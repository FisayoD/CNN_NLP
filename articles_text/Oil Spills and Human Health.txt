Lecture Notes in Computer Science
7389
Commenced Publication in 1973
Founding and Former Series Editors
Gerhard Goos Juris Hartmanis and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University UK
Takeo Kanade
Carnegie Mellon University Pittsburgh PA USA
Josef Kittler
University of Surrey Guildford UK
Jon M Kleinberg
Cornell University Ithaca NY USA
Alfred Kobsa
University of California Irvine CA USA
Friedemann Mattern
ETH Zurich Switzerland
John C Mitchell
Stanford University CA USA
Moni Naor
Weizmann Institute of Science Rehovot Israel
Oscar Nierstrasz
University of Bern Switzerland
C Pandu Rangan
Indian Institute of Technology Madras India
Bernhard Steffen
TU Dortmund University Germany
Madhu Sudan
Microsoft Research Cambridge MA USA
Demetri Terzopoulos
University of California Los Angeles CA USA
Doug Tygar
University of California Berkeley CA USA
Gerhard Weikum
Max Planck Institute for Informatics Saarbruecken Germany
DeShuang Huang Changjun Jiang
Vitoantonio Bevilacqua
Juan Carlos Figueroa Eds
Intelligent
Computing Technology
8th International Conference ICIC 2012
Huangshan China July 2529 2012
Proceedings
1 3
Volume Editors
DeShuang Huang
Changjun Jiang
Tongji University
School of Electronics and Information Engineering
4800 Caoan Road Shanghai 201804 China
Email dshuang cjjiangtongjieducn
Vitoantonio Bevilacqua
Polytechnic of Bari
Electrical and Electronics Department
Via Orabona 4 70125 Bari Italy
Email vitoantoniobevilacquagmailcom
Juan Carlos Figueroa
District University Francisco José de Caldas
Faculty of Engineering
Cra 7a No 4053 Fifth Floor Bogotá Colombia
Email jcﬁgueroagudistritaleduco
ISSN 03029743
eISSN 16113349
ISBN 9783642315879
eISBN 9783642315886
DOI 1010079783642315886
Springer Heidelberg Dordrecht London New York
Library of Congress Control Number 2012941229
CR Subject Classiﬁcation 1998 I2 I4 I5 H4 J3 F1 F2 C2 H3 H28 G22
K65
LNCS Sublibrary SL 3 – Information Systems and Applications incl InternetWeb
and HCI
© SpringerVerlag Berlin Heidelberg 2012
This work is subject to copyright All rights are reserved whether the whole or part of the material is
concerned speciﬁcally the rights of translation reprinting reuse of illustrations recitation broadcasting
reproduction on microﬁlms or in any other way and storage in data banks Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9 1965
in its current version and permission for use must always be obtained from Springer Violations are liable
to prosecution under the German Copyright Law
The use of general descriptive names registered names trademarks etc in this publication does not imply
even in the absence of a speciﬁc statement that such names are exempt from the relevant protective laws
and regulations and therefore free for general use
Typesetting Cameraready by author data conversion by Scientiﬁc Publishing Services Chennai India
Printed on acidfree paper
Springer is part of Springer Science+Business Media wwwspringercom
Preface
The International Conference on Intelligent Computing ICIC was started to
provide an annual forum dedicated to the emerging and challenging topics in
artiﬁcial intelligence machine learning pattern recognition image processing
bioinformatics and computational biology It aims to bring together researchers
and practitioners from both academia and industry to share ideas problems
and solutions related to the multifaceted aspects of intelligent computing
ICIC 2012 held in Huangshan China July 25–29 2012 constituted the 8th
International Conference on Intelligent Computing It built upon the success of
ICIC 2011 ICIC 2010 ICIC 2009 ICIC 2008 ICIC 2007 ICIC 2006 and ICIC
2005 that were held in Zhengzhou Changsha China Ulsan Korea Shanghai
Qingdao Kunming and Hefei China respectively
This year the conference concentrated mainly on the theories and method
ologies as well as the emerging applications of intelligent computing Its aim was
to unify the picture of contemporary intelligent computing techniques as an in
tegral concept that highlights the trends in advanced computational intelligence
and bridges theoretical research with applications Therefore the theme for this
conference was “Advanced Intelligent Computing Technology and Applications”
Papers focusing on this theme were solicited addressing theories methodologies
and applications in science and technology
ICIC 2012 received 753 submissions from 28 countries and regions All pa
pers went through a rigorous peerreview procedure and each paper received at
least three review reports Based on the review reports the Program Committee
ﬁnally selected 242 highquality papers for presentation at ICIC 2012 of which
242 papers are included in three volumes of proceedings published by Springer
one volume of Lecture Notes in Computer Science LNCS one volume of Lec
ture Notes in Artiﬁcial Intelligence LNAI and one volume of Communications
in Computer and Information Science CCIS
This volume of Lecture Notes in Computer Science LNCS includes 84
papers
The organizers of ICIC 2012 including Tongji University made an enormous
eﬀort to ensure the success of the conference We hereby would like to thank the
members of the Program Committee and the referees for their collective eﬀort
in reviewing and soliciting the papers We would like to thank Alfred Hofmann
executive editor from Springer for his frank and helpful advice and guidance
throughout and for his continuous support in publishing the proceedings In
particular we would like to thank all the authors for contributing their papers
VI
Preface
Without the highquality submissions from the authors the success of the con
ference would not have been possible Finally we are especially grateful to the
IEEE Computational Intelligence Society the International Neural Network So
ciety and the National Science Foundation of China for their sponsorship
May 2012
DeShuang Huang
Changjun Jiang
Vitoantonio Bevilacqua
Juan Carlos Figueroa
ICIC 2012 Organization
General Cochairs
Changjun Jiang China
Gary G Yen USA
Steering Committee Chair
DeShuang Huang China
Program Committee Cochairs
Jianhua Ma Japan
Laurent Heutte France
Organizing Committee Cochairs
Duoqian Miao China
Yang Xiang China
Jihong Guan China
Award Committee Chair
KangHyun Jo Korea
Publication Chair
Vitoantonio Bevilacqua Italy
WorkshopSpecial Session Chair
Juan Carlos Figueroa Colombia
Special Issue Chair
Michael Gromiha India
Tutorial Chair
Phalguni Gupta India
International Liaison Chair
Prashan Premaratne Australia
Publicity Cochairs
Kyungsook Han Korea
Ling Wang China
Xiang Zhang USA
Lei Zhang China
Exhibition Chair
Qiong Wu China
Organizing Committee Members
Zhijun Ding China
Hanli Wang China
Yan Wu China
GuoZheng Li China
Fanhuai Shi China
Conference Secretary
ZhiYang Chen China
VIII
ICIC 2012 Organization
Program Committee
Khalid Mahmood Aamir Italy
Vasily Aristarkhov Russian Federation
Costin Badica Romania
Vitoantonio Bevilacqua Italy
Shuhui Bi China
Danail Bonchev USA
Stefano Cagnoni Italy
ChinChih Chang Taiwan China
PeiChann Chang Taiwan China
Jack Chen Canada
ShihHsin Chen Taiwan China
WenSheng Chen China
Xiyuan Chen China
Yang Chen China
Ziping Chiang Taiwan China
Michal Choras Poland
Angelo Ciaramella Italy
Milan Cisty Slovakia
Jose Alfredo F Costa Brazil
Loganathan D India
Eng Salvatore Distefano Italy
Mariagrazia Dotoli Italy
Karim Faez Iran
Jianbo Fan China
Minrui Fei China
WaiKeung Fung Canada
JunYing Gan China
XiaoZhi Gao Finland
Dunwei Gong China
Valeriya Gribova Russia
M Michael Gromiha Japan
Kayhan Gulez Turkey
Anyuan Guo China
Ping Guo China
Phalguni Gupta India
Fei Han China
Kyungsook Han Korea
Nojeong Heo Korea
Laurent Heutte France
Martin Holena Czech Republic
WeiChiang Hong Taiwan China
Yuexian Hou China
Sanqing Hu China
Guangbin Huang Singapore
Peter Hung Ireland
Li Jia China
Zhenran Jiang China
KangHyun Jo Korea
DahJing Jwo Taiwan China
Yoshiaki Kakuda Japan
Vandana Dixit Kaushik India
Muhammad Khurram Khan
Saudi Arabia
Bora Kumova Turkey
Yoshinori Kuno Japan
Takashi Kuremoto Japan
Vincent C S Lee Australia
Bo Li China
Dalong Li USA
GuoZheng Li China
ShiHua Li China
Xiaoou Li Mexico
Hualou Liang USA
Honghuang Lin USA
Chunmei Liu USA
ChunYu Liu USA
Ju Liu China
Ke Lv China
Jinwen Ma China
Igor V Maslov Japan
Xiandong Meng USA
Filippo Menolascina Italy
Pabitra Mitra India
Ravi Monaragala Sri Lanka
Tarik Veli Mumcu Turkey
Primiano Di Nauta Italy
Ben Niu China
SimHeng Ong Singapore
Vincenzo Pacelli Italy
Shaoning Pang New Zealand
Francesco Pappalardo Italy
Young B Park Korea
Surya Prakash India
Prashan Premaratne Australia
Hong Qiao China
Daowen Qiu China
ICIC 2012 Organization
IX
K R Seeja India
Ajita Rattani Italy
Angel D Sappa Spain
Simon See Singapore
Akash K Singh USA
Jiatao Song China
Qiankun Song China
ZhanLi Sun Singapore
Stefano Squartini Italy
Evi Syukur Australia
Hao Tang China
ChuanKang Ting Taiwan China
Jun Wan USA
Bing Wang USA
JeenShing Wang Taiwan China
Ling Wang China
Shitong Wang China
Xuesong Wang China
Yong Wang China
Yufeng Wang China
Zhi Wei China
Xiaojun Wu China
Junfeng Xia USA
Shunren Xia China
Bingji Xu China
Shao Xu Singapore
Zhenyu Xuan USA
Yu Xue China
Tao Ye China
JunHeng Yeh Taiwan China
MyeongJae Yi Korea
ZhiGang Zeng China
Boyun Zhang China
Chaoyang Joe Zhang USA
Lei Zhang Hong Kong China
Rui Zhang China
Xiaoguang Zhao China
XingMing Zhao China
Zhongming Zhao USA
BoJin Zheng China
ChunHou Zheng China
Fengfeng Zhou China
Waqas Haider Khan Bangyal Pakistan
Yuhua Qian China
Reviewers
Kezhi Mao
Xin Hao
Tarik Veli Mumcu
Muharrem Mercimek
Selin Ozcira
Ximo Torres
BinSong Cheng
Shihua Zhang
Yu Xue
Xiaoping Luo
Dingfei Ge
Jiayin Zhou
Mingyi Wang
Chung Chang Lien
WeiLing Hwang
Jian Jia
Jian Wang
Zhiliu Zuo
Sajid Bashir
Faisal Mufti
Haﬁz Muhammad
Farooq
Bilal Ahmed
Maryam Gul
Gurkan Tuna
Hajira Jabeen
Chandana Gamage
Prashan Premaratne
Chathura R De Silva
Manodha Gamage
Kasun De Zoysa
Chesner Desir
Laksman Jayaratne
Francesco Camastra
R´emi Flamary
Antoninostaiano Alessio
Ferone
Raﬀaele Montella
Nalin Karunasinghe
Vladislavs Dovgalecs
Pierrick Tranouez
Antonio Maratea
Giuseppe Vettigli
Ranga Rodrigo
ChyuanHuei Yang
ReySern Lin
ChengHsiung Chiang
JianShiun Hu
YaoHong Tsai
HungChi Su
JH Chen
Wen Ouyang
Chong Shen
Yuan Xu
Cucocris Tano
TienDung Le
HeeJun Kang
X
ICIC 2012 Organization
HongHee Lee
NgocTung Nguyen
Ju Kunru
Vladimir Brusic
Ping Zhang
Renjie Zhang
Alessandro Cincotti
Mojaharul Islam
Marzio Pennisi
Haili Wang
Santo Motta
Keun Ho Ryu
Alfredo Pulvirenti
Rosalba Giugno
Ge Guo
ChihMin Lin
Yifeng Zhang
Xuefen Zhu
Lvzhou Li
Haozhen Situ
Qin Li
Nikola Paunkovic
Paulo Mateus
Jozef Gruska
Xiangfu Zou
Yasser Omar
YinXiang Long
Bjoern Schuller
Erikcam Bria
FaundezZanuy Marcos
Rui Zhang
Yibin Ye
Qinglai Wei
Guangbin Huang
Lendasse Amaury
Michele Scarpiniti
Simone Bassis
Morabito Carlo
Amir Hussain
Li Zhang
Emilio Soria
Sanqing Hu
Hossein Javaherian
Veselin Stoyanov
Eric Fock
YaoNan Lien
Liangjun Xie
Nong Gu
Xuewei Wang
Shizhong Liao
Zheng Liu
Bingjun Sun
Yuexian Hou
Shiping Wen
Ailong Wu
Gang Bao
Takashi Kuremoto
Amin Yazdanpanah
MengCheng Lau
Chi Tai Cheng
Jayanta Debnath
Raymond Ng
Baranyi Peter
Yongping Zhai
Baoquan Song
Weidi Dai
Jiangzhen Ran
Huiyu Jin
Guoping Lu
Xiaohua Qiao
Xuemei Ren
Mingxia Shen
Hao Tang
ZhongQiang Wu
Zhenhua Huang
Junlin Chang
Bin Ye
Yong Zhang
Yanzi Miao
Yindi Zhao
Jun Zhao
MeiQiang Zhu
Xue Xue
Yanjing Sun
Waqas Haider Khan
Bangyal
MingFeng Yang
GuoFeng Fan
Asma Nani
Xiangtao Li
Hongjun Jia
Yehu Shen
Tiantai Guo
Liya Ding
Dawen Xu
Jinhe Wang
Xiangyu Wang
Shihong Ding
Zhao Wang
Junyong Zhai
Haibo Du
Haibin Sun
Jun Yang
ChinSheng Yang
JhengLong Wu
JyunJie Lin
JunLin Lin
LiangChih Yu
SH Chen
ChienLung Chan
Eric Fan
XH Cloud
Yue Deng
Kun Yang
Badrinath Srinivas
Francesco Longo
Santo Motta
Giovanni Merlino
Shengjun Wen
Ni Bu
Changan Jiang
Caihong Zhang
Lihua Jiang
Aihui Wang
Cunchen Gao
Tianyu Liu
Pengfei Li
Jing Sun
Aimin Zhou
JiHui Zhang
Xiufen Zou
Lianghong Wu
H Chen
Jian Cheng
Zhihua Cui
ICIC 2012 Organization
XI
XiaoZhi Gao
Guosheng Hao
QuanKe Pan
Bin Qian
Xiaoyan Sun
Byungjeong Lee
Woochang Shin
Jaewon Oh
JongMyon Kim
YungKeun Kwon
Mingjian Zhang
Xiai Yang
Lirong Wang
Xi Luo
Weidong Yang
Weiling Liu
Lanshen Guo
Yunxia Qu
Peng Kai
Song Yang
Xianxia Zhang
Min Zheng
Weiming Yu
Wangjun Xiang
Qing Liu
Xi Luo
Ali Ahmed Adam
Ibrahim Aliskan
Yusuf Altun
Kadir Erkan
Ilker Ustoglu
Levent Ucun
Janset Dasdemir
Xiai Yan
Stefano Ricciardi
Daniel Riccio
Marilena De Marsico
Fabio Narducci
Atsushi Yamashita
Kazunori Onoguchi
Ryuzo Okada
Naghmeh Garmsiri
Lockery Dan
Maddahi Yaser
Kurosh Zareinia
Ramhuzaini Abd
Rahman
Xiaosong Li
Lei Song
Gang Chen
Yiming Peng
Fan Liu
Jun Zhang
Li Shang
Chunhou Zheng
Jayasudha John Suseela
Soniya Balram
KJ Shanti
Aravindan Chandrabose
Parul Agarwal
Deepa Anand
Ranjit Biswas
Nobutaka Shimada
Hironobu Fujiyoshi
Giuseppe Vettigli
Francesco Napolitano
Xiao Zhang
TorresSospedra Joaqu´ın
Kunikazu Kobayashi
Liangbing Feng
Fuhai Li
Yongsheng Dong
Shuyi Zhang
Yanqiao Zhu
Lei Huang
Yue Zhao
Yunsheng Jiang
Bin Xu
Wei Wang
Jin Wei
Kisha Ni
YuLiang Hsu
CheWei Lin
JeenShing Wang
Yingke Lei
Jie Gui
Xiaoming Liu
Dong Yang
Jian Yu
Jin Gu
Chenghai Xue
Xiaowo Wang
Xin Feng
Bo Chen
Jianwei Yang
Chao Huang
Weixiang Liu
Qiang Huang
Yanjie Wei
Ao Li
Mingyuan Jiu
Dipankar Das
Gianluca Ippoliti
Lian Liu
Mohammad Bagher
Bannae Shariﬁan
Hadi Afsharirad
S Galvani
Chengdong Wu
Meiju Liu
Aamir Shahzad
Wei Xiong
Toshiaki Kondo
Andrea Prati
Bai Li
Domenico G Sorrenti
Alessandro Rizzi
Raimondo Schettini
Mengjie Zhang
Gustavo Olague
Umarani Jayaraman
Aditya Nigam
Hunny Mehrotra
Gustavo Souza
Guilherme Barreto
Leandrodos Santos
Coelho
Carlos Forster
Fernando Von Zuben
Anne Canuto
Jackson Souza
Carmelo Bastos Filho
Daniel Aloise
Sergio P Santos
Ricardo Fabbri
XII
ICIC 2012 Organization
F´abio Paiva
SH Chen
TsungChe Chiang
ChengHung Chen
ShihHung Wu
Zhifeng Yun
Yanqing Ji
Kai Wang
JeHo Park
Junhong Wang
Jifang Pang
Thiran De Silva
Nalin Badara
Shaojing Fan
Chen Li
Qingfeng Li
Liangxu Liu
Rina Su
Hua Yu
Jie Sun
Linhua Zhou
Zhaohong Deng
Pengjiang Qian
Jun Wang
Puneet Gupta
Salim Flora
Jayaputera James
Sherchan Wanita
Helen Paik
Mohammed M Gaber
Agustinus B Waluyo
Dat Hoang
Hamid Motahari
Eric Pardede
Tim Ho
Jose AF Costa
Qiang Fan
Surya Prakash
Vandana Dixit K
Saiful Islam
Kamlesh Tiwari
Sandesh Gupta
Zahid Akhtar
MinChih Chen
Andreas Konstantinidis
Quanming Zhao
Hongchun Li
Zhengjie Wang
Chong Meng
Lin Cai
Aiyu Zhang
YangWon Lee
Young Park
Chulantha Kulasekere
Akalanka Ranundeniya
Junfeng Xia
Min Zhao
Hamid Reza Rashidi
Kanan
Mehdi Ezoji
Majid Ziaratban
Saeed Mozaﬀari
Javad Haddadnia
Peyman Moallem
Farzad Towhidkhah
Hamid
Abrishamimoghaddam
Mohammad Reza
Pourfard
MJ Abdollahi Fard
AranaArexolaleiba
Nestor
Carme Juli`a
Boris Vintimilla
Daniele Ranieri
Antonio De Giorgio
Vito Gallo
Leonarda Carnimeo
Paolo Pannarale
L´opezChau Asdr´ubal
Jair Cervantes
Debrup Chakraborty
Simon Dacey
WeiChiang Hong
Wenyong Dong
Lingling Wang
Hongrun Wu
ChienYuan Lai
MdKamrul Hasan
Mohammad Kaykobad
YoungKoo Lee
Sungyoung Lee
ChinChih Chang
Yuewang
Shinji Inoue
Tomoyuki Ohta
Eitaro Kohno
Alex Muscar
Sorin Ilie
Cosulschi Mirel
Min Chen
Wen Yu
LopezArevalo Ivan
Sabooh Ajaz
Prashan Premaratne
Weimin Huang
Jingwen Wang
Kai Yin
Hong Wang
Yan Fan
Niu Qun
Youqing Wang
Dajun Du
Laurence T Yang
Laurence Yang
Seng Loke
Syukur Evi
Luis Javier Garc´ıa
Villalba
Tsutomu Terada
Tomas Sanchez Lopez
Eric Cheng
Battenfeld Oliver
Yokota Masao
Hanemann Sven
Yue Suo
PaoAnn Hsiung
Kristiansen Lill
Callaghan Victor
Mzamudio Rodriguez
Victor
Sherif Sakr
Rajiv Ranjan
Cheong Ghil Kim
Philip Chan
ICIC 2012 Organization
XIII
Wojtek Goscinski
Jeﬀerson Tan
Bo Zhou
Huiwei Wang
Xiaofeng Chen
Bing Li
Wojtek Goscinski
Samar Zutshi
Rafal Kozik
Tomasz Andrysiak
Marian Cristian
Mihaescu
Michal Choras
Yanwen Chong
Jinxing Liu
Miguel Gonzalez
Mendoza
TaYuan Chou
Hui Li
Chao Wu
Kyung DaeKo
Junhong Wang
Guoping Lin
Jiande Sun
Hui Yuan
Qiang Wu
Yannan Ren
Dianxing Liu
M Sohel Rahman
Dengxin Li
Gerard J Chang
Weidong Chang
Xulian Hua
Dan Tang
Sandesh Gupta
Uma Rani
Surya Prakash
Narendra Kohli
Meemee Ng
Olesya Kazakova
Vasily Aristarkhov
Ozgur Kaymakci
Xuesen Ma
Qiyue Li
Zhenchun Wei
Xin Wei
Xiangjuan Yao
Ling Wang
Shujuan Jiang
Changhai Nie
He Jiang
Fengfeng Zhou
Zexian Liu
Jian Ren
Xinjiao Gao
TianShun Gao
Han Cheng
Yongbo Wang
Yuangen Yao
Juan Liu
Bing Luo
Zilu Ying
Junying Zeng
Guohui He
Yikui Zhai
Binyu Yi
Zhan Liu
Xiang Ji
Hongyuan Zha
Azzedine Boukerche
Horacio ABF Oliveira
Eduardo F Nakamura
Antonio AF Loureiro
Radhika Nagpal
Jonathan Bachrach
Daeyoung Choi
Woo Yul Kim
Amelia Badica
Fuqing Duan
HuiPing Tserng
RenJye Dzeng
Machine Hsie
Milan Cisty
Muhammad Amjad
Muhammad Rashid
Waqas Bangyal
Bo Liu
Xueping Yu
Chenlong Liu
Jikui Shen
Julius Wan
Linlin Shen
Zhou Su
Weiyan Hou
Emil Vassev
Anuparp Boonsongsrikul
Paddy Nixon
KyungSuk Lhee
Man Pyo Hong
Vincent CS Lee
YeeWei Law
Touraj Banirostam
HoQuocPhuong
Nguyen
Bin Ye
Huijun Li
Xue Sen
Mu Qiao
Xuesen Ma
Weizhen Chun
Qian Zhang
Baosheng Yang
Xuanfang Fei
Fanggao Cui
Xiaoning Song
Dongjun Yu
Bo Li
Huajiang Shao
Ke Gu
Helong Xiao
Wensheng Tang
Andrey Vavilin
Jong Eun Ha
MunHo Jeong
Taeho Kim
Kaushik Deb
Daenyeong Kim
Dongjoong Kang
HyunDeok Kang
HoangHon Trinh
Andrey Yakovenko
Dmitry Brazhkin
Sergey Ryabinin
Stanislav Efremov
Andrey Maslennikov
XIV
ICIC 2012 Organization
Oleg Sklyarov
Pabitra Mitra
Juan Li
Tiziano Politi
Vitoantonio Bevilacqua
Abdul Rauf
Yuting Yang
Lei Zhao
ShihWei Lin
Vincent Li
Chunlu Lai
Qian Wang
Liuzhao Chen
Xiaozhao Zhao
Plaban Bhowmick
Anupam Mandal
Biswajit Das
Pabitra Mitra
Tripti Swarnkar
Yang Dai
Chao Chen
Yi Ma
Emmanuel Camdes
Chenglei Sun
Yinying Wang
Jiangning Song
Ziping Chiang
Vincent Chiang
Xingming Zhao
Chenglei Sun
Francesca Nardone
Angelo Ciaramella
Alessia Albanese
Francesco Napolitano
GuoZheng Li
XuYing Liu
Dalong Li
Jonathan Sun
Nan Wang
Yi Yang
Mingwei Li
Wierzbicki Adam
Marcin Czenko
Ha Tran
Jeroen Doumen
Sandro Etalle
Pieter Hartel
Jerryden Hartog
Hai Ren
Xiong Li
Ling Liu
F´elix G´omez M´armol
JihGau Juang
HeSheng Wang
Xin Lu
KyungSuk Lhee
Sangyoon Oh
Chisa Takano
Sungwook S Kim
Junichi Funasaka
Yoko Kamidoi
Dan Wu
DahJing Jwo
Abdollah Shidfar
Reza Pourgholi
Xiujun Zhang
Yan Wang
Kun Yang
Iliya Slavutin
Ling Wang
Huizhong Yang
Ning Li
Tao Ye
Smile Gu
Phalguni Gupta
Guangxu Jin
Huijia Li
Xin Gao
Dan Liu
Zhenyu Xuan
Changbin Du
Mingkun Li
Haiyun Zhang
Baoli Wang
Giuseppe Pappalardo
Huisen Wang
Hai Min
Nalin Bandara
Lin Zhu
Wen Jiang
CanYi Lu
Lei Zhang
Jian Lu
Jian Lu
HongJie Yu
Ke Gu
Hangjun Wang
ZhiDe Zhi
Xiaoming Ren
Ben Niu
HuaYun Chen
Fuqing Duan
Jing Xu
Marco Falagario
Fabio Sciancalepore
Nicola Epicoco
Wei Zhang
MuChung Chen
Chinyuan Fan
ChunWei Lin
ChunHao Chen
LienChin Chen
Seiki Inoue
KR Seeja
Gurkan Tuna
Cagri Gungor
Qian Zhang
Huanting Feng
Boyun Zhang
Jun Qin
Yang Zhao
Qinghua Cui
Hsiao Piau Ng
Qunfeng Dong
Hailei Zhang
Woochang Hwang
Joe Zhang
Marek Rodny
BingNan Li
YeeWei Law
Lu Zhen
Bei Ye
Jl Xu
PeiChann Chang
Valeria Gribova
ICIC 2012 Organization
XV
Xiandong Meng
Lasantha Meegahapola
Angel Sappa
Rajivmal Hotra
George Smith
Carlor Ossi
Lijing Tan
Antonio Puliaﬁto
Nojeong Heo
Santosh Bbehera
Giuliana Rotunno
Table of Contents
Evolutionary Learning and Genetic Algorithms
PSO Assisted NURB Neural Network Identiﬁcation                  
1
Xia Hong and Sheng Chen
2D Discontinuous Function Approximation with RealValued
GrammarBased Classiﬁer System                                  
10
Lukasz Cielecki and Olgierd Unold
A Scatter Search Methodology for the Aircraft Conﬂict Resolution
Problem                                                        
18
ZhiZeng Li XueYan Song JiZhou Sun and ZhaoTong Huang
Selfadaptive Diﬀerential Evolution Based Multiobjective Optimization
Incorporating Local Search and IndicatorBased Selection             
25
Datong Xie Lixin Ding Shenwen Wang Zhaolu Guo
Yurong Hu and Chengwang Xie
Fuzzy Theory and Models
Application of Data Mining in Coal Mine Safety Decision System Based
on Rough Set                                                   
34
Tianpei Zhou
Study on Web Text Feature Selection Based on Rough Set            
42
Xianghua Lu and Weijing Wang
A Generalized Approach for Determining Fuzzy Temporal Relations    
49
Luyi Bai and Zongmin Ma
Applications on Information Flow and Biomedical Treatment of FDES
Based on Fuzzy Sequential Machines Theory                        
57
Hongyan Xing and Daowen Qiu
Discontinuous Fuzzy Systems and Henstock Integrals of Fuzzy Number
Valued Functions                                                
65
Yabin Shao and Zengtai Gong
Swarm Intelligence and Optimization
A Phased Adaptive PSO Algorithm for Multimodal Function
Optimization                                                    
73
Haiping Yu and Fengying Yang
XVIII
Table of Contents
The Comparative Study of Diﬀerent Number of Particles in Clustering
Based on ThreeLayer Particle Swarm Optimization                  
80
Guoliang Huang Xinling Shi Zhenzhou An and He Sun
Implementation of Mutual Localization of Multirobot Using Particle
Filter                                                           
87
Yang Weon Lee
Optimization of Orthogonal Poly Phase Coding Waveform Based on
Bees Algorithm and Artiﬁcial Bee Colony for MIMO Radar           
95
Milad Malekzadeh Alireza Khosravi Saeed Alighale and
Hamed Azami
Kernel Methods and Supporting Vector Machines
SVM Regularizer Models on RKHS vs on Rm                       
103
Yinli Dong and Shuisheng Zhou
Research on Performance Comprehensive Evaluation of Thermal Power
Plant under LowCarbon Economy                                 
112
Xing Zhang
Nature Inspired Computing and Optimization
Computing the Minimum λCover in Weighted Sequences             
120
Hui Zhang Qing Guo and Costas S Iliopoulos
A Novel Hybrid Evolutionary Algorithm for Solving Multiobjective
Optimization Problems                                           
128
Huantong Geng Haifeng Zhu Rui Xing and Tingting Wu
Heuristic Algorithms for Solving Survivable Network Design Problem
with Simultaneous Unicast and Anycast Flows                       
137
Huynh Thi Thanh Binh Pham Vu Long Nguyen Ngoc Dat and
Nguyen Sy Thai Ha
Systems Biology and Computational Biology
ProteinProtein Binding Aﬃnity Prediction Based on an SVR
Ensemble                                                       
145
Xueling Li Min Zhu Xiaolai Li HongQiang Wang and
Shulin Wang
A Novel TwoStage Alignment Method for Liquid Chromatography
Mass SpectrometryBased Metabolomics                            
152
Xiaoli Wei Xue Shi Seongho Kim Craig McClain and Xiang Zhang
Table of Contents
XIX
Reconstruction of Metabolic Association Networks Using
HighThroughput Mass Spectrometry Data                          
160
Imhoi Koo Xiang Zhang and Seongho Kim
Predicting Protein Subcellular Localization by Fusing Binary Tree and
ErrorCorrecting Output Coding                                   
168
Lili Guo and Yuehui Chen
Exponential Stability of a Class of HighOrder Hybrid Neural
Networks                                                       
174
Qian Ye Baotong Cui Xuyang Lou and Ke Lou
Knowledge Discovery and Data Mining
Mining Google Scholar Citations An Exploratory Study              
182
Ze Huang and Bo Yuan
Knowledge Acquisition of Multiple Information Sources Based on
Aircraft Assembly Design                                         
190
Liang Xia Lizhi Zhang and Zhenguo Yan
Graph Theory and Algorithms
Generalizing Suﬃcient Conditions and Traceable Graphs              
198
Kewen Zhao
Note on the Minimal Energy Ordering of Conjugated Trees           
206
Yulan Xiao and Bofeng Huo
Machine Learning Theory and Methods
An Ensemble Method Based on Conﬁdence Probability for
Multidomain Sentiment Classiﬁcation                              
214
Quan Zhou Yuhong Zhang and Xuegang Hu
Robust ISOMAP Based on Neighbor Ranking Metric                 
221
Chun Du Shilin Zhou Jixiang Sun and Jingjing Zhao
Modeling by Combining Dimension Reduction and L2Boosting        
230
Junlong Zhao
Geometric Linear Regression and Geometric Relation                 
236
Kaijun Wang and Liying Yang
XX
Table of Contents
Biomedical Informatics Theory and Methods
Optimal Control Strategies of a Tuberculosis Model with Exogenous
Reinfection                                                      
244
Yali Yang Xiuchao Song Yuzhou Wang and Guoyun Luo
Method of Smartphone Users’ Information Protection Based on
Composite Behavior Monitor                                      
252
Hua Zha and Chunlin Peng
Complex Systems Theory and Methods
Improved Digital Chaotic Sequence Generator Utilized in Encryption
Communication                                                  
260
Xiaoyuan Li Bin Qi and Lu Wang
Modeling and Adaptive Control for FlappingWing Micro Aerial
Vehicle                                                         
269
Qingwei Li and Hongjun Duan
Distributed Staﬀ’s Integral Systems Design and Implementation       
277
Qing Xie GuoDong Liu ZhengHua Shu BingXin Wang and
DengJi Zhao
PervasiveUbiquitous Computing Theory and
Methods
Research of QoCaware Service Adaptation in Pervasive Environment  
284
Di Zheng Qingwei Xu and Kerong Ben
Energy Eﬃcient Filtering Nodes Assignment Method for Sensor
Networks Using Fuzzy Logic                                       
293
Soo Young Moon and Tae Ho Cho
Sentiment Analysis with Multisource Product Reviews               
301
Hongwei Jin Minlie Huang and Xiaoyan Zhu
Intelligent Computing in Bioinformatics
Identifying CpG Islands in Genome Using Conditional Random
Fields                                                          
309
Wei Liu Hanwu Chen and Ling Chen
A Novel Gene Selection Method for Multicatalog Cancer Data
Classiﬁcation                                                    
319
Xuejiao Lei Yuehui Chen and Yaou Zhao
Table of Contents
XXI
A Novel Discretization Method for MicroarrayBased Cancer
Classiﬁcation                                                    
327
Ding Li Rui Li and HongQiang Wang
SequenceBased Prediction of ProteinProtein Interactions Using
Random Tree and Genetic Algorithm                               
334
Lei Zhang
Intelligent Computing in Pattern Recognition
A TwoStage Reduction Method Based on Rough Set and Factor
Analysis                                                        
342
Zheng Liu Liying Fang Mingwei Yu Pu Wang and
Jianzhuo Yan
Eyebrow Segmentation Based on Binary Edge Image                 
350
Jiatao Song Liang Wang and Wei Wang
Intelligent Computing in Image Processing
Xray Image Contrast Enhancement Using the Second Generation
Curvelet Transform                                              
357
Hao Li and Guanying Huo
MMW Image Blind Restoration Using Sparse ICA in Contourlet
Transform Domain                                               
365
Li Shang Pingang Su and Wenjun Huai
An Adaptive Non Local Spatial Fuzzy Image Segmentation
Algorithm                                                      
373
Hanqiang Liu and Feng Zhao
MMW Image Enhancement Based on Gray Stretch Technique and SSR
Theory                                                         
379
WenJun Huai Li Shang and PinGang Su
A Study of Images Denoising Based on Two Improved Fractional
Integral Marks                                                   
386
Changxiong Zhou Tingqin Yan Wenlin Tao and Shufen Lui
Leaf Image Recognition Using Fourier Transform Based on Ordered
Sequence                                                       
393
LiWei Yang and XiaoFeng Wang
Discriminant Graph Based Linear Embedding                       
401
Bo Li Jin Liu WenYong Dong and WenSheng Zhang
XXII
Table of Contents
Intelligent Computing in Robotics
A Performance Analysis of Omnidirectional Vision Based Simultaneous
Localization and Mapping                                        
407
Hayrettin Erturk Gurkan Tuna Tarik Veli Mumcu and
Kayhan Gulez
Trajectory Estimation of a Tracked Mobile Robot Using the
SigmaPoint Kalman Filter with an IMU and Optical Encoder         
415
Xuan Vinh Ha Cheolkeun Ha and Jewon Lee
Development of a Mobile Museum Guide Robot That Can Conﬁgure
Spatial Formation with Visitors                                    
423
Mohammad Abu Yousuf Yoshinori Kobayashi Yoshinori Kuno
Akiko Yamazaki and Keiichi Yamazaki
Intelligent Computing in Computer Vision
A Novel Image Matting Approach Based on Naive Bayes Classiﬁer     
433
Zhanpeng Zhang Qingsong Zhu and Yaoqin Xie
Detecting Insulators in the Image of Overhead Transmission Lines     
442
Jingjing Zhao Xingtong Liu Jixiang Sun and Lin Lei
Realizing Geometry Surface Modeling of Complicated Geological
Object Based on Delaunay Triangulation                           
451
Xiangbin Meng Panpan Lv Xin Wang and Hua Chen
Researching of the Evolution of Discontent in Mass Violence Event     
458
FanLiang Bu and YuNing Zhao
An Eﬃcient TwoStage Level Set Segmentation Framework for
Overlapping Plant Leaf Image                                     
466
XiaoFeng Wang and Hai Min
Intelligent Computing in Petri NetsTransportation
Systems
Behavior Analysis of Software Systems Based on Petri Net Slicing      
475
Jiaying Ma Wei Han and Zuohua Ding
Aircraft Landing Scheduling Based on Semantic Agent Negotiation
Mechanism                                                      
483
ZhaoTong Huang XueYan Song JiZhou Sun and ZhiZeng Li
A RealTime Posture Simulation Method for HighSpeed Train        
490
Huijuan Zhou Bo Chen Yong Qin and Yiran Liu
Table of Contents
XXIII
HighOrder Terminal SlidingMode Observers for Anomaly Detection   
497
Yong Feng Fengling Han Xinghuo Yu Zahir Tari Lilin Li and
Jiankun Hu
Intelligent Data Fusion and Information Security
An Automated Bug Triage Approach A Concept Proﬁle and Social
Network Based Developer Recommendation                         
505
Tao Zhang and Byungjeong Lee
A New Method for Filtering IDS False Positives with Semisupervised
Classiﬁcation                                                    
513
Minghua Zhang and Haibin Mei
Dualform Elliptic Curves Simple Hardware Implementation           
520
Jianxin Wang and Xingjun Wang
Genetic Based Autodesign of Fuzzy Controllers for Vector Controlled
Induction Motor Drives                                           
528
Moulay Rachid Douiri and Mohamed Cherkaoui
Very Short Term Load Forecasting for Macau Power System          
538
Chong Yin Fok and Mang I Vai
Intelligent Sensor Networks
A Method for the Enhancement of the Detection Power and Energy
Savings against False Data Injection Attacks in Wireless Sensor
Networks                                                       
547
Su Man Nam and Tae Ho Cho
Virtual Cluster Tree Based Distributed Data Classiﬁcation Strategy
Using Locally Linear Embedding in Wireless Sensor Network          
555
Xin Song Cuirong Wang Cong Wang and Xi Hu
Fault Detection and Isolation in Wheeled Mobile Robot              
563
Ngoc Bach Hoang HeeJun Kang and YoungShick Ro
Knowledge RepresentationReasoning and Expert
Systems
The Research on Mapping from DAML+OIL Ontology to BasicElement
and ComplexElement of Extenics                                  
570
Bin Wen
The Study of Codeswitching in Advertisements                      
579
Hua Wang
XXIV
Table of Contents
Special Session on Hybrid Optimization
New Theories and Developments
Powered Grid Scheduling by Ant Algorithm                         
586
Feifei Liu and Xiaoshe Dong
An Eﬃcient Palmprint Based Recognition System Using 1DDCT
Features                                                        
594
GS Badrinath Kamlesh Tiwari and Phalguni Gupta
An Eﬃcient Algorithm for Deduplication of Demographic Data       
602
Vandana Dixit Kaushik Amit Bendale Aditya Nigam and
Phalguni Gupta
A Transportation Model with Interval Type2 Fuzzy Demands and
Supplies                                                        
610
Juan C FigueroaGarc´ıa and Germ´an Hern´andez
Special Session on Bioinspired Computing
and Application
Unstructured Scene Object Localization Algorithm Based on Sparse
Overcomplete Representation                                      
618
Peng Lu Yuhe Tang Eryan Chen Huige Shi and Shanshan Zhang
Solving the Distribution Center Location Problem Based on
Multiswarm Cooperative Particle Swarm Optimizer                  
626
Xianghua Chu Qiang Lu Ben Niu and Teresa Wu
Improved Bacterial Foraging Optimization with Social Cooperation
and Adaptive Step Size                                           
634
Xiaohui Yan Yunlong Zhu Hanning Chen and Hao Zhang
Root Growth Model for Simulation of Plant Root System and
Numerical Function Optimization                                  
641
Hao Zhang Yunlong Zhu and Hanning Chen
BacterialInspired Algorithms for Engineering Optimization           
649
Ben Niu Jingwen Wang Hong Wang and Lijing Tan
Multiobjective Dynamic MultiSwarm Particle Swarm Optimization for
EnvironmentalEconomic Dispatch Problem                         
657
JaneJing Liang WeiXing Zhang BoYang Qu and TieJun Chen
Author Index                                                  
665
PSO Assisted NURB Neural Network
Identiﬁcation
Xia Hong1 and Sheng Chen2
1 School of Systems Engineering University of Reading UK
2 School of Electronics and Computer Science University of Southampton UK and
Faculty of Engineering King Abdulaziz University Jeddah 21589 Saudi Arabia
xhongreadingacuk
Abstract A system identiﬁcation algorithm is introduced for Hammer
stein systems that are modelled using a nonuniform rational Bspline
NURB neural network The proposed algorithm consists of two suc
cessive stages First the shaping parameters in NURB network are esti
mated using a particle swarm optimization PSO procedure Then the
remaining parameters are estimated by the method of the singular value
decomposition SVD Numerical examples are utilized to demonstrate
the eﬃcacy of the proposed approach
Keywords Bspline NURB neural networks De Boor algorithm Ham
merstein model pole assignment controller particle swarm optimization
system identiﬁcation
1
Introduction
The Hammerstein model comprising a nonlinear static functional transforma
tion followed by a linear dynamical model has been widely researched 21518
The model characterizationrepresentation of the unknown nonlinear static func
tion is fundamental to the identiﬁcation of Hammerstein model Various
approaches have been developed in order to capture the a priori unknown non
linearity by use of both parametric 16 and nonparametric methods 127 The
special structure of Hammerstein models can be exploited to develop hybrid
parameter estimation algorithms 14
Both the uniformnonrational Bspline curve and the nonuniformrational
Bspline NURB curve have also been widely used in computer graphics and
computer aided geometric design CAGD 5 These curves consist of many
polynomial pieces oﬀering much more versatility than do Bezier curves while
maintaining the same advantage of the best conditioning property NURB is
a generalization of the uniform nonrational Bsplines and oﬀers much more
versatility and powerful approximation capability The NURB neural network
possesses a much more powerful modeling capability than a conventional non
rational Bspline neural network because of the extra shaping parameters This
motivates us to propose the use of NURB neural networks to model the non
linear static function in the Hammerstein system The PSO 10 constitutes a
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 1–9 2012
c
⃝ SpringerVerlag Berlin Heidelberg 2012
2
X Hong and S Chen
population based stochastic optimisation technique which was inspired by the
social behaviour of bird ﬂocks or ﬁsh schools It has been successfully applied to
wideranging optimisation problems 1314
This paper introduces a hybrid system identiﬁcation consisting two succes
sive stages In the proposed algorithm the shaping parameters in NURB neural
networks are estimated using the particle swarm optimization PSO as the ﬁrst
step in which the mean square error is used as the cost function In order to
satisfy the shaping parameters constraints the normalisation are applied in PSO
as appropriate Once the shaping parameters are determined The remaining pa
rameters can be estimated by Bai’s overparametrization approach 1 which is
used in this study
2
The Hammerstein System
The Hammerstein system consists of a cascade of two subsystems a nonlinear
memoryless function Ψ• as the ﬁrst subsystem followed by a linear dynamic
part as the second subsystem The system can be represented by
yt = ˆyt + ξt
= −a1yt − 1 − a2yt − 2 −  − anayt − na
+b1vt − 1 +  + bnbvt − nb + ξt
1
vt − j = Ψut − j
j = 1  nb
2
where yt is the system output and ut is the system input ξt is assumed
to be a white noise sequence independent of ut with zero mean and variance
of σ2 vt is the output of nonlinear subsystem and the input to the linear
subsystem aj’s bj’s are parameters of the linear subsystem na and nb are
assumed known system output and input lags Denote a = a1  anaT ∈ ℜna
and b = b1  bnbT ∈ ℜnb It is assumed that Aq−1 = 1+a1q−1++anaq−na
and Bq−1 = b1q−1 +  + bnbq−nb are coprime polynomials of q−1 where q−1
denotes the backward shift operator The gain of the linear subsystem is given
by
G = lim
q→1
Bq−1
Aq−1 =
nb
j=1 bj
1 + na
j=1 aj
3
The two objectives of the work are that of the system identiﬁcation and the
subsequent controller design for the identiﬁed model The objective of system
identiﬁcation for the above Hammerstein model is that given an observational
inputoutput data set DN = yt utN
t=1 to identify Ψ• and to estimate
the parameters aj bj in the linear subsystems Note that the signals between
the two subsystems are unavailable
Without signiﬁcantly losing generality the following assumptions are initially
made about the problem
Assumption 1 Ψ• is a one to one mapping ie it is an invertible and con
tinuous function
Assumption 2 ut is bounded by Umin  ut  Umax where Umin and
Umax are assumed known ﬁnite real values
PSO Assisted NURB Neural Network Identiﬁcation
3
3
Modelling of Hammerstein System Using NURB
Neural Network
In this work the nonuniform rational Bspline NURB neural network is adopted
in order to model Ψ• De Boor’s algorithm is a fast and numerically stable al
gorithm for evaluating Bspline basis functions 3 Univariate Bspline basis
functions are parameterized by the order of a piecewise polynomial of order k
and also by a knot vector which is a set of values deﬁned on the real line that
break it up into a number of intervals Supposing that there are d basis func
tions the knot vector is speciﬁed by d + k knot values U1 U2 · · ·  Ud+k At
each end there are k knots satisfying the condition of being external to the input
region and as a result the number of internal knots is d − k Speciﬁcally
U1  U2  Uk = Umin  Uk+1  Uk+2  · · · 
Ud  Umax = Ud+1  · · ·  Ud+k
4
Given these predetermined knots a set of d Bspline basis functions can be
formed by using the De Boor recursion 3 given by
B0
j u =
1 if Uj ≤ u  Uj+1
0
otherwise
5
j = 1 · · ·  d + k
Bi
j u =
u−Uj
Ui+j−Uj Bi−1
i
u
+
Ui+j+1−u
Ui+j+1−Uj+1 Bi−1
j+1 u
j = 1 · · ·  d + k − i
⎫
⎪
⎬
⎪
⎭
i = 1 · · ·  k
6
We model Ψ• as the NURB neural network in the form of
Ψu =
d

j=1
N k
j
uωj
7
with
N k
j
u =
λjBk
j
u
d
j=1 λjBk
j
u
8
where ωj’s are weights λj  0’s the shaping parameters that are to be deter
mined Denote ω = ω1 · · ·  ωdT ∈ ℜd λ = λ1 · · ·  λdT ∈ ℜd For uniqueness
we set the constraint d
j=1 λj = 1 Our algorithm involves estimating the weights
and the shaping parameters in the NURB model With speciﬁed knots and over
the estimation data set DN λ ω a b may be jointly estimated via
min
λωab
J =
N

t=1
y − ˆyt λ ω a b2
9
subject to
λj ≥ 0 ∀j λT 1 = 1 and G = 1
10
4
X Hong and S Chen
in which G = 1 is imposed for unique solution We point out that this is still a
very diﬃcult nonlinear optimization problem due to the mixed constraints and
this motivates us to propose the following hybrid procedure It is proposed that
the shaping parameters λj’s are found using the PSO as the ﬁrst step of system
identiﬁcation followed by the estimation of the remaining parameters
4
The System Identiﬁcation of Hammerstein System
Based on NURB Using PSO
41
The Basic Idea
Initially consider using NURB approximation with a speciﬁed shape parameter
vector λ the model predicted output ˆyt in  can be written as
ˆyt = −a1yt − 1 − a2yt − 2 −  − anayt − na + b1
d

j=1
ωjN k
j
t − 1 + 
+bnb
d

j=1
ωjN k
j
t − nb
11
Over the estimation data set DN = yt utN
t=1 1 can be rewritten in a
linear regression form
yt = pxtT ϑ + ξt
12
where xt = −yt − 1  −yt − na ut − 1  ut − nbT is system in
put vector of observables with assumed known dimension of na + nb ϑ =
aT  b1ω1  b1ωd bnbω1  bnbωnbT ∈ ℜna+d·nb
pxt = −yt − 1  −yt − na
N k
1
t − 1  N k
d
t − 1 N k
1
t − nb
  N k
d
t − nbT
13
12 can be rewritten in the matrix form as
y = Pϑ + Ξ
14
where y = y1 · · ·  yNT is the output vector Ξ = ξ1  ξNT  and P
is the regression matrix
P =
⎡
⎢⎢⎣
p1x1 p2x1 · · · pna+d·nbx1
p1x2 p2x2 · · · pna+d·nbx2
                                     
p1xN p2xN · · · pna+d·nbxN
⎤
⎥⎥⎦
15
PSO Assisted NURB Neural Network Identiﬁcation
5
Denote B = PT P Performing the eigenvalue decomposition BQ = QΣ where
Σ = diagσ1 σr 0 · · ·  0 with σ1  σ2    σr  0 Q = q1 · · ·  qna+d·nb
followed by truncating the eigenvectors corresponding to zero eigenvalues we
have
ϑsvd
LS =
r

i=1
yT Pqi
σi
qi
16
Thus the mean square error can be readily computed from
Jλ = y − Pϑsvd
LS T y − Pϑsvd
LS N
17
for any speciﬁed λ Note that it is computationally simple to evaluate Jλ due
to the fact that the model has a linear in the parameter structure for a given λ
This suggests that we can optimize λ as the ﬁrst task
42
Particle Swarm Optimisation for Estimating the Shaping
Parameters λj’s
In the following we propose to apply the PSO algorithm 1011 and aim to solve
λopt = arg
min
λ∈d
j=1 Λj
Jλ
st λT 1 = 1
18
where 1 denotes a vector of all ones with appropriate dimension
d

j=1
Λj =
d

j=1
0 1
st
λT 1 = 1
19
deﬁnes the search space A swarm of particles λl
i S
i=1 that represent potential
solutions are “ﬂying” in the search space d
j=1 Λj where S is the swarm size
and index l denotes the iteration step The algorithm is summarised as follows
a Swarm initialisation Set the iteration index l = 0 and randomly generate
λl
i S
i=1 in the search space d
j=1 Λj These are obtained by randomly set each
element of λl
i S
i=1 as rand denoting the uniform random number between
0 and 1 followed normalizing them by
λ0
i
= λ0
i 
d

j=1
λ0
i |j
20
where •|j denotes the jth element of • so that λ0
i T 1 = 1 is valid
b Swarm evaluation The cost of each particle λl
i
is obtained as Jλl
i  Each
particle λl
i
remembers its best position visited so far denoted as pbl
i  which
provides the cognitive information Every particle also knows the best position
visited so far among the entire swarm denoted as gbl which provides the social
6
X Hong and S Chen
information The cognitive information pbl
i S
i=1 and the social information
gbl are updated at each iteration
For i = 1 i ≤ S i++
If Jλl
i   Jpbl
i 
pbl
i
= λl
i 
End for
i∗ = arg
min
1≤i≤S Jpbl
i 
If Jpbl
i∗   Jgbl
gbl = pbl
i∗ 
c Swarm update Each particle λl
i
has a velocity denoted as γl
i  to direct
its “ﬂying” The velocity and position of the ith particle are updated in each
iteration according to
γl+1
i
= μ0 ∗ γl
i
+ rand ∗ μ1 ∗ pbl
i
− λl
i 
+rand ∗ μ2 ∗ gbl − λl
i 
21
λl+1
i
= λl
i
+ γl+1
i

22
where μ0 is the inertia weight μ1 and μ2 are the two acceleration coeﬃcients
In order to avoid excessive roaming of particles beyond the search space 9 a
velocity space
d

j=2
Υj =
d

j=2
−Υjmax Υjmax
23
is imposed on γl+1
i
so that
If γl+1
i
|j  Υjmax
γl+1
i
|j = Υjmax
If γl+1
i
|j  −Υjmax
γl+1
i
|j = −Υjmax
Moreover if the velocity as given in equation 21 approaches zero it is reini
tialised proportional to Υjmax with a small factor ν
If γl+1
i
|j == 0 γl+1
i
|j = ±rand ∗ ν ∗ Υjmax
24
In order to ensure each element of λl+1
i
that it satisﬁes the constraint and stays
in the space we modiﬁed constraint check in the PSO as follows
If λl+1
i
|j  0
λl+1
i
|j = 0
then
λl+1
i
= λl+1
i

d

j=1
λl+1
i
|j
25
Note that the normalization step that we introduced here does not aﬀect the cost
function value rather it eﬀectively keeps the solution stay inside the bound
PSO Assisted NURB Neural Network Identiﬁcation
7
d Termination condition check If the maximum number of iterations Imax
is reached terminate the algorithm with the solution gbImax otherwise set
l = l + 1 and go to Step b
Ratnaweera and coauthors 14 reported that using a time varying accel
eration coeﬃcient TVAC enhances the performance of PSO We adopt this
mechanism in which μ1 is reduced from 25 to 05 and μ2 varies from 05 to 25
during the iterative procedure
μ1 = 05 − 25 ∗ lImax + 25
μ2 = 25 − 05 ∗ lImax + 05
26
The search space as given in equation 19 is deﬁned by the speciﬁc problem
to be solved and the velocity limit Υjmax is empirically set An appropriate
value of the small control factor ν in equation 24 for avoiding zero velocity is
empirically found to be ν = 01 for our application
43
Estimating the Parameter Vectors ω a b Using ϑsvd
LS
In this section we describe the second stage of Bai’s two stage identiﬁcation
algorithm 1 which can be used to recover ω a b from ϑsvd
LS λopt based on
the result of PSO above Our ﬁnal estimate of ˆa which is simply taken as the
subvector of the resultant ϑsvd
LS λopt consisting of its ﬁrst na elements
Rearrange the na + 1th to na + d + 1 × nbth elements of ϑsvd
LS λopt into
a matrix
M =
⎡
⎢⎢⎣
b1ω0 b1ω1 · · · b1ωd
b2ω0 b2ω1 · · · b2ωd
                    
bnbω0 bnbω1 · · · bnbωd
⎤
⎥⎥⎦ = bωT ∈ ℜnb×d+1
27
The matrix M has rank 1 and its singular value decomposition is of the form
M = Γ
⎡
⎢⎢⎣
δM 0 · · · 0
0 0 · · · 0
         
0 0 · · · 0
⎤
⎥⎥⎦ ΔT
28
where Γ = Γ 1  Γ nb ∈ ℜnb×nb and Δ = Δ1  Δd+1 ∈ ℜd+1×d+1
where Γ i i = 1  nb and Λi i = 1  d + 1 are orthonormal vectors δM is
the sole nonzero singular value of M b and ω can be obtained using
ˆb = δMΓ1
ˆω = Δ1
29
followed by
ˆb ←− βˆb
ˆω ←− ˆωβ
30
where β = 1 + na
j=1 ˆajnb
j=1 ˆbj
8
X Hong and S Chen
Note that the standard Bai’s approach as above may suﬀer a serious numerical
problem that the matrix M turns out to have rank higher than one resulting
in the parameters estimator far from usable The more stable modiﬁed SVD
approach 6 is used in our simulations
5
An Illustrative Example
A Hammerstein system is simulated in which the linear subsystem is Aq−1 =
1−12q−1+09q−2 Bq−1 = 17q−1−q−2 and the nonlinear subsystem Ψu =
2signu

|u| The variances of the additive noise to the system output is set as
001 low noise and 025 high noise respectively 1000 training data samples
yt were generated by using 1 and 2 where ut was uniformly distributed
random variable ut ∈ −15 15 The signal to noise ratio are calculated as
36dB and 22dB respectively The polynomial degree of Bspline basis functions
was set as k = 2 piecewise quadratic The knots sequence Uj is set as
−32 −24 −16 −08 −005 0 005 08 16 24 32
The proposed system identiﬁcation algorithm was carried out In the modiﬁed
PSO algorithm we set S = 20 Imax = 20 Υjmax = 0025 The resultant 8
NURB basis functions for the two data sets are plotted in Figure 1ab The
nonlinear subsystem obtained with σ2 = 001 data set was shown in Figure
1cThe plot obtained with σ2 = 025 data set has the same appearance except
for the external knots sequences The modelling results are shown in Table 1
for the linear subsystem
−15
−1
−05
0
05
1
15
0
01
02
03
04
05
06
07
08
09
1
u
−15
−1
−05
0
05
1
15
0
01
02
03
04
05
06
07
08
09
1
u
 
 
−4
−3
−2
−1
0
1
2
3
4
−3
−2
−1
0
1
2
3
 
 
NURB model prediction
True function
UiVi sequence external
UiVi sequence internal
a
b
c
Fig 1 The resultant Bspline solid line and NURB dotted line basis functions
formed using PSO a σ2 = 001 and b σ2 = 025 and the modelling result for the
nonlinear function Ψu σ2 = 001
Table 1 Results of linear subsystem parameter estimation for two systems
a1
a2
b1
b2
True parameter
−12
09
17
−1
Estimate parameters σ2 = 001 −12004 09004 17077 −10076
Estimate parameters σ2 = 025 −12015 09027 17424 −10412
PSO Assisted NURB Neural Network Identiﬁcation
9
6
Conclusions
This paper introduced a new system identiﬁcation algorithm for the Hammer
stein systems based on observational inputoutput data using the nonuniform
rational Bspline NURB neural network We propose utilising PSO for the es
timation of the shaping parameters in NURB neural networks The eﬃcacy of
the proposed approach is demonstrated using illustrative example
References
1 Bai EW Fu MY A Blind Approach to Hammerstein Model Identiﬁcation
IEEE Transactions on Signal Processing 507 1610–1619 2002
2 Billings SA Fakhouri SY Nonlinear System Identiﬁcation Using the Hammer
stein Model International Journal of Systems Science 10 567–578 1979
3 de Boor A Practical Guide to Splines Springer New York 1978
4 Chaoui FZ Giri F Rochdi Y Haloua M Naitali A System Identiﬁcation
Based Hammerstein Model International Journal of Control 786 430–442 2005
5 Farin G Curves and Surfaces for Comnputeraided Geometric Design a Practical
Guide Academic Press Boston 1994
6 Goethals I Pelckmans K Suykens JAK Moor BD Identiﬁcation of MIMO
Hammerstein Models Using Least Squares Support Vector Machines Automat
ica 41 1263–1272 2005
7 Greblicki W Stochastic Approximation in Nonparametric Identiﬁcation of Ham
merstein Systems IEEE Transactions on Automatic Control 4711 1800–1810
2002
8 Greblicki W Pawlak M Identiﬁcation of discrete Hammerstein Systems Using
Kernel Regression Estimate IEEE Transactions on Automatic Control AC311
74–77 1986
9 Guru SM Halgamuge SK Fernando S Particle Swarm Optimisers for Clus
ter Formation in Wireless Sensor Networks In Proc 2005 Int Conf Intelligent
Sensors Sensor Networks and Information Processing Melbourne Australia pp
319–324 2005
10 Kennedy J Eberhart R Particle Swarm Optimization In Proc of 1995 IEEE
Int Conf Neural Networks Perth Australia vol 4 pp 1942–1948 1995
11 Kennedy J Eberhart RC Swarm Intelligence Morgan Kaufmann 2001
12 Lang ZQ A Nonparametric Polynomial Identiﬁcation Algorithm for the Ham
merstein System IEEE Transactions on Automatic Control 42 1435–1441 1997
13 van der Merwe DW Engelbrecht AP Data Clustering Using Particle Swarm
Optimization In Proc CEC 2003 Cabberra Australia pp 215–220 2003
14 Ratnaweera A Halgamuge SK Watson HC Selforganizing Hierarchical Par
ticle Swarm Optimizer with Timevarying Acceleration Coeﬃcients IEEE Trans
Evolutionary Computation 8 240–255 2004
15 Stoica P S¨oderstr¨om T Instrumental Variable Methods for Identiﬁcation of
Hammerstein Systems International Journal of Control 35 459–476 1982
16 Verhaegen M Westwick D Identifying Mimo Hammerstein Systems in the Con
text of Subspace Model Identiﬁcation International Journal of Control 632 331–
349 1996
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 10–17 2012 
© SpringerVerlag Berlin Heidelberg 2012 
2D Discontinuous Function Approximation  
with RealValued GrammarBased Classifier System 
Lukasz Cielecki and Olgierd Unold 
Institute of Computer Engineering Control and Robotics 
Wroclaw University of Technology 
Wybrzeze Wyspianskiego 27 
50370 Wroclaw Poland 
lukaszcieleckiolgierdunoldpwrwrocpl 
Abstract Learning classifier systems LCSs are rulebased evolutionary 
learning systems Recently there is a growing interest among the researchers in 
exploring LCSs implemented in a realvalued environment due to its practical 
applications This paper describes the use of a realvalued Grammarbased 
Classifier System rGCS in a task of 2D function approximation rGCS is 
based on Grammarbased Classifier System GCS which was originally used 
to process context free grammar sentences In this paper we propose an exten
sion to rGCS called Simple Accept Radius SAR mechanism that filters 
invalid and unexpected input real values Performance evaluations show that the 
additional Simple Accept Radius mechanism enables rGCS to accurately ap
proximate 2D discontinuous function Performance comparisons with another 
realvalued LCS show that rGCS yields competitive performance 
Keywords Learning classifier systems Grammatical inference Contextfree 
grammar GCS 2D function estimation 
1 
Introduction 
A Learning Classifier System LCS is an evolutionary algorithm that operates on a 
population of rules used to classify an environmental situation The first learning clas
sifier system was introduced by Holland 6 shortly after he created Genetic Algo
rithms GAs 5 Many realworld problems are not conveniently expressed using the 
ternary representation typically used by LCSs true false and dont care symbol To 
overcome this limitation Wilson 11 introduced realvalued XCS classifier system 
for problems which can be defined by a vector of bounded continuous realcoded 
variables 
In 2 we introduced a new model of realvalued LCS  the rGCS  to classify real 
value data rGCS is based on Grammarbased Classifier System GCS which was 
used to process context free grammar CFG sentences 7 In rGCS terminal rewrit
ing rules were replaced with socalled environment probing rules That enabled rGCS 
 
2D Discontinuous Function Approximation with RealValued GCS 
11 
 
to explore environment described by real values First rGCS experiments utilized the 
checkerboard problem test sets This simple benchmark proved an ability to solve 
“realvalue input” problems and made new kind of data sets available for our system 
Following these preliminary experiments some more complex benchmarks were 
tested including 2D data and function classification but prior to that some improve
ments to the system were applied 
To improve 2D function classification a covering technique was developed This 
procedure implemented earlier in the GCS system replenishes the population of sys
tems classifiers with new ones created on the y to complete current state of grammar 
evolution As a result there is no need to launch a GA and wait for its results 3 
Current improvement to the rGCS  Simple Accept Radius or SAR  brought an 
ability to filter invalid and unexpected input values during the testing phase of an 
experiment In this paper we describe the use of rGCS with SAR mechanism in a task 
of 2D discontinuous function approximation The improved system was tested on 
benchmark functions introduced by Butz in 1 and compared with XCSR system 
2 
The rGCS 
The main idea behind the rGCS was to extend the space of eligible problems for 
grammarbased classifier system Our original system GCS  described in 7 was 
virtually limited to natural and formal language based tasks Improved rGCS is able to 
accept any data stored as a vector of real values which enables us to work with wide 
range of datasets 
Our rGCS operates in an environment formed by CYK table and evolves a CNF 
grammar An experiment is divided into some cycles Each cycle includes evolving 
new grammar testing every single example from the training set and modifying exist
ing rules population To fit to the real value input vectors we introduced environment 
probing rules which replace GCS standard terminal rewriting ones They cooperate 
with regular rules during the parsing phase of the learning process 
Automatic learning of CFG is realized with socalled grammatical inference from 
text 4 According to this technique system learns using a training set that in this case 
consists of sentences both syntactically correct and incorrect training or learning 
phase Grammar which accepts correct sentences and rejects incorrect ones is able to 
classify unseen so far sentences from a test set during the test phase Cocke
YoungerKasami CYK parser which operates in Θn3 · |G| time where n is the 
length of the parsed string and |G| is the size of the CNF grammar G 12 is used to 
parse sentences from the sets Environment of classifier system is substituted by an 
array of CYK parser Classifier system matches the rules according to the current 
environmental state state of parsing and generates an action or set of actions in 
GCS pushing the parsing process toward the complete derivation of the sentence 
The discovery component of the rGCS differs from the standard It extends LCS 
with “covering” procedure that adds some rules useful in the current state of parsing 
12 
L Cielecki and O Unold 
 
see 8 and a “fertility” technique which preserves rules dependencies in the parsing 
tree 10 
Environment Probing Rules  
Environment probing rules are to put into the very first row of the CYK table as they 
sense the input data and translate real numbers into the nonterminal symbols 
Each rule has the form of 
 
A → f  
where 
A is a nonterminal symbol f is a real number value 
f value rules factor or position is used during the matching process and the non
terminal A is to put into the first row of the CYK table 
Regular Grammar Rules 
These rules are identical to the ones used in the classic GCS They are used in the 
CYK parsing process and the GA phase They are in the form of 
 
B → BC  
where A B and C are nonterminal symbols 
Matching Phase 
During the matching phase a bundle of nonterminals is defined to put into the first 
row of the CYK table achieving the goal of translating real input values into the 
string of symbols capable of parsing 
First a list of distances between the element of an input vector real number value 
and each rule’s factor is created and then simply the nearest rule is selected As the 
result always one rule is put into the CYK cell This scheme used during training 
process uses only a single real value from the rule  a point with no accepting range 
around it explicitly labeled That approach differs from the one adopted in Wilson’s 
XCSR 10 where several interval predicates are defined to “catch” input values lo
cated inside its boundaries The main difference is that in the rGCS learning the single 
value method always chooses at least one rule  no matter where the input value is 
located This means that even with limited number of environment probing rules there 
are no input values that are left unrecognized This can be referred as some kind of 
interpolation of an input space 
Strategy where each rule can accept any real value without looking at the distance 
between them causes some problems however In many problems only some specific 
subset of points in the input space or in the certain dimension of an input space can 
exist in the positive examples Therefore we need to modify the matching phase to 
stop input values not included in that subset from being accepted To achieve that we 
set up a Single Accept Radius SAR parameter which describes every environment 
probing rule It is set up and updated and then not yet used during the matching 
phase of the learning process in the rGCS Simple accept radius is the longest distance 
between rule’s factor and any real value accepted by it that belongs to the positive  
 
 
2D Discontinuous Function Approximation with RealValued GCS 
13 
 
example During the test phase we accept only these real values that are located with
in the learned accept radius This protects us from accepting values that are outside 
the area which our terminal rewriting rule is supposed to be employed for 
Matching regular rules used in the CYK parsing process follows the CYK algo
rithm procedure and is same as in GCS 
Rules Adjustments and Evolution 
Environment probing rules adjustments were widely discussed in 2 As the envi
ronment probing rules match the input vector some data about the environment is 
collected to find the best possible value of each rule’s realvalue factor The aim is to 
distribute environment probing rules over the input space Modified Kohonen neural 
network adaptation techniques were implemented in the rGCS to allow classifiers’ 
realvalue factors to automatically tune to the environment 
Regular grammar rules are evolved just like in the classic GCS system during the 
evolutionary process GA is launched at the end of each learning cycle Fitness evalu
ation uses a fertility measurement technique see 10 for discussion for the rules 
that were present in any complete parsing tree generated during the cycle This tech
nique tries to keep all dependencies between rules in the parsing tree by promoting 
classifiers with large number of descendants 
3 
Function Estimation Experiments 
In 1 Butz presented some challenging two dimensional discontinuous functions to 
compare his modification of the XCSR with the original system 11 The task was to 
decide whether a point at given coordinates belongs to the function or not In this 
paper we test two Butz functions with rGCS classifier 
 
3
33

3
 33
 

1
y
x
f x y
+
=
6
24

 

2
y
x
x y
f
+
=
 
1 
where the  is a modulo operator 
 
Fig 1 Butz functions the axisparallel step function f1 and the axisdiagonal step function f2 
14 
L Cielecki and O Unold 
 
During these experiments Simple Accept Radius SAR technique was employed 
in the rGCS for the very first time For instance for function f1 and the Z axis all val
ues from positive examples are set exactly at five certain points 0 13 23 1 43  
therefore accept radius for perfectly adjusted located at these points during learning 
rules is equal to 0 That means that during the testing stage only these five real values 
will be translated into the corresponding nonterminal symbols while all other values 
will be simply rejected This filtering in an environment probing step is a key to suc
cess in the correct classification but to achieve a final recognition an additional task 
must be done by regular rules of the rGCS and its CYK table 
To compare with Butz results achieved by his improved XCSR we performed a set 
of experiments with different population sizes Fig 2 shows minimum mean and 
maximum fitness error percentage of incorrectly recognized examples averaged over 
20 individual runs Estimated error rate average 003 at 150 regular classifiers best 
result 001 is rewarding and comparable with Butz results 1 with XCSR best re
sults 100 with original system and 001 with modified one Interestingly we have 
not noticed any important change to the grammar evolution time always between 170 
and 220 cycles nor the best results minimum error rate equal to 001 of the single 
experiment while changing the population size On the other hand mean averaged 
over 20 runs is much better when employing more regular set of classifiers That 
means that it is still possible to evolve robust grammars even with small classifier 
population but having it bigger raises the chances of rGCS to succeed There is how
ever always a minimum population size below which it is absolutely impossible to 
evolve a proper grammar For instance during f2 function estimation we observed that 
50 regular rules was not enough to evolve grammar with 003 error rate Fig 2a For 
bigger grammars Fig 2b and 2c it was still possible to achieve 001 error rate 
which again corresponds to Butzs results 
31 
Induced Grammar Analysis 
Lets have a closer look at how rGCS actually works In contrast to many other ma
chine learning techniques its possible due to its nature where knowledge is 
represented by easy to interpret rules forming CFG grammar To do it we take some 
sentences from f2 dataset and analyze an environment probing and parsing done by 
rGCS In the f2 function axis z values for positive examples may be only located at 4 
certain points 0 025 05 and 075 These values form third last elements of all 
input vectors in the dataset and are classified by a dedicated class of environment 
probing rules 
Fig 3 shows rGCS actions for two true positive and one true negative classifica
tions depicted by parsing trees important properties of involved environment probing 
rules and input vector elements values In examples 1 and 8 environment probing 
rules have translated corresponding input values that are closest to theirs factors and 
fit into learned Simple Accept Radius SAR Last translated nonterminal symbol 
presented most to the right in the bottom row of each parsing tree always  
 
 
2D Discontinuous Function Approximation with RealValued GCS 
15 
 
 0
 005
 01
 015
 02
 025
 03
 035
 04
 045
 05
 0
 100
 200
 300
 400
 500
Fitness error
Learning cycle
a f1 50 regular classifiers
Minimum
Mean
Maximum
 0
 01
 02
 03
 04
 05
 06
 0
 100
 200
 300
 400
 500
Fitness error
Learning cycle
b f1 100 regular classifiers
Minimum
Mean
Maximum
 0
 01
 02
 03
 04
 05
 06
 0
 100
 200
 300
 400
 500
Fitness error
Learning cycle
c f1 150 regular classifiers
Minimum
Mean
Maximum
 0
 005
 01
 015
 02
 025
 03
 035
 04
 045
 05
 0
 100
 200
 300
 400
 500
Fitness error
Learning cycle
d f2 50 regular classifiers
Minimum
Mean
Maximum
 0
 005
 01
 015
 02
 025
 03
 035
 04
 045
 05
 0
 100
 200
 300
 400
 500
Fitness error
Learning cycle
e f2 100 regular classifiers
Minimum
Mean
Maximum
 0
 005
 01
 015
 02
 025
 03
 035
 04
 045
 0
 100
 200
 300
 400
 500
Fitness error
Learning cycle
f f2 150 regular classifiers
Minimum
Mean
Maximum
 0
 005
 01
 015
 02
 025
 03
 035
 04
 045
 05
 0
 100
 200
 300
 400
 500
Fitness error
Learning cycle
g f2 200 regular classifiers
Minimum
Mean
Maximum
 0
 005
 01
 015
 02
 025
 03
 035
 04
 045
 0
 100
 200
 300
 400
 500
Fitness error
Learning cycle
h f2 250 regular classifiers
Minimum
Mean
Maximum
 
Fig 2 Experimental results of rGCS over f1 and f2 functions 
corresponds to the axis z value of an input vector and therefore dedicated rules accept 
radius is set strictly to 0 in the learning phase for f2 function all positive examples 
were strictly located as 4 points mentioned above giving us zerotolerance rules for  
zaxis There is however no rule in sentence 6 that accepts last input value 02237 on 
axis z surely doesnt belong to f2 and SAR wont allow any rule to accept the value 
16 
L Cielecki and O Unold 
 
Consequently there is no rule which can let the parsing to complete and put a starting 
symbol S at the root of the parsing tree That classification is therefore considered as 
true negative Classifications of examples 1 and 8 with starting symbol S at the root 
are true positive ones 
 
Fig 3 Parsing trees for function f2 
4 
Conclusion 
In this paper realvalued Grammarbased Classifier System was extended by Simple 
Accept Radius mechanism Thanks to use SAR the system is able to reject input real 
 
2D Discontinuous Function Approximation with RealValued GCS 
17 
 
values that are located outside the learned accept radius Experimental results showed 
that the improvement enabled rGCS to solve discontinuous function approximation 
problems with low fitness error 
However there are some areas where future development seems to be necessary 
That includes building more sophisticated environment probing rules with complex 
accept conditions working in more than one dimensions and therefore checking more 
then one element of an input vector simultaneously A number of other mechanisms 
are being considered to improve grammar evolution 
References 
1 Butz M Kernelbased Ellipsoidal Conditions in the Realvalued XCS Classifier System 
In Proceedings of the 2005 Conference on Genetic and Evolutionary Computation pp 
1835–1842 ACM 2005 
2 Cielecki L Unold O Realvalued GCS Classifier System International Journal of Ap
plied Mathematics and Computer Science 174 539–547 2007 
3 Cielecki L Unold O Modified Himmelblau Function Classification with RGCS Sys
tem In Proc of 8th International Conference on Hybrid Intelligent Systems HIS 2008 
pp 879–884 IEEE 2008 
4 Gold E Language Identification in the Limit Information and Control 105 447–474 
1967 
5 Holland J Adaptation in Natural and Artificial Systems An Introductory Analysis with 
Applications to Biology Control and Artificial Intelligence U Michigan Press 1975 
6 Holland J Adaptation In Rosen R Snell F eds Progress in Theoretical Biology 
Academic Press 1976 
7 Unold O Contextfree Grammar Induction with Grammarbased Classifier System Arc
hives of Control Science 154 681–690 2005 
8 Unold O Cielecki L Grammarbased Classifier System In Issues in Intelligent Sys
tems Paradigms pp 273–286 2005 
9 Unold O Cielecki L How to use Crowding Selection in Grammarbased Classifier Sys
tem In Proceedings of 5th International Conference on Intelligent Systems Design and 
Applications ISDA 2005 pp 124–129 IEEE 2005 
10 Unold O Playing a ToyGrammar with GCS In Mira J Álvarez JR eds IWINAC 
2005 Part II LNCS vol 3562 pp 300–309 Springer Heidelberg 2005 
11 Wilson SW Get Real XCS with ContinuousValued Inputs In Lanzi PL Stolzmann 
W Wilson SW eds IWLCS 1999 LNCS LNAI vol 1813 pp 209–219 Springer 
Heidelberg 2000 
12 Younger D Recognition and Parsing of Contextfree Languages in Time n3 Information 
and Control 102 189–208 1967 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 18–24 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A Scatter Search Methodology  
for the Aircraft Conflict Resolution Problem 
ZhiZeng Li XueYan Song JiZhou Sun and ZhaoTong Huang 
School of Computer Science and Technology Tianjin University Tianjin China 
lzz0613126com 
Abstract Accurate conflict resolution is important to improve traffic capacity 
and safety In   this paper it shows how the evolutionary approach called 
scatter searchSS can be used to solve the problem of aircraft conflict under the 
free flight conditions The mathematic model of the conflict resolution problem 
is based on the path optimization problem In order to justify the choice of SS 
the paper describes the improvements that were used for solving the conflict 
resolution problem after a brief description of SS algorithm A large number of 
simulation results show that comparing with genetic algorithm SS can reduce 
the running time about 15s to get the equivalent effect In the same time the 
total cost is reduced and the fairness between the airlines is taken into account 
Keywords conflict resolution free flight scatter search air traffic 
1 
Introduction 
The increasing development of air traffic in the limited and crowd airspace stimulates 
people to explore the new way to increase the utilization of airspace Recent advances 
in navigation and data communication technologies make it feasible for individual 
aircraft to plan and fly their trajectories in the presence of other aircraft in the 
airspace This way individual aircraft can take advantage of the atmospheric and 
traffic conditions to optimally plan their paths34 Therefore how to solve the 
conflicts fast and accuracy is the key technology to ensure air safe and achieve free 
flight  
The problem of conflict resolution has been researched for a long time In total over 
60 different methods have been proposed by various researchers to address the conflict 
detection and resolution12 Karl D proposes the geometric optimization approach to 
aircraft conflict resolution5 Using the geometric characteristics of aircraft 
trajectories and information on aircraft position speed and heading the intuitive 
geometric can achieve more accurate conflict resolution In response to the largescale 
conflict Stefan and Nicolas propose the multiagent concept to solve the conflict67 
The technology of multiagent is suitable for the largescale complex structures and 
realtime problems The key feature is the negotiation mechanism between agents 
which is effective to the multiaircraft conflict 
Optimization algorithms like genetic algorithm8 are applied in the conflict 
resolution problem It is based on the path planning and has its efficiency to solve 
 
A Scatter Search Methodology for the Aircraft Conflict Resolution Problem 
19 
 
aircraft conflicts The authors note that the problem is simplified but it still has 
common constraints and objectives  
In this paper we propose a novel effective and efficient approach to solve the 
problem of conflict resolution namely the SS algorithm910 Like GA SS is also 
based the population In addition SS algorithm is less dependent on the randomness 
of the search process but the use of the dispersionconvergencegather intelligence 
iteration mechanism In comparison with the existing genetic algorithm the proposed 
approach improves both running time and fairness between airlines  
In the next section the paper introduces the aircraft conflict resolution problem 
Section 3 describes the SS implementation and section 4 contains the results of the 
simulation examples Finally it concludes in section 5 with some views on the success 
of this research 
2 
Problem Definition 
Conflict resolution can be described as when flights fly into the detection sector 
according to the comparison of intended tracks there may be appearing the risk of 
conflict in a moment The Optimization algorithm makes the replanning of the track 
to avoid conflict as show in Fig1 
 
 
Fig 1 Track changing diagram 
21 
Flight Safety Constraints 
Due to the particularity of the air traffic the relevant departments have expressly 
provided flight safety interval In this paper aircrafts take the free flight strategy in 
the cruise phase and fly in the same altitude Aircraft can only make the flight angle 
be variation to achieve conflict resolution in case of emergency The horizontal 
interval must be not less than 20 km  
Safety requirements 




2
2
20
i
j
i
j
d
x
x
y
y
km
=
−
+
−
≥
  
 1
22 
The Objective Function 
1 Minimum of the gap of distance between the point where the aircraft leave the 
detection sector scheduled and the point where the aircraft leave the detection sector 
actually 
20 
ZZ Li et al 
 
2
1
1
n
i
i
d
K
S
e
=
−
=
  
 2




2
2
i
i
j
i
j
d
x
x
y
y
=
−
+
−
 is the distance between the two points K is a constant 
namely the size of the initial population 
2 Minimum of the total voyage 
   
2
1
1
min
n
m
ij
i
j
S
s
=
=
=

    
3
n expresses the number of aircraft m is the number of point 
ijs expresses the more 
voyage of the i aircraft at the point j  
3 Minimum of the gap of total voyage between the aircrafts 
3
min
i
j
S
s
s
=
−
   
4




19
2
2
 
  1
 
  1
 
  1
i j
i j
i j
i j
i j
i j
s
x
x
y
y
+
+
=
=
−
+
−


is
is the total voyage of the 
aircraft i and
js is the total voyage of the aircraft j  
3 
Scatter Search Algorithm 
The SS algorithm is a kind of optimization algorithm to solve the problem of decision 
planning and constraint policy The basic idea build the reference set in accordance 
with the principle of diversity and get the better solution set through the operations of 
the reference set such as selection combination mutation and so on  
The SS algorithm consists of five main systematic sub methods diversification 
generation method reference set generation method subset generation method 
solution combination method reference set update method 
31 
Population Initialization 
The following are the strategies of gene encoding in a 200200 km detection sector 
the aircraft’s flight path is divided into 20 sections At each node position the aircraft 
will take action taking into account the passengers’ comfort and safety the paper sets 
the aircraft movements of three kinds 
 
1turn left 30 degrees and be coded as 01 
2turn right 30 degrees and be coded as 10 
3keep the original direction and be coded as 00 or 11 
Therefore the length of each binary string of code is 220=40 Like Fig2 
 
A Scatter Search Methodology for the Aircraft Conflict Resolution Problem 
21 
 
    
Fig 2 The gene encoding of aircraft track 
32 
Reference Set Generation 
In order to achieve the safety constrains the randomly generated solution set shall be 
screened by the equation 1 Then it can get the feasible solution set to achieve the 
population initialization 
In the SS algorithm the diversity solution set
1
R and the highquality solution 
set
2
R  together constitute the reference set R   
The diversity solution set generation in order to maintain the diversity of the 
solution the initial solution set is divided into n groups averagely according to the 
corresponding decimal value of solution Then select one solution randomly from 
each group and the final n solutions that have been selected constitute the diversity 
solution set 
The highquality solution set First of all according to the economy constrains 
formula 23 select 2n solutions Then from the 2n solutions that have been chosen 
according to the fairness constrains formula 4 select the n best solution to be the 
highquality solution set 
33 
Subset Generation 
The size of the subset is 2 and every two solutions should combine in one subset So 
if the number of reference set is B the number of subset is BB12  
34 
Solutions Combination Method 
1 Crossover it takes the singlepoint crossover method Select the point where the 
binary code of the first aircraft track is end as the cross point Like Fig3 
2 Mutation select the mutation point randomly and change the code Like Fig4 
 
 
 
Fig 3 Crossover 
Fig 4 Mutation 
22 
ZZ Li et al 
 
35 
Reference Set Update Method 
It can determine that whether the new solutions can replace the solutions in the high
quality set If it can replace return to the step of reference set generation Until there 
are no new solutions can replace the solutions in the highquality set 
4 
Performance Evaluation 
In the Fig5 there is a 200200km detection sector If there is no any intervention in 
the black dashed box the horizontal interval of the two aircrafts will be less 
than 20km and have a risk of collision  
 
Fig 5 Aircraft collision model 
In this paper establish the simulation system of aircraft conflict resolution using 
Matlab The PC for simulation CPU 24GHz CoreTM4 memory 2GB In SS the 
size of reference set is 20 the initial population is 50100200 respectively each 
instance we run 10 times and select the best results as shown in Table1 
From the Table1 we can see that 
Table 1 Result comparison of different methods’ 
Instance 
The size of  
initial 
population 
The number 
of  
iteration 
GA
SS
Economic 
constraints 
Fairness
gap 
Running
time 
Economic 
constraints 
Fairness 
gap 
Running 
time 
AP50G20 
50
20 
78371
43
4s
72384
38 
4 
AP50G40 
50
40 
86231
37
7s
81363
49 
9 
AP50G60 
50
60 
87540
46
11s
86542
42 
13 
AP50G80 
50
80 
86913
39
14s
 
BP100G20 
100
20 
63417
24
6s
73677
32 
7s 
BP100G40 
100
40 
76243
27
11s
97273
29 
13s 
BP100G60 
100
60 
95392
26
17s
 
BP100G80 
100
80 
97365
23
24s
 
CP200G20 
200
20 
65461
24
8s
77134
21 
8s 
CP200G40 
200
40 
75470
23
13s
89387
26 
14s 
CP200G60 
200
60 
87412
21
19s
99011
20 
20s 
CP200G80 
200
80 
91915
19
26s
 
CP200G100 
200
100 
98624
26
34s
 
 
 
 
A Scatter Search Methodology for the Aircraft Conflict Resolution Problem 
23 
 
1GA will run until reach the number of iteration but the SS will stop when the 
new solutions can’t replace the solutions in the reference set 
2The goal of the economy constrains we achieve using the SS algorithm is almost 
equal with the effect using the GA It can fully demonstrate the effectiveness of the 
SS algorithm 
3The fairness between airlines not only decreases but also tends to balance The 
fairness is improved 
4According to the running time it just spend 13s to get the best solution genetic 
algorithm requires 24s It fully demonstrates the high efficiency of the algorithm in 
this paper 
Based on the instance of BP100 the Fig6 is the track using the SS algorithm and the 
Fig7 is the track using the genetic algorithm The Figs can directly perceive the 
problem of aircraft conflict resolution through the senses 
 
0
20
40
60
80
100
120
140
160
180
200
0
20
40
60
80
100 120 140 160 180 200
X
Y
 
0
20
40
60
80
100
120
140
160
180
200
0
20
40
60
80
100 120 140 160 180 200
X
Y
c
 
Fig 6 The track after relief using SS     
Fig 7 The track after relief using GA 
In the final simulate the conflict resolution of threeaircraft using the SS 
0
20
40
60
80
100
120
140
160
180
200
0
20
40
60
80
100 120 140 160 180 200
X
Y
 
Fig 8 The three aircrafts’ track after relief 
From the Fig8 the SS is suitable for the conflict resolution of many aircrafts So 
there are some reasons to say SS has some advantages for the conflict resolution 
problem 
24 
ZZ Li et al 
 
5 
Conclusion 
A new approach to aircraft conflict resolution called scatter search algorithm has 
been presented Using the approach in this paper the conflict resolutions are optimal 
in the sense that they not only achieve the equal effect that is got by using the genetic 
algorithm but also spend less time and achieve the balance between airlines 
Especially the less time can meet the need the requirement of realtime in air traffic 
The performance of the algorithm has been illustrated with simulation examples 
 
Acknowledgements This study is supported by grants from National Natural Science 
Foundation of China and Civil Aviation Administration of China61039001 and 
Tianjin Municipal Science and Technology Commission11ZCKFGX04200 
References 
1 Kuchar JK Yang LC A Review of Conflict Detection and Resolution Modeling 
Methods IEEE Transactions on Intelligent Transportation Systems 14 179–189 2000 
2 Kuchar JK Yang LC Survey of Conflict Detection and Resolution Modeling Methods 
In AIAA Guidance Navigation and Control Conference New Orleans LA August 11
13 1997 
3 Schulz R Shaner D Zhao Y Free Flight Concept In AIAA Guidance Navigation 
and Control Conference New Orleans LA August 1113 1997 
4 Menon PK Sweriduk GD Optimal Strategies for Freeflight Air Traffic Conflict 
Resolution AIAA Journal of Guidance Control and Dynamics 222 202–211 1999 
5 Bilimoria KD A Geometric Optimization Approach to Aircraft Conflict Resolution In 
AIAA Guidance Navigation and Control Conference Denver CO August 1417 2000 
6 Tomlin C Pappas GJ Sastry SS Conflict Resolution for Air Traffic Management A 
Study in Multiagent Hybrid Systems IEEE Transactions on Automatic Control 434 
509–521 1998 
7 Resmerita S Heymann M Conflict Resolution in MultiAgent Systems In 42nd IEEE 
Conference on Decision and Control Hyatt Regency Maui December 912 2003 
8 Durand N Alliot JM Noailles J Automatic Aircraft Conflict Resolution Using 
Genetic Algorithms In Proceedings of the 1996 ACM Symposium on Applied 
Computing pp 289–298 ACM Press New York 1998 
9 Marti R Laguna M Laguna F Principle of Scatter Search European Journal of 
Operationl Research 169 359–372 2006 
10 Burke EK Curtois T A Scatter Search Methodology for the Nurse Rostering Problem 
Journal of the Operational Research Society 61 1667–1679 2010 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 25–33 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Selfadaptive Differential Evolution Based  
Multiobjective Optimization Incorporating Local  
Search and IndicatorBased Selection 
Datong Xie12 Lixin Ding1 Shenwen Wang1 Zhaolu Guo1 Yurong Hu1  
 and Chengwang Xie3 
1 State Key Laboratory of Software Engineering School of Computer Wuhan University 
Wuhan 430000 PR China 
2 Department of Information Management Engineering Fujian Commercial College Fuzhou 
350012 PR China 
3 East China Jiao Tong University Nanchang 330000 PR China 
2010lxdingwangshenwenwhueducn  
gzl990137yuronghu118gmailcom chengwangxie163com 
Abstract As an efficient and effective evolutionary algorithm Differential 
evolution DE has received everincreasing attention over recent years 
However how to make DE suitable for multiobjective optimization is still 
worth further studying Moreover various means from different perspectives 
are promising to promote the performance of the algorithm In this study we 
propose a novel multiobjective evolutionary algorithm ILSDEMO which 
incorporates indicatorbased selection and local search with a selfadaptive DE 
In this algorithm we also use orthogonal design to initialize the population In 
addition the knearest neighbor rule is employed to eliminate the most crowded 
solution while a new solution is ready to join the archive population The 
performance of ILSDEMO is investigated on three test instances in terms of 
three indicators Compared with NSGAII IBEA and DEMO the results 
indicate that ILSDEMO can approximate the true Pareto front more accurately 
and evenly 
Keywords multiobjective optimization selfadaptive differential evolution 
orthogonal design indicatorbased selection local search knearest neighbor 
1 
Introduction 
In multiobjective optimization a single method usually cannot perform well in terms 
of all performance indicators 1 such as convergence uniformity time costs 
stability etc In order to get an overall promotion in multiple indicators various 
means from different perspectives are worth trying It is preferable to employ simple 
and efficient variation operators such as Differential evolution DE 2 to improve 
the convergence performance Several wellknown DEbased multiobjective 
evolutionary algorithms such as PDE DEMO GDE3 εMyDE 3 had been 
developed However most of them use DErandnbin E MezuraMontes 4 
26 
D Xie et al  
 
experimentally validated that the most competitive one of 8 DE variants based on the 
quality and robustness of the results regardless the characteristics of the problem to 
be solved was DEbest1bin In addition using different fitness assignment 
strategies and selection operators to adjust the selection pressure or employing local 
search operators to improve the accuracy of the results are also popular in recent years 
56 Recently selfadaptive parameters and strategies have also received increasing 
attention due to the problemdependent and stagedependent parameters and 
strategies 
Based on the above considerations we extend the two variants of DE 
DEBest1bin and DEBest1exp and present a novel method ILSDEMO to 
solve multiobjective problems More specifically the proposed approach 
incorporates orthogonal design local search indicatorbased selection 7 knearest 
neighbor rule with a selfadaptive DE 
The remainder of the paper is organized as follows In section 2 we describe the 
proposed algorithm Then in section 3 the performance of the algorithm is 
investigated experimentally on three test instances in terms of three indicators and the 
results are compared with those of NSGAII IBEA and DEMO Finally conclusions 
are drawn in section 4 
2 
The Proposed Method 
21 
Population Initialization 
The state of the initial population usually has an effect on the results Experimental 
design methods such as uniform design and orthogonal design OD are statistically 
sound 8 and popular in many areas OD had been used successfully in evolutionary 
algorithms 8910 In our method OD is utilized to generate an initial population 
where the individuals are scattered uniformly over the feasible solution space so that 
the algorithm can scan the search space evenly and locate the promising solutions 
quickly  
22 
The Variation Operators 
Exploration and Exploitation capabilities mainly propelled by variation operators are 
equally important in multiobjective evolutionary algorithms They are realized via 
selfadaptive DE and a local search operator respectively in our method 
1 The Selfadaptive Differential Evolution 
In our selfadaptive DE not only the two control parameters but also the strategies 
are determined selfadaptively to avoid early stagnation More precisely the control 
parameters and the strategy are encoded into the individuals and undergo variation 
Initially the scaling factor and crossover probability are randomly initialized within 
02 08 and 07 10 respectively Meanwhile one of the strategies DEbest1bin 
or DEbest1exp is randomly assigned to every individual At the end of each 
iteration the means and variances of two parameter values collected from the 
improved individuals are calculated to build two Gaussian models The unimproved 
 
Selfadaptive DE Based Multiobjective Optimization Incorporating Local Search 
27 
 
individuals will be assigned new parameter values by sampling the Gaussian models 
Moreover their strategies will be reinitialized according to the probabilities of the two 
strategies improving the individuals 
2 Local Search 
The primary purpose of local search methods is to enhance the exploitation 
capability of the algorithm Previous researches 111 had been focused on local 
search in multiobjective optimization Different from other local search methods in 
the literature of multiobjective optimization we use qGaussian mutation 12 to 
perform exploitation once the trial individual is better than the target individual In q
Gaussian probability density function the parameter q can control the shape of the q
Gaussian distribution As described in 12 larger values of q result in longer tails of 
the qGaussian distribution Thus qGaussian mutation can adjust the range of local 
search with different values of q 
In our algorithm if the individual obtained by local search dominates the target 
individual the latter will be replaced by the former Otherwise the target individual 
remains unchanged In either case the trial individual should be used to update the 
archive population in order to avoid neglecting any possible improvement on the 
distribution of the archive population 
23 
The Selection Operator 
In this study we utilize indicatorbased selection to generate a new parent population 
based on the union of the old parent population and the archive population First the 
duplicated individuals are eliminated from the union If the number of the remaining 
individuals is not beyond the parent population size all the individuals will be 
propagated into the new parent population Moreover the new parent population may 
need to be supplemented by some new initialized individuals Otherwise the 
indicatorbased selection will work Specifically indicator values of the remaining 
individuals will be worked out and one extreme point for each objective will be 
ensured to survive to the new parent population via assigning small enough indicator 
values to them Then an individual with a maximum indicator value is deleted from 
the union and the indicator values of the other individuals are updated concurrently 
The procedure is repeated until the number of the remaining individuals is equal to 
the parent population size 
24 
The Updating Rule of the Archive Population 
Although indicatorbased selection integrating with the preservation of extreme 
solutions is an effective method in favor of finding a welldistributed solution set it 
still cannot obtain ideal uniformity as well as some other algorithms such as SPEA2 
Therefore we will introduce the knearest neighbor rule to update the archive 
population In detail when the number of nondominated solutions exceeds the 
archive population size the cumulative Euclidean distance to its k nearest neighbors 
for every nondominated solution will be calculated Here k is set to 2  m  1 where 
m is the objective number Then the individual with the smallest distance is removed 
28 
D Xie et al  
 
25 
The Framework of the Proposed Algorithm 
Based on the above design the procedure of the proposed ILSDEMO algorithm for 
solving multiobjective problems is summarized in Algorithm 1 
Algorithm 1 ILSDEMO 
1 Set n=0 Initialize PnXi Fi CRi Sii∈1 NP 
using OD and Evaluate Pn 
2 Copy the nondominated solutions to the archive 
population PA 
3 While the termination criteria are not satisfied do 
4  Set n = n + 1 
5  for i=1 to NP do 
6    select r1≠r2 ∈1 2 … NP randomly 
7    best∈1 2 … fsize fsize is the size of the 
nondominated set in PA 
8    Perform program segment 1 or 2 using Si Fi CRi 
9    Evaluate Vi 
10   If Vi is nondominated by Xi then 
11      If Vi dominates Xi then 
12        Mark Xi as an improved individual 
13      End if 
14      Perform qGaussian mutation on Vi 
15      If Vi  is nondominated with the individuals in PA 
16        Add Vi to PA using the knearest neighbor rule 
17      Else 
18        Remove the dominated individuals from PA and 
add Vi to PA 
19      End if 
20   End if 
21   If Vi dominates Xi then 
22      Xi = Vi 
23   End if 
24  Endfor 
25  Build two Gaussian Models using the parameters of 
the improved individuals 
26  Calculate the probabilities of the strategies based 
on the improved individuals 
27  Generate Pn+1 based on Pn and PA using indicatorbased 
selection 
28  Reinitialize F CR S for the unimproved individuals 
according to the models 
29 Endwhile 
The variation operators of DEbest1bin and DEbest1exp are shown as follows 
  
 
Selfadaptive DE Based Multiobjective Optimization Incorporating Local Search 
29 
 
Program segment 1 
Program segment 2 
jrand = randint 1 D 
for j=1 to D do 
  if rand01CRi || j == jrand then 
     Vj
i = Xj
best + FiXj
r1Xj
r2 
  else 
     Vj
i=Xj
i 
  endif 
endfor 
j=randint 1 D 
k=0 
do 
  Vj
i = Xj
best + FiXj
r1Xj
r2 
  j = j+1  D 
  k++ 
while rand01CRi  kD 
 
In the above pseudocodes Pn is the parent population of the nth generation Fi CRi 
and Si are the two control parameters and the strategy corresponding to the individual 
Pi
n D is the dimension of decision variables randint1 D denotes generating a 
random integer number between 1 and D r1 and r2 are two randomly generated 
integers with uniform distribution and their values are lower than or equal to the 
population size and mutually different best is a randomly generated integer with 
uniform distribution and less than the size of the nondominated solution set 
3 
Experiments 
In this section we experimentally investigate the performance of ILSDEMO 
compared with NSGAII IBEA and DEMOparent 13 on DTLZ1 DTLZ3 and 
DTLZ7 using three performance metrics All the experiments are implemented in 
VC60 Matlab 71 Sigmaplot 120 and run on Intel i5 760 28GHz machines with 
4GB DDR3 RAM 
31 
Performance Indicators 
We employ three performance metrics namely hypervolume HV 14 inverted 
generational distance IGD 15 and Spacing 16 to compare the nondominated 
solutions obtained by the four algorithms It must be pointed out that we use the HV 
difference between the Pareto front and the obtained nondominated front to substitute 
the HV indicator In the following experiment the HV difference is denoted as HV 
Then a small value of HV corresponds to a high quality Also noticeable is that we 
use an improved version of spacing where the Euclidean distances between the two 
extreme ends of nondominated front and the corresponding ends of the Pareto front 
are considered 
32 
Parameters Settings 
According to our preliminary experiments the common parameters are set as follows 
NP=50 NA=150 F∈02 08 CR∈0710 RUNs=30 MaxEvals=100000 Here 
NP represents the parent population size NA denotes archive population size F and 
CR are the scaling factor and crossover rate of DE respectively RUNs and MaxEvals 
indicate the number of runs and evaluations in one run respectively In addition the 
30 
D Xie et al  
 
factor k is set to 002 in indicatorbased selection In IBEA and NSGAII the offspring 
population size is set to 50 the probabilities of simulated binary crossover SBX and 
polynomial mutation are set to 08 and 01 respectively the distribution indices of 
SBX and polynomial mutation are set to 20 In the calculation of performance 
indicators we use the objective values normalized by arctan function and finally 
mapped onto 0 1 and each objective value of the reference point is set to 10 
33 
Experimental Results 
In this section we present and discuss the experimental results obtained by 
ILSDEMO NSGAII IBEA and DEMO In DTLZ1 DTLZ3 and DTLZ7 the number 
of decision variable is equal to M+k−1 where M is the number of objective functions 
and k can be set by the user In this experiment M is set to 3 and k is set to 5 for 
DTLZ1 10 for DTLZ3 20 for DTLZ7 respectively 
It can be seen from Fig 1 that ILSDEMO approaches to the true Pareto fronts of the 
DTLZ test instances more accurately and more evenly In addition For DTLZ1 the 
solutions obtained by IBEA are more evenly distributed while NSGAII maintains a 
worst distribution For DTLZ3 the approximate front obtained by NSGAII is the 
worst owing to some outliers on f1 and f2 and DEMO is better than IBEA Although 
the centers of the fronts obtained by IBEA are evenly distributed to a certain extent 
not only large gaps exist between the centers and the edges but also some  
extreme points of the three objectives are lost For DTLZ7 the nondominated front 
obtained by NSGAII is similar to the one of DEMO while IBEA obtains the worst 
result 
 
0
05
0
05
0
05
f1
DTLZ1 ILSDEMO
f2
f3
0
05
1
0
05
1
0
05
1
f1
DTLZ1 NSGAII
f2
f3
0
05
1
0
05
1
0
05
1
f1
DTLZ1 IBEA
f2
f3
0
05
1
0
05
1
0
05
1
f1
DTLZ1 DEMO
f2
f3
0
05
1
0
05
1
0
05
1
f1
DTLZ3 ILSDEMO
f2
f3
0
2
4
0
2
4
0
1
2
f1
DTLZ3 NSGAII
f2
f3
0
1
2
0
1
2
0
1
2
f1
DTLZ3 IBEA
f2
f3
0
1
2
0
1
2
0
1
2
f1
DTLZ3 DEMO
f2
f3
0
05
1
0
05
1
0
5
10
f1
DTLZ7 ILSDEMO
f2
f3
0
05
1
0
05
1
0
5
10
f1
DTLZ7 NSGAII
f2
f3
0
05
1
0
05
1
0
5
10
f1
DTLZ7 IBEA
f2
f3
0
05
1
0
05
1
2
4
6
f1
DTLZ7 DEMO
f2
f3
 
Fig 1 The best approximate Pareto fronts of 25 runs obtained by the four algorithms on 
DTLZ1 DTLZ3 DTLZ7 according to HV value 
 
 
Selfadaptive DE Based Multiobjective Optimization Incorporating Local Search 
31 
 
DTLZ1
DTLZ1
DTLZ1
DTLZ3
DTLZ3
DTLZ3
ILSDEMO     NSGAII        IBEA           DEMO
IGD values
000
02
04
06
08
10
ILSDEMO     NSGAII        IBEA           DEMO
Spacing values
5
6
7
8
9
10
ILSDEMO     NSGAII        IBEA           DEMO
HV values
0000
001
002
003
004
005
ILSDEMO     NSGAII        IBEA           DEMO
IGD values
000
05
10
15
20
25
30
ILSDEMO     NSGAII        IBEA           DEMO
Spacing values
4
6
8
10
12
14
ILSDEMO     NSGAII        IBEA           DEMO
HV values
000
02
04
06
08
10
DTLZ7
DTLZ7
DTLZ7
ILSDEMO     NSGAII        IBEA           DEMO
IGD values
004
006
008
010
012
014
016
018
020
ILSDEMO     NSGAII        IBEA           DEMO
Spacing values
5
6
7
8
9
10
ILSDEMO     NSGAII        IBEA           DEMO
HV values
00000
0002
0004
0006
0008
0010
0012
 
Fig 2 The box plots of the three indicator values for DTLZ1 DTLZ3 and DTLZ7 
Besides the boxplots in Fig 2 statistically reflect the differences of the four 
algorithms by three performance indicators The overall performance of ILSDEMO is 
highly competitive with the other three methods Meanwhile seen from the medians 
on DTLZ1 IBEA exhibits better performance than NSGAII and also slightly 
outperforms DEMO For DTLZ3 although there exist some worse outliers for IBEA 
NSGAII only shows better performance than IBEA regarding IGD while it has worse 
spacing and HV values For DTLZ7 the four algorithms have consistent 
performance ranks on IGD and Spacing ie ILSDEMO has the best indicator values 
and IBEA obtains the worst indicator values while DEMO is better than NSGAII 
However IBEA is better than NSGAII and DEMO in terms of HV metric 
4 
Conclusions 
In this paper we have presented an efficient multiobjective evolutionary algorithm 
based on differential evolution Multiple approaches from different perspectives have 
been applied to boost the performance of the algorithm To generate a statistically 
sound population orthogonal design is used in this algorithm Selfadaptive 
parameters and strategies are employed to tune parameters automatically in order to 
adapt them to different problems or different stages In the greedy selection of 
differential evolution we use qGaussian mutation to enhance the local search 
capability Moreover the knearest neighbor rule is utilized to eliminate the crowded 
individual so as to maintain an evenly distributed archive population with a fixed size 
To keep the parent population distributed evenly we also employ indicatorbased 
32 
D Xie et al  
 
selection to form a new parent population based on the old parent population and the 
archive population Compared with NSGAII IBEA and DEMO on three test 
instances we have observed that our approach can converge to the true Pareto front 
more accurately and evenly 
 
Acknowledgments The work is supported by the National Natural Science 
Foundation of China No 60975050 the Fundamental Research Funds for the 
Central Universities No 6081014 the National Natural Science Foundation of 
China No 61165004 and the Natural Science Foundation Project of Fujian 
Province China No 2012J01248 
References 
1 Knowles JD LocalSearch and Hybrid Evolutionary Algorithm for Pareto Optimization 
PhD Thesis Department of Computer Science University of Reading Berkshire 2002 
2 Storn R Price K Differential Evolution–A Simple and Efficient Adaptive Scheme for 
Global Optimization over Continuous Spaces Tech Rep TR95012 Berkeley pp 1–12 
1995 
3 MezuraMontes E ReyesSierra M Coello CA MultiObjective Optimization Using 
Differential Evolution A Survey of the StateoftheArt In Chakraborty UK ed 
Advances in Differential Evolution SCI vol 143 pp 173–196 Springer Heidelberg 
2008 
4 MezuraMontes E VelázquezReyes J Coello CA A Comparative Study of 
Differential Evolution Variants for Global Optimization In Proceedings of the 8th Annual 
Conference on Genetic and Evolutionary Computation GECCO 2006 Seattle pp 485–
492 2006 
5 Das S Suganthan PN Differential Evolution A Survey of the StateoftheArt IEEE 
Trans Evol Comput 151 4–31 2011 
6 Noman N Iba H Accelerating Differential Evolution Using an Adaptive Local Search 
IEEE Trans Evol Comput 121 107–125 2008 
7 Zitzler E Künzli S IndicatorBased Selection in Multiobjective Search In Yao X 
Burke EK Lozano JA Smith J MereloGuervós JJ Bullinaria JA Rowe JE 
Tiňo P Kabán A Schwefel HP eds PPSN 2004 LNCS vol 3242 pp 832–842 
Springer Heidelberg 2004 
8 Zhang QF Leung YW An Orthogonal Genetic Algorithm for Multimedia Multicast 
Routing IEEE Trans Evol Comput 31 53–62 1999 
9 Leung YW Wang Y An Orthogonal Genetic Algorithm with Quantization for Global 
Numerical Optimization IEEE Trans Evol Comput 51 41–53 2001 
10 Zeng SY An Orthogonal Multiobjective Evolutionary Algorithm for Multiobjective 
Optimization Problems with Constraints Evol Comput 121 77–98 2004 
11 Sindhya K Sinha A Deb K Miettinen K Local Search Based Evolutionary Multi
objective Optimization Algorithm for Constrained and Unconstrained Problems In 
Proceedings of the 2009 IEEE Congress on Evolutionary Computation CEC 2009 
Trondheim pp 2919–2926 IEEE Press 2009 
12 Tinos R Yang SX SelfAdaptation of Mutation Distribution in Evolutionary 
Algorithms In Proceedings of the 2007 IEEE Congress on Evolutionary Computation 
CEC 2007 Singapore pp 79–86 2007 
 
Selfadaptive DE Based Multiobjective Optimization Incorporating Local Search 
33 
 
13 Robič T Filipič B DEMO Differential Evolution for Multiobjective Optimization In 
Coello Coello CA Hernández Aguirre A Zitzler E eds EMO 2005 LNCS 
vol 3410 pp 520–533 Springer Heidelberg 2005 
14 Zitzler E Evolutionary Algorithms for Multiobjective Optimization Methods and 
Applications PhD Thesis ETH Zurich 1999 
15 Veldhuizen DA Multiobjective Evolutionary Algorithms Classifications Analyses and 
New Innovations PhD Thesis Department of Electrical and Computer Engineering 
Graduate School of Engineering Air Force Institute of Technology WrightPatterson 
AFB Ohio 1999 
16 Deb K Pratap A Agarwal S Meyarivan T A Fast and Elitist Multiobjective Genetic 
Algorithm NSGAII IEEE Trans Evol Comput 62 182–197 2002 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 34–41 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Application of Data Mining in Coal Mine Safety Decision 
System Based on Rough Set 
Tianpei Zhou  
Xuzhou College of Industrial and Technology Xuzhou China 
Zhoutianpei001163com 
Abstract The coal mine safety decision systems such as ventilation safety 
monitoring system underground water inrush monitoring system underground 
coal and gas emission monitoring system have been established in many large 
and mediumsized coal mines A large amount of original data had accumulated 
in these systems How to transform data into information for scientific decision 
was a problem worth to consider for coal mine safety production The rough set 
theory quantitative analysis of incomplete imprecise and uncertainty know
ledge provided a new method and tool for data mining A kind of heuristic ge
netic algorithm for continuous attributes discretization was put forward to solve 
the problem of continuous attribute discretization of decision table a kind of 
heuristic immune algorithm for attribute reduction was presented to conquer the 
shortage of existing attribute reduction algorithm in order to solve the problem 
of reasoning and decision in incomplete and imprecise information a kind of 
default rule mining model based on reduction lattice was proposed Finally data 
mining system based on rough set was designed which was applied to data 
mining analysis of underground gas emission good results were achieved  
Keywords rough set data mining attribute reduction genetic algorithm  
Immure algorithm reduction lattice default rule 
1 
Introduction 
Coal and gas outburst was a complex geological disaster occurred in coal mines the 
complexity mainly as follows 1 the factors affecting coal and gas outburst itself 
was inaccurate and fuzzy 2 the relationship between outburst factors and outburst 
events was fuzzy 3 for the occurrence mechanism of coal and gas outburst that was 
not yet fully clear 4 all prediction index and prediction method of coal and gas out
burst was presented based on the experience which was difficult to generally applica
ble to different gas geological conditions 1 
Due to the above reasons the practical applications of prediction method of coal 
and gas outburst were subject to greater restrictions the effect was worst The differ
ent prediction methods using different parameters has used seven or eight but also 
choose more than a dozen two dozen for many penetration workers the problem that 
how to choose better predict effect index is a headache To solve the problems uncer
tainty ambiguity nonlinear of existing prediction methods a feasible way is to find 
Application of Data Mining in Coal Mine Safety Decision System Based on Rough Set 
35 
 
new theory and method2 Data mining system based on rough set was designed 
potential valuable information rules and knowledge were mined and discovered from 
a large number of original data thus to effectively guide and predict safety produc
tion operation and management of coal mining enterprises good results were 
achieved  
2 
Model for Default Rule Mining  
Default rule reflecting general relationships of data a certain degree of generality 
relatively simple expression form can deal the problems with some degree of incom
pleteness the socalled incompleteness are shown that some attributes have no value 
in decision system3 Model for default rule mining based on reduction lattice was 
presented and its idea is for given confidence threshold 
C
μ  and support threshold 
S
μ of target rules starting from the sample data according to rough set theory reduc
tion lattice was firstly built and searched through some search strategy the rule set 
that meet the requirements of credibility and support was calculated according to 
searched reduction nodes default rule was applied to reasoning or decision due to the 
incomplete information it was matched layer by layer in the lattice in accordance 
with existing information the optimal solution of problem was obtained according to 
a priority decision algorithm 
 
Ni q
 was represented as the ith node of the qth layer the corresponding decision 
subsystem for 
 
 
 
U Ci q
 D
 hich corresponded to a set of simplified rules set 
 
1






q
i
i
U IND C
X
X
=
 
1



    
j
U IND D
Y
Y
=
 if the credibility and support 
meet 


 
 






 
C
i
j
C
C
i
j
i
j
i
X Y
X Y
card X
Y
card X
μ
μ
μ

=
∩
 
1


 
 






 
C
i
j
C
C
i
j
i
j
i
X Y
X Y
card X
Y
card X
μ
μ
μ

=
∩
      
2
A default rule can be generated 

 
 

       






i
j
C
i
j
S
i
j
Des X
Des Y
X Y
X Y
μ
μ
→
 
3
And the rules were stored in the rule set of the node 
For node in reduction lattice a reduction node may be found from top to bottom 
and from left to right rule set was generated if confidence of the node relative to a 
particular decision class Yj rule set was less than confidence threshold all subsequent 
nodes relative to the decision class Yj rule set was not calculated then a reduction 
node may be found from t bottom o top and from right to left rule set was generated 
if support of the node relative to a particular decision class Yj rule set was less than 
support threshold all predecessor nodes relative to the decision class Yj rule set was 
not calculated and so forth finally all reduction node was traversed the rules set of 
all nodes were obtained4 
36 
T Zhou 
 
3 
Data Mining System Based on Rough Set 
Data mining system based on rough set achieved the main process data preprocess
ing data mining model interpretation and model evaluation as shown in Fig 1 the 
whole system was composed of four modules data preprocessing attribute reduc
tion rule extraction rule evaluation and interpretation5 
Database
Data selection
Default value
Handle
Attributes
Discretization  
General method
Heuristic 
method
Global search 
method
Attribute 
reduction
Neural network 
integrated 
Default rule
Rule evaluation 
Rule  
interpretation
Data preprocessing
Attribute reduction
Rule extraction
Rule evaluation and 
interpretation
Data 
warehouse
 
Fig 1 Frame of data mining system based on rough set 
31 
Data Preprocessing Module 
Data preprocessing was very important in data mining the success or failure of a 
data mining was affected not only by algorithm model selection but also by data pre
processing The results of data analysis were directly affected by incomplete noisy 
and inconsistent data Data pretreatment technology can improve the quality of the 
data and help to improve accuracy and performance of the mining Data pre
processing module included three functions data selection data default value 
processing and data discretization Data selection including the selection of data 
tables and the selection of data object in tables data default value processing includ
ing several filling and ignore methods of default value discretization of continuous 
attribute value was a generalization process of decision table Heuristic genetic algo
rithm for discretization of continuous attributes was adopted6 the algorithm was as 
follows 
Step 1 the genetic algorithm parameters maximum evolution algebra G popula
tion size Psize initial temperature T0Pc1Pc2Pm1Pm2 were set 
Step 2 evolutionary algebraically counts t=0 according to the rules of initial can
didate cut dots selection initial candidate cut sets were selected the population Pt 
was initialized 
Step 3 each individuals fitness value of the initial population Pt was calculated 
Step 4 population P′t was obtained by selection operation for population Pt 
Step 5 the population P′′t was obtained by crossover and mutation operation for 
population P′t 
Step 6 the next population P t +1 was obtained through corrective operation for 
population P′′t 
Application of Data Mining in Coal Mine Safety Decision System Based on Rough Set 
37 
 
Step 7 the calculation was end if t = G or optimal individual satisfying the re
quirements otherwise each individuals fitness value of the initial population Pt+1 
was calculated went to Step 4 
32 
Attribute Reduction Module 
Attribute reduction module the framework based on rough set theory the core of all 
the applications of rough set theory main function was reduction of data attributes 
and concentration and generalization of data Attribute reduction methods included 
general reduction heuristic reduction methods and global search attribute reduction 
method where heuristic immune algorithm for attribute reduction was adopted7 the 
algorithm was as follows 
Step 1 the relative difference comparison table of decision table was firstly solved 
then core attribute of attribute reduction in decision table was calculated according to 
the relative difference comparison table 
Step 2 the algorithm parameters were initialized antibody group size for N the 
memory size for M iterations for epochs crossover probability for Pc mutation prob
ability for Pm weighting factor for γ  
Step 3 antibody Ag was defined as attribute number of attribute reduction set 
Step 4 attribute reduction was encoded each antibody contained core properties 
only the noncore properties of antibody was encoded the size of antibody group 
A1was set to N＋M 
Step 5 corrective operation new antibody in the antibody group A1 was corrected 
by correction strategy so that each antibody was effective antibody 
Step 6 affinity calculation affinity fj and concentration cj of each antibody Abj in 
AkN+M was calculated 
Step 7 antibody evaluation expected reproduction rate of antibody Abj for Prj = 
Prf j+ γ Prcj 
Step 8 Memory update the poor antibody was replaced by M excellent antibodies 
with more affinity in antibody group Ak which can ensure that the average affinity of 
actual antibody in the memory is greater than or equal to the prevenient antibody 
Step 9 termination condition judgment whether it reached iterations antibody af
finity function value whether did not change after a certain number of successive 
iterations all attribute reduction sets of memory were output operation end Other
wise continue to the next step 
Step 10 genetic operation for antibody Ak comprehensive evaluation methods 
based on affinity and concentration were adopted as parent group Bk N antibodies 
were selected by roulette wheel selection method in Ak For Bk the corresponding 
antibodies were selected respectively in accordance with the crossover probability Pc 
and mutation probability Pm Bk+1 was obtained through the crossover and mutation 
operations 
Step 11 corrective operation new antibody in the antibody Bk+1was corrected by 
correction strategy so that each antibody was effective antibody effective antibody 
B’k+1 was obtained 
Step 12 a new antibody group generation a new antibody group Ak+1 included 
antibodies in memory antibody group and offspring antibodies B’k+1 with corrective 
operation then went to Step6 
38 
T Zhou 
 
33 
Rule Extraction Module 
The function of rule extraction module was to extract rules from the data set attribute 
value reduction method based on rough set theory was adopted8 
34 
Rule Evaluation and Interpretation Module 
The module included the prediction accuracy of classification rule confidence and 
support analysis the number of rules and the final interpretation of the rules9 
4 
Application 
Gas emission in underground mines as an example the relationship between gas 
emission and its affecting factors was found for scheduling decision help mine leader 
to timely analyze and guide coal mine production These data were collected from 
July 18 2010 to September 20th some were obtained by measure of underground gas 
detection system and mine coal belt system some were calculated a total of 1560 
records 20 data were randomly selected from records as shown in table 1 S1 S2 S3 
S4 S5 S6 as condition attributes S1 for the burial depth S2 for coal seam thickness S3 
for gas content S4 for daily progress S5 for coal seam pitch S6 for daily output D for 
the gas emission1 for nonemergent 2 for general 3 for emergent10 
Table 1 Data set of gas emission 
 
S1m 
S2 m 
S3 m3t 1 
S4 dt 1 
S5 m 
S6 dt 1 
D 
1 
409 
21 
192 
442 
21 
1824 
1 
2 
421 
19 
214 
413 
18 
1750 
1 
3 
434 
24 
250 
471 
16 
2082 
1 
4 
449 
23 
243 
432 
17 
1995 
1 
5 
528 
26 
280 
325 
10 
1978 
2 
6 
543 
28 
316 
381 
12 
2206 
2 
7 
562 
31 
368 
353 
11 
2409 
2 
8 
589 
60 
421 
285 
19 
3238 
3 
9 
606 
62 
434 
277 
16 
3086 
3 
10 
625 
63 
403 
264 
17 
3353 
3 
11 
639 
64 
467 
275 
15 
3411 
3 
12 
415 
21 
215 
416 
23 
1526 
1 
13 
431 
24 
258 
467 
18 
2077 
1 
14 
457 
23 
240 
451 
21 
2102 
1 
15 
515 
29 
322 
345 
13 
2241 
2 
16 
518 
27 
310 
351 
19 
2089 
2 
17 
530 
30 
335 
368 
12 
2287 
2 
18 
550 
29 
361 
402 
14 
2325 
2 
19 
628 
65 
462 
280 
20 
3455 
3 
20 
633 
66 
480 
292 
16 
3649 
3 
 
Application of Data Mining in Coal Mine Safety Decision System Based on Rough Set 
39 
 
Table 2 Data set of gas emission after discretization 
 
S1 
S2 
S3 
S4 
S5 
S6 
D 
1 
1 
1 
1 
4 
4 
1 
1 
2 
1 
1 
2 
3 
4 
1 
1 
3 
1 
2 
3 
4 
3 
2 
1 
4 
2 
2 
3 
3 
2 
1 
1 
5 
4 
2 
4 
2 
1 
1 
2 
6 
4 
3 
4 
2 
1 
2 
2 
7 
5 
3 
5 
2 
1 
3 
2 
8 
5 
6 
6 
1 
3 
5 
3 
9 
5 
6 
6 
1 
3 
5 
3 
10 
5 
6 
5 
1 
2 
5 
3 
11 
6 
6 
7 
1 
2 
6 
3 
12 
1 
1 
2 
3 
5 
1 
1 
13 
1 
2 
3 
4 
3 
2 
1 
14 
2 
2 
3 
4 
4 
2 
1 
15 
4 
2 
4 
2 
1 
1 
2 
16 
4 
3 
4 
2 
3 
2 
2 
17 
4 
3 
4 
2 
1 
2 
2 
18 
5 
3 
5 
3 
2 
3 
2 
19 
6 
7 
7 
1 
4 
6 
3 
20 
6 
7 
7 
1 
2 
6 
3 
41 
Discretization of Continuous Attributes 
Due to condition attributes as the continuous discretization of continuous firstly was 
carried out before extracting rule 500 records that randomly selected from the gas 
emission data set were analyzed discretization of the condition attributes such as 
burial depth coal seam thickness gas content daily progress coal seam pitch and 
daily output was made by heuristic genetic algorithm to get the minimum discrete 
points 
Burial depth data was discretized into 6 intervals coal seam thickness data was 
discretized into 7 intervals gas content data was discretized into 7 intervals daily 
progress data was discretized into 4 intervals coal seam pitch data was discretized 
into 5 intervals daily output data was discretized into 6intervals data set of instance 
of gas emission after discretization from table 1 as shown in Table 2 
42 
Attribute Reduction 
The discrete data sets with records for attribute reduction by using heuristic immune 
algorithm attribute reduction was burial depth coal seam thickness gas content daily 
progress coal seam pitch and daily output 
43 
Default Rules Mining 
Gas emission data set was analyzed and mined by using model for default rule mining 
based on rough set The classification rules were obtained in condition that the mini
mum reliability was 045 and the minimum support degree was 002 f which five 
rules were interpreted as follows 
40 
T Zhou 
 
Table 3 Test data set of gas emission 
 
S1m 
S2 m 
S3 m3t 1 
S4 d t 1 
S5 m 
S6 d t 1 
D 
1 
419 
20 
152 
432 
22 
1722 
1 
2 
467 
22 
291 
421 
22 
2004 
1 
3 
498 
28 
320 
341 
19 
2083 
1 
4 
545 
28 
331 
422 
15 
2227 
2 
5 
580 
29 
336 
381 
13 
2108 
2 
6 
615 
64 
445 
284 
18 
3259 
3 
7 
638 
64 
442 
270 
21 
3357 
3 
8 
425 
25 
235 
426 
24 
1423 
1 
9 
431 
18 
262 
433 
19 
1653 
1 
10 
444 
23 
270 
431 
17 
2084 
1 
11 
459 
25 
233 
442 
18 
1896 
1 
12 
525 
30 
312 
335 
14 
2144 
1 
13 
540 
33 
365 
378 
13 
2188 
2 
14 
620 
62 
435 
295 
16 
3312 
3 
15 
650 
56 
440 
242 
15 
3546 
3 
 
S1burial depth =1  S2coal seam thickness=1  S3gas content=1  Dgas 
emission =1 nonemergent 073 020 
S1burial depth=4  S2coal seam thickness=3  S3gas content=4  S4daily 
progress=2   S5coal seam pitch=1  S6daily output= 2  Dgas emis
sion=2general 070032  
S1burial depth=4  S2coal seam thickness=2  S3gas content=4  S4daily 
progress=2   S5coal seam pitch=1  S6daily output=1  Dgas emis
sion=2general 082024 
S1burial depth=5S2coal seam thickness=6S3gas content=6S5coal seam 
pitch= 3S6daily output=5  gas emission=3emergent 073002 
S1burial depth=6S2coal seam thickness=7S3gas content=7S5coal seam 
pitch= 2S6daily output=6 Dgas emission=3emergent 0850021 
The gas emission data were tested after obtaining the gas emission classification rules 
15 samples of test data set as shown in Table 3 Firstly the test data set was discre
tized then discretized test data set was predicted for classification through the classi
fication rules prediction accuracy rate reached 98 indicating that it was effective to 
data mining based rough set on for extracting the gas emission classification rules 
Finally the rules were evaluated and interpreted and the rules were stored in know
ledge base for the prediction of the gas emission or auxiliary decision 
5 
Summary 
The rough set theory quantitative analysis of incomplete imprecise and uncertainty 
knowledge has been a research hotspots in recent years In order to solve the problem 
of continuous attribute discretization of decision table a kind of heuristic genetic 
algorithm for continuous attributes discretization was put forward the performance of 
the algorithm was improved A kind of heuristic immune algorithm for attribute  
 
Application of Data Mining in Coal Mine Safety Decision System Based on Rough Set 
41 
 
reduction was presented to conquer the shortage of existing attribute reduction algo
rithm In order to solve the problem of reasoning and decision in incomplete and im
precise information a kind of default rule mining model based on reduction lattice 
was proposed which was used the research method of Topdown and Bottomup in 
reduction lattice alternately to mining default rule By setting rule credible degree and 
rule support degree based on RS the algorithm introduced some research strategies 
which were used to optimize the process of mining default rule and reduce the time 
complexity of the algorithm Finally data mining system based on rough set was de
signed which was applied to data mining analysis of underground gas emission good 
results were achieved 
References 
1 Niu LD Mine Gas Linkage Monitoring Method Based on Data Mining Approach China 
Safety Science Journal 21 62–68 2011 
2 Ni LQ Zhou HT Gao SS A More Effective Data Mining Approach That Adroitly 
Combines Rough Set Theory with Evidence Theory Journal of Northwestern Polytechnic
al University 28 927–931 2010 
3 Zhao ZP Yin ZM Chen JC Mine Hidden Danger Data Digging Model and Applica
tive Digging Algorithm Coal Science and Technology 38 67–69 2010 
4 Zheng HZ Liu Y Zhan DC Default Rules Frame of Nonmonotonous Problems 
Based on Data Mining Computer Science 33 181–182 2006 
5 Wang YY Knowledge Discovery Methods Research Based on Rough Set Theory 
Shanghai Jiao Tong University Shanghai 2006 
6 Xu X Zhai JM Multiscale Genetic Algorithms for Discretization in Rough Set on 
Trees Modern Manufacturing Engineering 10 1–4 2009 
7 Liu Y Li WH Chen YL Research on Intelligent Fault Diagnosis Based on Artificial 
Immune System Computer Measurement  Control 18 2694–2696 2010 
8 Zhao LS Shi JH Real Value Attribute Reduction Method Based on Rough Sets Jour
nal of Inner Mongolia University 41 97–101 2010 
9 Liu B Pan JH Liu PS Rule Evaluation Method and Data Quality Mining System 
Computer Integrated Manufacturing Systems 15 1436–1441 2009 
10 Hou GZ Focus on Gas Safety Management Face Ventilation and Gas Management 
Means Coal Technology 28 199–200 2009 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 42–48 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Study on Web Text Feature Selection Based on Rough Set 
Xianghua Lu and Weijing Wang 
Department of Computer and Information Engineering Luoyang Institute of Science  
and Technology Luoyang PR China 
xhxianghuasohucom 
Abstract This paper uses vector space model as the description of the Web 
text analyses the feature of the Web pages which are written in HTML and 
improves the traditional formula of TFIDF The feature weight is calculated 
according to the term location in the document In addition a text classification 
system based on Vector Space Model is studied In the article feature selection 
and text classification is connected and feature terms are selected depending on 
the term’s importance to classification and then the paper proposes a feature  
selection algorithm based on rough set Experiments show that this method can 
effectively improve the classification accuracy It can not only reduce the di
mension of feature space but also improve the accuracy of classification 
Keywords feature weight feature selection rough set text classification 
1 
Introduction 
With the rapid development of WWW technology Internet has become the largest 
information gathering place As these large heterogeneous Web information re
sources with great potential value people urgently need tools to find the resources and 
knowledge from the Web rapidly and efficiently Web text mining is a kind of method 
and tool to find the implicit knowledge and pattern from a large number of Web doc
uments 1 Web text classification is an important technology in Web text mining 
and it refers to classify each of the Web documents into a predefined category 2 
Through the Web text classification a lot of web pages can be classified automatical
ly to help the users get their interested text information quickly and accurately  
reduce the search space accelerate the retrieval speed and improve the query  
precision etc 
The development of Web text classification is based on traditional text classifica
tion among which some technology are applicable in the Web text classification as 
well However there is a fundamental difference between the two kinds of classifica
tion The former deal with Web texts and the latter do with plain texts Web text is 
mainly described in the form of webpage which contains a lot of marker symbols 
those symbols provide much richer information for Web text classification In addi
tion the Web document also has high dimension features sparse sample and less 
obvious characteristics 3 The classification performance of some frequentlyused 
classifier with superior performance usually dropped sharply in high dimension  
 
Study on Web Text Feature Selection Based on Rough Set 
43 
 
feature spaces while the huge computation cost makes it difficult for various algo
rithms to deal with largescale text collections In view of the features above the cal
culation method of feature weight has been improved and then this paper proposed a 
feature selection method based on attribute reduction of rough set Experiments 
proved that when reduced to the same feature number classification can get higher 
accuracy using training subset extracted by the method 
2 
The Improvement of Weight Calculation Formula 
Vector Space Model VSM is usually used to describe the texts in Web text classifi
cation In VSM each text is described as a feature vector of n dimension  
 
 
 
 


1
 1
 
 
i
i
n
n
V d
t w d
t w d
t w
d
=
Where 
it  stands for feature 
item 
iw  
d is the weight of term 
it  in text d  which is generally defined as the 
function 
of 
the 
frequency 
 
t if d
that 
it
occurs 
in 
text
d
 
name
ly
 
 


i
i
w d
=ϕ tf d
 Weight of feature describes the importance for expressing 
text content so feature selection and weight calculation turns out important particular
ly In VSM TFIDF is usually used to calculate feature weight and the following is a 
commonlyused formula for weight calculation 
  log

001
 
2
2
  log 

001
1
i
i
i
i
i
tf d
N n
w d
n
tf
d
N n
i
∗
+
=
∗
+

=
 
1 
N is the total number of training text n  is the character number 
in  is the document 
frequency of the item it  and 
 
itf d  is the frequency of 
it  occurs in the text d  
 


1
2
1

 
max
ti
i
ti
n
t
freq
f
d
t
t t
t
i
freq
=
∈
 
2 
The corresponding part IDF measures the inverse document frequency 


log

001
i
idf
N n
i =
+
  
3 
When text feature weight calculated the TFIDF function only aims at the informa
tion about the times that the feature item appears in the text and the distribution of 
item in the whole text space It doesn’t consider the information of the feature loca
tion in the text and the uneven distribution In a specific text the description ability of 
feature which appears in different location is different Its contribution to text  
44 
X Lu and W Wang 
 
classification is also obviously different On the other hand in the entire training set 
the different position of feature in each text can also reflect the different performance 
to describe text 4 So the item location should be considered when its weight calcu
lated At present in most of text classifications based on position weight absolute 
weighting method is the common method to calculate the weight of feature according 
to the main structure features In the weight calculation based on position what com
monly used is the absolute value weighted method it is a matter of experience 
Firstly the feature items in different positions are given different weighting coeffi
cient secondly final weight is calculated together with the feature items in the main 
body of the text 5 This method can improve the classification effect but only to a 
certain extent The disadvantage of the method is that weighting coefficient is fixed 
value which has different effect to long text and short text So the influence of struc
ture feature to body text will weaken with the length of body text increase  To solve 
the problem the reference 6 proposed an improved weighting method namely rela
tive weighting method The method considers both the length of HTML document’s 
body and the length of the text which is written in labels Then the method expands 
words of each element in proportion according to the length of the document so as to 
reduce the disadvantages that the effect each element plays on the text is always influ
enced by the document length 
Web document contains a large number of HTML tags Accordingly the term in
cluded in different tags has different ability to describe text’s content or distinguish 
text categories Therefore the traditional TFIDF algorithm is not so effective on the 
Web text So according to HTML structural feature the weight of feature in different 
position should be calculated using different methods 
Based on the above analysis each HTML document will be divided into three sec
tions according to the Web document structure in this paper title anchor text and 
body The same sections are included in a collection respectively so that the entire 
training set can be divided into three “pseudo text set” 7 

iS i = 1 23
where 
iS  is 
made of the text segments corresponding to the training set of the i  section In each 
pseudo text set
iS  formula 4 and 5 are used to calculate the feature frequency 
corresponding to the feature item and the inverted text frequency 
 




1
2
1


 

123
max
i
i
t
j
i j
i
n
t
j
freq
f
d
t
t t
t
j
freq
=
∈
=
   
4 




log

001
i j
i j
idf
N n
=
+
 
5 
Where 
n i j
is the number of texts containing the feature 
it in the text set 

j j = 1 23
 
In the entire training set the weight function of the feature it  can be improved as 
 
Study on Web Text Feature Selection Based on Rough Set 
45 
 




3
3
 
log


001
1
1
 
2
3
3
 
log


001
1
1
1
j
i j
j
i j
i
j
i j
j
i j
tf
d
N n
j
j
w d
n
tf
d
N n
i
j
j
λ
λ
λ
λ
∗
∗
+


=
=
=




∗
∗
+





=
=
=


  
6 
where
3

1
1
2
3
1
i
j
λ
λ
λ
 λ


=
=
 it’s determined by the experiment as the follow
ing
1
2
3
046
036
018
λ
λ
λ
=
=
=
 Then make use of formula 6 to weight each feature 
item 
3 
Feature Selection Based on Rough Set 
A part of features all of which have larger weight than those unselected is selected to 
construct the feature vector Where the number of the selected features is far litter 
than the total of all items in text And then texts are classified using KNN automati
cally However the experiment shows that the classification efficiency is not very 
ideal because it is not easy to determine an appropriate value of m its would affect 
the classification accuracy whether m is too big or too small  To solve the problem 
this paper put forward a method of feature selection based on rough set which is a 
math tool to deal with incertitude and uncertain data 8 The main idea of rough set 
theory is to extract decision rules by attribute reduction and value reduction in the 
premises of keeping the ability of classification  Attribute reduction based on 
attribute significance can remove those feature items which have little effect on the 
classification or even no effect and reduce the dimension of feature vector However 
it exists obvious shortcomings when directly using the attribute reduction of rough 
sets for feature selection because that feature dimension of text is too large and  the 
mathematics amount of attribute reduction is too high 9 Extremely uneven attribute 
value may cause null able core of some attributes This paper proposes a method of 
feature selection based on rough set Firstly making use of the previouslymentioned 
TFID algorithm a certain number of features are selected to constitute primitive 
feature sets secondly beneficial features to classification are selected from the primi
tive feature sets by attribute reduction of rough sets in the premise of keeping the 
accuracy of classification Further feature selection can reduce the feature dimension 
to a few hundred dimensions  
Web text feature selection based on rough set theory is founded on the vector space 
model VSM obtained in the previous stage Firstly the weighted features are discre
dited and a decision table is established  the entire training sets are  regarded as U 
feature subset as conditional  attribute and text categories set as decision attribute D 
Secondly conditional  attributes are filtered properly depending on its significance 
which  is evaluated through dependence of conditional attribute So the feature subset 
46 
X Lu and W Wang 
 
including minimum features is obtained and the vector space model is reconstructed 
The specific steps are as follows 10 
• Calculate the positive region
Posc  
D that decision attributes D is relative to con
dition attribute C in the twodimensional decision table 
C
X U D
Pos
CX
∈
= 
 
7 
• According to the dependence function of rough set 






 
card Pos
c D
K
c D
card U
= γ
=
 
8 
K reflects the degree that decision attributes D depend on attribute C the larger the 
value of K the higher the dependence degree 
• To each attribute it  in the twodimensional decision table calculate its importance 


IM
ti D
 to decision attribute D 




  

ti
C
ti
IM
D
D
D
γc
γ
−
=
−
 
9 
The larger the value of 
IM it  
D  is the more important the attribute 
it is to classi
fication If the value of 
IM it  
D is zero attribute 
it  has no effect on classifica
tion And it should be removed 
• Set a threshold value δ and remove all attributes that meet the condition 
IMti
 δ
This method won’t result in a loss of information and can delete vast re
dundant attributes so as to reduce the original dimension to a certain degree 
4 
Experiments and Analysis of Eight Categories 
This paper collects 800 webpage documents from Sohu website as dataset those doc
uments are divided into eight categories cars finance and economics health travel 
entertainment sports real estate and military and each category contain 100 webpage 
documents 600 of them as training set 200 as the test set Those documents are clas
sified by the most commonlyused classification kNN In the experiment the classi
fication results are evaluated by a general comprehensive classification rate namely 
F1 which comprehensively considers the precision and recall of text classification Its 
specific calculation formula is as follows 
 
Study on Web Text Feature Selection Based on Rough Set 
47 
 
2
F1
precision recalll
precision
recall
∗
∗
=
+
 
10 
Web pages are cleaned to extract the title anchor text and body those constitute 
plain texts And then the Chinese plain texts are segmented into words with 
ICTCLAS which is a high precision Chinese segmentation tool from CAS The high 
frequency words lowfrequency words and stop words are eliminated Weight of 
every term is calculated respectively using the improved weighting method and the 
traditional TFIDF method 
The experiments are divided into two levels In order to verify the effect of the me
thod based on position weighting the experiments in the first level compared the im
proved TFIDF weighting algorithm and the traditional Seven hundred and eighty terms 
are selected to construct the feature vector respectively all of those selected terms have 
larger weight than others And then they are classified using KNN classifier 
The experiments in the second level are based on the improved weighting method 
before and after using rough set theory and then by the experiment comparison vali
date the effect of method based on rough set theory for further feature selection Fea
tures are selected preliminary from the terms whose weight is calculated using the 
improved weighting method there seven hundred and eighty terms are selected as 
before Feature dimensions are reduced by mean of the attribute reduction of rough 
sets finally one hundred and eighty dimensions is determinedF1 test value of the 
classification results is shown in the following table 
Table 1 F1 test value 
Category 
weight formula 
TFIDF 
Improved TFIDF 
Direct 
reduction of 
rough  sets 
Cars 
0781 
0829 
0865 
Military 
0759 
0796 
0839 
Finance and Economics 
0672 
0768 
0812 
Health 
0703 
0759 
0821 
Travel 
0802 
0856 
0866 
Entertainment 
0798 
0812 
0854 
Sports 
0751 
0823 
0846 
Real Estate 
0697 
0737 
0805 
The experimental results prove that this improved algorithm of term weighting 
based on position in kNN classifier can improve the precision of text classification 
The method of feature selection based on rough set can not only achieve the purpose 
of dimension reduction but also increase the final text classification accuracy 
48 
X Lu and W Wang 
 
5 
Conclusion 
With the extensive application of Web text classification technology in search engine 
digital library information filtering information retrieval Internet information monitor
ing and other fields the research of Web text classification has been an advanced issue in 
the field of information processing Searching for effective text feature reduction method 
will be the key technology to improve the efficiency of automatic text classification 
Feature selection method based on evaluation function can reduce feature to over a thou
sand dimensions The quantity of the dimensions is still very large to machine learning 
further reducing the dimensions will affect classification accuracy 
The feature selection method based on rough was proposed in this paper Firstly 
the improved weight algorithm based on location to select a certain number of fea
tures is used and then the attribute reduction algorithm of rough set theory to further 
reduce the feature dimensions was used The experiments showed the method is effec
tive It could get higher classification accuracy when the same number of features is 
reduced and the method to extract the feature subset training is used 
Acknowledgement The project was supported by Key Project on Science and Tech
nology of the Education Department of Henan Province NO12A520031 Youth 
Foundation of Luoyang Institute of Science and Technology NO2011QZ15 
References 
1 Wang JC Pan JG Zhang FY Research on Web Text Mining Journal of Computer 
Research and Development 37 513–520 2007 in Chinese 
2 Liu H Research on Some Problems in Text Classification Jilin University Jilin 2009 
3 Liu L The Research and Implementation of Automatic Classification for Chinese Web 
Text University of Changchun for Science and Technology Changchun 2007 
4 Chu JC Liu PY Wang WL Improvement Approach to Weighting Terms in Web 
Text Computer Engineering and Applications 43 192–194 2007 in Chinese 
5 Tai DY Xie F Hu XG Text Categorization Based on Position Weight of Feature 
Term Journal of Anhui Technical College of Water Resources and Hydroelectric 3 64–
66 2008 in Chinese 
6 Tan JB Yang XJ Li Y An Improved Approach to Term Weighting in Automatic 
Web Page Classification Journal of the China Society for Scientific and Technical Infor
mation 27 56–61 2008 in Chinese 
7 Liu HF Zhao H Liu SS An Improved Method of Chinese Text Feature Selection 
Based on Position Library and Information Service 53 102–105 2009 in Chinese 
8 Wang GY Rough Sets Theory and Knowledge Acquisition Xi’an JiaoTong University 
Press Xi’an 2001 in Chinese 
9 Zhang BF Shi HJ Improved Algorithm of Automatic Classification Based on Rough 
Sets Computer Engineering and Applications 47 129–131 2011 in Chinese 
10 Chen SR Zhang Y Yang ZY The Research of the Feature Selection Method Based 
on Rough Set Computer Engineering and Applications 42 159–161 2006 in Chinese 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 49–56 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A Generalized Approach for Determining Fuzzy 
Temporal Relations 
Luyi Bai and Zongmin Ma 
College of Information Science  Engineering 
Northeastern University 
Shenyang China 
mazongminiseneueducn 
Abstract Fuzzy temporal relations have been defined to support temporal 
knowledge representation and reasoning in the presence of fuzziness which are 
still open issues In this paper we propose a generalized approach for determin
ing fuzzy temporal relations assuming that fuzzy temporal intervals are all 
fuzzy We firstly present the basics of representation of fuzzy temporal relations 
from two aspects fuzzy time point and fuzzy time interval and then give defi
nitions of their fuzzy relations On this basis correspondences between fuzzy 
and crisp temporal relations are investigated Finally a general formalized algo
rithm for determining fuzzy temporal relations is proposed 
Keywords Fuzzy time point Fuzzy time interval Fuzzy temporal relations 
1 
Introduction 
Temporal relations is a topic of great importance in various application domains such 
as scheduling and planning 78 natural language understanding 17 question 
answering 10 etc Starting from Allen’s seminal work on temporal interval relations 
1 increasingly more expressive efforts have been proposed both in qualitative and 
quantitative aspects of reasoning about temporal data combining qualitative and me
tric temporal data 1214 specifying metric constraints on points and durations 15 
and studying disjunctions of temporal constraints 1113 etc 
However the fact that notion of time is usually fuzzy in the real world has led to 
emergence of various kinds of representation and reasoning approaches for dealing 
with fuzzy temporal information The fuzzy set theory 20 and the possibility theory 
21 seem to provide a efficient framework for dealing with fuzzy temporal informa
tion and are applied in 4 and 6 The two theories allow relating the qualitative 
linguistic terms to a quantitative interpretation and providing a sort of interface be
tween the qualitative and quantitative levels of descriptions 9 Moreover it has been 
proven 5 the fuzzy methodologies can account for both preference and fuzziness 
Since fuzzy temporal relations play a fundamental role in the real world applica
tions the problems that emerge are how fuzzy temporal relations should be forma
lized to determine their fuzzy relations To fill this gap a lot of efforts 
50 
L Bai and Z Ma 
 
23161819 have been done in developing fuzzy temporal relations in the re
cent years In 3 Badaloni and Giacomin integrate the ideas of flexibility and fuzzi
ness into Allen’s intervalbased temporal framework defining a new formalism called 
IAfuz which extends classical Interval Algebra IA Furthermore Badaloni and Falda 
2 study the problem of representing different forms of fuzzy temporal relations in 
order to develop a more general way to represent it The work of Ribarić et al 16 
introduce a geometrical approach to validation of the possibility and necessity meas
ures for temporal relations between two fuzzy time points and a fuzzy time point and 
a fuzzy time interval Unfortunately one of the most import limitations of their efforts 
for practical applications is that the fuzzy temporal relations are computationally ra
ther expensive to evaluate In order to solve the problem Schockaert et al 19 inves
tigate how the evaluation of the fuzzy temporal interval relations for piecewise linear 
fuzzy intervals boils down to the evaluation for linear fuzzy intervals and provide a 
characterization for linear fuzzy intervals that is both efficient to evaluate and easy to 
implement What’s more Schockaert and De Cock 18 present a framework for rea
soning about qualitative and metric temporal relations between fuzzy time intervals 
which can be drawn upon wellestablished results for solving disjunctive temporal 
reasoning problems 
Following the previous ideas of building a model capable to effectively reason 
about fuzzy temporal relations we propose a generalized approach for determining 
fuzzy temporal relations in this paper assuming that fuzzy temporal intervals are all 
fuzzy We firstly present the basics of representation of fuzzy temporal relations from 
two aspects fuzzy time point and fuzzy time interval and then give definitions of 
their fuzzy relations On this basis correspondences between fuzzy and crisp temporal 
relations are investigated which has been received little attention although fuzzy 
temporal relations have been extensively studied Finally a general formalized algo
rithm for determining fuzzy temporal relations is proposed 
The rest of the paper is organized as follows After presenting basics of representa
tion of fuzzy temporal relations and giving definitions of them in Section 2 Section 3 
investigates correspondences between fuzzy and crisp fuzzy temporal relations Sec
tion 4 proposes a general formalized algorithm for determining fuzzy temporal rela
tions and Section 5 concludes the paper 
2 
Representation of Fuzzy Temporal Relations 
In this section we present the basics of representation of fuzzy temporal relations 
from two aspects which are fuzzy time point and fuzzy time interval Among each 
aspect fuzzy temporal data and their relations are defined 
21 
Fuzzy Time Point 
The time of occurrence of an instantaneous event may be vaguely known and called a 
fuzzy time point FTP A fuzzy time point is interpreted as a representation of a set 
of possible time points along with the degrees of possibility that an event occurs 
 
A Generalized Approach for Determining Fuzzy Temporal Relations 
51 
 
Definition 1 Fuzzy time point For a fuzzy time point FTP = T δ we have 
 T is a fuzzy time point 
 δ is the membership degree of the fuzzy time point where 0 ≤ δ ≤ 1 
The fuzzy relations of two fuzzy time points contain three cases which are fuzzy 
equal fuzzy disjoint and fuzzy overlap denoted as FTPequal FTPdisjoint FTPover
lap respectively Here we introduce a mathematic symbol supp where supp A = u | 
u ∈ U A u  0 
Definition 2 Fuzzy relations of fuzzy time points For two fuzzy time points FTP1 = 
T1 δ1 and FTP2 = T2 δ2 t1i δ1i ∈ FTP1 and t2i δ2i ∈ FTP2 we have 
 FTPequal FTP1 FTP2 min supp t1i δ1i = min supp t2i δ2i ∨ max supp 
t2i δ2i = max supp t1i δ1i 
 FTPdisjoint FTP1 FTP2 max supp t1i δ1i  min supp t2i δ2i ∨ max 
supp t2i δ2i  min supp t1i δ1i 
 FTPoverlap FTP1 FTP2 min supp t1i δ1i  min supp t2i δ2i ∧ min 
supp t2i δ2i  max supp t1i δ1i  max supp t2i δ2i ∨ min supp t2i δ2i  
min supp t1i δ1i ∧ min supp t1i δ1i  max supp t2i δ2i  max supp t1i 
δ1i ∨ max supp t1i δ1i = min supp t2i δ2i ∨ min supp t1i δ1i = max supp 
t2i δ2i ∨ min supp t1i δ1i  min supp t2i δ2i  max supp t2i δ2i  min supp 
t1i δ1i ∨ min supp t2i δ2i  min supp t1i δ1i  max supp t1i δ1i  min supp 
t2i δ2i 
Theorem 1 For two fuzzy time points FTP1 = T1 δ1 and FTP2 = T2 δ2 ∝ denotes 
precedence we have 
i FTP1 = FTP2 ⇔ T1 = T2 ∧ δ1 = δ2 
ii FTP1 ∝ FTP2 ⇔ supp T1 δ1  supp T2 δ2 
Proof Because fuzzy time point represent all the possible time point so that it is 
straightforward that two fuzzy time points FTP1 = FTP2 if and only if each ti ∈ T1 and 
tj ∈ T2 that ti = tj and δi = δj In the case of precedence similarly if and only if each ti 
∈ T1 and tj ∈ T2 with positive possibility that ti  tj in other words there is no ti ∈ T1 
and tj ∈ T2 that tj ∝ ti FTP1 ∝ FTP2                                                 
Theorem 2 Let FTP1 = T1 δ1 and FTP2 = T2 δ2 be two fuzzy time points and poss 
E denotes the possibility distribution of event E is true 
i max poss FTP1 = FTP2 poss FTP1 ∝ FTP2 poss FTP2 ∝ FTP1 = 1 
ii min poss FTP1 = FTP2 poss FTP1 ∝ FTP2 poss FTP2 ∝ FTP1 = 0 
Proof It is completely possible that either FTP1 = FTP2 or FTP1 ∝ FTP2 or FTP2 
∝ FTP1 and impossible that FTP1 = FTP2 and FTP1 ∝ FTP2 and FTP2 ∝ FTP1 
This interpretation is intuitive and understandable                                    
22 
Fuzzy Time Interval 
A fuzzy time interval FTI is a time interval with fuzzy bounds which is interpreted 
as representation of a set of possible time intervals along with degrees of possibility 
52 
L Bai and Z Ma 
 
Definition 3 Fuzzy time interval For a fuzzy time interval we have FTI = Ts δ Te 
δ’ including 
 Ts and Te are fuzzy starting and ending time point respectively where Ts ∝ Te 
 δ and δ’ are the membership degrees of the fuzzy starting and ending time points 
being the time Ts and Te respectively 
Theorem 3 Let FTI = Ts δ Te δ’ be a fuzzy time interval the membership degree 
of the fuzzy time interval δFTI is 
i if δ = 1 δFTI = δ’ 
ii if δ’ = 1 δFTI = δ 
iii 
if 0  δ  1 and 0  δ’  1 δFTI = δ × δ’ 
Proof The membership degree of the fuzzy time interval computation can be divided 
into three cases  
 Case 1 If the starting time point of the fuzzy time interval is crisp we have δFTI 
= 1 × δ’ = δ’ 
 Case 2 If the ending time point of the fuzzy time interval is crisp we have δFTI = 
δ × 1 = δ 
 Case 3 If the two ending time points of the fuzzy time interval are all fuzzy we 
have δFTI = δ × δ’ because the two membership degrees of the two ending time points 
are assumed to be independent                                                       
The fuzzy relations of two fuzzy time intervals contain seven cases which are fuzzy 
before fuzzy equal fuzzy meet fuzzy overlap fuzzy during fuzzy start and fuzzy 
finish denoted as FTIbefore FTIequal FTImeet FTIoverlap FTIduring FTIstart 
FTIfinish respectively 
Definition 4 Fuzzy relations of fuzzy time intervals For two fuzzy time intervals 
FTI1 = T1s δ1 T1e δ1’ and FTI2 = T2s δ2 T2e δ2’ we have 
 FTIbefore FTI1 FTI2 supp T1e δ1’  supp T2s δ2 
 FTIequal FTI1 FTI2 supp T1s δ1 = supp T2s δ2 ∧ supp T1e δ1’ = supp 
T2e δ2’ 
 FTImeet FTI1 FTI2 supp T1e δ1’ = supp T2e δ2’ 
 FTIoverlap FTI1 FTI2 supp T2s δ2  supp T1e δ1’  supp T2e δ2’ ∧ supp 
T1s δ1  supp T2s δ2  supp T1e δ1’ 
 FTIduring FTI1 FTI2 supp T2s δ2  supp T1s δ1  supp T2e δ2’ ∧ supp 
T2s δ2  supp T1e δ1’  supp T2e δ2’ 
 FTIstart FTI1 FTI2 supp T1s δ1 = supp T2s δ2 
 FTIfinish FTI1 FTI2 supp T1e δ1’ = supp T2e δ2’ 
3 
Correspondences between Fuzzy Temporal Relations and 
Temporal Relations 
In this section we present correspondences between fuzzy and crisp temporal rela
tions on the basis of the studies in the above section The correspondences come from 
two cases between fuzzy time points and between fuzzy time intervals 
 
A Generalized Approach for Determining Fuzzy Temporal Relations 
53 
 
Fig 1 shows the three fuzzy relations of two fuzzy time points and their corres
ponding crisp relations Take FTPequal for example the possible fuzzy time intervals 
of FTP1 and FTP2 are the same from t1 to t2 However the corresponding crisp rela
tions can be either disjoint or equal because the time point is fuzzy and may be exist 
in any time point between the possible fuzzy time interval from t1 to t2 Then we 
consider FTPdisjoint that the possible fuzzy time intervals of FTP1 and FTP2 are dis
joint The corresponding crisp relation must be disjoint because the possible fuzzy 
time intervals of FTP1 and FTP2 are disjoint FTP1 = t1 t2 ∩ FTP2 = t3 t4 = 
∅ Finally we define the relation of fuzzy time points is FTPoverlap if there is 
common area in the possible time area The corresponding crisp relations of FTPover
lap can also be either disjoint or equal as FTPequal Note that FTPoverlap is differ
ent from FTPequal although the corresponding crisp relations are either disjoint or 
equal It can be considered from two points of view one is similarity as FTPdisjoint if 
t ∈ t1 t2 ⊂ FTP1 ∧ t’ ∈ t3 t4 ⊂ FTP2 and the other is similarity as FTPequal if t ∈ 
t2 t3 ⊂ FTP1 = t’ ∈ t2 t3 ⊂ FTP2 It is noted that it is only illustrated in one of the 
cases that T1 and T2 are overlap visually Other cases like meet and contain can be 
extended in a similarly way 
Fig 2 shows fuzzy relations of fuzzy time intervals and their corresponding crisp 
relations The dashed area represents fuzzy time area of each fuzzy time interval 
which can be any subinterval of the fuzzy area FTIbefore is the fuzzy relation if any 
possible time interval of one fuzzy time interval stays before the other fuzzy time 
interval FTIequal is the fuzzy relation if the minimum and the maximum possible  
 
 
Fig 1 Fuzzy relations of fuzzy time points 
 
Fig 2 Fuzzy relations of fuzzy time intervals 
54 
L Bai and Z Ma 
 
 
Fig 3 Correspondences between fuzzy and crisp temporal relations 
Time intervals of the two fuzzy time intervals are equal FTImeet is the fuzzy rela
tion if the minimum possible time point of one fuzzy time interval equals to the max
imum possible time point of the other fuzzy time interval FTIoverlap is the fuzzy 
relation if the minimum possible time point of one fuzzy time interval stays between 
another possible time interval of the fuzzy time interval and the maximum possible 
time point of the other fuzzy time interval stays between this possible time interval of 
the fuzzy time interval FTIduring is the fuzzy relation if any possible time intervals 
of one fuzzy time interval are contained by any possible time intervals of the other 
fuzzy time interval FTIstart is the fuzzy relation if the minimum possible time points 
of the two fuzzy time intervals are equal FTIfinish is the fuzzy relation if the maxi
mum possible time points of the two fuzzy time intervals are equal 
The corresponding crisp temporal relations of fuzzy relations between fuzzy time 
points and between fuzzy time intervals are shown in parentheses in Fig 1 and Fig 2  
In order to present the correspondences between fuzzy and crisp temporal relations 
more intuitively Fig 3 shows their correspondences In Fig 3 it is denoted as cross if 
there are no correspondences it is denoted as tick if there are correspondences and the 
fuzzy relation has the corresponding crisp relation it is blank if there are correspon
dences but the fuzzy relation has no corresponding crisp relation 
4 
Determination of Fuzzy Temporal Relations 
In this section we present how to determine fuzzy temporal relations A general for
malized algorithm for determining fuzzy temporal relations is firstly proposed and 
then an example is given to explain it 
 
Algorithm Frelation Y Z 
01 
for k = 1 k = X k++ 
02 
let δk = 0 
03 
end for 
04 
for m = 1 m = i i++ 
05 
  for n = 1 n = m n++ 
06 
    for r = 1 r = X r++ 
 
07 
    if Frrelation Y Z 
08 
    δr = δr + δX 
09 
    end for 
10 
  end for 
11 
end for 
12 
if true FTrelation 
13 
    return δT  
=
X
i
i
1
δ  
 
A Generalized Approach for Determining Fuzzy Temporal Relations 
55 
 
The algorithm Frelation is a general algorithm for determining fuzzy temporal re
lations It contains two loops in order to compare all possible fuzzy temporal rela
tions The membership degrees of the fuzzy temporal relation employ cumulative way 
to compute The finally returned value should be transformed into relative one since it 
is obtained by two membership degrees which composed of two fuzzy time points  
For the algorithm Y and Z indicate two fuzzy temporal data it can be further ex
plained by their representing time points X indicates number of fuzzy relations be
tween Y and Z Frrelation Y Z compares fuzzy relations X times according to the 
definition 2 or 4 T indicates the number satisfying fuzzy relation ranging from 1 to X 
and the returned value is divided by all the possible membership degrees 
Consider FTI1 = 2 06 5 07 3 04 5 07 3 04 6 03 and FTI2 = 3 
08 4 06 4 02 5 02 4 02 6 02 Because there are two fuzzy time inter
vals Y and Z are FTI1 and FTI2 and their representing time points are T1si δ1i T1ei 
δ’1i and T2sj δ2j T2ej δ’2j There are seven fuzzy relations between two fuzzy time 
intervals so that X equals to 7 According to the definition 4 we can get the fuzzy 
relation of FTI1 and FTI2 is FTIfinish Then we get the during finish overlap start 
relation pair Accordingly we compute the membership degrees of each relation δ5 = 
01392 δ7 = 00328 δ4 = 0028 δ6 = 0192 Finally the returned value is divided by 
all the possible membership degrees the possibility that the relation is during approx
imately amounts to 0355 is finish approximately amounts to 0084 is overlap ap
proximately amounts to 0071 and is start approximately amounts to 0490 
5 
Conclusion 
In the scope of this paper we propose a general characterization of representing and 
determining fuzzy temporal relations Definitions of fuzzy temporal data and their 
fuzzy relations are defined from mathematical point of view assuming that fuzzy tem
poral intervals are all fuzzy On this basis correspondences between fuzzy and crisp 
fuzzy temporal relations are investigated Finally a general formalized algorithm for 
determining fuzzy temporal relations is proposed Future work is extending fuzzy 
temporal relations to fuzzy spatiotemporal relations 
Acknowledgments This work is supported by the National Natural Science Founda
tion of China 60873010 61073139 and the Fundamental Research Funds for the 
Central Universities N090504005 and in part by the Program for New Century 
Excellent Talents in University NCET050288 
References 
1 Allen JF Maintaining Knowledge about Temporal Intervals Communications of 
ACM 2611 832–843 1983 
2 Badaloni S Falda M Classical and Fuzzy Neighborhood Relations of the Temporal Qu
alitative Algebra In Proceedings of the 16th International Symposium on Temporal Re
presentation and Reasoning pp 147–154 2009 
56 
L Bai and Z Ma 
 
3 Badaloni S Giacomin M The Algebra IAfuz A Framework for Qualitative Fuzzy Tem
poral Reasoning Artificial Intelligence 17010 872–908 2006 
4 Barro S Marin R Mira J Paton AR Model and Language for the Fuzzy Representa
tion and Handling of Time Fuzzy Sets and System 612 153–175 1994 
5 Dubois D Fargier H Prade H Possibility Theory in Constraint Satisfaction Problems 
Handling Priority Preference and Uncertainty Applied Intelligence 64 287–309 1996 
6 Dubois D Prade H Processing Fuzzy Temporal Knowledge IEEE Transaction on Sys
tem Man and Cybernetics 194 729–744 1989 
7 ElKholy A Richards B Temporal and Resource Reasoning in Planning the parcPLAN 
Approach In Proceedings of the 12th European Conference on Artificial Intelligence 
ECAI 1996 pp 614–618 1996 
8 Fox M Long D PDDL21 An Extension to PDDL for Expressing Temporal Planning 
Domains Journal of Artificial Intelligence Research 20 61–124 2003 
9 Freksa C Spatial and Temporal Structures in Cognitive Processes In Freksa C Jant
zen M Valk R eds Foundations of Computer Science LNCS vol 1337 pp 379–387 
Springer Heidelberg 1997 
10 Harabagiu S Bejan C Question Answering Based on Temporal Inference In AAAI 
2005 Workshop on Inference for Textual Question Answering Pittsburgh PA 2005 
11 Jonsson P Bäckström C A Unifying Approach to Temporal Constraint Reasoning Ar
tificial Intelligence 1021 143–155 1998 
12 Kautz H Ladkin P Integrating Metric and Qualitative Temporal Reasoning In The 9th 
National Conference on Artificial Intelligence pp 241–246 1991 
13 Koubarakis M Tractable Disjunctions of Linear Constraints Basic Results and Applica
tions to Temporal Reasoning Theoretical Computer Science 2661 311–339 2001 
14 Meiri I Combining Qualitative and Quantitative Constraints in Temporal Reasoning Ar
tificial Intelligence 871 343–385 1996 
15 Navarrete I Sattar A Wetprasit R Marin R On PointDuration Networks for Tem
poral Reasoning Artificial Intelligence 14012 39–70 2002 
16 Ribarić S Bašić BD Maleš L An Approach to Validation of Fuzzy Qualitative Tem
poral Relations In Proceedings of the 24th International Conference on Information 
Technology Interfaces pp 223–228 2002 
17 Sanampudi SK Kumari GV Temporal Reasoning in Natural Language Processing A 
Survey International Journal of Computer Applications 14 53–57 2010 
18 Schockaert S De Cock M Temporal Reasoning about Fuzzy Intervals Artificial Intelli
gence 17289 1158–1193 2008 
19 Schockaert S De Cock M Kerre EE An Efficient Characterization of Fuzzy Tempor
al Interval Relations In Proceedings of the IEEE International Conference on Fuzzy Sys
tems pp 1894–1901 2006 
20 Zadeh LA Fuzzy Sets Information and Control 84 338–353 1965 
21 Zadeh LA Fuzzy Sets as a Basis for Theory of Possibility Fuzzy Sets and Systems 11 
3–28 1978 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 57–64 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Applications on Information Flow and Biomedical 
Treatment of FDES Based on Fuzzy Sequential Machines 
Theory 
Hongyan Xing13 and Daowen Qiu12 
1 Department of Computer Science Sun Yatsen University Guangzhou 510006 PR China 
issqdwmailsysueducn 
2 SQIGInstituto de Telecomunicações Departamento de Matemática Instituto Superior 
Técnico TULisbon Av Rovisco Pais 1049001 Lisbon Portugal 
3 Faculty of Applied Mathematics Guangdong University of Technology Guangzhou 510090 
PR China 
Abstract In order to more effectively cope with the real world problems of 
vagueness impreciseness and subjectivity fuzzy discrete event systems 
FDES were proposed and developed in recent ten years In this paper we 
study the applications of FDES to information flow inference and then to 
biomedical control treatment planning and decision making based on fuzzy 
sequential machines FSM theory Through modeling security system and 
biomedical decision problem with FDES as an FSM we extend propositions 
and  procedures to decide the equivalent states display the ideas for checking 
the observation label based on eventstate approach to decide whether hidden 
massage flow exists 
Keywords fuzzy sequential machines fuzzy discrete event systems 
information flow supervisory control 
1 
Introduction 
Discrete event systems DES 1 are dynamical systems whose evolution in time is 
governed by the abrupt occurrence of physical events at possibly irregular time 
intervals In most of engineering applications the states of a DES are crisp However 
this is not the case in many other applications such as biomedical systems and 
economic systems in which vagueness impreciseness and subjectivity are typical 
features Notably Lin and Ying 2 initiated significantly the study of fuzzy discrete 
event systems FDES by combining fuzzy set theory with classical DES Then Qiu 
3 established a supervisory control theory of FDES and designed a testalgorithm 
for checking the existence of supervisors As was known the main task of supervisory 
control is to find a supervisor that restricts the plant behavior modeled by a machine 
in order to comply with a specification behavior In classical DES under certain 
condition we can check within finite number of steps whether the controllability 
condition holds by utilizing the finiteness of states in crisp finite automata but the 
infiniteness of fuzzy states in fuzzy DESs modeled by maxproduct automata gives 
58 
H Xing and D Qiu 
 
rise to considerable complexity for formulating a uniform fashion to check the fuzzy 
controllability conditions However when FDESs are modeled by maxmin automata 
the author in 3 derived a uniform criterion to check this condition  
With a desire to provide further applications of FSM theory in FDES in this paper 
we first reformulate DES using state vectors and event transition matrices and then 
extend to fuzzy vectors and matrices by allowing their elements to take values in 
01 We deal with information flow analysis which is closely related to 
confidentiality  an important security requirement in many secure systems 
As is known in practice information safety directly decides the safety of the 
whole systems in other words secrecy is an oftenappearing problem in finding and 
preventing the spread of secret or other important messages It also plays an important 
role in describing the exact synthesis in communication networks Theoretically 
information flow analysis of the security systems is a fundamental way to label and 
analyze the hidden information flow and its skill and relative study have attracted 
attention for long times see 4 and its references For example Goguren and 
Mesegurer  proposed the first theoretical model in 1984 to describe the system 
confidentiality and security 5 called as noninterferencemodel This model 
considered the condition of the conservation of the safety as forbidding high security 
part from executing influence to low security part In 6 Zakintihinos put forward a 
general theory and gave out a unified form on the speciality of information flow 
However we still cannot completely make sure that confidential information never 
leaks out because the mechanisms cannot prevent hidden information flow and 
confidential information can stealthily leak out In real systems many safety 
strategies are described as statetransition rules and also they are formally described 
as a relative model of statetransition rules easily 78 This process puts the system 
as a model firstly taking the real safe system as a discrete event system Motivated by 
this procedure we can formalize the safe strategy as a fuzzy sequential machine 
FSM a statebased transition system driven by events in analyzing hidden 
information flow As it is well known sequential machines are simple but important 
models of computation such as in software engineering and lexical analysis 9 and 
so do their fuzzycounterparts which have still more practical importance in 
description of natural languages  
The rest of the paper is organized as follows Section II recalls some required 
notations properties and methods deciding the equivalence between states and FSM 
Then in Section III we describe the idea to analyze security systems modeled as FSM 
In this model security policies in actual secure systems are described a state and 
eventbased approach is displayed to decide whether hidden information flow exists 
and a computation procedure to decide the equivalence is also proposed in this 
Section Afterwards we discuss an example in Section IV to illustrate the applications 
of the derived results and methods in fuzzy supervisory control of FDES Finally 
concluding remarks and further considerations are made in Section V 
2 
Preliminaries 
This section serves as recalling some preliminaries concerning fuzzy sequential 
machines FSM In what follows N denotes the set of natural numbers X  the 
 
Applications on Information Flow and Biomedical Treatment of FDES 
59 
 
cardinality of set X    an alphabet and 

  the free monoid generated from   with 
the operation of concatenation empty string ε  is identified with the identity of 

  
FX 
 or X  

X
10
 and 
PX 
 are used to represent the classes of all 
10
valued 
fuzzy subsets on X  and classical subsets of X  respectively 
Definition 1 A quadruple 

δ 


M = Q 0 1
 is called a fuzzy sequential machine 
simply FSM where 
1
0

Q  
 are finite sets and

10

1
0
→
× × ×
Q
δ Q
 a fuzzy 
subset of 
Q
Q
×  × ×
1
0
 is the transition function of FSM M  
As usual 
01
 stand for input and output alphabets respectively and 

qn
q q
Q


2
1
=
 a set of internal states We can regard 


10



∈
=
ir
jk
k
r
i
j
a
δ q u v q
 
as the grade of the proposition that  FSM M  enters state 
kq  and produces output 
rv  given that the present state is 
jq  and input is 
iu  
We call finite sequences of elements of 

0 1 
 input output as tapes or strings 
The collection of all input output tapes will be denoted by 

 
1

0 
 The length of 
tape θ
 is denoted by 
θ
 so it is clear that 
ε = 0
 Moreover 





γ 
θ
γ
θ γ θ
=
∈Σ
∈Σ
=
 ×




1

0

1
0
 and 



0 1
θ γ ∈  ×
 Furthermore we 
assume all machines have the same input event set 
0
  output event set 
1
  and the 
inputoutput pair 
θγ 
 satisfying 



0 1
θ γ ∈  ×
 
Definition 2 Let 

qn
q q
Q


2
1
=
 be a set 
Q = n
 A vector set pertaining to Q  
denoted by VQ  is defined as 
 





21  
10 


2
1
n
i
a
a
a a
q
VQ
i
n T
=
∈
=
=
 
ie column matrix 


an T
a a
q


2
1
=
 corresponds to Q  with which elements 
ia  
stands for the membership degree that the current state is 
n
qi i
21 
 =
 
In an FSM 

δ 


M = Q 0 1
 δ  can be defined by 
 
ir 
jk
i
j
a
q u
=

δ
 ie 
 



1
0
Q
V
Q
Σ ×
×Σ →
δ
 
and corresponding to δ  matrix 

ir 
jk
u v
a
M
i r
=
 also written as 

M ui vr 
 
characterizes the work of M  Note that the transition function δ  can be extended to 

δ  from 


Q
Q
×
×  ×

1
0
 into 
10
 defined inductively in the natural way 
Let us consider the words 





1
1 2

0
1 2
∈Σ
=
∈Σ
=
m
m
v
v v
u
u u
γ
θ
 and the 
matrice



r 
i
jk
u v
u v
a
M
i r
=
where 



 
10



∈
=
=
k
r
i
j
ir
jk
r
i
jk
q u v q
a
u v
a
δ
 With 
the operation of matrices 
max− min
 or max product denoted by   we obtain 
the expression 
60 
H Xing and D Qiu 
 
um vm
v
u
u v
M
M
M
M
o
o
o

2
2
1 1
θ γ =
 where  
 1











1 2 
2
1
2
2
1
1
2
2
1 1
v v
u u
a
v
u
a
u v
def a
M
M
jk
jk
jk
v
u
u v
=
o
o
and 
2
















max
max
2
2
1
1
2
2
1
1
1 2
2
1
k
s
s
j
Q
q
sk
js
Q
q
jk
q u v q
q u v q
v
u
a
u v
a
v v
u u
a
s
s
δ
δ


∈
∈
=
=
 
3
Assume 

qn
q q
Q


2
1
=
 ie 
Q = n
 Then 
 

 


1 2
2
1
m
m
jk
jk
v
v v
u
u u
a
a
M


=
=
θ γ
θ γ
 where 
 






max


1
1 2
1
1
1 2
2
2
1
1

 
1
1 2
2
1
m
m
k
i
ii
ji
n
i
i
i
m
m
jk
v
u
a
v
u
a
u v
a
v
v v
u
u u
a
m
m
−
− ≤
≤
=





 
4 
Definition 3 An initial distribution of FSM 

δ 


M = Q 0 1
 is a mapping η  
from Q to 
10
 ie η  is concentrated at 
q∈Q
 denoted as 
q
η  if  
η q =1
 and 0 
elsewhere We will also use symbol η  to denote the row vector whose i th entry is 
 
η iq
 The ordered pair 
M η
 is called an initialized FSM If η  is concentrated at 
q∈Q
 we write 
M q
 for 
M η
 
Definition 4 Let 

δ 


M = Q 0 1
 and let 
M η
 be an initialized FSM We set 




P1
M
S
M


θ γ
η
γ
η θ
=
 
5
an entry indicating the maximal membership degree for the inputoutput pair 
θ γ
 
For the language theory the generated and marked languages of 
M η
 denoted 
by 

L M η
 and 

Lm Mη
 respectively or just simply 

L M 
 and 

Lm M 
 respectively 
can be described as 







0



1
0

∈ Σ ×Σ
=
θ γ
θ γ
η
Sη m
L M
 
6









max



1
0
m
m
m
Q
q
m
m
Q
q
q
M
S
M
L
m
m
∈
=
∈ Σ ×Σ
=
∈
o
o
θ γ
η
η
θ γ
θ γ
η
 
7
Definition 5 Two initialized FSM 
M η
 and 
M  η 
 where 

δ 


M = Q 0 1
 






1
0
M = Q   δ
 
are 
equivalent 
denoted 
by 







η
η
M
M
 
if 






η
η
L M
L M
=
 or more clearly for all 


θ γ ∈ Σ×Σ1 
 






M
M
S
S
θ γ
θ γ
η
η
=
 
8
In particular If 






M q
M q
 that is η  is concentrated at q  and 

η  at 
q  ie 






M
q
M
q
S
S
θ γ
θ γ
=
 then state q  and 
q  are equivalent denoted by 
q  q
 
 
Applications on Information Flow and Biomedical Treatment of FDES 
61 
 
Definition 6 Observationequivalence Let 

δ 


M = Q 0 1
 be an FSM M  is 
in reduced form if for each 
Q
q q
 ∈
 relation 
q  q
 implies 
q
q =
 Suppose 
two states 
Q
q q
 ∈
 are in reduced FSM M  We call 
qq
 to be observation
equivalent if 






M q
M q
 
Definition 7 Synchronous composition of two FSMs Let 
i 
i
M η

 be two initialized 
FSMs where 


21




1
0
=
 
=
i
Q
M
i
i
i
δ
 The synchronous composition of 
M1η1 
 
and 
M2η2 
 is a new machine 
M η
 here 







2
1
2
1
1
0
2
1
2
1
η η
η
δ δ
=
Σ Σ
×
=
=
Q
Q
M M
M
 
9
where  



21 



2
1
2
1
=
∈
=
×
Q i
q
q q
Q
Q
i
i
 and for any 

2
1
1 2
Q
Q
q q
×
∈
 and any 
1
0


∈Σ
∈Σ
σ
σ
 
 










 

 


 
min



 

2
2
2
1
1
1
2
1
2
1
2
1
q
q
q
q
q
q
q q
σ σ
δ
σ σ
δ
σ σ
δ δ
=
 
3 
Message Analyses Based on Equivalence Relations 
In reality information flow conforms to the demand of security and the secrecy of the 
system demands as follows there exist two class of users highclass user and low
class user for simplicity written as H  user and L  user respectively Every user 
has its own special operation and observation  
Definition 8 For initialized FSM 
M η
 

δ 


M = Q 0 1
 the projection of 
M  from H  users onto L  users is also an initialized FSM 
M p η
 where 

p 
p
p
p
Q
M
δ


1
0


=
 satisfying 
0
0
Σ p ⊆ Σ
 
1
1
Σ p ⊆ Σ
 which denotes input and 
output events executable for L  users respectively and the transition function  
10


1
0
→
×
×
×
Q
Q
p
p
δp
 defined as  









∉Σ
=
∈Σ
=

1
   


  
  

0
0
p
p
p
p
a
q
q
a
q a y q
q a y q
ε ε
δ
δ
δ
 
10
ie 
p
p
p
1
0
×Σ
δ = δ Σ
 
Definition 9 Given projection 
M p η
 of 
M η
 where 



 






1
0
1
0
δ
δ
 
=


=
Q
M
Q
M
p
p
p
p
 
11
We say there exists hidden information flow if 
≠ φ
×
× −

p
p
1
0
1
0
  
Remark 1 In DES theory usually a mask m  is defined as a function 
mQ → Γ
 
that maps elements from the machine state space Q  to the observation space Γ   In 
an FDES we can also assume mask m  in system 

δ 


M = Q 0 1
 constructed as 
m Q → Γ
 such that if 
Q
q
q
j
i
∈
∀ 
 satisfy 




j
M
i
M
P
M
P
M




θ γ
η
θ γ
η
=
 for 
62 
H Xing and D Qiu 
 
some 



1
0
θ γ ∈ Σ ×Σ
 then 
 
 
j
i
m q
m q
=
 and 
i qj
q 
 to be indistinguishable relative 
to 
θ γ
 In other words two states that are reached by some inputoutput string 
having same membership degree are relativeindistinguishable and the mask m  may 
be constructed timely to map these states the same observation 
Procedure 1 Computing Steps decide distinguished states and distinct buttons 
Step 1  For two states 
i qj
q 
 waiting for differentiate in FSM M  set two initial 
distribution 
j
i
q
q
η
η
η
η
=
=
 
 of FSM M  
Step 2 Choose 



1
0
θ γ ∈ Σ ×Σ
 write out the transition matrix 

M θ γ 
 decide 
whether 




j
i
P
M
P
M


θ γ
θ γ
=
 for ``yes go to step 3 for ``no go to step 4 
Step 
3 
Compute 

η M θ γ 

 and 

η M θ γ 

 
decide 
whether 



θ γ 
η
θ γ
η
M
M


= 
 for answer ``yes  
j
i
q  q
 otherwise go to step 4 
Step 4 Note down the inputoutput symbol 
θ γ
 as the distinctbutton of 
i qj
q 
 
4 
Application to Medical Decision Systems 
When a DES is modeled by a finite automaton 

q Qm 
Q
G

 

0
Σ δ
=
 language 
 
L G
 
generated by G  may be interpreted as the physically possible behavior In order to 
alter the behavior of G  a supervisor S  is introduced Formally S  is defined as a 
function from 
 
L G
 to 
 
P Σ
 It is interpreted that for each 
 
s∈ L G
  
 
 

L G 
s
S s
∈
∩
σ  σ
 represents the set of enabled events after the occurrence of 
s  Furthermore it is required that for any 
 
s∈ L G
 
 


 

S s
L G
s
uc
⊆
∈
∈Σ
∩
Σ
σ
σ
 
12
As before we use FSM M  to model FDES in this section Suppose each fuzzy event 
σ  is associated with a degree of controllability so the uncontrollable set 
uc
Σ  and 
controllable set 
c
Σ  are two fuzzy subsets of Σ  ie 
 
Σ
Σ ∈
Σ
F
uc c
 and satisfy for 
any 
 
σ ∈ F Σ
 
 
 
= 1
+ Σ
Σ
σ
σ
c
uc
 
13
A supervisor S  of  FDES M  is defined as a function 






1
0

1
0
Σ ×Σ
→
Σ ×Σ
F
F
S
 
14
Similar to the admissibility condition 12 for crisp supervisors S  is usually 
required to satisfy that for any 
s ∈Σ
 and 
Σ
σ ∈
 
  




  

min
σ
σ
σ
S s
L M s
uc
≤
Σ
 
15
 
Applications on Information Flow and Biomedical Treatment of FDES 
63 
 
Notations concerning prefixclosed property in the sense of FDES are for any fuzzy 
string 
s ∈ Σ

  

 




s
tr
r
t
pr s
=
∃ ∈Σ
∈Σ
=
For any fuzzy language L  over 

Σ  its 
prefixclosure 
 

10


Σ →
pr L
 is defined as
  
   
sup
L t
pr L s
s∈pr t
=
So 
  s
pr L
 
denotes the possibility of string s  belonging to the prefixclosure of L  By means 
of the formulation of the above concepts now we can present the controllability 
theorem concerning FDESs The following proposition is changed from Theorem 1  
in 3 
Proposition 1 Let an FDES be modeled by FSM 

δ 


M = Q 0 1
 Suppose fuzzy 
uncontrollable subset 

1 
0

Σ ×Σ
∈Σ =
Σ
F
uc
 and fuzzy legal subset 
K ∈Σ
 that 
satisfies 

K ⊆ L M 
 and 
 
K ε =1
 Then there exists supervisor 
∈Σ → Σ

 


S
 such 
that S  satisfies the fuzzy admissibility condition Eq 15 and 

 
pr K
L S M
=

 if 
and only if for any 
s ∈ Σ
 and any 
Σ
σ ∈
 
  
  




 



min
σ
σ
σ
pr K s
L M s
pr K s
uc
≤
Σ
 
16
where Eq 16 is called fuzzy controllability condition of K  with respect to M  
and 
Σuc
 
5 
Concluding Remarks 
We further developed FDES by dealing with information flow analysis based on 
sequential machine theory Through modeling a security system by using a fuzzy 
sequential machine the idea of a stateeventbased approach to decide whether hidden 
massage flows exist was displayed Furthermore we study the supervisory control 
theory in FDES and discussed a medicine issue modeled by FSM The technical 
contributions are mainly as follows i we reformulated the parallel composition of 
crisp DES and defined the parallel composition of FDES ii we gave a stateevent
based approach to decide whether hidden massage flows exist and a computing 
process was introduced to get the equivalent states and by means of this we can 
search for all possible distinguishable states and their distinctbuttons 
With the results obtained in 3 and this paper it is worth further considering to 
apply the supervisory control theory of FDES to practical control issues particularly 
in economic and traffic control systems Moreover dealing with FDES modeled by 
fuzzy petri nets 10 is of interest as the issue of DES modeled by Petri nets 1 and 
we deem it a significant research direction 
Acknowledgments This work is supported in part by the National Natural Science 
Foundation Nos 60873055 61073054 the Natural Science Foundation of Guang
dong Province of China No 10251027501000004 the Research Foundation for the 
Doctoral Program of Higher School of Ministry of Education of China No 
20100171110042 the Fundamental Research Funds for the Central Universities  
 
64 
H Xing and D Qiu 
 
No 10lgzd12 and the project of SQIG at IT funded by FCT and EU FEDER projects 
QSec PTDCEIA676612006 FCT project PTDCEEATEL1034022008 QuantPriv
Tel FCT PEstOEEEILA00082011 AMDSC UTAustinMAT00572008 IT Project 
QuantTel Network of Excellence EuroNF  
References 
1 Cassandras CG Lafortune S Introduction to Discrete Event Systems Kluwer Boston 
1999 
2 Lin F Ying H Modeling and Control of Fuzzy Discrete Event Systems IEEE Transac
tions on System Man Cybern B 324 408–415 2002 
3 Qiu DW Supervisory Control of Fuzzy Discrete Event Systems A Formal Approach 
IEEE Transactions on System Man Cybern B 352 72–88 2005 
4 Zi XC Yao LH Li L A Statebased Approach to Information Flow Analysis J of 
Computers 298 1460–1467 2006 in Chinese 
5 Goguren JA Messegurer J Security Policies and Security Models In Proceedings of 
the IEEE Symposium on Security and Privacy California USA pp 75–85 1984 
6 Zakintihinos A Lee ES A General Theory of Security Properties In Proceedings of 
the IEEE Symposium on Security and Privacy California USA pp 94–102 1997 
7 Shayman MA Kumar R Supervisory Control of Nondeterministic Systems with Dri
ven Events via Prioritized Synchronization and Trajectory Models SIAM J Control Op
tim 332 469–497 1995 
8 Thomason MG Marinos PN Deterministic Acceptors of Regular Fuzzy Languages 
IEEE Transactions on System Man and Cyber 41 228–230 1974 
9 Kumar R Heymann M Masked Prioritized Synchronization for Interaction and Control 
of Discrete Event Systems IEEE Transactions on Automatic Control 4511 1970–1982 
2000 
10 Looney CG Fuzzy Petri Nets for Rulebased Decision making IEEE Trans Syst Man 
Cybern 181 178–183 1988 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 65–72 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Discontinuous Fuzzy Systems  
and Henstock Integrals of Fuzzy Number Valued  
Functions 
Yabin Shao1 and Zengtai Gong2 
1 College of Mathematics and Computer Science 
 Northwest University for Nationalities Lanzhou 730030 China 
2 College of Mathematics and Information Science 
Northwest Normal University Lanzhou 730070 China 
ybshao163com 
Abstract In this paper using the properties of the strong Henstock integrals 
of fuzzynumbervalued functions and controlled convergence theorem 
we prove the existence theorem for the discontinuous fuzzy system 
  
x = f t x
′
in fuzzy number space where f is strong fuzzy Henstock in
tegrable 
Keywords fuzzy number strong fuzzy Henstock integrals cauchy prob
lem existence of solution 
1 
Introduction 
The Henstock integral is designed to integrate highly oscillatory functions which 
the Lebesgue integral fails to do It is known as nonabsolute integration and in
cludes the Riemann improper Riemann Lebesgue and Newton integrals 7 The 
Riemanntype definition of nonabsolute integration was introduced more recently 
by Henstock in 1963 and also independently by Kurzweil the  definition  is now 
simple  and  furthermore the  proof involving  the  integral also turns  out to be 
easy  
It is well known that the theory of fuzzy sets provides an effective means of de
scribing the behavior of system which are too complex or too illdefined to admit 
precise mathematical analysis by classical methods and tools We have combined the 
above theories and discussed the fuzzy Henstock integrals of fuzzynumbervalued 
functions which extended Kaleva  integration In order to complete the theory of 
fuzzy calculus and to meet the solving need of transferring a fuzzy differential 
equation  into a fuzzy integral  equation we also have defined the strong fuzzy 
Henstock integrals and discussed some of their  properties  and the controlled  
convergence theorem 
On the other hand the characterization of the derivatives in both real and 
fuzzy analysis is an important problem Bede and Gal 1 have subsequently 
66 
Y Shao and Z Gong 
introduced a more general definition of a derivative for fuzzynumbervalued func
tion enlarging the class of differentiable of fuzzy number valued functions  
The Cauchy problems for fuzzy differential equations have been studied by sev
eral authors 46811 on the metric space 



E n D
of normal fuzzy convex set 
with the distance D  given by the maximum of the Hausdorff distance between 
the corresponding level sets  In 8 the author has been proved the Cauchy prob
lem has a uniqueness result if f  was continuous and bounded Wu  and Song 
11 changed  the initial  value problems of fuzzy differential  equations  into a 
abstract differential equations on a closed convex cone in a Banach space by the 
operator j which is the isometric embedding from 



E n D
 onto its range in 
the Banach space X They obtained the existence theorems under the compact
nesstype conditions In 2002 Xue and Fu 12 established solutions to fuzzy  
differential equations with righthand side functions satisfying Caratheodory con
ditions on a class of Lipschitz fuzzy sets However the study on fuzzy disconti
nuous systems is insufficient  
In this paper according to the idea of 12 and the operator j  which is the isome
tric embedding from 



E n D
 onto its rang in the Banach space X  we shall deal 
with the Cauchy problem of discontinuous systems as following 



∈
=
=
′

0

   


0
En
x
x
f t x t
x t
 
1
where 
n
E is a fuzzy number space and 
En
B
f
→
×
0
 
γ
 is strong fuzzy Hens
tock integrable and 
0

0

 0


0

+
≤
=
b b
D x
x D x
B
 
2 
Preliminaries 
Let 


k Rn
P
 denote the family of all nonempty compact convex subset of 
n
R  
and define the addition and scalar multiplication in 


k Rn
P
 as usual Let A  
and B  be two nonempty bounded subset of
n
R  The distance between 
A  
and B  is defined by the Hausdorff metric 2 
|
|supinf |
maxsupinf |
  
a
b
b
a
A B
d
b B a A
a A b B
H
−
−
=
∈
∈
∈
∈
 
Denote 
u
u R
E
n
n
 10 
 
→
=
satisfies 
4
1
−
 below is a fuzzy number 
space Where 
1 u  is normal ie there exists an 
x ∈ Rn
0
 such that 
1


u x0 =
 
2 u  is fuzzy convex ie 
min    
 
1

u x u y
y
x
u
≥
−
+
λ
λ
 for any 
 
Discontinuous Fuzzy Systems and Henstock Integrals 
67 
Rn
x y
∈

and 
1
0
≤ λ ≤
 
3 u   is upper semicontinuous 
4 
0
|  



0

∈
=
u x
R
cl x
u
n
 is compact 
Define 

 0

+∞
→
×
n
n
E
D E
 
 10 


  
sup
  
∈
=
α
u α v α
d
D u v
H
 
where 
dH
 is the Hausdorff metric defined in 


k Rn
P
  Then it is easy see that D  
is a metric in
n
E  Using the results 3 we know that the metric space 



En D
 has 
a linear structure it can imbedded isomorphically as a cone in a Banach space of 
function
R
S
I
u
n
→
×
−
∗
1

 where 
Sn−1
 is the unit sphere in 
n
R  which an im
bedding function 
u = ju
∗
 defined by
x
r x
u
u

sup
 

 
α
α
α∈
∗
=
 
It is well know that the H derivative for fuzzy functions was initially introduced by 
Puri and Ralescu 2 and it is based in the condition 
H
of sets In this paper we 
consider a more general definition of a derivative for fuzzy number valued functions 
enlarging the class of differentiable fuzzy number valued functions which has been 
introduced in 1 
Definition 11 Let 
En
a b
f
  →
 
 and 
 

0
x ∈ a b
 We say that f  is diffe
rentiable at 
0x  if there exist an element 

 

0
En
f t
∈
′
 such that 1 for all 
h  0
 
sufficiently small there exist 





0
0
f x
h
x
f
−H
+
 and  





0
0
f x
h
x
f
−H
+
 
the limits 

 




lim




lim
0
0
0
0
0
0
0
x
f
h
h
f x
f x
h
f x
h
x
f
H
h
H
h
′
=
−
−
=
−
+
→
→
 
2 for all 
h  0
 sufficiently small there exist 





0
0
h
f x
x
f
H
+
−
 and  





0
0
f x
h
x
f
−H
−
 the limits 

 




lim




lim
0
0
0
0
0
0
0
x
f
h
f x
h
f x
h
h
f x
x
f
H
h
H
h
′
=
−
−
−
=
−
+
−
→
→
 
3  for all 
h  0
 sufficiently small there exist 





0
0
f x
h
x
f
−H
+
 and  





0
0
f x
h
x
f
−H
−
 the limits 

 




lim




lim
0
0
0
0
0
0
0
x
f
h
f x
h
f x
h
f x
h
x
f
H
h
H
h
′
=
−
−
−
=
−
+
→
→
 
4 for all 
h  0
 sufficiently small there exist 





0
0
h
f x
x
f
H
+
−
 
68 
Y Shao and Z Gong 
and  





0
0
h
f x
x
f
H
−
−
 the limits 

 




lim




lim
0
0
0
0
0
0
0
x
f
h
h
f x
f x
h
h
f x
x
f
H
h
H
h
′
=
−
−
=
−
+
−
→
→
 
3 
The Strong Henstock Integrals of Fuzzy Number Valued 
Functions in 
n
E  
In this section we define the strong Henstock integrals of fuzzy number valued func
tions in fuzzy number space 5 and we give some properties of this integral 
Definition 29 A function 
En
a b
f
  →
 
 is strong fuzzy Henstock integrable on 
  
a b


  

a b En
f ∈SFH
 if there exist a function 
En
a b
F
  →
 
 with the 
following property given 
ε  0
 there exist a positive  function δ on 
  
a b
 such 
that 
 


i
i
i
s
c d
P =
 is a tagged partition of  
  
a b
  then  
 ε
−

=
n
i
i
i
i
i
i
F c d
c
d
f s
D
1

  
  
 
where D is the Hausdorff metric of the distance between fuzzy numbers  
Theorem 1 Let 
En
a b
f
  →
 
 be a strong fuzzy Henstock integrable on 
  
a b
 
and let 
= 
t
a
f s ds
F t
 
 
for each
  
s ∈ a b
 Then 
1 F  is continuous on 
  
a b
 
2 F  is differentiable almost everywhere on 
  
a b
 
3 f is measurable 
Proof Because of the fuzzy number space 



E n D
is a complete metric space the 
proof is similar to the real valued function in Ref 10 15 we omit it here 
Theorem 2 5 Suppose 
 

nf
 is a sequence of SFH integrable functions on 
  
a b
 
satisfying the following conditions 
 
1 
 
  
f x
fn x
→
 a e in 
  
a b
 as 
n → ∞
  
2 the primitives 
n
F  of 
nf  are 
∗
ACG uniformly in n  
3 the primitives 
n
F  converge uniformly on 
  
a b
 
 
Then f  also SFH integrable on 
  
a b
 and 


=
∞
→
b
a
b
a
n
n
f x dx
x dx
f

 
  
lim
 
 
Discontinuous Fuzzy Systems and Henstock Integrals 
69 
4 
The Existence of Solutions for Discontinuous Fuzzy Systems 
In this section we will prove the existence theorem for the problem 1 For any 
bounded subset A  of the Banach space X  we denote 
αA
 the Kuratowski 
measure of noncompactness of A  ie the infimum of all 
ε  0
 such that there exist 
a finite covering of A  by sets of diameter less than ε  For the properties of α we 
refer to 3 for example 
Lemma 1 3 Let 



X
C I
H
γ
⊂
 be a family of strong equicontinuous functions 
Then  



 

sup


γ
α
α
α
γ
H I
H t
H
t I
=
=
∈
 
where 
αH
 denote the Kuratowski measure of noncompactness in 



X
C I
γ
and 
the function 
 

H t
t
→α
is continuous 
Let 

0

 0

0



 

0
0
0
b
D x
x D x
x
C I
x
C x
+
≤
=
∈
=
γ
γ
b and γ  are 
some positive numbers Obviously the fuzzy set 
 

C x0 γ
 is closed and convex 
Let 
x
F  be defined by  
+ 
=
t
x
f s x s ds
x
t
F
0
0
   
  
 or 
⋅
+ −
=
t
x
f s x s ds
x
t
F
0
0
   
1

  
 
for 
t ∈ γI
 and 
 

x∈C x0 γ
where the integrals in the sense of SFH  
Definition 3  A fuzzy number valued function f  is a Caratheodory function if  
1 f  is measurable for any 
x∈ En
 
2 f  is continuous for any 
t ∈ γI
 
Lemma 2 Let V be equicontinuous bounded set in 



En
C I
γ
 f is a Caratheod
ory function and 
 
 
f ⋅ x ⋅
 be a SFH integrable function for each 
x∈V
 Let 
 

 

C x0 γ
x
F
F
x
∈
=
 be equicontinuous and uniformly 
∗
ACG on 
γI  Then 

   


   

0
0
ds
f s V s
j
f s V s ds
j
t
t


≤


α
α
 
whenever 
 
 
 

s
f s V s
j
ϕ
α
≤

 for 
s∈ γI
ae 
ϕs
is a Lebesgue integra
ble function and  


  
   

   

0
0


∈
=
t
t
V s
f s x s ds x s
f s V s ds
 
70 
Y Shao and Z Gong 
Proof Because 
 
 
f ⋅ x ⋅
 be a SFH integrable function then 
−
αf  and 
+
αf is 
Henstock integrable for all
10
α ∈
 According to the theory of real analysis the 
conclusion is hold true 
Theorem 3 3 Let D  be a closed convex subset of X  and Let F  be a conti
nuous function from D  into itself If for 
x∈ D
 the implication 
V
F V
x
con
V

∪
=
 
 
 
2
is relatively compact then  F  has a fixed point 
Definition 43 A nonnegative function 
  
  
h t r
t r
→
 is a Kamke function on 
I × R+
 if  
1 
 
 r
h t
 satisfies the Caratheodory conditions 
2 
0
 0 
=
h t
and the function indentically equal to zero is the unique continuous 
solution of equation 
= 
t
h s u s ds
u t
0

   
 
 for 
t ∈ I
satisfying 
0
0
=
u
 
Next we give the main results for this paper 
Theorem 4 If for each continuous 
En
x I
 γ →
  
 
 
f ⋅ x ⋅
 be a SFH integrable  
f  is Caratheodory function and 


 

 

X
j
h t
f t X
j


α
α
≤
 
3
for each bounded subset 
X ⊂ En
  where h  is a Kamke function Let 
 

 

C x0 γ
x
F
F
x
∈
=
 be equicontinuous and uniformly 
∗
ACG on 
γI  Then 
there exist a solution of problem 1 on 
βI  for some 
0  β ≤ γ
 
Proof By equicontinuous of F  there exist a number β  and 
0  β ≤ γ
 such that 
b
f s x s ds
D
t
≤

0
   

0
 for 
t ∈ βI
and 
 

x∈C x0 γ
 By assumption the op
eraor 
x
F is well defined and maps 



C x0 β
into 



C x0 β
 Using Theorem 2 for 
the SFH integral we deduce that F is continuous  
Suppose that 
 
 
F V
x
con
V
∪
=
for some bounded 



V ⊂ C x0 β
 We 
shall prove that V is relatively compact thus 2 is satisfied In fact
 
 V
F
is equi
continuous the function 
 

 
j V t
v t

→α
is continuous on 
βI  and  
 
Discontinuous Fuzzy Systems and Henstock Integrals 
71 

   

  

0 0
+
=
t
f s x s ds
x
F V t
or 

   
1


  

0
0
⋅
+ −
=
t
f s x s ds
x
F V t
 
By Lemma 2 and 3 we have 



≤
≤
∈
≤
t
t
t
ds
j V s
h s
ds
f s V s
j
V s
f s x s ds x s
j
F V t
j
0
0
0

 
  
   

 
  
   


  





α
α
α
α
 
Since
 
 
F V
x
con
V
∪
=
 by the property of measure of noncompactness we 
have 
  

 

j F V t
j V t


α
α
≤
and 
≤ 
=
t
h s v s ds
j V t
v t
0

   
 

 

α
 
Hence we have 
 

 
j V t
v t

= α
 By Lemma 1 V is relatively compact So by 
Theorem 3 has a fixed point which is a solution of 1 The proof is completed 
5 
Conclusions 
In this paper we deal with the Cauchy problem of discontinuous fuzzy differential 
equations involving the strong fuzzy Henstock integral in fuzzy number space The 
function governing the equations is supposed to be discontinuous with respect to 
some variables and satisfy nonabsolute fuzzy integrablility  Our result improves the 
result given in Ref 4 6 8 and 12 where uniform continuity was required as well 
as those referred therein 
 
Acknowledgements The authors wish to thanks the referees for the careful reading 
and valuable remarks which improved the presentation of the paper This work is 
supported by the National Natural Science Fund of China No11161041 and 
No7106013  
References 
1 Bede B Gal S Generalizations of the Differentiability of FuzzyNumberValued
Functions with Applications to Fuzzy Differential Equation Fuzzy Sets and Syst 151 
581–599 2005 
2 Diamond P Kloeden P Metric Space of Fuzzy Sets Theory and Applications World 
Scientific Singapore 1994 
3 Banas J Goebel K Measure of Noncompactness in Banach Space Lecture Notes in 
Pure and Appl Math vol 60 Mercel Dekker New York 1980 
4 Gong Z Shao Y Global Existence and Uniqueness of Solutions for Fuzzy Differential 
Equations under Dissipativetype Conditions Computers  Math with Appl 56 2716–
2723 2008 
72 
Y Shao and Z Gong 
5 Gong Z Shao Y The Controlled Convergence Theorems for the Strong Henstock Inte
grals of FuzzyNumberValued Functions Fuzzy Sets and Syst 160 1528–1546 2009 
6 Kaleva O Fuzzy Differential Equations Fuzzy Sets and Syst 24 301–319 1987 
7 Lee P Lanzhou Lectures on Henstock Integration World Scientific Singapore 1989 
8 Nieto JJ The Cauchy Problem for Continuous Fuzzy Differential Equations Fuzzy Sets 
and Syst 102 259–262 1999 
9 Puri ML Ralescu DA Differentials of Fuzzy Functions J Math Anal Appl 91 552–
558 1983 
10 Wu CX Ma M On Embedding Problem of Fuzzy Number Spaces Part 2 Fuzzy Sets 
and Syst 45 189–202 1992 
11 Wu C Song S Existence Theorem to the Cauchy Problem of Fuzzy Differential Equa
tions under Compactnesstype Conditions Inf Sci 108 123–134 1998 
12 Xue X Fu Y Caratheodory Solution of Fuzzy Differential Equations Fuzzy Sets and 
Syst 125 239–243 2002 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 73–79 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A Phased Adaptive PSO Algorithm for Multimodal 
Function Optimization  
Haiping Yu1 and Fengying Yang2 
1 Faculty of Information Engineering City College Wuhan University of Science and  
Technology Wuhan China 
2 College of information engineering Huanghuai University Henan China 
yhp0308yahoocomcn ziying661163com 
Abstract Particle swarm optimization is a powerful algorithm that has been 
applied to various kinds of problems However it suffers from falling into local 
minimum and prematurity especially on multimodal function optimization 
problems In this paper a phased adaptive particle swarm optimizationPAPSO 
is proposed to solve such problem The process is divided into the initial particle 
presearching phase and the postsearching cooperative phase In the post phase 
the strategy of selecting randomly a certain number of particles for entering the 
reverselearning is one of the most effective ways of escaping local stagnation 
The illustrative example is provided to confirm the validity as compared with the 
SPSO Dynamic Inertia Weight PSOPSOW and Tradeoff PSOPSOT in 
terms of convergence speed and the ability of jumping out of the local optimal 
value Simulation results confirm that the proposed algorithm is effective and 
feasible  
Keywords particle swarm optimization multimodal function adaptive 
1 
Introduction 
Particle swarm optimization is a swarm intelligence technique developed by Kennedy 
and Eberhart in 19951 because it is a simple and effective various improved algo
rithms have been proposed in recent years For aspects of inertia weight such as  
references23 for hybrid algorithms such as references46etc Particle swarm 
optimization has been used increasingly as an effective technique for solving complex 
optimization problems successfully such as multiobjective optimization training 
neural network and emergent system identification79 Researchers have found the 
algorithm suffers from premature convergence and falls into local minima So it is of 
great significance to solve the above shortcoming As a result  accelerating conver
gence rate and avoiding local optima have become the most important and appealing 
goals in PSO research10To overcome the above limitation a phased adaptive particle 
swarm optimizationPAPSO is proposed in this paper which can search global value 
for multimodal functions in highspeed of convergence 
This paper is organized as follows Section 2 is devoted to the standard particle 
swarm optimization In section 3 the proposed technique of PAPSO are described and 
analyzed In section 4 six multimodal functions are tested by the PAPSO and the 
results are analyzed in detail Finally the paper closes with conclusion and ideas for the 
further research about PSO in section 5 
74 
H Yu and F Yang 
 
2 
The Standard Particle Swarm Optimization  
 
21 
Algorithm Background 
Many scientists have created computer simulations of various interpretations of the 
movement of organisms in a bird flock or fish school Notably Reynolds  Heppner and 
Grenander presented simulations of bird flocking choreography and Heppner a zo
ologist was interested in discovering the underlying rules that enabled large numbers 
of birds to flock synchronously often changing direction suddenly scattering and 
regrouping etc Both of these scientists had the insight that local processes such as 
those modeled by cellular automata might underlie the unpredictable group dynamics 
of bird social behavior Both models relied heavily on manipulation of interindividual 
distances1 
Particle Swarm optimizerPSO is an optimization algorithm first proposed by 
James Kennedy and Russell Beernaert1 The original algorithm is based on the so
ciological behavior associated with bird flock or fish school Because of the less algo
rithm parameters the PSO algorithm is applied in many fields such as network training 
optimization and fussy control and so on 
22 
The Theory of the Particle Swarm Optimization  
In PSO each swarm member called a particle represents a potential solution to an 
optimization problem in searching space Each particle adjusts the search direction by 
learning from its own experience and the global particles experiences Specifically 
each particle velocity is updated by following two optimum values The first one is the 
best solution fitness that has been achieved so far This value is called pbest The 
second one is the global best value obtained so far by any particle in the swarm This 
best value is called gbest11 Each particle updates its velocity by using formula 1 
and each particle updates its position by using formula2 At the same time when a 
particle finds a better position than the previous one its location is stored in memory In 
other words  the algorithm works on the social behavior of particles in the swarm In 
the end particles can find the globalbest position by simply adjusting their own posi
tion The formula is as follows 
 
 

 
 

 
1

2 2
1 1
t
x
t
c r gbest
t
x
t
c r pbest
t
wv
t
v
ij
gj
ij
ij
ij
ij
−
+
−
+
=
+
 
1
1

 
1

+
+
=
+
t
v
t
x
t
x
ij
ij
ij
 
2
This equation shows that the new velocity is not only related to the old velocity but also 
associated with the position of the particle itself and of the global best one And the 
degree of influence depends on the inertia weight and the two cognitive factors So 
reasonable choice of parameters has a great influence on the algorithm In 2002 a  
 
 
 
A Phased Adaptive PSO Algorithm for Multimodal Function Optimization 
75 
 
constriction named λ coefficient is used to prevent each particle from exploring too far 
away in the range of min and max since λ applies a suppression effect to the oscillation 
size of a particle over time This method constricted PSO suggested by Clerc and 
Kennedy is used with λ set it to 07298 according to the formula1213 
However PSO lacks the ability of global search in searching space and it easily falls 
into local optimum especially in solving the multimodal function optimization There 
are some reasons for this phenomenon The multimodal function shape is more com
plicated In addition because of the parameter settings of the particle number and 
particle number for improper the diversity of particles gradually disappears so the 
algorithm is easy to fall into local optimal solution As the result a new method will be 
demonstrated by the multimodal function optimization 
Table 1 Definition of the Parameters 
Parameters 
Descriptions 
t 
iteration number 
w 
inertia weight 
r1 r2 
random vectors  the range is 01 
c1 
selfcognitive factor 
c2 
socialcognitive factor 
i 
equals 123NNthe swarm size 
j 
equals 123DDthe dimension of searching place 
v 
the velocity of each particle 
x 
the position of each particle 
3 
A Phased Adaptive Particle Swarm OptimizationPAPSO 
According to the above description it is very difficult to find the global optimum just 
by using the standard PSO In order to overcome this limitation we have proposed a 
new concept named a phased adaptive particle swarm optimizationPAPSO The main 
idea is described as follows  
In primary phase particles have stronger selflearning ability than the global one 
therefore first let each particle search around its own personal best by setting a control 
parameter If the number of failures reaches the control parameter adjust the particle 
search strategy by way of setting the current optimum equals the global optimum Then 
the searching enters an advanced phase In this phase the synergies among particles 
become stronger We set the current value equals the global optimum at the same time 
adjust the learning factor If the number of failures reaches the control parameter 
randomly select a certain number of particles for entering the reverse learning The 
searching process iterates until the particles find the global optimum or reach to the 
maximum number of iterations 
 
76 
H Yu and F Yang 
 
The algorithm flow diagram are as follows 
Step1 Define the Basic Conditions 
Define the parameters the parameters of the algorithm involved includes that pop
ulation size  the minimum and maximum rangesaccording to the problem the range 
of velocity the dimension the control parameter 
Step2 Initialize the Particle Velocity and Position 
Initialize the locations and velocities of all particles randomly in the searching space 
and let the initial pbest of each particle equal its current position 
Step3 Update Velocity  
In primary phase the velocity of the particles changes as the formula 3 in ad
vanced phase update the velocity using formula 1 the c1 changes as the formula4 
c2=2  
 
 

 
1

1 1
t
x
t
c r pbest
t
wv
t
v
ij
ij
ij
ij
−
+
=
+
 
3
2
max
1
iter
iter
c =
 
4
where iter means the current times maxiter means the maximum number of iterations  
Step4 Update Position 
In primary phase each particle modifies its position according to the formula 2 In 
advanced phase When the particle position has not changed in the range of the control 
parameter it seems that the particles are likely to fall into local optimum In order to 
avoid the phenomenon we select a certain number of particles and calculate their 
opposite particles as follows1415 
10






rand
P
b
a
P
i j
j
j
i j
=
−
+
=
λ
λ
 
5
where Pij is the jth vector of the ith candidate in the particle 
i j
P   is the transformed 
one the range of the population is between aj and bj in the jth dimension 
Step5 Update pbests and gbests 
Step6 Repeat and Check Convergence 
Steps35 are repeated until all particles are gathered around the global best or a 
maximum iteration is encountered 
4 
Multimodal Function Test and Results 
Six testing multimodal functions have been used to verify the performance of the 
proposed algorithm A procedure description of the proposed algorithm is provided by 
Table 2 and the experimental settings is provided by Table 3 the results are showed in 
Table4  
In table 2 the six test function used in the experiments where D is the dimension of 
the function R is the definition domain and Fmin means the minimum value of the 
function 
 
A Phased Adaptive PSO Algorithm for Multimodal Function Optimization 
77 
 
Table 2 Definition of the Multimodal Function 
Name 
F 
D 
R 
Fmin 
Sine 

=
×
=
n
i
i
i
x
i
x
x
f
1
2
20
1


 cos
sin
 
π
 
30 
PIPI 
0 
Schewefel 

=
−
=
n
i
i
i
x
x
x
f
1
2

sin



 
30 
500500 
0 
Griewank 
1

cos
 
1
1
2
4000
1
3
+
− ∏

=
=
=
n
i
i
n
i
i
i
x
x
x
f
 
30 
600600 
0 
Rastrigin 

+
−
=
10
10cos2

 
2
4
i
i
x
x
x
f
π
 
30 
512512 
0 
Rosenbrock 
 

1

 x 
100
 
2
2
1
i
2
i
1
5
−
+
= 
=
+
i
n
i
x
x
x
f
 
30 
3030 
0 
Ackley 

−

−
+
=
=
−
cos2
1
1
5
1
6
1
2
20
20


n
i
i
n
x
n
e
e
e
x
f
30 
3232 
0 
Table 3 The Experimental Parameter Settings 
Algorithm 
PS 
c1 
c2 
wmaxwmin 
Itermax 
SPSO 
50 
149618 
149618 
0729843 
150000 
PSOW 
50 
205 
205 
09504 
150000 
PSOT 
50 
2 
2 
0904 
150000 
PA PSO 
50 
 
2 
0904 
150000 
 
Table 4 Results of the Experiment Comparing with Other PSOs 
 
 
f1 
f2 
f3 
f4 
f5 
f6 
SPSO 
Avg 
211E+1 
775E+03 
177E02 
457E+01 
238E+01 
118 
StdD 
175 
603E+02 
192E02 
111E+01 
313E+01 
660E01 
PSOW 
Avg 
212E+1 
839E03 
157E02 
362E+01 
443E+01 
171E01 
StdD 
242 
443E+02 
206E2 
915 
310E+1 
481E01 
PSOT 
Avg 
135E+1 
952E+03 
113E2 
313E+1 
267E+1  
784E06 
StdD 
172E+01 
304E+02 
896E3 
490 
537E+1  
837E6 
PAPSO 
Avg 
204E+1 
706E+03 
892E4 
120E+01 
226E+1 
726E15 
StdD 
186 
402E+02 
327E3 
590 
161 
203E15 
78 
H Yu and F Yang 
 
In table 3 where PS is the size of population WmaxWmin is the inertia weight  
gradient descent range Itermax is the maximum number of iterations 
In table 4 Avg means the average of the best value Std means the standard deviation 
PSOT is the reference 3 algorithm The bold number is the best one 
5 
Conclusion 
A Phased Adaptive PSO Algorithm for Multimodal function optimization is proposed 
in this paper It keeps the basic concepts of the PSO at the same time embeds the search 
mechanism by controlling parameter and reversing learning strategies The testing 
results show that it avoids falling into local minimum and premature convergence 
compared to conventional PSO PSOW and PSOT Moreover the PAPSO has a 
unique advantage in many multimodal functions optimization However the PAPSO is 
not suitable for all kinds of problems Further study is to investigate the effectiveness of 
PAPSO in the near future            
 
References 
1 Eberhart R Kennedy J A New Optimizer Using Particle Swarm Theory In Proc 6th Int 
Symp Micromach Hum Sci Nagoya Japan pp 39–43 1995 
2 Nie P Ji GQ Zhi G Selfadaptive Inertia Weight PSO Test Case Generation Algorithm 
Considering Prematurity Restraining International Journal of Digital Content Technology 
and its Applications 59 125–133 2011 
3 Chen F Tradeoff Strategy Between Exploration and Exploitation for PSO In Seventh 
International Conference on Natural Computation pp 1216–1222 2011 
4 Abdel K Rehab F An Improved Discrete PSO with GA Operators for Qosmulticase 
Routing International Journal of Hybrid Information Technology 223–238 2011 
5 Wang XH Li JJ Hybrid Particle Swarm Optimization with Simulated Annealing In 
Proceedings of the Third International Conference on Machine Learning and Cybernetics 
pp 26–29 2004 
6 Li ST Tan MK Ivor WT A Hybrid PSOBFGS Strategy for Global Optimization of 
Multimodal Functions IEEE Trans on Systems Man and Cybernetics 414 2011 
7 Wang YF Zhang YF A PSObased Multiobjective Optimization Approach to the In
tegration of Process Planning and Scheduling In 8th IEEE International Conference on 
Control and Automation pp 614–619 2010 
8 Hu X Eberhart R Multiobjective Optimization Using Dynamic Neighborhood Particle 
Swarm Ptimization In Congress on Evolutionary Computation CEC 2002 vol 2 pp 
1677–1681 IEEE Service Center Piscataway 2002 
9 Y S Design of Neural Network Gain Scheduling Flight Control Law Using a Modified 
PSO Algorithm Based on Immune Clone Principle In Second International Conference on 
Intelligent Computation Technology and Automation pp 259–263 2009 
10 Ho SY Lin HS Liauh WH Ho SJ OPSO Orthogonal Particle Swarm Optimization 
and its Application to Task Assignment Problems IEEE Trans Syst Man Cybern A Syst 
Humans 382 288–298 2008 
 
 
A Phased Adaptive PSO Algorithm for Multimodal Function Optimization 
79 
 
11 Sotirios K Goudos VM Theodoros S Application of a Comprehensive Learning Par
ticle Swarm Optimizer to Unequally Spaced Linear Array Synthesis With Sidelobe Level 
Suppression and Null Control IEEE Antennas and Wireless Propagation Letters 9 125–129 
2010 
12 Clerc M Kennedy J The Particle Swarmexplosion Stability and Convergence in a 
Multidimensional Complex Space IEEE Trans Evol Comput 58–73 2002 
13 Li XD Niching Without Niching Parameters Particle Swarm Optimization Using a Ring 
Topology IEEE Transactions on Evolutionary Computation 150–169 February 2010 
14 Rahnamayan S Tizhoosh HR Salama MMA Oppositionbased Differential Evolu
tion IEEE Trans Evolut Comput 12 64–79 2008 
15 Wang H Zhi JW Shahryar R Enhancing Particle Swarm Optimization Using Genera
lized Oppositionbased Learning Information Sciences 181 2011 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 80–86 2012 
© SpringerVerlag Berlin Heidelberg 2012 
The Comparative Study of Different Number of Particles 
in Clustering Based on ThreeLayer Particle Swarm 
Optimization  
Guoliang Huang Xinling Shi Zhenzhou An and He Sun 
School of Information Science and Engineering  
Yunnan University Kunming 650091 China  
 hglwan2008163com 
Abstract To study how the different number of particles in clustering affect the 
performance of threelayer particle swarm optimization THLPSO that sets the 
global best location in each swarm to be the position of the particle in the swarm 
of the next layer ten configurations of the different number of particles are 
compared Fourteen benchmark functions being in seven types with different 
circumstance are used in the experiments The experiments show that the 
searching ability of the algorithms is related to the number of particles in clus
tering which is better with the number of particles transforming from as little as 
possible to as much as possible in each swarm when the function dimension is 
increasing from less to more Finally the original algorithm and THLPSO are 
compared to illustrate the efficiency of the proposed method 
Keywords Particle swarm optimization hierarchy cluster 
1 
Introduction  
Particle swarm optimization PSO algorithm is based on the evolutionary computation 
technique It has been used increasingly as a novel technique for solving complex 
optimization problems Many researchers have expanded on the original ideas of PSO 
and some improving approaches such as the idea of hierarchy and cluster have been 
reported In 1 a dynamically changing branching degree of the tree topology was 
proposed for solving intractable large parameter optimization problems In 2 the 
particles have been clustered in each iteration and the centroid of the cluster was used 
as an attractor instead of using the position of a single individual In 3 the population 
was partitioned into several subswarms each of which was made to evolve based on 
the PSO In 4 the PSO approach that used an adaptive variable population size and 
periodic partial increasing or declining individuals in the form of ladder function was 
proposed In 5 a twolayer particle swarm optimization TLPSO was proposed for 
unconstrained optimization problems 
                                                           
  This paper is supported by the National Natural Science Foundation of China No 61062005 
 
The Comparative Study of Different Number of Particles in Clustering 
81 
 
To study the influence on the ability to search function optimization among the 
configurations of the different number of particles in clustering some previous work 
6 based on a twolayer particle swarm optimization have been done It has come to a 
conclusion that a good efficiency of searching ability is related to the number of par
ticles transforming from as little as possible to as much as possible in each swarm when 
the function dimension is increasing from less to more In order to further study the 
affection degree among different configurations of the different number of particles in 
clustering a threelayer particle swarm optimization THLPSO is proposed for the 
comparison Through the experiment it confirms the accuracy of the results of 6 and 
deeper summarizes the conclusion To illustrate the efficiency of THLPSO the original 
algorithm is used to compare with the proposed method in the end 
The rest of this article is organized as follows Section 2 describes the main idea and 
the basic process of THLPSO Section 3 presents ten configurations of the different 
number of particles in clustering and seven classifications for the functions In section 
4 ten configurations of the different number of particles in clustering are compared in 
the benchmark functions existing in the seven types then the following passages is the 
contrasting between the original algorithm and THLPSO Finally section 5 draws 
conclusions about the comparison among the ten configurations of the different number 
of particles testing in the seven types 
2 
ThreeLayer Particle Swarm OptimizationTHLPSO 
In the THLPSO approach there will be three layers of the structure bottom layer 
middle layer and top layer M swarms of particles B swarms of particles and one swarm 
of particles are generated in the three layers respectively In the initial population M 
swarms of N particle

12

12
bxdi
d
M i
N
=
=


 are randomly generated in the bottom 
layer where 
di
bx  is the position of the ith particle in the dth swarm of the bottom layer 
According to the fitness contrasting among the particles in the dth swarm of the bottom 
layer the global best location of the dth swarm

1 2
d
ym
d
M
=

 is determined Then the 
global best location of each swarm of the bottom layer is set to be in the middle layer 
that is to say the number of particles in the middle layer has been determined which is 
M In the middle layer B swarms of F particle 

12

12
hk
ym
h
B k
F
=
=

  are also ran
domly generated where 
hk
m
y  is the position of the kth particle in the hth swarm of the 
middle layer According to the fitness contrasting among the particles in the hth swarm 
of the middle layer the global best location of the hth swarm

1 2
tzh
h
B
=

 is deter
mined Then the global best location of each swarm of the middle layer is set to be in 
the top layer that is to say the number of particles in the top layer has also been de
termined which is B Therefore the global best location of the swarm in the top layer 
will be determined according to the fitness contrasting among B particles in the top 
layer Furthermore two mutation operations are added into the particles of each swarm 
in the bottom layer and middle layer respectively Consequently the diversity of the 
population in the THLPSO increases so that the THLPSO has the ability to avoid 
trapping into a local optimum The structure of the proposed THLPSO is shown in 
Fig1 
82 
G Huang et al 
 
 
Fig 1 The structure of the THLPSO 
3 
Classification and Analysis 
31 
Ten Configurations of the Number of Particles in Clustering  
and Computation Cost Analysis  
In this experiment the total number of particles is set to be one hundred and twenty the 
number of swarms in the bottom layer M is set to be sixty From the principle analysis 
of THLPSO in section 2 there will be sixty particles in the middle layer which are 
chosen from each swarm with the global best location in the bottom layer respectively 
Ten different classifications about the sixty particles in the middle layer are shown in 
Table 1 where B means the total number of swarms in the middle layer and F stands for 
the average number of particles in clustering 
Table 1 The number of particles in clustering 
B 
2 
3 
4 
5 
6 
10 
12 
15 
20 
30 
F 
30 
20 
15 
12 
10 
6 
5 
4 
3 
2 
 
Suppose the computation cost of one particle in the algorithms is c then the total 
computation cost of the algorithms for one generation is MNc+Mc+Bc where MNc 
stands for the computation cost of the bottom layer and Mc stands for the computation 
cost of the middle layer and Bc stands for the computation cost of the top layer 
Therefore the computation cost is increasing along with the increasing of the number 
of swarms in the middle layer In other words it corresponds to the decreasing of the 
number of particles in clustering  
32 
Function Classification and Experiment Parameters Configuration 
In 7 fourteen benchmark functions 
1
1 4
f
− f
 which were chosen for their variety 
were generated and divided into three types Functions 
1
3
f
− f
 were simple unimodal 
problems 
4
9
f
− f
 were highly complex multimodal problems with many local  
 
The Comparative Study of Different Number of Particles in Clustering 
83 
 
minima and 
1 0
1 4
f
− f
 were multimodal problems with few local minima In this 
section they are classified with the different dimensions in detail shown as follows 
• type 1 Unimodal function in two dimensions  
• type 2 Unimodal function in ten dimensions  
• type 3 Unimodal function in one hundred dimensions  
• type 4 Highly complex multimodal function with many local minima in two di
mensions  
• type 5 Highly complex multimodal function with many local minima in ten di
mensions  
• type 6 Highly complex multimodal function with many local minima in one hun
dred dimensions  
• type 7 Multimodal function with few local minima in two or four dimensions 
4 
Experiments   
41 
Experimental Results Based on THLPSO 
In this section the seven types of the functions are employed to examine the efficiency 
of searching function optimization with the ten configurations of the different number 
of particles shown in Table 1 respectively The simulation results about the ten con
figurations of the different number of particles in clustering testing in the seven types 
are shown in Fig2 In the figure the horizontal coordinate stands for the ten configu
rations of the different number of particles in clustering and the vertical coordinate 
stands for the average value of all the global optimum values produced in each cycle 
operation except for the maximum and the minimum values And the figure of the left 
edge is the mark of the types from type 1 to type 7 
In Fig2 type 1 type 4 and type 7 reveal the same results that the better searching 
ability is corresponding to the less number of particles in clustering in two or four 
dimensions and type6 reveals the opposite that the more number of particles in clus
tering in one hundred dimensions shows better efficiency Combined with type 2 type 
3 type 5 and the results of 6 it comes to a conclusion that different types of functions 
will go through three stages of change along with the dimension of the functions 
transforming from less to more The three stages of change are listed as follow 
• Stage 1 The less number of particles in clustering the better searching ability for the 
function  
• Stage 2 The searching ability is better with the number of the particles transforming 
from as little as possible to as much as possible in clustering 
• Stage 3 The more number of particles in clustering the better searching ability for 
the function 
42 
The Contrast between Original Algorithm and THLPSO 
Suppose the computation cost of one particle in the algorithms is c then the total 
computation cost of THLPSO for one generation is MNc+Mc+Bc which has been 
84 
G Huang et al 
 
analysed above On the other hand the original algorithm OPSO has no the middle 
layer and top layer of the particles so that the computation cost for one generation is 
MNc Therefore the computation cost of the THLPSO is greater than that of the OPSO 
by Mc+Bc However the THLPSO spends Mc+Bc computation time on the movement 
of the particles in the middle layer and top layer for the global search In order to 
illustrate the above results Table 2 compares our results the minimum average value 
among the ten configurations of the proposed THLPSO approach with the results of 
the OPSO for the benchmark functions 
14
1
f − f
 with three different dimensions re
spectively In Table 2 D means the dimensions A means the algorithms and F means 
the functions all the values represents the average of all the global optimum values 
produced in each iteration 
 
Fig 2 Ten configurations of the number of particles compared from type 1 to type 7 
 
The Comparative Study of Different Number of Particles in Clustering 
85 
 
Table 2 Comparison the Results between THLPSO and OPSO 
D 
D=2 or D=4 
D=10 
D=100 
A 
F 
OPSO 
THLPSO 
OPSO 
THLPSO 
OPSO 
THLPSO 
1f  
02769 
00385 
1156134 
11289 
322E+03 
00922 
2f  
02898 
00521 
46743 
02496 
311E+04 
00404 
3f  
13763 
02357 
52231 
12576 
2802511 
19668 
4f  
8099008 
8378077 
364e+003 
419e+003 
269e+004 
409e+004 
5f  
03345 
01809 
82818 
11524 
3668465 
345E13 
6f  
09323 
02638 
55007 
13133 
6631 
00186 
7f  
01345 
00689 
10029 
0115 
299999 
00567 
8f  
04475 
03228 
52959 
0304 
739E+05 
00102 
9f  
00913 
00709 
116456 
01617 
471E+05 
00043 
10f  
00156 
00104 
 
 
 
 
11f  
02023 
01746 
 
 
 
 
12f  
18778 
24379 
 
 
 
 
13f  
19577 
24978 
 
 
 
 
14f  
20649 
25444 
 
 
 
 
From the comparison it can be seen that the accuracy of THLPSO are much better 
than OPSO in different dimensions with the same configuration parameters It reveals 
that the OPSO might lead to the earlier convergence so that the result of the OPSO is 
trapped into the local optimum solution owing to the lack of swarm’s diversity On the 
other hand the proposed THLPSO has more diversity such that it is hard to be trapped 
into the local optimum owing to having more searching choices for the particle swarm 
Therefore the THLPSO structure with three layers is necessary 
5 
Conclusion 
In this article ten configurations of the different number of particles in clustering have 
been compared in the fourteen benchmark functions existing in different circumstance 
respectively According to the simulation results of the seven types it can reach three 
general conclusions listed as follows 
• The less number of particles in clustering the better searching ability for the func
tion in less dimensions 
• The searching ability is better with the number of the particles transforming from as 
little as possible to as much as possible in clustering when the function dimension is 
increasing from less to more 
• The more number of particles in clustering the better searching ability for the 
function in more dimensions 
86 
G Huang et al 
 
References 
1 
Janson S Middendorf M A Hierarchical Particle Swarm Optimizer and Its Adaptive 
Variant IEEE Transactions on Systems Man and CyberneticsPart B Cybernetics 35 
1272–1282 2005 
2 
Kennedy J Stereotyping Improving Particle Swarm Performance with Cluster Analysis 
In Proceedings of the 2000 Congress on Evolutionary Computation vol 2 pp 1507–1512 
2000 
3 
Jiang Y Hu T Huang CC Wu X An Improved Particle Swarm Optimization algo
rithm App Math Comp 193 231–239 2007 
4 
Chen DB Zhao CX Particle Swarm Optimization with Adaptive Population Size and Its 
Application App Soft Comp 9 39–48 2009 
5 
Chen CC Twolayer Particle Swarm Optimization for Unconstrained Optimization 
Problems App Soft Comp 11 295–304 2011 
6 
Huang G Shi X An Z The Comparative Study of Different Number of Particles in 
Clustering Based on TwoLayer Particle Swarm Optimization In Tan Y Shi Y Ji Z 
eds ICSI 2012 Part I LNCS vol 7331 pp 109–115 Springer Heidelberg 2012 
7 
Bratton D Kennedy J Defining a Standard for Particle Swarm Optimization In Pro
ceedings of the 2007 IEEE Swarm Intelligence Symposium pp 120–127 IEEE Press 
Honolulu 2007 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 87–94 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Implementation of Mutual Localization of Multirobot 
Using Particle Filter 
Yang Weon Lee 
Department of Information and Communication Engineering 
Honam University Seobongdong Gwangsangu Gwangju 506714 South Korea 
ywleehonamackr 
Abstract This paper describes an implementation of mutual localization of 
swarm robot using particle filter Robots determine the location of the other ro
bots using wireless sensors Measured data will be used for determination of the 
robot itself moving method It also effects on the other robots formation such as 
circle and line type formation We discuss the problem in circle formation en
closing target which moves This method is the solution about enclosed invader 
in circle formation based on mutual localization of multirobot without infra
structure We use trilateration which does not need to know the value of the 
coordinates of reference points So specify enclosed point for the number of 
robots based on between the relative positions of the robot in the coordinate 
system Particle filter is used to improve the accuracy of the robots location 
The particle filter is well operated for mutual location of robots than any other 
estimation algorithm Through the experiments we show that the proposed 
scheme is stable and works well in real environments  
Keywords swarm robot particle filter tracking 
1 
Introduction 
Swarm robotics is an approach to robotics that emphasizes many simple robots in
stead of a single complex robot A robot swarm has much in common with an ant 
colony or swarm of bees No individual in the group is very intelligent or complex 
but combined they can perform difficult tasks Swarm robotics has been an experi
mental field but many practical applications have been proposed A traditional robot 
often needs complex components and significant computer processing power to ac
complish its assigned tasks In swarm robotics each robot is relatively simple and 
inexpensive As a group these simple machines cooperate to perform advanced tasks 
that otherwise would require a more powerful more expensive robot Using many 
simple robots has other advantages as well Robot swarms have high fault tolerance 
meaning that they still will perform well if some of the individual units malfunction 
or are destroyed Swarms also are scalable so the size of the swarm can be increased 
or decreased as needed  
In this paper we present a particle filter localization system for cooperative mul
tiple robots in an environment where they can move automatically as a group and 
extract position information in a given map of the environment from stationary  
88 
YW Lee 
landmarks For assessment of the system we consider a scenario where some robots 
cannot detect landmarks In this case uncertainty in position estimates for all robots is 
expected to increase We suppose that each mobile robot in the environment not only 
can detect and calculate the distance to other robots but also can share its position 
information with other robots Consequently each mobile robot acts as an additional 
mobile landmark However it can introduce some amount of error to the position 
information from stationary landmarks and localization could become unstable Still 
we desire selflocalization performed by each robot to be improved by extra position 
information from other robots   
2 
Localization Algorithm 
Localization is the process of finding both position and orientation of a vehicle in a 
given referential system 20 Drumheller in 12 Navigation of mobile robots in
doors usually requires accurate and reliable methods of localization Many transporta
tion systems now using wireguided automated vehicles may benefit from the in
creased layout design flexibility provided by a wirefree localization method such as 
triangulation with active beacons  
21 
Triangulation 
Triangulation is based on the measurement of the bearings of the robot relatively to 
beacons placed in known positions It differs from trilateration which is based on the 
measurement of the distances between the robot and the beacons These beacons are 
also called landmarks by some authors According to 3 the term beacon is more 
appropriate for triangulation methods When navigating on a plane three distinguish
able beacons  at least  are required for the robot to localize itself Fig1 ߣଵଶ is the 
oriented angle “seen” by the robot between beacons 1 and 2 It defines an arc between 
these beacons which is a set of possible positions of the robot An additional arc be
tween beacons 1 and 3 is defined by ߣଷଵ The robot is in the intersection of the two 
arcs Usually the use of more than three beacons results in redundancy 
22 
Trilateration 
Trilateration is a method to determine the position of an object based on simultaneous 
range measurements from three stations located at known sites This is a common 
operation not only in robot localization 789 but also in kinematics aeronautics 
crystallography and computer graphics It can be trivially expressed as the problem 
of finding the intersection of three spheres that is finding the solutions to the follow
ing system of quadratic equations 
ሺݔ െݔଵሻଶ ൅ ሺݕ െݕଵሻଶ ൅ ሺݖ െݖଵሻଶ = ݈ଵ
ଶ
ሺݔ െݔଶሻଶ ൅ ሺݕ െݕଶሻଶ ൅ ሺݖ െݖଶሻଶ = ݈ଶ
ଶ
ሺݔ െݔଷሻଶ ൅ ሺݕ െݕଷሻଶ ൅ ሺݖ െݖଷሻଶ = ݈ଷ
ଶ
                  1 
where ݔଵݔଶݔଷ are the coordinates of station  ݅ and ݈௜  is the range measurement 
associated with it 
 
Implementation of Mutual Localization of Multirobot Using Particle Filter 
89 
 
Fig 1 Threeobject triangulation 
 
Fig 2 Threeobject triangulation 
In Figure 2 thick segments between stations define the base plane and thin ones 
those connecting the moving object and the stations correspond to the range mea
surements 
23 
Bounding Box 
Bounding Box Method uses a simple boxshaped ranging area shown in Figure 3 
which can be specified by lower left extreme coordinates ሺݔ௟ݕ௟ሻ and upper right 
extreme coordinates ሺݔ௨ݕ௨ሻ Figure 5 shows an example where node i is bounded 
by the ranging area of its neighbors nodes 1 2 and 3 Thus the bounding area for the 
node ݅ can be specified as follows 
ሺݔ௟ሺ݅ሻݕ௟ሺ݅ሻሻ = ሺmaxሺݔ௟ሺ݆ሻ|݆ ∈ ܰሺ݅ሻሻ  max ሺݕ௟ሺ݆ሻ|݆ ∈ ܰሺ݅ሻሻሻ
ሺݔ௨ሺ݅ሻݕ௨ሺ݅ሻሻ = ሺmaxሺݔ௨ሺ݆ሻ|݆ ∈ ܰሺ݅ሻሻ  max ሺݕ௨ሺ݆ሻ|݆ ∈ ܰሺ݅ሻሻሻ      
2
Bounding Box Method can be implemented as a distributed algorithm Simic and 
Sastry 9 suggest anchors broadcast their respective position estimates periodically 
and nodes broadcast any changes in their estimates upon reception of any broadcast 
 
90 
YW Lee 
 
Fig 3 Bounding box 
3 
Condensation Algorithm 
31 
Particle Filter Algorithms 
The particle filter approach to track motion also known as the condensation algorithm 
4 and Monte Carlo localization uses a large number of particles to ’explore’ the state 
space Each particle represents a hypothesized target location in state space Initially the 
particles are uniformly randomly distributed across the state space and each subsequent 
frame the algorithm cycles through the steps illustrated  as  follows  
1 Deterministic drift particles are moved according to a deterministic motion model 
a damped constant velocity motion model was used 
2 Update probability density function PDF Determine the probability for 
every new particle location 
3 Resample particles 90 percent of the particles are resampled with replacement 
such that the probability of choosing a particular sample is equal to the PDF at that 
point the remaining 10 percent of particles are distributed randomly throughout the 
state space 
4 Diffuse particles particles are moved a small distance in state space under Brow
nian motion 
This result in particles congregating in regions of high probability and dispersing 
from other regions thus the particle density indicates the most likely target states 
See3 for a comprehensive discussion of this method The key strengths of the par
ticle filter approach to localization and tracking are its scalability computational re
quirement varies linearly with the number of particles and its ability to deal with 
multiple hypotheses and thus more readily recover from tracking errorsHowever 
the particle filter was applied here for several additional reasons 



1
1
u
u
y
x



2
2
u
u
y
x



3
3
u
u
y
x



1
1
l y l
x



3
3
l
l
y
x



2
2
l
l
y
x
 
Implementation of Mutual Localization of Multirobot Using Particle Filter 
91 
− It provides an efficient means of searching for a target in a multidimensional state 
space 
− Reduces the search problem to a verification problem ie is a given hypothesis 
facelike according to the sensor information 
− Allows fusion of cues running at different frequencies 
32 
Application of Particle Filter for Multirobots 
In order to apply the particle filter algorithm to hand motion recognition we extend 
the methods described by Black and Jepson 1011 Specifically a state at time ݐ is 
described as a parameter vector ݏ௧ = ሺµ ߶௜  ߙ௜ߩ௜ሻwhere µ is the integer index of 
the predictive model ߶௜indicates the current position in the model ߙ௜refers to An 
amplitude scaling factor and ߩ௜ is a scale factor in the time dimension 
1 Initialization 
The sample set is initialized with N samples distributed over possible starting states 
and each assigned a weight ofభ
ಿ Specifically the initial parameters are picked un
iformly according to 
ߤ∈ ሾ1ߤ௠௔௫ሿ
߶௜ =
ଵି√௬
√௬ ݕ∈ ሾ01ሿ
ߙ௜= ሾߙ௠௜௡ߙ௠௔௫ሿ
ߩ௜∈ ሾߩ௠௜௡ߩ௠௔௫ሿے
ۑ
ۑ
ۑ
ې
     3
2 Prediction 
In the prediction step each parameter of a randomly sampled ݏ௧is used to ݐ൅1 de
termine based on the parameters of that particular ݏ௧ Each old state ݏ௧ is randomly 
chosen from the sample set based on the weight of each sample That is the weight 
of each sample determines the probability of its being chosen This is done efficiently 
by creating a cumulative probability table choosing a uniform random number on 0 
1 and then using binary search to pull out a sample14 The following equations are 
used to choose the new state 
ߤ௧ାଵ =ߤ௧
߶௧ାଵ
௜
= ߶௧
௜ ൅ߩ௧
௜ ൅ ܰሺߪథሻ
ߙ௧ାଵ
௜
=ߙ௧
௜ ൅ ܰሺߪఈሻ
ߩ௧ାଵ =ߩ௧
௜ ൅ ܰሺߪఘሻے
ۑ
ۑ
ۑ
ې
   4
whereNሺߪכሻ refers to a number chosen randomly according to the normal distribution 
with standard deviation ߪכ This adds an element of uncertainty to each prediction 
which keeps the sample set diffuse enough to deal with noisy data For a given drawn 
sample predictions are generated until all of the parameters are within the accepted 
range If after a set number of attempts it is still impossible to generate a valid pre
diction a new sample is created according to the initialization procedure above 
 
92 
YW Lee 
3 Updating 
After the Prediction step above there exists a new set of N predicted samples which 
need to be assigned weights The weight of each sample is a measure of its likelihood 
given the observed data ܼ௧ = ሺݖ௧ݖ௧ଵ ⋯ ሻ We define ܼ௧௜ = ൫ݖ௧௜ݖሺ௧ିଵሻ௜ ⋯ ൯ as a 
sequence of observations for the ݅௧௛ coefficient over time specifically let 
ܼሺ௧ଵሻ ܼሺ௧ଶሻ ܼሺ௧ଷሻ ܼሺ௧ସሻ be the sequence of observations of the horizontal velocity of 
the left robot the vertical velocity of the left robot the horizontal velocity of the right 
robot and the vertical velocity of the right robot respectively Extending Black and 
Jepson 5 we then calculate the weight by the following equation 
pሺݖ௧|ݏ௧ሻ = ∏
݌൫ܼ௧௜|ݏ௧൯
ସ
௜ୀଵ
   
5
p൫ݖ௧௜|ݏ௧൯ =
ଵ
√ଶగ ݁ݔ݌
ି∑
ቀ௭ሺ೟షೕሻ೔ିఈכ௠ሺഝషഐכೕሻ೔
ഋ
ቁ
ഘషభ
ೕసబ
ଶሺఠିଵሻ
   
6
where ω is the size of a temporal window that spans back in time Note that ߶כߙכ 
and ߩכ refer to the appropriate parameters of the model for the blob in question and 
that ߙכ݉ሺథכିఘכ௝ሻ௜
ሺఓሻ
 refers to the value given to the ݅௧௛ coefficient of the model µ 
interpolated at time ߶כ െߩכ݆ and scaled by ߙכ 
4 
Experiment Result 
To test the proposed particle filter scheme we used MATLAB and visual studio 
MATLAB is used for simulation of particle filter and visual studio is used to calculate 
the mutual localization of intruder and robot Through experiment we confirmed that 
accuracy of robot localization using particle filter is more than localization of using 
only sensor information Therefore it is necessary to use particle filter to localize the 
robot when there is no more information except triangular measurement data This 
information is shown in Figure 7 and 8 
Also we evaluate the algorithm of intruder enclosed formation using trilateration 
First of all we assume that initially there are 6 robot and continue experiment alterna
tively swapped each robots position and intruders position Figure 9 and 10 is 
shown each robot make a circle formation to enclose intruder 
 
Fig 4 Result 1 of experiment by particle filter 
 
Implementation of Mutual Localization of Multirobot Using Particle Filter 
93 
5 
Conclusions 
In this paper we have developed the particle filter for the swarm robot localization 
and circle formation enclosing target which moves It is method about enclosed in
vader in circle formation based on mutual localization of swarm robot without infra
structure Therefore we use trilateration that do not need to know the value of the 
coordinated of reference points So we specify the enclosed point for the number of 
robots base on between the relative positions of the robot in the coordinate system 
This scheme is important in providing a computationally feasible alternative to classi
fy the robot motion in real system We have proved that given an environment par
ticle filter scheme classify the robot location in real time 
 
 
Fig 5 Result 2 of experiment by particle filter 
References 
1 Huang DS Jia W Zhang D Palmprint Verification Based on Principal Lines Pattern 
Recognition 414 1316–1328 2008 
2 Huang DS Lawken K Ip H Chi Z Zeroing Polynomials Using Modified Con
strained Neural Network Approach IEEE Trans on Neural Networks 163 721–732 
2005 
3 Huang DS Ip H Chi Z A Neural Root Finder of Polynomials Based on Root Mo
ments Neural Computation 168 1721–1762 2004 
4 Huang DS A Constructive Approach for Finding Arbitary Roots of Polyminals by Neur
al Networks IEEE Trans on Neural Networks 152 477–491 2004 
5 Huang DS Radial Basis Probabilistic Neural Networks Model and Application Interna
tional Journal of Pattern Recognition and Artificial Intelligence 137 1083–1101 1999 
6 Huang DS The Local Minima Free Condition of Feedforward Neural Networks for Out
er Supervised Learning IEEE Trans on Systems Man and Cybernetics 28B3 477–480 
1998 
94 
YW Lee 
7 Yeo TK Hong S Jeon BH Latest Tendency of Underwater multirobots Institute of 
Control Robotics and Systems 161 23–34 2010 
8 Arai T Pagello E Parker LE Editorial Advances in MultiRobot Systems IEEE 
Transactions on Robotics and Automation 185 2002 
9 Isard M Blake A CONDENSATIONconditional Density Propagation for Visual 
Tracking International Journal of Computer Vision 291 5–28 1998 
10 Lee YW Adaptive Data Association for Multitarget Tracking Using Relaxation In 
Huang DS Zhang XP Huang GB eds ICIC 2005 Part I LNCS vol 3644 pp 
552–561 Springer Heidelberg 2005 
11 Lee YW Seo JH Lee JG A Study on The TWS Tracking Filter for MultiTarget 
Tracking Journal of KIEE 414 411–421 2004 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 95–102 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Optimization of Orthogonal Poly Phase Coding 
Waveform Based on Bees Algorithm and Artificial Bee 
Colony for MIMO Radar 
Milad Malekzadeh1 Alireza Khosravi1 Saeed Alighale1 
 and Hamed Azami2 
1 Department of Electrical and Computer Engineering  
Babol Industrial Univsity Iran  
2 Department of Electrical Engineering Iran University of Science and Technology Iran 
mmalekzadestunitacir akhosravinitacir 
 saeedalighalegmailcom hamedazamiieeeorg 
Abstract Multi input multi output MIMO radars have multiple antenna radars 
that each of them can transmit signals For that matter to avoid interference 
transmitted waveform must be mutually orthogonal This paper has proposed a 
new approach by using bee algorithm BA and artificial bee colony ABC to 
design orthogonal discrete frequency coding waveforms DFCWs which have 
desirable autocorrelation and cross correlation characteristic for orthogonal 
MIMO radars The results represent a various ability of these algorithms The 
cross correlation and auto correlation are better designed by the BA and ABC 
respectively 
Keywords Bee algorithm artificial bee colony artificial bee colony poly 
phase and MIMO radars 
1 
Introduction 
Multi input multi output radar MIMO is a great achievement in communication 
science in two past decades The MIMO radar is a multiple antenna radar system 
which is capable of transmitting arbitrary waveform from each antenna element  
Multiple transmitting antennas transmit orthogonal signals and multiple receiving 
antennas receive returns 1 
The MIMO radar technology has rapidly drawn considerable attention from many 
researchers Several advantages of the MIMO radar have been discovered by many 
different researchers such as increased diversity of the target information excellent 
interference rejection capability improved parameter identify ability and enhanced 
flexibility for transmit beam pattern design However the ability to detect low speed 
target on the background of clutter and the detection ability of weak target on the 
background of strong clutter causes that MIMO radar performance to be considered in 
different projects 2 
96 
M Malekzadeh et al 
According to the MIMO radars’ structure there are different transmitted signal so   
receiving the transmitted signal without interference is the most important purpose 
Solving this problem the transmitted signals must be mutually orthogonal 3 
The MIMO radar uses L orthogonal waveforms that are transmitted from different 
phase centers and N receiving phase centers The received signals are matched filtered 
for each of the transmitted waveforms forming NL channels This assumption denotes 
the necessity of low cross correlation properties between waveforms In addition to 
have high resolution for multiple target detection the low autocorrelation sidelobe 
peaks levels for transmitted signals is required 4 
Assume a MIMO radar system that compose of L transmitters where each    
transmits a distinct signal from an orthogonal code set 

 
123
ls t l
L
=
 in which 
any two signals in the set are uncorrelated and the aperiodic autocorrelation function 
of any code 
 
ls t  should be close to an impulse function 
The capability of being achieved of MIMO radars relates to the feasibility of a set 
of orthogonal signals with low autocorrelation and crosscorrelation properties As a 
consequence the acceptable design of such orthogonal code sets is very important for 
putting into effective MIMO radar systems 5 MIMO radar systems can be coded 
with binary sequences polyphase sequences or frequencyhopped sequences Poly
phase code has some advantages over binary code as might be expected polyphase 
code is increasingly becoming a desirable alternative for radar signals 6 
In this paper we intend to demonstrate impressive algorithms bee algorithm BA 
and artificial bee colony ABC for the designing of orthogonal ployphase code sets 
that can be used in MIMO radars The remainder of this paper is organized as follows 
in the second section the problem of the polyphase code set design will be presented 
Then we will introduce two kinds of swarm algorithms to numerically optimize poly
phase code sets and the results from designing are presented Finally some           
conclusions are drawn in last section 
2 
Orthogonal Polyphase Signal Design for MIMO 
Consider that orthogonal polyphase code comprise L signals with each signal    con
taining N subpulses represented by a complex number sequence the signal set can be 
shown as follows 3 
 
 

12

12
j l
n
ls t
e
n
N l
L
ϕ
=
=
=
 
1
where 
 0
 
2 
l
l
n
n
ϕ
ϕ
π
≤
≤
 is the phase of subpulse n of signal L in the signal set 
Assume a polyphase code set s with code length of N set size of L one can      
concisely represent the phase values of s with the following L×N phase matrix 
1
1
1
2
2
2
1
2

 
1
2

 
 

1
2

 
L
L
L
n
n
s L N
n
ϕ
ϕ
ϕ
ϕ
ϕ
ϕ
ϕ
ϕ
ϕ






= 






 
2 
 Optimization of Orthogonal Poly Phase Coding Waveform Based on Bees Algorithm 
97 
where the phase sequence in row l 1≤ l ≤ L is the polyphase sequence of signal l 
and all the elements in the matrix can only be chosen from the phase set in 2 
From the autocorrelation and cross correlation characteristic of orthogonal          
polyphase codes we get 
1
1
1
exp 
 


0 0
  
12
1
exp 
 


0
0
N k
l
l
n
l
N
l
l
n
k
j
n
n
k
k
N
N
A
k
l
L
j
n
n
k
N
k
N
ϕ
ϕ
ϕ
ϕ
ϕ
−
=
=− +

−
+
=




=
=



−
+
=
−





 
3
1
1
1
exp   

 0 0
 
 
12
1
exp   

 0
0
N k
q
p
n
p
q
N
q
p
n
k
j
n
n k
k
N
N
C
k
l
L
j
n
n k
N k
N
ϕ
ϕ
ϕ ϕ
ϕ
ϕ
−
=
=− +

−
+
=
 


=
=



−
+
=
−
 



 
4
where 

l  
A
ϕ k
and 


 
p
q
C
k
ϕ ϕ
 are the aperiodic autocorrelation function of       
polyphase sequence 
ls  and the crosscorrelation function of sequences 
ps  and 
qs  
and k is the discrete time index Therefore designing an orthogonal polyphase code 
set is equivalent to the constructing a polyphase matrix in 2 with 

l  
A
ϕ k
and 


 
p
q
C
k
ϕ ϕ
constraints in 3 and 4 
For the design of orthogonal polyphase code sets used in MIMO radar systems an 
optimization criterion is not only to minimize the autocorrelation sidelobe peak ASP 
and the cross correlation peaks CP but also minimize the total autocorrelation   
sidelobe energy and crosscorrelation energy in 3 and 4 The peak and energy based 
cost function to be used for MIMO radar signals design is as follows 7 
2
2
1
1
1
1

1
1
1
1
1

max
  


 
L
L
L
N
N
l
p
q
K
k
N
l
p
q
p
E
A
k
C
k
μ
φ
μ
φ φ
−
−
−
=
=−
−
=
=
= +
=
−
+

  
 
5
3 
Evolutionary Algorithms for DFCW 
In past decades engineers have concentrated to present heuristic methods to solve 
optimization problems In this way they have tried to inspire from nature and from 
this view they success to achieve different evolutionary algorithms We purpose to 
present the BA and ABC in this paper At the first step the ABC algorithm is pre
sented 8 
98 
M Malekzadeh et al 
31 
Artificial Bee Colony Algorithm Optimization of DFCW 
ABC is one of the most recently defined algorithms proposed by Karaboga and Bas
tutk in 2006 is inspired by the intelligent behavior of honeybees In the ABC algo
rithm the colony of artificial bees contains three groups of bees employed onlookers 
and scouts The performance of this algorithm has been shown at 9 steps as follows 
• Initialize the population of solutions 
• Evaluate the population 
• Produce new solutions for the employed bees 
• Apply the greedy selection process 
• Calculate the probability values 
• Produce the new solutions for the onlookers 
• Apply the greedy selection process 
• Determine the abandoned solution for the scout and  replace it with a new ran
domly produced  solution 
• Memorize the best solution achieved so far 
In this algorithm first half of the colonies are selected as the employed artificial bees 
and the rest of them are chosen as the onlookers For every food source there is only 
one employed bee In other words the number of employed bees is equal to the num
ber of food sources around the hive Scout is an employed bee whose food source has 
been abandoned Generally the ABC algorithm consists of local and global searches 
to find an optimum answer in desired space In this algorithm the position of a food 
source represents a possible solution to the optimization problem and the   nectar 
amount of a food source corresponds to the quality or fitness of the associated solu
tion The number of the employed or the onlooker bees is equal to the number of solu
tions in the population An onlooker bee assesses the information of the nectar taken 
from all employed bees and selects a food source with a probability pi related to its 
fitness value 8 
1
i
i
SN
n
n
fit
p
fit
=
=

 
6
where SN and fiti are the size of population and the fitness value of the solution i 
which is proportional to the nectar amount of the food source in the position i respec
tively In order to produce a candidate food position from the old one in memory the 
ABC uses the following expression 


kj
ij
ij
ij
ij
x
x
x
v
−
+
=
φ
 
7
where 
ij
φ denotes a random number and selected between 11 and k ∈1 2 SN 
and j∈12D are randomly selected indexes Each solution xi i = 1 2 N is 
 Optimization of Orthogonal Poly Phase Coding Waveform Based on Bees Algorithm 
99 
represented by a Ddimensional vector where D denotes the number of parameters to 
be optimized and each parameter is real coded
 
4 
Bees Algorithm Optimization for DFCW 
The BA which imitates the food foraging behavior of honey bee colony is a novel 
swarmbased search algorithm developed by DT Pham 9 This algorithm is based 
on a kind of neighborhood search combined with random search and can be used for 
multiobjective optimization The performance of this algorithm has been shown at 8 
steps as follows 
• Initialize population with random solutions 
• Evaluate fitness of the population 
• While stopping criterion not met Forming new population                                 
• Select sites for neighbourhood search 
• Recruit bees for selected sites more bees for best e sites and evaluate fitnesses 
• Select the fittest bee from each patch 
• Assign remaining bees to search randomly and evaluate their fitnesses 
• End While 
The optimization of DFCW with Bee algorithm is summarized as follows 
At the first step the initial phase are produced by random solution Then according 
to the equation 5 we evaluate the fitness function In order to minimize the fitness 
function It is sorted in descending form Now the m sites and best sites e out of m 
are selected with respect to fitness The recruited bees investigate the selected sites 
New values are produced with this equation 
 
  

2

  11
u i
x i
ngh
ngh
rand size x i
=
−
+
∗
⋅∗
 
8
According to new value the new fitness function would be derived After comparing 
this fitness with old fitness and repeating this cycle the BA processing will be  
done 10 
5 
Simulation Results 
In this section according to described optimization algorithm we can design many 
different lengths of sequences However in this paper we will present only the     
correlation properties of sequences of the three code sequences with length N=128 
and L=3 In the Figures 1 and 2 the results of ASPs and CPs for these sequences are 
shown for the BA and ABC respectively Also Tables 1 and 2 list the three code  
sequences with length N=128 and L=3 for optimizing by BA and ABC respectively 
As can be seen in Tables 1 and 2 to design the CPs applying of the BA is better than 
the ABC while for designing the ASPs applying of the ABC is better than BA  
100 
M Malekzadeh et al 
150
100
50
0
50
100
150
0
001
002
003
004
005
006
007
008
009
01
code1code2
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig 1 Autocorrelation and crosscorrelation functions of sequences with code length N=128 
and set size L=3 the BA results 
Table 1 ASP and CP of the designed DFCW set with N= 128 and L=3 
 
CODE 1 
CODE 2 
CODE3 
CODE1 
03835 
00561 
00649 
CODE2 
00561 
03224 
00573 
CODE3 
00649 
00573 
03032 
150
100
50
0
50
100
150
0
001
002
003
004
005
006
007
008
009
01
code1code3
150
100
50
0
50
100
150
0
001
002
003
004
005
006
007
008
009
01
code2code3
150
100
50
0
50
100
150
0
01
02
03
04
05
06
07
08
09
1
code1
150
100
50
0
50
100
150
0
01
02
03
04
05
06
07
08
09
1
code2
150
100
50
0
50
100
150
0
01
02
03
04
05
06
07
08
09
1
code3
 Optimization of Orthogonal Poly Phase Coding Waveform Based on Bees Algorithm 
101 
 
 
 
Fig 2 Autocorrelation and crosscorrelation functions of sequences with code length N=128 
and set size L=3 the ABC results 
Table 2 ASP and CP of the designed DFCW set with N=128 and L=3 
 
CODE 1 
CODE 2 
CODE3 
CODE1 
01254 
01896 
01565 
CODE2 
01896 
01227 
01676 
CODE3 
01565 
01676 
01657 
102 
M Malekzadeh et al 
6 
Conclusion 
The MIMO radars can track the target from different angles and it is very important to 
diagnose objects in space In order to attain desirable properties the transmitted signal 
must be orthogonal This paper has presented an effective method to design          
orthogonal code for MIMO radars by using two evolutionary algorithms namely BA 
and ABC These approaches have tried to demonstrate desirable ASPs and CPs The 
simulation results show the different ability of these algorithms To design the     
obtained CPs by the BA is better than the ABC while for designing the ASPs      
applying of the ABC is better than the BA It should be mentioned that both of them 
are attractive optimization algorithm to solve this problem 
References 
1 Fishler E Haimovich A Blum R Cimini L Chizhik D Valenzuela R MIMO Ra
dar an Idea Whose Time Has Come In Proc of the IEEE Int Conf on Radar Philadel
phia PA April 2004 
2 Fishler E Haimowich A Blum R Cimini L Chizhik D Valenzuela R Statistical 
MIMO Radar In 12th Conf on Adaptive Sensor Array Processing 2004 
3 Deng H Polyphase Code Design for Orthogonal Netted Radar Systems IEEE Transac
tions on Signal Processing 52 3126–3135 2004 
4 Deng H Synthesis of Binary Sequences with Good Autocorrelation and Crosscorrelation 
Properties by Simulated Annealing IEEE Trans Aerosp Electron Syst 32 98–107 
1996 
5 Grandjean E Linear Time Algorithms and NPcomplete Problems SIAM J Com
put 233 573–597 1994 
6 Liu B He Z Zeng J Liu B Polyphase Orthogonal Code Design for MIMO Radar 
Systems In International Conference on Radar CIE 2006 pp 16–19 2006  
7 Liu B He Z He Q Optimization of Orthogonal Discrete Frequencycoding Waveform 
Based on Modified Genetic Algorithm for MIMO Radar In Communications Circuits 
and Systems ICCCAS 2007 
8 Karaboga D Bastürk B A Powerful and Efficient Algorithm for Numerical Function 
Optimization Artificial Bee Colony ABC Algorithm J Global Optim 393 459–471 
2007 
9 Pham DT Ghanbarzadeh A Koc E Otri S Rahim S Zaidi M The Bees Algo
rithm a Novel Tool for Complex Optimization Problems In International Virtual Confe
rence on Intelligent Production Machines and Systems pp 454–459 2006 
10 Pham DT Ghanbarzadeh A Multiobjective Optimization Using the Bees Algorithm 
In International Virtual Conference on Intelligent Production Machines and Systems 
Scotland 2007 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 103–111 2012 
© SpringerVerlag Berlin Heidelberg 2012 
SVM Regularizer Models on RKHS vs on Rm 
Yinli  Dong1 and Shuisheng Zhou2  
1 Foundation Department Xian Eurasia University PR China 701165 
2 School of Science Xidian University PR China 710071 
sszhoumailxidianeducn 
Abstract There are two types of regularizer for SVM The most popular one is 
that the classification function is normregularized on a Reproduced Kernel 
Hilbert SpaceRKHS and another important model is generalized support vec
tor machineGSVM in which the coefficients of the classification function is 
normregularized on a Euclidean space Rm In this paper we analyze the differ
ence between them on computing stability computational complexity and the 
efficiency of the Newtontype algorithms Many typical loss functions are  
considered The results show that the model of GSVM has more advantages 
than the other model Some experiments support our analysis 
Keywords representer theorem regularizer newtontype algorithm 
1 
Introduction  
Based on the Vapnik and Chervonenkis’ structural risk minimization principle12 
SVMs are proposed as powerful machine learning methods for supervised learning 
They are popular methods in the past 10 more years and widely used in classification 
and regression problems such as character identification disease diagnoses face 
recognition the time series prediction etc Historically the optimal classification 
function obtained by SVMs has an offset But the investigations of the generalization 
performance do not suggest that the offset offers any improvement for a large feature 
kernel space like Gaussian kernel23 For simplicity here we study the SVMs with
out offset for the nonlinear classification problems with Gaussian kernel Actually the 
offset can be considered with an extra attribute 1 added to every sample4 
11 
Representer Theory  
Representer theorem is an important tool for the learning model with many samples 
as the inputs of the problem It states that the learning solution is a linear combination 
of the kernel functions of input samples It can transform the high or infinite dimen
sional learning problem into finite dimensional problems with the size of the input 
learning data where the finite combination coefficients are solved according to the 
input data Many kernel learning problems admit this such as SVMs1 PCA5 etc 
                                                           
  This work is supported by NNSFC under Grant No 61072144 61179040 61173089 and 
11101322  
 Corresponding author 
104 
Y Dong and S Zhou 
 
Quantitatively speaking given an input set T=x1 y1… xm ym for samples 
xi∈Rd and labels yi∈−1 1 a kernel learning classification problem is to learn a 
classification function f in a reproduced kernel Hilbert spacesRKHS Ħ correspond
ing to a kernel function kRm×Rm→R with a good generalized capacity RKHS has the 
reproducing property that admits fx=fk·xĦ and k·x k·zĦ=kxz for f∈Ħ 
and x z∈Rm Because of high dimensions the learning problem on Ħ can not be 
solved efficiently By the representer theory the solution f∈Ħ can be represented as 


 =1
⋅
=
m
j
j
j
x
k
f
α
 
1
This is a finite linear combination of the basic hypotheses in Ħ and the finite op
timal combination coefficients are solved to represent the optimal hypothesis in Ħ 
12 
Two Regularized Models for SVMs  
There are two regularization forms for SVMs One model regularizes the classifica
tion function on Ħ and is converted into a finite dimensional problem by representer 
theorem467 or duality1810 Another regularizes the combination coefficients 
on Rm which is called Generalized Support Vector MachinesGSVMs1113 
Regularization Model on RKHS With the hypothesis f normregularized in Ħ a 
popular model is defined as 
M1  


 =
∈

⋅

−
+
m
i
H
i
i
H
f H
x
f k
y
L
f
1
2
2



1
min λ
 
2
with the regularized parameter λ and the loss LR→R+ Plugging 1 into 2 we have 





=
=
∈
−
+
m
i
m
j
j
i
j
i
i j
j
i
j
i
R
k x x
y
L
x x
k
m
1
1

2

 
1

 
min
α
α α
λ
α
 
3
Solving problem 3 we obtain m combination coefficients to represent the learning 
result of M1 as 1 no matter how high the dimension of RKHS is 
Another way to convert 2 into a finite dimensional problem is the duality for a 
given loss 1810 We can derive a general form by conjugate duality14 




=
∈
−
+
m
i
i
i
i j
j
i
j
i
j
i
R
L
y y k x x
m
1


2
 

 
max
γ
γ
γ γ
λ
γ
 
4
where LR→R+∪+∞ is the conjugate function of L the formulation 3 is always 
more complicated than 4 which explains why so many researchers focus on the dual 
418915 etc However the dual has m dimensional even the reduced representa
tion as Eq 5 is used to obtain an approximate solution 471216 


 ∈
⋅
=
j J
i
i
x
k
f
α
 
5
where J is random chosen12 16 with |J|≤01m or wellchosen4 with less  
cardinal Training the reduced SVMs by Newton methods in dual space is not a good 
choice 
Regularization Model on Rm While the combinations coefficients noted as β for 
the hypothesis f in 1 are normregularized on Rm the model called GSVMs sloves 
 
SVM Regularizer Models on RKHS vs on Rm 
105 
 
M2    




=
=
∈
−
+
m
i
m
j
j
i
j
i
R
k x x
y
L
m
1
1
2
2

 
1
min
β
β
λ
β
 
6
It is first proposed by Mangasarian11 and is used to design algorithms with squared 
hinge loss12 and with least square loss13 Here β may be different from α in 3 
Relationship of Two Models On the one hand Mangasarian11 points out that the 
dual of 6 can meet the same form with the dual of 2 for a wellchosen gβ instead 
of ||β||2 On the other hand from 2 and 6 we observe that the difference of two 
models is on the first item of objective function which is called the regularizer The 
regularizer of the former is on L2 norm of hypothesis f in RKHS Ħ and the regulariz
er of the latter is on L2 norm of the combination coefficients in Euclidean space Rm 
The rest of the paper is organized as follows We analyze Newtontype algorithms 
for the reduced SVMs with different smooth loss in Section 2 give experimental re
sults in Section 3 and conclude the paper in Section 4 
2 
Newton Methods for Reduced Models with Different Losses  
Let f in 1 be approximated as 5 with J is wellchosen or random chosen and r=|J| 
be the reduced set size For M1 and M2 the reduced form of 3 or 6 is 


 =
∈
−
+
m
i
iJ
i
R
y K z
L
Az
z
m
1
2 T
1
min λ
α
 
7
where z can be αJ or βJ and A is KJJ for M1 and I for M2 The only difference between 
them is the regularizer αJ
TKJJαJ versus βJ
TβJ  Commonly a kernel matrix K∈Rm×m has 
σ1≥1≥σm≥0 where σ1σm is the largestsmallest eigenvalue of K We have 
κλI+Q≤κλK+Q where κ· is the condition number of a matrix and Q is a semi
positive definite matrix induced by the second part of 7 Roughly speaking we have 
Proposition1 The model of M2 is stabler than the model of M1 
The most popular and meaningful loss function is the hinge loss function 
Lu=max0 u but it is not differentiable and Newtontype methods do not work for 
it Some losses are adopted to smooth it including least squared loss10 13 squared 
hinge loss4 15 Huber loss1 7 17 and logistic loss12 Next those two regula
rizer problems with different loss functions are compared by Newtontype algorithms 
on performance of the classification and efficiency of the algorithms 
21 
Setup of NewtonType Algorithms  
Given a smooth loss in 7 Newtontype algorithms18 have quadratic convergence 
rate Specifically for a given ztcan be αt
J or βt
J  at iteration t let ξi
t=1−yiKiJzt and 
It=i∈M |Lξi
t0 and z  be the solution to Newton equations 





∈
∈
∈
∇
−
∇
=
∇
+
t
t
t
i I
t
i
t
i I
t
i
i I
it
L
z
L
z
L
A






2
2
ξ
ξ
ξ
λ
 
8
 
 
106 
Y Dong and S Zhou 
 
If the full Newton step is acceptable then zt+1= z  otherwise zt+1=τ z +1−τzt where τ 
is chosen by linesearch scheme Experiments show the full step is always acceptable 
and Armijo linesearch scheme is pretty well while the full step fails 
22 
Specification of Algorithms with Smooth Loss Functions  
Specification forms of 8 are analyzed according to different smooth loss including 
least squared loss squared hinge loss Huber loss and logistic loss as follows 
Least Squared Loss With least squared loss Lu=u2 Eq 8 is independent to zt and 
its solution is obtained by solving the following system of linear equations 9  


y
K
z
K
K
A
MJ
MJ
MJ
=
+
T
λ
 
9
The coefficients matrix in 9 for M2 is always positive definite while the coefficient 
matrix in 9 for M2 may be semipositive definite need a ridge4 added 
Squared Hinge Loss With squared hinge loss Lu=max0u2 Eq 8 is 


t
t
t
t
I
I J
I J
I J
y
K
z
K
K
A
=
+
T
λ
 
10
In 4 they designed a complicated procedure to iteratively update the Cholesky fac
torization of 
I J
I J
K t K t
A
T
λ +
 to reduce the computational complexity for solving 10 
For simplicity in this paper we use 
J
I
J
I
J
I
J
I
t
t
out
out
in
in
K
K
K
K
B
B
T
T
1
−
+
=
−
 to update Bt 
iteratively as 15 then solve Newton equation 10 by “” operator in Matlab where 
Iin=i|i∈It i∉It−1 Iout=i|i∉It i∈It−1 and |I 
t||Iin|+|Iout| always holds This scheme 
works well while the reduced set is randomly chosen in advanced 
Huber loss With Huber loss 



≤
+

=
δ
δ
δ
δ
δ
|
|
  4

|
 |
max 0


2
u
u
u
u
L u
 8 is simplified as 


t
t
t
t
t
t
I
T
I J
I
T
I J
I J
T
I J
y
K
y
K
z
K
K
A
+
+
+
=
+
+
0
0
0
0
2
1
2
1
δ
δ
δ
λ
 
11
where I0
t=i∈M| |ξt
i|δ I+
t=i∈M| |ξt
i|≥δ At the beginning of the algorithm we 
should start the algorithm with a big δ0 like δ0=1 reduce it by δk=01δk−1 while the 
current solution is good under some criterions such as ||gt||≤1 and repeat the algo
rithm until δk≤10−4 and the final stop criterions are reached 
Logistical loss With Logistical loss Lpu=maxu0+log1+exp−pup 8 is  




0
0
0
0
0
0
t
I
p
T
I J
t
I J
t
I
T
I J
I J
t
I
T
I J
L
K
z
K
K
z
K
K
A
+
+
′
+
Λ
=
Λ
+
ξ
λ
 
12
where I+=i∈M|L′
pξi
t≥υ and I0=i∈M| L′′
pξi
t ≥υ Λt=diagL′′
pξ1
t ··· L′′
pξm
t for 
a tiny number υ=10−10 or less In order to make the Newton method working good we 
should set p not very large such as p=10 at the beginning then set p=10p if ||g|| is 
small and repeat the algorithm until p=104 and ||g||≤ε for a given ε 
23 
SMW Identity and Advantage of GSVM 
SMW identity18 A+UTΛU−1=A−1−A−1UTΛ−1+UA−1UT−1UA−1 can be used to re
duce the computational complexity for calculating A+UTΛU−1 while A−1 is very 
simple and Λ−1+UA−1UT has a small size Based on the analysis above Σi∈It L′′ξi
t  
 
 
SVM Regularizer Models on RKHS vs on Rm 
107 
 
always has the form UTΛU in Eq 8 where U has the size of r×|It| and Λ is an |It|×|It| 
diagonal matrix If A=I for GSVMs the solution of Eq 8 is 
z=λb−λUTλΛ−1+UUT−1Ub 
13
where b is the right hand side of 8 So while |It|r which always happens for some 
problems that have a sparse solution the solution to Eq 8 is obtained by 13 with 
the computation complexity Or|It|2 less than Or3 This trick is not valid if A=KJJ 
for M1 As a conclusion we have 
Proposition 2  If |It|r Newtontype algorithm based on M2 has less computational 
complexity than the same algorithm based on M1 
3 
Experimental Results  
In this section we do two sets of experiments on some datasets to compare the reduced 
models with Newtontype methods The first is a nonlinearly dataset called “tried and 
true” checkerboard1215 and the second is on practical datasets from site19 
31 
Artificial Data with Reduced Methods  
In this section we give some experiments on an artificial dataset to compare the per
formance of two kinds of regularization The “tried and true” checkerboard datasets 
have often been used to show the effectiveness of nonlinear kernel methods1215 It 
is generated by uniformly discretized the region 0199×0199 to 2002=40000 
points and is labeled two classes spaced by 4×4 grids The training set is random 
sampled from the total points and the reminders are left in the testing set  
OverFitting of M1 with Least Square Loss Here we show that the resulted classi
fication hyperplanes based on M1 with least squared loss have strange phenomena but 
the hyperplanes based on M2 haven’t The training results are plotted in Fig 1 It 
shows that the classification lines of two methods are similar but the high confidence 
areasatisfying yifxi≥1 is very different For M2right the high confidence area is 
normal ie the more central the grid the higher the confidence and the exception 
happens only on the four corners But for M1left the high confidence area is strange 
on nearly every grid ie the central of the grids are not always in the high confidence 
area These phenomena should be a kind of over fitting 
M1 
20
40
60
80
100
120
140
160
180
200
20
40
60
80
100
120
140
160
180
200
             
M2
 
20
40
60
80
100
120
140
160
180
200
20
40
60
80
100
120
140
160
180
200
 
Fig 1 Plots with least square lossm=8000 r=75m λ=001 The blank lines are the classifi
cation fx=0 and the bluegreen lines are support lines to fx=±1 respectively 
108 
Y Dong and S Zhou 
 
Comparison M1 and M2 with Different Smooth Losses The results in Table 1 are 
obtained by Newtontype algorithm with same stop criterion and λ=001 where the 
averaged test errors averaged training time averaged iterations of Newton step and 
numbers of the low confidence training samples that satisfying yifxi1 are listed with 
standard deviation All the results are mean values on 20 random trials 
Table 1 Comparison of two regularizations with different smooth loss functionsλ=001 
rithms 
m=2000 
m=4000 
m=8000 
m=12000 
m=20000 
 
r=150 
r=300 
r=600 
r=800 
r=800 
 
Test error on the rest data with test 40000−m samples 
M1+SH 
152±023 
070±011 
023±006 
011±004 
006±003 
M2+SH 
127±026 
036±007 
010±003 
006±002 
004±002 
M1+Log 
192±023 
097±013 
035±007 
022±005 
016±005 
M2+Log 
165±025 
048±011 
018±005 
010±003 
007±003 
M1+Hub 
192±023 
097±013 
035±007 
022±005 
016±005 
M2+Hub 
165±025 
048±011 018±005 
010±003 
007±003 
 
Training times 
M1+SH 
013±000 
070±002 
357±040 
750±022 
1198±025 
M2+SH 
011±008 
057±003 
289±006 
638±017 
1067±032 
M1+Log 
067±012 
230±045 
1127±129 
2327±457 
3198±363 
M2+Log 
032±006 
148±021 
604±047 
1327±165 
2473±321 
M1+Hub 
051±002 
210±011 
983±086 
1977±111 
2652±137 
M2+Hub 
046±005 
210±042 
775±071 
1436±105 
2149±228 
 
Iterations of Newton step 
M1+SH 
109±10 
141±11 
157±07 
169±07 
191±07 
M2+SH 
104±08 
117±07 
163±11 
206±16 
234±20 
M1+Log 
980±232 
762±130 
840±133 
848±229 
773±131 
M2+Log 
658±121 
789±107 
945±120 
918±140 
973±188 
M1+Hub 
1128±53 
1107±83 
1133±95 
1175±60 
1173±86 
M2+Hub 
1857±193 
2213±320 
2656±312 
2684±203 
2435±282 
 
Lowconfidence training samples Number yifxi1 
M1+SH 
2238±101 
3519±94   
5924±167 
8420±140 
12779±178 
M2+SH 
1985±130 
2706±85      3183±85 
3276±95 
4266±108 
M1+Log 
1298±95 
2239±60 
3726±100    4636±89 
6160±75 
M2+Log 
1232±102 
1428±78      1566±77 
1763±78 
2432±77 
M1+Hub 
1303±97 
2243±57 
3726±100    4636±86 
6162±74 
M2+Hub 
1232±102 
1429±79 
1566±78 
1763±77 
2432±77 
 
From the results in Table 1 we can get the following conclusions  
• It is observed that M2 nearly wins most all aspects Firstly it is clear that the test 
errors corresponding to M2 are always better than the results corresponding to M1 
Secondly on the training time aspect M2 excels M1 much for every loss functions 
although the iterations of Newton step corresponding to M2 are often longer than 
that of M1 This is coherent with our conclusion in Proposition 2 Thirdly for M2 
often has less lowconfidence training samples than M1 has  
• Compared three types of loss functions the squared hinge loss gets the best gene
ralization errors No matter how large the training set there are a few itera
tionsless than 2logm of Newton steps needed  
 
SVM Regularizer Models on RKHS vs on Rm 
109 
 
• The advantages of the logistical loss and Huber loss are that they always get less 
lowconfidence training samplesor called support vectors than others Especially 
for M2 there always have a small number of lowconfidence training samples  
 
By Table 1 we can conclude that M2 has some advantages over M1 With the same 
parameter settings the algorithms based on M2 always faster than those based on M1 
The former is stabler in computing than the latter who often needs a ridge on the Hes
sian matrix4 to keep the Newton direction welldefined  
From those experimental results M2 with squared hinge loss is the best model to 
train reduced SVM with Newton method In Section 32 we only do some experi
ments to compare M1 and M2 equipped with square hinge loss 
32 
Benchmark Data Experiments Comparison  
Six practical data sets from UCI repository of machine learning databases19 are 
adopted to evaluate the related algorithms The first five data are preprocessed in 20 
but we exchange the training and the testing in order to get a large training set to im
plement the reduced technique The sixth data is the Adult with a large training set 
For simplicity Gaussian kernel function kx y=exp−γ||x−y||2 with different spread 
parameters γ is used for all dataset Parameters γ and λ are roughly chosen by grid 
search method with γ∈2−10… 22 and λ∈10−5… 100 The test errors and the 
training time are listed in Table 2 
Table 2 Experiments on 6 data sets from UCI repository of machine learning databases 
DataSet 
Banana 
Ringnorm Splice 
Twonorm 
Waveform Adult 
train test 4900 400 7000 400 2175 100 7000 400 4600 400 30162 15060 
Red size 
368 
525 
163 
525 
345 
800 
γ λ 
2−1 10−3 2−610−1 
2−810−3 
2−810−1 
2−510−1 
2−410−3 
 
Error 
M1+SH 
923±123  166±064 1271±124 219±063 850±154  1550±003 
M2+SH 
925±128 170±067  1267±121 217±063 829±143 1556±004 
 
Training Times 
M1+SH 
219±017 377±018 028±003 370±013 186±012  6545±771 
M2+SH 
164±018 317±010 020±002 320±009 168±013 2259±284 
 
From the results in Table 2 the mean test errors are varied with the different me
thods with a little the difference At the same time it clearly sees that algorithms 
based on M2 are always faster than those based on another This is mainly because its 
Hessian matrix has a better condition number than those of M1 In our experiments a 
ridge is added to the Hessian matrix of M1 to maintain the calculation stability 
4 
Conclusion  
There are two main regularization models of SVMs One is normregularized the 
classification function in a reproduced kernel Hilbert space The other is  
110 
Y Dong and S Zhou 
 
normregularized the coefficients of the classification function in a Euclidean space 
All of them are converted to m dimension problems by representer theorem 
We compare M1 and M2 with the reduced methods by Newtontype algorithm The 
Hessian matrix of M2 is always positive and is simper than the Hessian matrix of M1 
and the SMW identity can be used to reduce the computation complexity of the cor
responding algorithms for M2 but cannot be used in algorithm for M1 Our analysis 
and the experimental results support that M2 has some advantages over M1 such as 
simple in computing the Hessian matrix stable in solving the Newton direction and 
the algorithm based on M2 is also faster and stabler than the algorithm based on M1 
with the similar test errors This work is valuable to extend the using of GSVMs  
References  
1 Vapnik VN The Nature of Statistical Learning Theory Springer NY 2000 
2 Steinwart I Christmann A Support Vector Machines Springer 2008 
3 Steinwart I Sparseness of Support Vector Machines JMLR 4 1071–1105 2003 
4 Keerthi SS Chapelle O Decoste D Building Support Vector Machines with reduced 
classifier complexity JMLR 7 1493–1515 2006 
5 Schölkopf B Smola A Müller KR Nonlinear component analysis as a kernel eigen
value problem Neural Comput 105 1299–1319 1998 
6 Schölkopf B Herbrich R Smola AJ A Generalized Representer Theorem In Helm
bold DP Williamson B eds COLT 2001 and EuroCOLT 2001 LNCS LNAI 
vol 2111 pp 416–426 Springer Heidelberg 2001 
7 Chapelle O Training a Support Vector Machine in the primal Neural Computa
tion 195 1155–1178 2007 
8 Joachims T Support Vector Machine 1998 
httpwwwcscornelledupeopletjsvmlight 
9 Platt JC Fast training of Support Vector Machines using Sequential Minimal Optimiza
tion In Schölkopf B et al eds Advances in Kernel MethodSupport Vector Learning 
pp 185–208 MIT Cambridge 1999 
10 Suykens J Vandewalle J Least Square Support Vector Machine Classifiers Neural 
Processing Letters 93 293–300 1999 
11 Mangasarian OL Generalized Support Vector Machine In Smola AJ et al eds Ad
vances in Large Margin Classifiers pp 135–146 MIT Press 2000 
12 Lee Y Mangasarian OL RSVM Reduced Support Vector Machines Data Mining In
stitute Computer Sciences Department University of Wisconsin pp 1–7 2001 
13 Fung G Mangasarian OL Proximal Support Vector Machine classifiers In Provost 
Srikant R eds Proceedings KDD 2001 Knowledge Discovery and Data Mining San 
Francisco CA August 2629 pp 77–86 ACM New York 2001 
14 Boyd SP Vandenberghe L Convex Optimization 7th edn Cambridge University 
Press Cambridge 2009 
15 Zhou S Liu H Zhou L Ye F Semismooth Newton Support Vector Machine Pattern 
Recognition Letters 28 2054–2062 2007 
16 Lin KM Lin CJ A Study on Reduced Support Vector Machines IEEE Trans on 
Neural Networks 146 1449–1459 2003 
 
 
SVM Regularizer Models on RKHS vs on Rm 
111 
 
17 Ye F Liu H Zhou S Liu S A Smoothing Trustregion NewtonCG method for Mi
nimax Problem Applied Mathematics and Computation 1992 581–589 2008 
18 Golub GH Loan CFV Matrix Computations The John Hopkins University Press 
Baltimore 1996 
19 Blake CL Merz CJ UCI Repository of Machine Learning Databases 1998 
httpwwwicsuciedumlearnMLRepositoryhtml  
20 Rätsch G Benchmark Repository 2003 
httpusersrsiseanueduauraetschdataindexhtml 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 112–119 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Research on Performance Comprehensive Evaluation  
of Thermal Power Plant under LowCarbon Economy 
Xing Zhang 
Department of Economy Management North China Electric Power University  
Baoding 071003 Hebei China 
zx83316126com 
Abstract A performance evaluation index system of thermal power plant is 
established under low carbon economy and a comprehensive evaluation model 
based on principal component analysis PCA support vector machine SVM 
and quick sort algorithm is presented Then experiments are made by using the 
real data from 17 thermal power plants and the sequence of them is obtained 
ultimately The results show that the model proposed has high accuracy and 
comparing with BP network SVM shows better performance in the condition of 
few data  
Keywords performance evaluation thermal power plant PCA SVM binary 
tree quick sort algorithm 
1 
Introduction 
Low carbon and energy conservation has become a hot topic around the world since 
Copenhagen conference Low carbon economy is the direction of economic 
development in China Thermal power plant as an industry which consumes large 
amounts of energy in low efficiency and damages the environment seriously inevitably 
becomes the focus Thermal power plant should transform its development mode to low 
carbon and energy saving How to evaluating the performance of Thermal power plant 
synthetically under low carbon economy becomes an important topic 
SVM is a new and promising machine learning technique proposed by Vapnik 1 
which can solve the nonlinear small sample and high dimensional problems 
effectively It has been attracting more and more researcher’s attention in recent years 
because of its good generation performance In this paper SVM is introduced to 
compare two thermal power plants by vector connection and then we obtain the 
sequence by means of quick sort algorithm Experiment results show that the model has 
high accuracy hence it is effective to evaluate the performance of thermal power plant 
2 
Performance Evaluation Index System under LowCarbon 
Economy 
According to the characteristic of thermal power plant and the requirement of low 
carbon economy this paper designs the performance evaluation index system from 
economic performance environmental performance and social performance 23 
 
Research on Performance Comprehensive Evaluation of Thermal Power Plant 
113 
 
Table 1 Performance evaluation index system under lowcarbon economy 
The first level 
The second level 
The third level 
Economic 
performance 
 
Profit ability 
Return on total asset S1 
Return on net asset S2 
Return on low carbon asset S3 
Debt paying ability 
Debt Asset ratio S4 
Interest Protection Multiples S5 
Turnover ability 
Total Assets Turnover S6 
Current Assets Turnover S7 
Development 
ability 
Rate of sales growth S8 
Rate of Capital Accumulation S9 
Environmental 
performance 
Pollutants 
discharge 
CO2 emission per unit power S10 
SO2 emission per unit power S11 
NOX emission per unit power S12 
Effluent emission per unit power S13 
Smoke emission per unit power S14 
Noise at the boundary of plant S15 
Resource 
utilization efficiency 
Standard coal consumption rate S16 
Water consumption per unit power S17 
Plant water loss rate S18 
Plant power consumption rate S19 
Industrial water recycling rate S20 
 
Social 
performance 
Customer 
satisfaction 
Contract compliance rate S21 
Annual number of complaints S22 
Staff satisfaction 
Education funding Per capita S23 
Staff retention rate S24 
3 
Principal Component Analysis 
Principal component analysis which is also known as KarhunenLoeve KL 
transform Principal component analysis reorganizes a large number of originally 
indexes into a small amount of comprehensive indexes by linear transformation 
Thereby it simplifies the data and reveals the relationship between variables 4 The 
main process of PCA is as following 
1 Convert the original matrix Y into standardized matrix Z 
ij
j
ij
j
y
y
z
s
−
=

12

i
n
=

 
12

j
p
=

 
 1
where
1
1
n
j
ij
j
y
y
n
=
= 
 


2
1
1
1
n
j
ij
j
i
s
y
y
n
=
=
−
− 
 
2 Obtain the correlation matrix R of the standardized matrix Z 
1
T
ij
p p
Z Z
R
r
n
×


=
=


−
  
12

i j
p
=

   
 2
114 
X Zhang 
 
where 
1
1
n
ki
kj
k
ij
z z
r
n
= =
−

  
3 Compute the Eigen values
j
λ and the corresponding eigenvectors 
jb of the 
covariance matrix R respectively The Eigen values 
j
λ are just the nonnegative real 
roots of
0
p
R
− λI
=
 order them from large to small
1
2
0
p
λ
λ
λ
≥
≥
≥
≥

 then 
the corresponding eigenvectors 
jb  can be obtained by
j
j
j
Rb
= λ b
  
4 Determine the principal components  p new variables are composed of the 
eigenvectors as follows 
1
11 1
12
2
1
2
21 1
22
2
2
1 1
2
2
p
p
p
p
p
p
p
pp
p
A
b z
b z
b z
A
b z
b z
b z
A
b z
b z
b z
=
+
+
+


=
+
+
+



=
+
+
+





 
3
The first m principal components are selected as principal components when their 
accumulative contributive rate  
1
1
085
m
j
j
p
m
j
j
w
λ
λ
=
=
=
≥


   
4
4 
Support Vector Machine 
SVM is the theory based on statistical learning theory It realizes the theory of VC 
dimension and principle of structural risk minimum 5 The following is the brief 
introduction of SVM 
Given the training set in the case of linear separation 



1
 1


l 
l
x y
x y

 
n
x
∈ R


1 1
y∈ + −
 there exists a separating hyper plane with the target functions 
0
w x
b
⋅
+
=
  w represents the weight vector and b the bias The linear SVM for 
optimal separating hyper plane has the following optimization problem 
 




1
2
 
1
1
i
i
Min
w
w w
s t
y w x
b
i
l
φ
=
⋅
⋅
+
≥
= 
   
5
 
Research on Performance Comprehensive Evaluation of Thermal Power Plant 
115 
 
Considering the nonseparable case slack variable 
iξ ≥ 0
 is introduced and soft 
margin optimal hyper plane is constructed 
 
 




1
1
2
 
1
l
i
i
i
i
i
Min
w
w w
C
s t
y w x
b
φ
ξ
ξ
=
=
⋅
+
⋅
+
≥ −

   6
C ≥ 0
 is the error penalty coefficient chosen by users 
The solution to above optimization problem can be converted into its dual problem 
We can search the nonnegative Lagrange multipliers by solving the following 
optimization problem 6 
 
1

1
1
1
2
 
00
l
l
T
i
i
j
i
j
i
j
i
i j
l
i
i
i
i
Max
Q
y y x x
s t
y
C
α
α
α α
α
α
=
=
=
=
−
=
≤
≤



  
7
As to the nonlinear case SVM maps input vectors x  into a high dimensional linear 
feature space by a nonlinear mapping  
ϕ   and searches the optimal separating hyper 
plane 
 
0
w
x
b
⋅ϕ
−
=
 in the space SVM use kernel function


i 
K x xj
of input 
space instead of the operation  


i
j
x
x
ϕ
⋅ϕ
of high dimensional feature space Then 
the target function of 6 is changed to be 
 


1

1
1
2
l
l
i
i
j
i
j
i
j
i
i j
Max
Q
y y K x x
α
α
α α
=
=
=
−
⋅


 
 8
It has been proved that only the Lagrange coefficient of support vectors are not zero 
that is only support vectors can influence the result of classification So the optimal 
weight vectors are 
i
i
i
w
y
α x
= 
SV
  
9
The optimal biases are  
 


1
1
1
2
b
w
x
w
x
∗
∗


=
⋅
+
⋅
−

   
10
where 
x∗  1
is one of the support vectors in the first class and 

x∗ − 1
 is one of the 
support vectors in the second class 
Hence the classification function is 78 
 
 


sgn
i
i
i
f x
y
K x x
b
α


=
⋅
+





SV
  
11
116 
X Zhang 
 
5 
Performance Comprehensive Evaluation Model 
Comprehensive evaluation usually has two kinds of problems one is classification the 
other is ranking This paper reduces the performance evaluation of thermal power plant 
to a ranking problem The main process of the model is as follows 
1 PCA preprocessing  
Because of a large number of performance evaluation indexes the input dates of SVM 
are too complex it is difficult to get good result in rapidity and accuracy This paper 
introduces PCA into SVM at first the sample set is preprocessed by PCA and several 
principal components are obtained instead of multiple original indexes The 
preprocessed sample set is used to train and test SVM 
2 Vector Connection 
SVM is established for classification In order to introduce SVM to the model vector 
connection is necessary Given two thermal power plants A and B the eigenvector of A 
is

1
2
5



A
A
A
x
x
 x
and the eigenvector of B is

1
2
5



B
B
B
x
x
 x
 connect the 
eigenvectors of A and B

1
1
2
2
5
5






A
B
A
B
A
B
x
x
x
x
x
x

 and use it as the input 
vector of SVM the output has three classes higher lower and equivalent  
3 Establish twolayer SVM classifier based on SVM and binary tree 
The standard SVM is only used to classify two classes but the output of the model have 
three classes so a twolayer SVM classifier which combines SVM and binary tree is 
established For the first SVM if A is higher than B the output is +1 otherwise the 
output is –1 and then the second SVM if A is equivalent to B the output is +1 
otherwise the output is –1 It is illustrated by Fig 2 
 
 
 
 
 
 
 
 
 
 
Fig 1 Twolayer SVM classifier 
4 Sort by quick sort algorithm  
Quick sort algorithm is a recursive algorithm based on division Given array S  
v
∈S
is chosen as pivot and 
 
S
− v
 is divided into two incompatible sets 
 


1
|
S
x
S
v
x
v
=
∈ −
≤
and
 


2
|
S
x
S
v
x
v
=
∈
−
≥
 then repeat respectively the 
process for
1S and
2
S until the array is ranked The Time complexity of quick sort 
SVM1 
SVM2 
Equivalent、Lower1
Higher +1 
Equivalent +1
Lower 1
 
Research on Performance Comprehensive Evaluation of Thermal Power Plant 
117 
 
algorithm is


O N log
N  Quick sort algorithm is considered as one of the best 
sorting algorithms 
6 
Experiments 
61 
Date Preprocessing by PCA 
The credit data of 17 thermal power plants are obtained in this study SPSS115 is 
utilized to simplify the index system The accumulative contribution rate of the former 
five principal components can reach 85 that is the 24 indexes are reduced to five 
indexes 
Table 2 Contribution rates of principal components 
Principal components 
Contribution rate 
Accumulative contribution rate 
A1 
64478 
64478 
A2 
8219 
72697 
A3 
6277 
78974 
A4
5327 
84301 
A5 
4108 
88409 
62 
TwoLayer SVM Training and Testing 
We randomly separate the dataset preprocessed by PCA into two parts The data of 10 
thermal power plants are used as training set and then we obtain 90 input vectors by 
vector connection The data of 7 thermal power plants are used as testing set 
Training set is used to train the two layer SVM classifier and LIBSVM Version 
26 is utilized in this paper RBF function 
2
2
  
exp
2

k x y
x
y
δ
=
−
−
 is 
used as the kernel function δ is the breadth of RBF function and its optimal value is 
0685313 C is the error penalty coefficient and its optimal value is 90517457 
Testing set is tested by twolayer SVM classifier and its main process is as follows 
Suppose 7 thermal power plants P1 P2 P3 P4 P5 P6 P7 P4 is chosen as pivot and the 
rest is divided into two parts compare P4 with the first part  P1 P2 P3 and the second 
part P5 P6 P7 respectively that is input the vector connection of Pi  i =123567 
and P4 to twolayer SVM classifier and adjust the position of Pi according to the output 
to make sure that the left of P4 are all lower than P4 and the right of P4 are all higher 
than P4 repeat the process for the two parts until all thermal power plants are ranked 
In order to verify the effectiveness of twolayer SVM classifier BP network is also 
used to assess the same data The neuron number of input layer interlayer and output 
118 
X Zhang 
 
layer is 3 6and 2 and sigmoid function is used as active function Table 3 is the 
comparison of the results by using twolayer SVM classifier and BP network 
Table 3 Comparison of ranking results 
Number 
Original 
Rank 
SVM  
BP  
P1 
P2 
P3 
P4 
P5 
P6 
P7 
Accuracy 
1 
3 
6 
7 
5 
2 
4 
— 
1 
3 
6 
7 
5 
2 
4 
100 
2 
3 
6 
7 
4 
1 
5 
7142 
 
From Table 3 we can see that the accuracy of twolayer SVM classifier is higher 
than BP network which because the theory of BP and SVM is different SVM is 
established with principle of structural risk minimum while BP is established with 
empirical risk minimum In the condition of few data SVM use few support vectors to 
represent the whole sample set and has good generalization performance while BP has 
poor generalization performance because of over fitting Only trained by sufficient data 
can BP network overcome the problem however this is difficult to realize so SVM has 
greater use value than BP in practice 
7 
Conclusions 
A comprehensive index system of thermal power plant performance evaluation is 
established under low carbon economy and a synthetic evaluation model based on 
PCA SVM and quick sort algorithm is proposed The method shows two merits in the 
research ①After dimension reduction by PCA the training set is reduced the training 
time is shorten and the prediction accuracy is improved ② SVM as the latter 
processor has good generalization performance in the condition of small sample 
Experiment results show that the model has great effectiveness for performance 
evaluation 
References 
1 
Vladimir NV Zhang XG Nature of Statistics Theory Tsinghua University Press 
Beijing 2000 
2 
Yao XY Li Y Wen Q Lowcarbon Economy Evaluation Index System of Thermal 
Power Industry Journal of Ningxia University Natural Science Edition 12 389–392 
2010 
3 
Li J Research on Performance Evaluation Index System for ThermoPower Enterprises 
Based on Sustainable Development Journal of Beijing Polytechnic College 1 114–118 
2010 
 
Research on Performance Comprehensive Evaluation of Thermal Power Plant 
119 
 
4 
Yang KR Meng FR Liang ZZ Adaptively Weighted PCA Algorithm Computer 
Engineering and Applications 3 189–191 2012 
5 
Wang Y Yang JA Liu H Geng Q A SVM Incremental Learning Algorithm Based on 
Inner Hull Vectors Journal of Circuits and Systems 6 109–113 2011 
6 
Ding SF Sun JG Chen DL Li Y Jiang XL Improved SVM Decisiontree and Its 
Application in Remote Sensing Classification Application Research of Computers 3 
1146–1148 2012 
7 
Feng GH Parameter Optimizing for Support Vector Machines Classification Computer 
Engineering and Applications 3 123–124 2011 
8 
Zhang ZZ Dong CL Chen ZZ He XL Improved Fast Classifier Based on SVM and 
Density Clustering Computer Engineering and Applications 2 136–138 2011 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 120–127 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Computing the Minimum λCover in Weighted  
Sequences 
Hui Zhang1 Qing Guo2 and Costas S Iliopoulos3 
1 College of Computer Science and Technology 
Zhejiang University of Technology Hangzhou 310023 China 
zhanghzjuteducn 
2 College of Computer Science Zhejiang University Hangzhou 310027 China 
13385718936189cn 
3 Department of Computer Science Kings College London Strand 
London WC2R 2LS England 
csidcskclacuk 
Abstract Given a weighted sequence X of length n and an integer constant λ 
the minimum λcover problem of weighted sequences is to find the sets of λ fac
tors of X each of equal length such that the set covers X and the length of each 
element in the set is minimum By constructing the Equivalence Class Tree and 
iteratively computing the occurrences of a set of factors in weighted sequences 
we tackle the problem in On2 time for constant alphabet size 
Keywords Weighted sequence the minimum λcover problem λcombination 
Equivalence Class Tree 
1 
Introduction 
It has long been an effort to investigate special areas in a biological sequence by their 
structure in biological sequence analysis especially those repetitive genomic 
segments The motivation comes from there exists high quantities of repetitive 
segments in the genome some examples are tandem repeats long interspersed nuclear 
sequences and short interspersed nuclear sequences14  
A repeat of a string x is a substring that repeatedly occurs in x A substring w of x is 
defined as a cover of x if and only if x can be constructed by concatenations and 
superpositions of w For instance aba is a cover of x=abababa The study of covers 
dates back to the pioneering work of Apostilico etc1 who first introduced the 
notion of covers and presented a lineartime algorithm for finding the shortest cover 
of a string A series of lineartime algorithms then followed to compute either the 
shortest cover or all the covers of a normal string 2 8 11 
Considering a set of repeated substrings “cooperatively” cover a given string 
Iliopoulos and Smyth 10 introduced the kcovers and the minimum kcover problem 
that is to compute a set w of substrings of x of minimum cardinality such that every 
element of w is of length k and every position of x lies within an occurrence of some 
                                                           
 Corresponding author 
 
Computing the Minimum λCover in Weighted Sequences 
121 
elements in w Lots of work have been done on this problem It was proved to be  
NPComplete based on a reduction to 3SAT3 Then by reducing to RVCPk a 
minimum kcover can be approximated to within a factor k in polynomial time9 
Inspired by the kcovers we introduced the idea of λcovers 7 by restricting the 
size instead of the length of each element of a generalized cover to be a given 
constant Given a string x of length n and an integer λ the λcover problem is to find 
all the sets W=w1 w2 wλof substrings of x such that 1 |w1|=|w2|==|wλ| and 2 
the set W covers the string x Through the construction of Equivalence Class Tree 
ECT and iterative computation of λcombinations the λcovers of every possible 
length can be found in On2  time 
This paper focuses on the λcovers in weighted sequences A weighted sequence is 
a string that allows a set of characters to occur at each position of the sequence with 
respective probability Weighted sequences are apt at summarizing poorly defined 
short sequences eg transcription factor binding sites and the profiles of proteins6 
The minimum λcover problem in weighed sequences is mainly investigated that 
is to find the λcovers such that the length of each element in the set is minimum If 
λ=1 this problem is reduced to compute the minimum cover of weighted sequences 
which has been solved in On2 time 4 12 Thus we simply consider the case λ1 
2 
Preliminaries 
Definition 1 Let an alphabet be Σ = σ1 σ2     σl  A weighted sequence X over Σ 
denoted by X1 n = X1X2    Xn is a sequence of n sets Xi for 1≤ i≤ n such that 
 =
=
≥
≤
≤
=
l
j
j
i
j
i
j
i
j
and
l
j
i
X
1
1


0



 |1



 
π σ
π σ
π σ
σ
 
Each Xi is a set of couples σj πiσl where πiσl is the nonnegative weight of σj 
at position i  representing the probability of having character σj at position i of X  
Let X be a weighted sequence of length n σ be a character in Σ We say that σ occurs 
at position i of X if and only πi σ 0 written as σ∈Xi A nonempty nonweighted 
string f 1 m m∈1 n occurs at position i of X  if and only if position i + j − 1 is 
an occurrence of the character f j in X for all 1≤ j≤ m Then f is said to be a factor of 
X and i is an occurrence of f in X The probability of the presence of f at position i of 
X  is called the weight of f at i written as πif can be obtained by using the cumula
tive weight defined as the productive weight of the character at every position of f 
that is 
  


1
1
∏ =
+ −
=
m
j
i j
i
f j
f
π
π
    
For instance consider the following weighted sequence 
 






























=
5 
0

5 
0

0 25

0 25

0 25

0 25

4 
0

6 
0

0 25

0 25

5 
0

T
C
T
G
C
A
C
A
G
G
C
A
X
 
 
The weight of a factor f =CGAT at position 1 of X is π1 f  = 025 × 1 × 06 × 025 
=00375 That is CGAT occurs at position 1 of X with probability 00375 
Definition 2 A factor f of a weighted sequence X is called a repeat in X if there exists 
at least two distinct positions of X that are occurrences of f in X 
122 
H Zhang Q Guo and CS Iliopoulos 
As scientists pay more attention to the pieces with high probabilities in DNA se
quences we fix a constant threshold 1k 1 for the presence probability of the motif 
Then only those occurrences with probability not less than this threshold are counted 
called real factors 
Definition 3 Given a weighted sequence X of length n and λ λ2 factors of X such 
that |w1|=|w2|==|wλ|=p we say that a set W=w1 w2 wλ is a λ pcover of X if 
and only if every position of X lies within an occurrence of some wi 
To avoid trivialities we suppose 1 p  nλ Then the minimum λcover problem is to 
find such λ pcovers of X with smallest p for a certain constant λ Consequently we 
need to iteratively record all the repeats of length p then check if there exists combi
nations of these repeats that cover x until we find a λ pcover of X 
We have presented an algorithm for locating all the repeats in a weighted sequence 
based on the following idea of equivalence relation on positions of a string 5 
Definition 4 Given a string x of length n over Σ an integer p∈1 2     n S be a 
set of positions of x 1 2 …  n − p + 1 then Ep is defined to be an equivalence 
relation on S such that for two i j∈ S i j∈Ep if xi i + p −1 = xj j + p −1 
Definition 5 Consider a factor f of length p in a weighted sequence X 1 n An Ep
class associated with f is the set Cf p of all positionprobability pairs i πi f such 
that f occurs at position i with probability πif ≥1k 
Cf p is an ordered list that contains all the positions of X where a real factor f occurs 
For this reason the probability of each appearance of a factor should be recorded and 
kept for the next iteration Briefly we first partition all the  positions of X to build 
E1 then iteratively compute Ep from Ep1 using Corollary 1 for p ≥2 The computation 
stops at stage L once no new EL+1classes can be created or each ELclass is a single
ton For more information readers may refer to 13 
 
Corollary 1 Let p∈1 2     n  i j∈1 2 …  n − p Then 
i πi f  j πj f∈Cf p  iff i πi f’  j πj f’∈Cf’ p1  and                  
i + p − 1 πi+p1σ j + p − 1 πi+p1σ∈Cσ 1  
Where σ∈Σ f and f are two factors of length p and p1 respectively such that f = fσ 
and πjf≥1k πjf≥1k 
3 
Constructing the ECT 
We follow to use the technique for computing λcovers in nonweighted sequence To 
help recording the λcovers we reinterpret the idea of the Equivalence Class Tree 
ECT for weighted sequences that was first introduced for nonweighted strings 7 
The ECT is a rooted tree built upon the building of equivalence classes which ex
presses the relationship between each Ep1class and its corresponding 
p
E classes by 
scanning characters from left to right in the ECT 
 
Computing the Minimum λCover in Weighted Sequences 
123 
Let f1 f2… fr be the real factors of length p1 of X 
1

1


1
−
−
p
C
p
C
rf
f
 be 
the Ep1classes associated with these factors denoted by 



1
rf
f
C
C
for simplifica
tion The ECT is created as follows The root has label 0 There are r nodes of depth 
p1 each of which is a pair C fi fi i∈1 r To simplify we label each node by fi 
instead of the pair in the ECT The children of fi are the Epclasses partitioned by 
if
C  
according to Corollary 1 corresponding to those real factors of length p produced by 
each fi reading one character to the right The construction of the ECT proceeds along 
with the computation of equivalence classes until all the nodes are not repeats of X 
For example consider the following weighted sequence and let the constant k=5 
X = TATA05 C 03 T 02ATA05 C 05AC05 T 05A  In
itially there are three children of the root in the ECT A T and C When p=2 CA is 
partitioned into three E2classes CAA CAT CAC each of which has node A as the par
ent in the ECT A subtree of the ECT of X rooted at A is shown in Fig 1 
 
Fig 1 A subtree of the ECT of X rooted at A 
We label each factor that ends to position n in the ECT called an endaligned fac
tor It is easily observed that in any branch of the ECT every internal node represents 
a proper prefix of the leaf of decreasing length from bottom to top 
The ECT provides a graphic model for the partitioning which store the occur
rences of all the repeats of X in different ways The declaration the ECT is constructed 
along with the partitioning of equivalence classes implies that the construction takes 
time no more than the partitioning that is On2 time 
4 
Outline of Our Algorithm 
Definition 6 Let X be a weighted sequence of length n overΣ λ be an integer We say 
that a set W=w1 w2 wλ is a λcombination of X if each wj  for 1≤ j≤ λ is a factor 
of X W is a λ pcombination of X if each wj is of length p 
Consider a λcombination W=w1 w2 wλ of X we say that W occurs at position j 
of X if any wi 1≤ i≤ λ occurs at position j Obviously a λ pcombination W identi
fies a set of λ factors of length p which contributes a candidate λ pcover of  X 
124 
H Zhang Q Guo and CS Iliopoulos 
Definition 7 Suppose W=w1 w2 wλ is a λcombination of X A position list is a 
doubly linked list that stores all the occurrences of W in X in ascending numerical 
order denoted by LW=j1 j1… jr where for each element j in LW jprev and jnext 
points to the previous and the next position respectively 
Our method for finding the minimum λcovers of a given weighted sequence X is then 
explicit We begin with p=2 compute all the λ 2combinations of X if existed then 
check each if it is a λ 2cover of X If there exists at least a λ 2cover the mini
mum λcover has been found otherwise we increment p to deduce all the λ p
combinations from the corresponding λ p1combinations and check if there exists a 
λ pcover of X by its position list for p=3 4 5…The iterative computation stops 
until at least a λ pcover was found at one stage 
Now we present a general algorithm for finding possible λ pcovers W of a given 
weighted sequence X based on the ECT for a certain p∈1 nλ Our algorithm main
ly consists of two steps 
1 
Compute all the λ pcombinations of X and their position lists 
In our algorithm the computation of any λ pcombination and its position list 
proceeds simultaneously Once a λ pcombination is obtained the corresponding 
position list is updated 
1 Base case 
Let mp be the number of 
p
E classes ie the number of distinct factors of length p 
of X First consider p=1 the number of E1classes is at most m1= min |Σ| n We 
store each λ 1combination for further computation Note that the only possibility 
that there exists a trivial  λ 1cover of X is in the case of λ= m1 If m1≤ λ we save 
this m1 1combination and move on to considering p=2 3 4… the base case is the 
smallest p such that mp λ which allows us to find qualified λ pcombinations for 
iteration 
2 Inductive step 
Assume that we have created the ECT for X and we have stored the position 
strings for all the λ pcombinations at stage p Consider a certain λ pcombination  
W=w1 w2 wλ it might produce a series of λ p+1combinations as a result of 
the partitioning of each 
p
E class 
iw
C 1≤ i≤ λ according to the ECT Let the number 
of sons of wi in the ECT be δi Then the relevant substrings of length p+1 can be de
noted by 
i
i
i
i
s
s
s
 δ

2
1
 where δi ≤ |Σ| Let the number of sons of wi chosen to form the 
λ p+1combinations be τiwhere 0 ≤ τi ≤ δi  
From i=1 to i=λ we successively substitute τi among these δi factors of length p+1 
respectively for wi Every current combination obtained after wi being replaced is 
saved to be further updated denoted by Si In other words we first compute S1 then 
update each S1 to obtain a set of S2s etc until Sλ is iteratively created Obviously 
Sλ consists of all the λ p+1combinations associated with the given λ p
combination W 
We simply describe the process of generating S1 by considering w1 since 
updating Sj to obtain a set of  Sj+1’s follows the same method 
 
Computing the Minimum λCover in Weighted Sequences 
125 
1  δ1 λ We update W by replacing w1 with τ1 among δ1 sons of w1 where 0≤ τ1≤ 
δ1 
a 
τ1≠0 We need to select λτ1 elements from the remaining λ1 wj s j ≥ 2 to 
compose λcombinations which leads to






−
−







=
1
1
1
1
1
1
1
τ
λ
λ
τ
δ
δ
τ
S1’s The cardinality of 
S1’s is dependent on λ and |Σ| which is a constant 
b 
τ1=0 Not choosing any 
j
is to substitute w1 we simply keep the λ1 p
combination w2 w3 wλ as an eligible S1 to be further updated 
2 δ1≥λ The algorithm runs almost the same with the case δ1 λ with the major 
distinction that 0 ≤τ1 ≤ λ In this case as δ1|Σ| the number of S1’s is at most 
1
1
|
|
1
1
1
1
 +





−
−





 Σ

=
τ
λ
λ
τ
λ
τ
 also a constant independent of p 
The position list 
LS2
 for any S1 can be iteratively updated along with the induction 
of S1 from W Consider a certain S1 
 
Case 1 w1 is an unmarked internal node in the ECT 
As τ1 sons of w1 are chosen those occurrences that are included in each of the τ1 cor
responding 
Ep+1
classes but not included in the class 


Cw1 p
 and those of wk’s 2 ≤ k 
≤ λ that are not included in  S1  should be eliminated from LW When an occurrence 
j needs to be removed the node previous and next to it in the position list ie jprev 
and jnext will be directly linked After all the “removed” positions are examined we 
get the position list for S1 
 
Case 2 w1 is an unmarked internal node in the ECT 
In this case position np+1 should be removed Besides this the algorithm runs the 
same with Case 1 
Observe the fact that position 1 should be always the first element in the position 
list of a λ pcover so during the iterative process only those λcombinations whose 
position list has position 1 as the first element are eligible to be stored then to be 
further partitioned and updated This will greatly decrease the amount of 
combinations that needs to be further processed 
2 Check every λ pcombination of  X if it is a  λ pcover 
 1  Filtering step 
The last occurrence of a λ pcover must ends to n  Only those λ–combinations  
whose last element in its position list is np+1 will be remained as candidate λ p
covers 
2 Comparison step 
This step determines whether a candidate λ pcover is true or not based on the 
fact that any distance between adjacent occurrences of a λ pcover in X should not 
exceed the length of p We attempt to maintain the maximum difference between 
adjacent occurrences of a λcombination in X denoted by MAXGAP 
Let MAXGAP be g0 for the given λ pcombination W=w1 w2 wλ We can 
iteratively compute the value of MAXGAP for a λ p+1combination Sλ at the 
126 
H Zhang Q Guo and CS Iliopoulos 
iterative step along with the induction of Sλ from W As mentioned earlier when a 
position j is removed the position previous and next to it in the position list will be 
directly linked then the distance between two adjacent positions is correspondingly 
updated to jnextjprev MAXGAP is then maintained as the larger between this 
updated distance and the current MAXGAP When a Sλ is produced MAXGAP can 
also be achieved Eliminating those candidates with MAXGAPp from all those 
candidates we obtain the true λ pcovers 
Theorem 1 The minimum λcover problem of a weighted sequence X of length n can 
be solved in On2 time 
ProofAs we discussed in section 3 the ECT of weighted sequences is constructed 
along with the partitioning of equivalence classes which costs On2 time12 
Base step processes at most On λcombinations Consider the iterative step from 
p to p+1 




2
1
wλ
L w w
 is updated according to the partitioning of each wi 
Computing the position list for any λ p+1combination simply removes some nodes 
of the list 




2
1
wλ
L w w
 which takes O1 time for each removed position 
As we have analyzed before there are constant numbers of combinations produced 
after w1 is processed dependent on |Σ| and λ Hence after all these λ wj’s are 
processed we obtain constant numbers of λ p+1combinations related to a given λ 
pcombination w1 w2 wλ Taking them all into account every position is to be 
removed for O|Σ| times Therefore at stage p+1 Step 1 costs On time for all the n 
positions That is the total number of  λ pcombinations is On 
The filtering step takes O1 time for each  λ pcombination since it simply 
checks the last element of its position list The comparing step is performed along 
with the iterative step thus computing maximal differences does not take more time 
than computing the position lists Therefore Step 2 runs in O1 time to determine 
whether a λ p+1combination is a λ p+1cover or not hence On time for all 
On combinations 
To sum up finding the λ p+1covers requires On time Since 1 p  nλ there 
are at most On stages then the overall complexity for finding the minimum λcover 
is On2 Therefore our algorithm needs On2 time in total 
5 
Conclusions 
In this paper we introduce the minimum λcover problem of a weighted sequence and 
present an efficient solution to this problem with the time complexity On2 The main 
idea of our algorithm lies in the construction of the Equivalence Class Tree and 
iterative computations of the occurrences of a set of factors in weighted sequences As 
opposed to its nonweighted version the solution does not take more time  
Acknowledgments This work was partially supported by Zhejiang Provincial 
Natural Science Foundation under Grant No Y1101043 and Foundation of Zhejiang 
Provincial Education Department under Grant No Y201018240 of China  
 
Computing the Minimum λCover in Weighted Sequences 
127 
References 
1 Apostolico A Farach M Iliopoulos CS Optimal Superprimitivity Testing for Strings 
Information Processing Letters 39 17–20 1991 
2 Breslauer D An Online String Superprimitivity Test Information Processing Letters 44 
345–347 1992 
3 Cole R Iliopoulos CS Mohamed M Smith WF Yang L Computing the Minimum 
kcover of a String In Proc of the 2003 Prague Stringology Conference PSC 2003 pp 
51–64 2003 
4 Christodoulakis M Iliopoulos CS Mouchard L Perdikuri K Tsakalidis A Tsich
las K Computation of Repetitions and Regularities on Biological Weighted Sequences 
Journal of Computational Biology 136 1214–1231 2006 
5 Crochemore M An Optimal Algorithm for Computing the Repetitions in a Word Infor
mation Processing Letters 125 244–250 1981 
6 Gusfield D Algorithms on Strings Trees and Sequences Computer Science and Compu
tational Biology Cambridge University Press 1997 
7 Guo Q Zhang H Iliopoulos CS Computing the λcovers of a String Information 
Sciences 177 3957–3967 2007 
8 Iliopoulos CS Moore DWG Park K Covering a String Algorithmica 16 288–297 
1996 
9 Iliopoulos CS Mohamed M Smyth WF New Complexity Results for the kcovers 
Problem Information Sciences 181 251–255 2011 
10 Iliopoulos CS Smith WF An Online Algorithm of Computing a Minimum Set of k
covers of a String In Proc of the Ninth Australian Workshop on Combinatorial Algo
rithms AWOCA pp 97–106 1998 
11 Li Y Smyth WF Computing the Cover Array in Linear Time Algorithmica 321 95–
106 2002 
12 Zhang H Guo Q Iliopoulos CS Varieties of Regularities in Weighted Sequences In 
Chen B ed AAIM 2010 LNCS vol 6124 pp 271–280 Springer Heidelberg 2010 
13 Zhang H Guo Q Iliopoulos CS Loose and Strict Repeats in Weighted Sequences 
Protein and Peptide Letters 179 1136–1142 2010 
14 The Human Genome Project HGP httpwwwnbgrinihgovHGP 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 128–136 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A Novel Hybrid Evolutionary Algorithm for Solving 
MultiObjective Optimization Problems  
Huantong Geng1 Haifeng Zhu1 Rui Xing2  and Tingting Wu1 
1 College of Computer and Software  
2 College of Atmospheric Science  
Nanjing University of Information Science  Technology Nanjing China 
htgengnuisteducn 
Abstract This paper proposed a novel hybrid evolutionary algorithm for solving 
the multiobjective optimization problems MOPs The algorithm uses the idea 
of simulated annealing to combine coevolution with genetic evolution set sev
eral model sets according to the principle of “model student” and employs 
εdominant and crowding distance sorting to search the excellent population In 
the coevolution model cluster analysis is used to classify the model set and the 
estimation of distribution algorithm EDA is used to establish the probabilistic 
model for each class The individuals are generated by sampling through the 
probabilistic model in genetic evolution model populations evolve based on the 
model set This algorithm takes full advantage of the global and local search ab
ilities and makes comparison with the classical algorithm NSGAII the expe
rimental results show that our algorithm for solving the multiobjective problem 
has better convergence and distribution 
Keywords MOEAs Coevolution EDA Model Student 
1 
Introduction 
Many problems are affected by human activities and a number of factors in the natural 
environment and many have multiple conflicting objectives and can only get a group 
Pareto optimal solutions 1 As a group search algorithm evolutionary algorithm is 
very suitable for solving MOPs 2  
The classic multiobjective evolutionary algorithms MOEAs such as SPEAII 3 
and NSGAII 4 are all based on Pareto dominance relationship However the 
MOEAs are mainly used for the twoobjective optimization problems As the number 
of objectives increases the number of nondominated individuals in population will 
increase exponentially this can greatly weaken the selection and search capabilities 
based on Pareto sorting and the problem of premature appears so the traditional 
MOEAs based on Pareto sorting are needed to improve for the optimization problem 
This paper combined εPareto dominance 5 with Pareto dominance and based on 
crowding distance sorting to achieve the purpose of selecting excellent individuals 
In recent years our ISMOEA 6 and SBMSMOEA 7 are also proposed on the 
basis of previous work In these evolutionary algorithms the individual often 
 
A Novel Hybrid Evolutionary Algorithm for Solving MOPs 
129 
represents only one solution of the problem without taking into account the interaction 
between the individuals and the surrounding environment that is without considering 
the coevolution This may lead to lack of diversity of the solutions and may affect the 
convergence and diversity However cooperative coevolutionary genetic algorithm 
CCGA 8 solves the above problems to some extent In this paper the idea of CCGA 
is referred specific to the decision variableindependent MOPs that is if the function 
has V decision variables uses V subpopulations and each individual in the 
subpopulations only represents one part of the problem 
In the standard genetic algorithm its evolution has excellent local optimization 
ability but poor global search capability At the late stages of the evolution the evo
lution entirely dependents on the mutation which makes the subsequent evolution 
become a completely aimlessly random search For solving the complex problems 
such as the problem of multiple local optima the algorithm often takes on premature 
convergence phenomenon EDA 9 is different from the traditional evolutionary 
algorithms EDA has a good global search capability and can lead the population search 
toward the Pareto front In order to converge to the Pareto front more effectively a 
hybrid algorithm with better local and global search capability is needed 
This paper employs the idea of simulated annealing to combine the coevolution al
gorithm which is based on EDA with genetic evolution based on “model student” and 
proposes a new hybrid evolutionary algorithm for solving the multiobjective problem 
CEDAMSGA This algorithm balances the relationship of the local search and the 
global search dynamically In this paper seven test functions are used to evaluate the 
performance of the algorithm and comparison with the NSGAII is also done 
2 
CEDAMSGA  
CEDAMSGA combines Pareto dominance with εdominance to find the model pop
ulation sorts the model population based on crowding distance to keep the diversity 
and distribution of the model set uses the clustering to classify the model set and 
establish the appropriate probabilistic model of each class During the process of 
evolution simulated annealing is used to combine the coevolution algorithm which is 
based on EDA with genetic evolution based on the model student 
21 
Individual Evaluation Method 
Pareto dominance and εdominance are combined in this algorithm compares p with q 
by Pareto dominated relations that is if p and q do not dominate mutually and p has 
weaker performance on one objective only but greatly improves on the other objec
tives that is p meets the condition of εdominance then considering that p is better 
than q ie p dominate q  
22 
Model Student Set 
Model student the assessment of the model student in real world is that the student 
who gets highest scores in one subject among the students who have good consolidated 
130 
H Geng et al 
results will be chosen as the class representative of the subject if the student acts 
excellently on two or three subjects he can become the model undoubtedly However 
NSGAII only simply uses the Pareto front and less model individuals 
In CEDAMSGA according to model student concept a number of model sets are 
established correspondingly Taking three objective functions as an example set A B 
C that are used to store the individuals with outstanding performance in one two and 
three objectives respectively are set and their capacities are set to M1 M2 and M3  and 
if the problem has three objectives M1 = M2 = 3 M3=200 
Generation Method of Model Set One individual from the initial population Pop is 
randomly selected to the set A B and C in the first generation then all individuals in 
Pop are compared with the individuals in C one by one for Pi∈Pop if Pi can’t be 
dominated by any individual add it to C if the number of individuals in A and B is less 
than their maximal capacity add Pi to A and B directly Otherwise only when Pi can’t 
be dominated by any individual in C and is better than the corresponding objective of 
some individual Pi can add to the set to replace the original location of the corres
ponding individual if some individuals in C is dominated by Pi these individuals will 
be removed from C From the second generation the individuals of populations 
through the evolution operations adjust C based on the domination relations the update 
method used in A and B is same as in the first generation The model sets are not needed 
to be sorted in the algorithm thus the computing cost is greatly reduced 
The number of the excellent individuals in C is increasing as evolving Taking the 
computational efficiency into account once the number of individuals stored is more 
than M3 the set C is needed to be trimmed Whether each individual will be removed 
depends on its crowding distance those poor individuals are removed to make the 
individuals maintain a certain distance in the set In this way the set not only maintain 
the diversity but also have a better distribution 
Role of Model Sets Firstly take these model sets as the genetic evolutionary popula
tions and their studying objects respectively ie make the individuals which have 
three excellent objectives learn from the individuals which have one or two excellent 
objectives make them mate with each other The principle is shown as Fig1 first 
take C as the genetic evolutionary population and select one individual randomly to the 
mating pool then select one individual randomly from A or B as the studied object to 
the mating pool too and finally do the evolutionary operation to the individuals in the 
mating pool to achieve the purpose that the evolutionary population can learn from the 
model individual Using this method can make the searched front evolve toward the 
true front direction and more completed  
Secondly take C as a collection of excellent individuals and use the clustering 
algorithm to classify these individuals according to the idea of EDA and establish the 
appropriate probabilistic model for each dimension of individuals in each class mul
tiprobabilistic models can make the algorithm have better distribution and this me
thod can make all the coevolutionary populations evolve toward a hopeful prospect 
and find the complete Pareto front by this algorithm rapidly 
Considering the solutions of MOPs are an optimal solution set EDA is used here if 
only one probabilistic model is established apparently this model can’t make the 
 
A Novel Hybrid Evolutionary Algorithm for Solving MOPs 
131 
population evolve towards all the fine directions and the true Pareto front is most likely 
unable to find In order to cover the Pareto optimal front better the clustering algo
rithmleaderfollower 10 is used to classify C before the algorithm establishes the 
probabilistic model to obtain a number of different classes and then establish the 
corresponding probabilistic model on each dimension for each class according to the 
idea of EDA Considering C based on crowding distance sorting maintains the diver
sity calculate the weight coefficient of each class according to the number of indi
viduals in each class and use the roulette wheel sampling algorithm based on the 
weight coefficient to select the corresponding model to generate new individuals form 
next population and it will also maintain this diversity distribution character 
 
 
Fig 1 Model set in genetic evolution  
Fig 2 Effect figure of the population cluster 
Leaderfollower can meet the requirement of uncertainty number of each generation 
probabilistic models without predetermining the number of classes and ensure that all 
searching areas are searched extensively and this can make the algorithm get better 
distribution Fig2 shows that four classes are searched in the twodimensional search 
space 
Finally the model sets save the excellent solutions In the process of evolution the 
algorithm always exploits and explores based on the best populations  
23 
Genetic Evolution and Coevolution Based on Simulated Annealing 
The principle of the individuals mixed is the individuals come from different algo
rithms In our algorithm the individuals are generated from coevolution and GA The 
algorithm combined the coevolution with GA using simulated annealing in the early 
stages of evolution global search is needed to be done by EDA of coevolution to find 
the Pareto front quickly and coevolution plays a dominant role while in the late stages 
of evolution exploit need be strengthened around the Pareto front using the local search 
ability of GA and GA plays a key role here As shown in Fig3 the proportion of the 
individuals generated by coevolution gradually decreases as the number of evolutional 
generation increases therefore the algorithm not only maintains the local search abil
ity but also improves the global search ability 
In order to control the role played by coevolution and GA at different period in the 
algorithm simulated annealing method is used to control the scale factor pt  pt can be 
calculated by the equation set as follows 
132 
H Geng et al 
1
max
min
1
min



t
t
p
p
p
p
p
p
α
−
=
=
+
−
 
1
In formula 1 pmax and pmin is the upper and lower limit scale factor respectively 
and α is the annealing factor which interval is 0 ≤ α ≤ 1 
This algorithm generates new individuals by comparing random number rand with 
pt When rand pt use the coevolutionary approach to generate new individuals when 
rand ≥pt use the crossover and mutation of GA to generate new individuals 
Generating Method of Parent Population The random initialization is used to gen
erate the first parent population and the subsequent generation parent population is 
taking the excellent individuals entered the model sets which have greater probability 
to be selected as the candidate set of the next parent population In this way the suc
cessive new subpopulation will be searched and explored based on the excellent 
populations to find better individuals to supply the model sets therefore the model sets 
and evolutionary population will be promoted mutually and the algorithm will con
verge to the true Pareto front more rapidly  
 
 
Fig 3 Sketch map of proportion 
Fig 4 Coevolutionary parent population 
In the coevolutionary algorithm parent populations are generated by the approach 
shown as in Fig4 that is N complete individuals are randomly selected from model set 
C and each complete individual has V componentsIn Fig4 V = 7 Merging the 
components of the N complete individuals can reform the next generation parent pop
ulations which have V subpopulationsIn Fig4 Population 1  Population 7 and 
each size of the subpopulation is N 
As shown in Fig1 the parent population of GA comes from set A B and C and the 
parent individuals which are randomly selected from set A B and C respectively 
3 
Implementation Process of the Algorithm 
The detailed process of CEDAMSGA is described as follows 
Step1Initialization set the generation counter T to 1 coevolutionary population 
size to N the number of the subpopulations to V and create model sets A = B = C = ф 
set the maximum capacity of A B and C to M1 M2 and M3 the population size of the 
genetic evolution to V × N and initialize the populations respectively 
Step2Population assessment Assess each individualij i = 1 2  V j = 1 2  N 
in the population the individual which is assessed is chosen from coevolution  
 
A Novel Hybrid Evolutionary Algorithm for Solving MOPs 
133 
populations or GA population according to the relationship of random probability and 
simulated annealing scale factor When rand pt goto Step21 otherwise goto Step22 
The step2 ends until all individuals are evaluated 
Step21Assessment in coevolution evaluate the jth individual of the ith 
subpopulation in coevolution and a complete solution is generated by the cooperation 
result from the individual and other subpopulations understanding through Fig4 this 
individual randomly combines with the individuals from other V1 subpopulations to a 
complete individual Different from CCGA using this way can greatly reduce the 
consumption of computing resources and make a decision to update the model set or 
discard it according to the dominant concept goto Step2  
Step22 Assessment in genetic evolution when T=1 evaluate the i × jth indi
vidual in the population of genetic evolution and decide to update the model set or 
discard it according to the dominant relationship Otherwise as shown in Fig1 select a 
individual randomly from set A B and C respectively crossover and mutate in the 
mating pool to generate a new individual and then decide to update the model set or 
discard it Return to Step2 
Step3 Trimming of set C  If | C | M3 trim the C and make the size of C equal to 
M3 if | C | ≤M3 goto Step4 
Step4 Generating parent population the parent population of genetic evolution is 
generated in Step22 while the parent populations of coevolution are generated by 
using the N complete individuals the specific method is shown in section 23 
Step5 Evolutionary operator genetic evolution is completed in Step22 about the 
coevolution classifying the parent populations of coevolution to get different classes 
and establishing the appropriate probabilistic model of each class The new populations 
will be generated based on the probabilistic model 
Step6 Termination T = T +1 If the termination conditions are satisfied then stop 
and output the set C as the final solution set otherwise goto Step2 
4 
Experimental Results and Discussions 
41 
Test Problems 
To verify the idea of CEDAMSGA and its effectiveness the performance of 
CEDAMSGA is assessed using the experiments and compared with NSGAII we 
adopted five standard test functions 4 ZDT1ZDT4 and ZDT6 minimization 
problems of two objectives and two standard test functions in the references 11 
DTLZ1 DTLZ2 minimization problems of three objectives They have different 
typical characteristics that is convex or nonconvex and searching space which has a 
bias The difficulties of using the EAs for solving these functions are representative  
42 
Performance Metrics  
1 Approximation the distance between the solutions set in the objective space and the 
true Pareto front is minimized The coverage metrics C 12 the convergence measure 
M1 12 and distance indicators GD 13 are used  
134 
H Geng et al 
2 Distribution the solution set in the objective space acquires the best distribution 
pattern and this paper adopted the distribution measure SP 12 and SD 14  
In addition the graphs of Pareto front are drawn to make the comparison 
43 
Design of Experiments 
CEDAMSGA and NSGAII are realcoded and the mutation probability Pm is 1V V 
is the number of independent variables The size of coevolutionary subpopulations is 
N in CEDAMSGA and the amount of subpopulations is V The GA population size is 
V×N and the scale factor pmax=09 pmin= 03 α=09 and the threshold b of clustering 
algorithm is 05 For each test function two algorithms run 30 times independently  
To prove the idea of CEDAMSGA and its effectiveness for solving MOPs the 
classical NSGAII is used to solve the same MOPs and the results are compared with 
CEDAMSGA The subpopulations size N is set to 30 in CEDAMSGA 
Experiment 1 In two objectives the algorithm is terminated when the number of 
objective evaluation equals 90000 The maxcapacity M3 of the model set C is set to 
100 and crossover probability Pc is 09 Population size N in NSGAII equals 100 
Experiment 2 In three objectives when the algorithm is terminated when the 
number of objective evaluation equals 100000 the maxcapacity M3 of the model set C 
is set to 200 and crossover probability Pc is 1 Size N in NSGAII is equal to 200 
44 
Experimental Results and Analysis 
Experiment 1 Table 1 shows the average results of running CEDAMSGA and 
NSGAII 30 times independently on M1 C and SD metrics in each test function Here 
CCEDAMSGA NSGAII is abbreviated as CCN and CNSGAII CEDAMSGA 
as CN C Fig5 is the box plot which depicts the coverage metrics CC N and C N 
C results obtained from CEDAMSGA and NSGAII on 5 test functions Fig6 shows 
that the Pareto front searched by CEDAMSGA and NSGAII and these figures indi
cate that the front searched by CEDAMSGA is more completed and more approxi
mation to the true front The experimental results show that CEDAMSGA is signifi
cantly better than NSGAII on the test functions ZDT1 ZDT4 and ZDT6 in the aspects 
of convergence and distribution 
Experiment 2 Table 2 and Fig7 show that CEDAMSGA is better than NSGAII in 
the aspects of convergence GD measure and distribution SP measure in 
threeobjective test optimization problems DTLZ1 and DTLZ2  
Table 1 Results obtained by NSGAII and CEDAMSGA on ZDT1ZDT4 ZTD6 
Problems 
CEDAMSGA 
NSGAII 
M1 
SD 
CCN 
M1 
SD 
CNC 
ZDT1 
00001 
00031 
10000 
00179 
00126 
00000 
ZDT2 
00001 
00018 
10000 
00222 
00603 
00000 
ZDT3 
00003 
00052 
09424 
00133 
00158 
00000 
ZDT4 
00002 
00020 
10000 
10123 
00134 
00000 
ZDT6 
00004 
00015 
10000 
05859 
00053 
00000 
 
A Novel Hybrid Evolutionary Algorithm for Solving MOPs 
135 
Table 2 Results obtained by NSGAII and CEDAMSGA on DTLZ1 and DTLZ2 
Problems 
CEDAMSGA 
NSGAII 
GD 
SP 
GD 
SP 
DTLZ1 
0000181 
0011234 
0032412 
0016642 
DTLZ2 
0000313 
0052755 
0122403 
0070044 
 
 
Fig 5 CCN and CNC for twoobjective optimization problems 
 
a             b               c             d               e 
Fig 6 Pareto fronts obtained by NSGAII and CEDAMSGA for ZDT1 ZDT4 ZDT6 are 
shown in subfigure ae 
 
Fig 7 Pareto fronts obtained by CEDAMSGA or NSGAII for DTLZ1DTLZ2 
5 
Conclusions and Future Works 
This paper proposed a novel hybrid evolutionary algorithm to solve the multiobjective 
problem The algorithm using the intrinsic character of diversity attached to CCGA 
and combining coevolution algorithm which is based on EDA with genetic evolution 
based on model student method makes the new algorithm possess better capability of 
05
0
05
ZDT1
ZDT2
ZDT3
ZDT4
ZDT6
CNC
ZDT6
ZDT4
ZDT3
ZDT2
ZDT1
07
075
08
085
09
095
1
ZDT1
ZDT2
ZDT3
ZDT4
ZDT6
CCN
ZDT6
ZDT4
ZDT3
ZDT2
ZDT1
0
05
1
0
02
04
06
08
1
CEDAMSGA             o
NSGAII                     +
paretooptimal front
0
05
1
0
05
1
15
CEDAMSGA                o
NSGAII                        +
paretooptimal front
0
05
1
1
05
0
05
1
15
CEDAMSGA             o
NSGAII                     +
paretooptimal front
0
02
04
06
08
1
0
1
2
3
4
CEDAMSGA               O
NSGAII                       +
paretooptimal front
02
04
06
08
1
0
05
1
15
2
CEDAMSGA             O
NSGAII                     +
paretooptimal front
0
05
1
15 0
05
1
15
0
05
1
CEDAMSGA
0
05
1 0
05
0
05
1
15
NSGAII
0
05 0
05
1
0
05
1
CEDAMSGA
0
1
2 0
1
2
0
1
2
NSGAII
136 
H Geng et al 
local and global search Each strategy plays an important role during the different 
evolutionary period In the seven standard test optimization problems compared with 
NSGAII experimental results show that our algorithm has better search ability and 
the Pareto solutions have better performance in convergence and distribution The 
future work will further research the solving effect of our algorithm on other types of 
test functions and make more comparisons with other classical algorithmssuch as 
SPEAII etc and further apply it to high dimensional multiobjective problem to 
verify its effectiveness 
References 
1 Horn J Nafpliotis N Goldberg DE A Niched Pareto Genetic Algorithm for Mul
tiobjective Optimization In Michalewicz Z ed Proc lst IEEE Conf on Evolutionary 
Computation pp 82–87 IEEE Service Center Piscataway 1994 
2 Deb K MultiObjective Optimization Using Evolutionary Algorithms John Wiley  
Sons Chichester 2001 
3 Zitzler E Laumanns M Thiele L SPEA2 Improving the Strength Pareto Evolutionary 
Algorithm Technical Report 103 Computer Engineering and Networks Laboratory TIK 
Zurich Switzerland ETH Zurich pp 1–21 2001 
4 Deb K Pratap A Agarwal S et al A Fast and Elitist Multiobjective Genetic Algo
rithm NSGAII IEEE Transactions on Evolutionary Computation 62 182–197 2002 
5 Laumanns M Thiele L Deb K et al Combining Convergence and Diversity in Evo
lutionary Multiobjective Optimization Evolutionary Computation 103 263–282 2002 
6 Geng H Zhang M Huang L Wang X Infeasible Elitists and Stochastic Ranking Se
lection in Constrained Evolutionary Multiobjective Optimization In Wang TD Li X 
Chen SH Wang X Abbass HA Iba H Chen GL Yao X eds SEAL 2006 
LNCS vol 4247 pp 336–344 Springer Heidelberg 2006 
7 Geng HT Song QX Wu TT Liu JF A Multiobjective Constrained Optimization 
Algorithm Based on Infeasible Individual Stochastic BinaryModification In Proceedings 
of 2009 IEEE International Conference on Intelligent Computing and Intelligent Systems 
pp 89–93 IEEE Shanghai 2009 
8 Potter MA De Jong KA A Cooperative Coevolutionary Approach to Function Opti
mization In Davidor Y Männer R Schwefel HP eds PPSN 1994 LNCS vol 866 
pp 249–257 Springer Heidelberg 1994 
9 Zhou SD Sun ZQ Estimation of Distribution Algorithms AAS 332 113–124 2007 
in Chinese 
10 Duda O Hart E Stork G Pattern Classification 2nd edn pp 450–452 John Wiley  
Sons Inc USA 2001 
11 Deb K Thiele L Laumanns M et al Scalable Multiobjective Optimization Test 
Problems In Proceedings of the Congress on Evolutionary Computation vol 1 pp 
825–830 IEEE Service Center Piscataway 2002 
12 Zitzler E Deb K Thiele L Comparison of Multiobjective Evolutionary Algorithms 
Empirical Results Evolutionary Computation 82 173–195 2000 
13 Coello CAC A Comprehensive Survey of Evolutionarybased Multiobjective Optimi
zation Techniques Knowledge and Information Systems 13 269–308 1999 
14 Schott JR Fault Tolerant Design Using Single and Multicriteria Genetic Algorithm Op
timization Master’s thesis Department of Aeronautics and Astronautics Massachusetts 
Institute of Technology Cambridge MA 1995 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 137–144 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Heuristic Algorithms for Solving Survivable Network 
Design Problem with Simultaneous Unicast  
and Anycast Flows 
Huynh Thi Thanh Binh Pham Vu Long Nguyen Ngoc Dat and Nguyen Sy Thai Ha  
School of Information and Communication Technology Hanoi University of Science and 
Technology Hanoi Vietnam 
binhhtsoicthuteduvn 
longopddatthientainguyensythaihagmailcom 
Abstract Given a connected weighted undirected graph G = V E a set of 
nodes a set of links with modular cost based on ACMC model 2 and a set of 
customers’ demands This paper proposes two heuristic algorithms for solving 
the ACMC Survivable Network Design Problem ASNDP The goal is to de
sign connections based on customers’ demands with the smallest network cost 
to protect the network against all failures This problem is NPhard The expe
rimental results are reported to show the efficiency of proposed algorithm  
comparing to the Tabu Search algorithm 2 
Keywords survivable network design anycast unicast ASNDP 
1 
Introduction 
In the recent years there are many kinds of transmissions have been used included 
unicast and anycast flow which are two of the most popular types A transmission 
connected one host to another is called unicast An anycast is defined as onetoone
ofmany transmission to deliver a packet to one of many host which is applied in 
Domain Name Service DNS Web Service Overlay Network peertopeer P2P 
systems Content Delivery Network CDN software distribution…2 The populari
ty of anycast technology will increase in the near future since many new services that 
can use anycast paradigm are developed 1  
The impact of Internet and other computer networks can be seen in almost all areas 
of our lives including business science technology so on and their importance will 
grow Thus any network failure can cause serious consequences affect to many 
people and corporations Therefore designing the survivable network is a crucial 
problem Because we solve the SNDP problem based on ACMC All Capacities 
Modular Cost model which is defined in 2 we call this problem by ASNDP 
ACMC Survivable Network Design Problem 
ASNDP problem is defined as the following Given an undirected graph G = V 
E Each link divides into some bandwidth levels each level has a corresponding cost 
Link cost is the cost of total of corresponding bandwidths using in this link And  
138 
TT Binh Huynh et al 
network cost is total of link cost A connection from a node to another is a unicast 
transmission An anycast is also a connection from a node to another but the differ
ence is that the destination node has a replica server which backs up for it In this 
problem a demand required by customer is a connection between two nodes with 
corresponding bandwidth and type of connection The goal of problem is to find a set 
of connections for all demands such that the network cost is minimal  
We can formulate the problem as following 
Find an appropriate connection for each customer’s demand so as to minimize 

=
i
ic
NCost
 
1
With condition is that if 



−
j
k
k
B
Rij
B
1
 then ci = Ck where ci is cost of link i 
Bk Ck is bandwidth and corresponding cost with level k 
To solve ASNDP we have to build a set of path for all demands to minimize the 
network cost To guarantee the survivability of connections we also use backup path 
approach 2 3 4 5 In particular each connection anycast or unicast is divided 
into working path and backup path they can use some same links but not complete
ly  If the working path is broken the backup path must be restored and conversely  
In order to minimize the network cost we propose a new approach called Free 
Bandwidth based Heuristic Algorithm FBB This approach is based on utilizing the 
redundant bandwidth corresponding with paid cost level in each link We propose two 
heuristic algorithms FBB1 and FBB2 The FBB1 applies the main idea of FBB algo
rithm Whereas the FBB2 combines the idea of FBB1 and Local Search algorithm 
14 for solving ASNDP We hope that the results found by FBB1 and FBB2 are 
better than the one found by Tabu Search 2 
The rest of this paper is organized as following Section II describes the related 
works In section III we present the proposed algorithms to solve ASNDP Our ex
periments and computational and comparative results are given in section IV The 
paper concludes with section V with some discussions on the future extension of this 
work   
2 
Related Works 
The SNDP is generally presented in 10 when considering both economics and re
liability in telecommunication network design Thus there are two problems re
quested in the ASNDP Those are to guarantee the survivability of network system 
and to minimize the network cost  
The most popular way mentioned in many researches is the single backup path ap
proach The main idea of this method is as following Each connection has a working 
path and a backup path The working path is used for transmitting data in normal 
failurefree state of the network After a failure of the working path the failed con
nection is switched to the backup path 2 3 4 5 
In the literature there are several papers research on SNDP problem 2 8 10 
They use branch – and – bounds or branch – and – cut methods to find optimal pre
cisely solutions These methods can only use for small networks with 30 nodes and 
100 edges For larger network they may propose evolutionary algorithms tabu search 
 
Heuristic Algorithms for Solving Survivable Network Design Problem 
139 
2 and simulated annealing 8 In 10 Vissen and Gold apply the evolution strate
gy ES Following the discussion of 10 the quality of result is very flexible and 
effective The graph instance which has m nodes n edges is called by m n – ES  
Overall the 30 200 – ES with discrete recombination delivers the best solution The 
10 50 – ES operates with a smaller population size uses much less resources  and 
still delivers good cost results but it is not as good as the 30 200 – ES Clearly 
when using ES a larger population helps to achieve a better result than a smaller one 
by avoiding or delaying convergence on local suboptimal However this algorithm is 
useful in the network which has only unicast flows 
With the network which has both anycast and unicast flows KWalkowiak and Ja
cub Gladysz 13 presented a heuristic algorithm for solving ASNDP The main idea 
of this algorithm is based on Flow Deviation method 8 and Local Search algorithm 
14 They achieve the quite good result with Polska 12 nodes 36 links 65 unicast 
12 anycast network the detail is that the average gap of the proposed heuristic algo
rithm to optimal results is 711 Furthermore K Walkowiak and Jakub Gładysz 2 
have built Tabu search algorithm based on hill climbing algorithm with some heuris
tics to solve this problem They experimented this algorithm with three large instances 
which are Polska 12 nodes 36 links 65 unicast 12 anycast Germany 17 nodes 52 
links 119 unicast 13 anycast Atlanta 26 nodes 82 links 234 unicast 22 anycast 
and received effective results In particular with Polska network they achieve the 
average gap to optimal results is 257 for 70 anycast30 unicast case and 200 
for 80 anycast 20 unicast case However their Tabu Search algorithm is quite 
simple and their results cannot be optimal completely  
In the next section we introduce two algorithms for solving ASNDP We hope 
that our proposed algorithms will have a better result than previous algorithms 
3 
Proposed Algorithms 
Almost the current heuristic algorithms for solving ASNDP use local search method 
Each step they try to minimize the cost of the network using different path finding 
algorithms with the default link cost But in those problem where link cost is mod
ular there is another type of link cost can be used to minimize the network cost 
This type of link cost based on the characteristic of modularlinkcost the cost for 
redundant bandwidth is free For instance we choose a 20 MB cable type for a link 
which needs 15 MB of bandwidth consider there is only 20MB and 10 MB cable 
types so 5MB of bandwidth will be unused If we route another connection which 
needs no more than 5 MB of bandwidth through this link the cost is totally free We 
call this 5 MB of bandwidth the FreeBandwidth If we can route connections through 
those FreeBandwidth that would be a greatly decrease of network cost That is the 
ideal of the FreeBandwidth based heuristic algorithms FBB 
This algorithm is based on the utilizing the redundant bandwidth Consider we have 
an initial solution a network with a set of identified links and a routing table for all 
requests First we remove the connection v of a demand d from the routing table and 
recalculate the cable type for all link of the network And then we recalculate the 
complementary cost of all links in the current network if the new connection of de
mand go through those as following demand d requires bandwidth rb For every link 
we add rb to its bandwidth if the result is greater than the max value of bandwidth 
level which is used in this link so 
140 
TT Binh Huynh et al 
Ccost = CostnextLevel – CostcurrentLevel 
2
Otherwise Ccost = 0 This is a new way to calculate graph costs and it helps the path 
finding algorithms easier find a better solution 
31 
Free Bandwidth Based Heuristic Algorithm 1 FBB1 
Base on the general idea FBB1 can find a new connection for demand d and the new 
link cost is recalculated If the new network cost is lower than the old one we have a 
new best current solution  
However if we consider cost of any link is totally free the path finding algorithms 
may not work well For instance we can see in the graph below 
 
 
Fig 1 A misleading situation for path finding algorithms 
In this instance we can see that the FBB1 cannot recognize the differences between 
two paths above Although they both have free cost the second path seems to be bet
ter In some worse situations the path finding algorithms may enter a loop In order to 
overcome this drawback instead of considering cost for redundant bandwidth is free 
we set it a minor value compare to the network link cost so that the path finding 
algorithms still try to go through these links as possible they are able to distinguish 
the better “free” path and will not enter any loop The pseudo code for Free
Bandwidth based algorithm solving ASNDP is shown below 
1 Procedure FBB1 
2  G represent graph with identified link 
3  C represent connections for all request 
4  GC  initial Solution 
5 currentNetworkCost  initial Solution Cost 
6 While terminateCondition 
7   d  randomRequest 
8   c  CgetConnectionsd 
9   V = Vv v belong to c 
10   For j = 1 to numberOfLinks do 
11    if  requiredBandwidthd + usedBandwidthj= 
maxBandwidthj  
12     costj = MINORVALUE 
13    else costj = costLevelnextLevelj costLe
velcurrentLevelj 
14  Endfor 
 
Heuristic Algorithms for Solving Survivable Network Design Problem 
141 
15  c’  findConnectionsForRequestd 
16  C=C cc’ 
17  newNetworkCost  ß recalculateSolutionCost 
18  if currentNetworkCost  newNetworkCost  
19  C = C cc’ 
20  else C = C 
21 End While 
22 End Procedure 
32 
Free Bandwidth Based Heuristic Algorithm 2 FBB2 
In general there are not much redundant bandwidths which are applied in FBB1 So 
in order to improve we could use some normal path finding algorithms such as 
Dijkstra shortest path DFS etc That is the idea of FBB2 First we remove the con
nection v of a demand d from the routing table and recalculate the cable type for all 
link of the network And then we find two new paths one using the same algorithm 
in FBB1 and the other using shortest path with normal link cost Last we choose the 
best of the current solutions and consider that is the current best solution  
4 
Experimental Results 
41 
Problem Instances 
In our experiments we used three real world instances They are Polska 12 nodes 36 
links 65 unicast 12 anycast Germany 17 nodes 52 links 119 unicast 13 anycast 
Atlanta 26 nodes 82 links 234 unicast 22 anycast With each instance we random
ly create 10 test sets which are different from the content of customers’ demands  
42 
Experiment Setup 
We experiment two our proposed algorithms independently and compare their per
formance with together and Tabu Search in 2  
43 
System Setting 
In the experiment the system was run 50 times for each test set All the programs 
were run on a machine with Intel Core 2 Duo U7700 RAM 2GB Windows 7 Ulti
mate and were installed by C++ and Java language  
44 
Computational Results 
The experiments show that 
• Figure 2 shows that on Polska network the results found by FBB2 algorithm are 
the best The best results found by Tabu Search and FBB1 are more equivalent 
with the 5 first test sets the ones of FBB1 are better than Tabu Search and con
versely  
142 
TT Binh Huynh et al 
• Figure 3 and 4 show that on the larger instance Germany and Atlanta network 
the best results found by our proposed algorithms and Tabu Search 2 are quite 
equivalent But in almost test sets the results found by both FBB1 and FBB2 are 
better than the best results found by Tabu Search 
• Figure 5 6 and 7 shows that the average results found by FBB1 and FBB2 are 
better than Tabu Search 2 
 
Fig 2 The best result of Polska network found 
by Tabu Search 2 FBB1 FBB2 over 50 
runing times 
Fig 3 The best result of Germany network 
found by Tabu Search 2 FBB1 FBB2 over 
50 running times 
 
Fig 4 The best result of Atlanta network 
found by Tabu Search 2 FBB1 FBB2 over 
50 running times 
Fig 5 The average result of Polska network 
found by Tabu Search 2 FBB1 FBB2 over 
50 running times 
 
Fig 6 The average result of Germany network 
found by Tabu Search 2 FBB1 FBB2 over 
50 running times 
Fig 7 The average result of Atlanta network 
found by Tabu Search 2 FBB1 FBB2 over 
50 running times 
 
Heuristic Algorithms for Solving Survivable Network Design Problem 
143 
5 
Conclusion 
In this paper we proposed two new heuristic algorithms for solving ASNDP called 
FBB1 and FBB2 FBB1 algorithm is based on utilizing the redundant bandwidth cor
responding with paid cost level in each link FBB2 is the combination of FBB1 and 
Local Search algorithm 14 We experimented on three instances which are Polska 
Germany and Atlanta network 2 13 With each instance we randomly create 10 
test sets which are different from the content of customers’ demands The results 
show that our proposed approach is quite effective with ASNDP On all instances 
FBB1 and FBB2 have better results than Tabu Search in most of test sets  
In the future we are planning to improve the algorithm for solving larger instances 
Moreover we hope that we can find the other approach with better results for A
SNDP 
 
Acknowledgement We would like to thank Prof Jakub Gładysz Prof Krzysztof 
Walkowiak for providing us the ASNDP problem instances as well as sending us the 
materials related to their works on ASNDP problem This work was partially 
supported by the project “Models for next generation of robust Internet” funded 
by the Ministry of Science and Technology Vietnam and the project “Some 
Advanced Statistical Learning Techniques for Computer Vision” funded by the 
National Foundation for Science and Technology Development under grant number 
10201201117 The Vietnam Institute for Advanced Study in Mathematics provided 
part of the support funding for this work 
References 
1 Walkowiak K A Flow Deviation Algorithm for Joint Optimization of Unicast and Any
cast Flows in ConnectionOriented Networks In Gervasi O Murgante B Laganà A 
Taniar D Mun Y Gavrilova ML eds ICCSA 2008 Part II LNCS vol 5073 pp 
797–807 Springer Heidelberg 2008 
2 Gładysz J Walkowiak K Tabu Search Algorithm for Survivable Network Design Prob
lem with Simultaneous Unicast and Anycast Flows Intl Journal of Electronics and Tele
communications 561 41–48 2010 
3 Walkowiak K A New Function for Optimization of Working Paths in Survivable MPLS 
Networks In Levi A Savaş E Yenigün H Balcısoy S Saygın Y eds ISCIS 2006 
LNCS vol 4263 pp 424–433 Springer Heidelberg 2006 
4 Grover W Meshbased Survivable Networks Options and Strategies for Optical MPLS 
SONET and ATM Networking Prentice Hall PTR Upper Saddle River 2004 
5 Sharma V Hellstrand F Framework for MPLSbased Recovery RFC 3469 2003 
6 Vasseur J Pickavet M Demeester P Network Recovery Protection and Restoration of 
Optical SONETSDH IP and MPLS Morgan Kaufmann San Francisco 2004 
7 Kasprzak A Algorithms of Flow Capacity and Topology Structure in Computer Net
works Monography Wroclaw Polish 1989 
8 Pioro M Medhi D Routing Flow and Capacity Design in Communication and Com
puter Networks Morgan Kaufmann Publishers 2004 
144 
TT Binh Huynh et al 
9 Walkowiak K Anycast Communication – A New Approach to Survivability of Connec
tionOriented Networks In Gorodetsky V Kotenko I Skormin VA eds MMM
ACNS 2007 CCIS vol 1 pp 378–389 Springer Heidelberg 2007 
10 Nissen V Gold S Survivable Network Design with An Evolution Strategy In Yang 
A Shan Y Bui LT eds Success in Evolutionary Computation SCI vol 92 pp 263–
283 Springer Heidelberg 2008 
11 Johnson Deering Reserved IPv6 Subnet Anycast Addresses RFC 2526 1999 
12 Anycast vs Unicast httpcommunitydnseuAnycastpdf 
13 Gladysz J Walkowiak K Optimization of Survivable Networks with Simultaneous Un
icast and Anycast Flows In ICUMT Poland pp 1–6 2009 
14 Battiti R Brunato M Mascia F Reactive Search and Intelligent Optimization Sprin
ger New York 2008 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 145–151 2012 
© SpringerVerlag Berlin Heidelberg 2012 
ProteinProtein Binding Affinity Prediction Based  
on an SVR Ensemble 
Xueling Li1 Min Zhu2 Xiaolai Li1 HongQiang Wang1 and Shulin Wang3 
1 Intelligent Computing Lab Hefei Institute of Intelligent Machines  
Chinese Academy of Sciences Hefei Anhui 230031 PR China 
xlliiimaccn 
2 Robot Sensor and HumanMachine Interaction Laboratory Hefei Institute of Intelligent 
Machines Chinese Academy of Sciences Hefei Anhui 230031 PR China 
zhuminiimaccn 
3 School of Computer and Communication Hunan University Changsha  
Hunan 410082 PR China 
Abstract Accurately predicting generic proteinprotein binding affinities 
PPBA is essential to analyze the outputs of protein docking and may help in
fer real status of cellular proteinprotein interaction subnetworks However  
accurate PPBA prediction is still extremely challenging Machine learning me
thods are promising to address this problem We propose a twolayer support 
vector regression TLSVR model to implicitly capture binding contributions 
that are hard to explicitly model The TLSVR circumvents both the descriptor 
compatibility problem and the need for problematic modeling assumptions In
put features for TLSVR in first layer are scores of 2209 interacting atom pairs 
within each distance bin The base SVRs are combined by the second layer to 
infer the final affinities Leaveoneout validation on our heterogeneous data 
shows that the TLSVR method obtains a very good result of R=080 and 
SD=132 with real affinities Comparison experiment further demonstrates that 
TLSVR is superior to the previous stateofart methods in predicting generic 
PPBA  
Keywords Proteinprotein interaction affinity machine learning twolayer 
support vector machine potential of mean force 
1 
Introduction  
The affinity is the bridge of function and structure Revealing the energetic characte
ristics of cellular multimolecular complex is critical to understand the protein func
tion Four general scoring approaches have been developed to evaluate proteinprotein 
docking results and to predict proteinprotein binding affinity PPBA They are phys
icalbased force fields 1 empirical scoring functions 2 knowledgebased statistic
al potentials where volume correction is always considered to improve the prediction 
accuracy 36 and hybrid scoring functions 7 8 These scoring functions are suc
cessful in proteinprotein docking evaluation and some are successful in protein  
binding affinity prediction However existing scoring functions for proteinprotein 
146 
X Li et al 
 
docking usually do not hold capacity to predict the binding affinity of a complex 9 
Kastritis and Bonvin presented a protein−protein binding affinity benchmark consist
ing of binding constants Kd for 81 complexes to assess the performance of nine 
commonly used scoring algorithms Their results revealed a poor correlation R03 
between binding affinity and scores for all algorithms tested and concluded that accu
rate prediction of binding affinity remains beyond these methods reach Therefore 
improvement in proteinprotein interaction affinity is still in great need  
In previous work we developed a distanceindependent residue level potential of 
mean force to predict PPBA 10 on a small data set of PPBA including 80 protein
protein complexes Machine leaning methods can achieve satisfactory results without 
assuming any predefined model when used for affinity prediction However the gene
ralization of methods based on one single classifier is often limited Furthermore a 
great number of features and small data set lead to the curse of dimensionality Thus 
researchers seek for classifier ensembles 11 12 or multiple instance learning 13 to 
improve prediction accuracy and generalization and at the same time overcome the 
high feature dimensional disaster For example an ensemble learning method random 
forest as an ensemble is recently reported in diverse proteinligand binding affinity 
prediction with higher prediction accuracy 14 In our previous study we used sup
port vector regression models and rough set reduction model to predict TAPpeptide 
binding affinity and specificity 18 19 This method has a high interpretability  
In this work we build a twolayer support vector regression TLSVR model with 
greater generality and prediction accuracy by capturing the nonlinear combination 
effects on affinities of interacting atom pairs within each bin and between bins Se
condly we construct a new data set by considering that structure diversity will greatly 
affect the accuracy of PPBA prediction Finally we evaluate our method with LOO 
cross validation 
2 
Methods 
21 
Data Set 
1056 heterogeneous protein complexes were obtained from PDBbindCN 2010 ver
sion which includes complexes with a single residue mutation or multiple residue 
mutations 17 18 The dataset was then filtered with sequence similarity 50 by 
PDBculled httpdunbrackfccceduGuoliPISCESInputBphp with complex enti
ties criteria and other default parameters We finally integrated 49 proteins from 4 
which did not exist in the dataset to get a heterogeneous larger data set For simplici
ty only complexes with two chains were held except 1CHD with four chains EFGI 
Thus 180 proteinprotein interaction complexes were kept in our final data set 
22 
Input Features 
Machine learning methods require equal length of vectors How to represent a protein 
complex structure in an equallength vector as the input of TLSVR is the first prere
quisite 47 types of heavy atoms were defined as reported by Su et al 4 The occur
rence numbers of 2209 contact atom pairs within each 02 Å width bin were counted 
by Eq1 at each protein complex interface Our preliminary experiment shows that 
 
ProteinProtein Binding Affinity Prediction Based on an SVR Ensemble 
147 
 
TLSVR achieves best prediction results when the cutoff distance of a pair of contact 
atoms equals 16 Å Thus 71 distance bins ie 80 bins minus 9 bins with distance 
threshold below 18 Å were kept The final features consist of scores of 2209 contact 
atom pairs obtained with Eq2 which are obtained by multiplying the occurrence 
number of each contact atom pair with the corresponding atom pair potential The 
interacting atom pair’s potentials were defined as the natural logarithm of the ratio of 
observed number of interacting atom pairs to those expected The smoothed potentials 
were generated as illustrated in 4 Thus 2209 smoothed atom pair scores features 
were finally generated Specifically  




 
 
ij
obs
f
N
i j r
P i j r
=
×
 
1 
where 
Nobs    
i j r is the observed number of interacting atom pairs  
i j  within the 
distance shell 

 
r r
− Δr r
 in a given proteinprotein binding structure 

02
r
r
Δ
Δ =
 
Å is the bin width for 18Å 
≤ rcut
≤ 16 Å between two interface chains or proteins at 
th
n distance bin of width 02 Å  
exp
 
   
   
log

12347
12347
   
cor
Nobs
i j r
P i j r
f
i
j
N
i j r
=
=
=
） 
2
The bins 
 
r r
− Δr r
range from 18 Å to 16 Å at 02 Å intervals The interfacial atom 
pair potential 
exp   
 
i
j
total
N
i j r
X
X
N
r
=
×
×
 is the expected number of interacting 
atom pairs of  
i j  between two interface chains or proteins if there are no preferential 
interactions between them 
i
X  is the mole fraction of atom type i and is calculated as 
Ni 
N  where Ni and N are the total number of atom type i and all atoms respectively  
while 
Ntotal  
r  is the total number of interacting atom pairs derived from the refer
ence database 4 
fcor
 is the correction factor derived from smoothing 
exp
    
   
Nobs
i j r
N
i j r  ranging from 18 Å to 16 Å by a moving window of 30 Å 
width for bin of width 02 Å  
23 
TwoLayer Support Vector Regression Model  
Support Vector Machine SVM is introduced by Vapnik 19 While SVM classifica
tion outputs binary results binding or nonbinding Support Vector Regression SVR 
model produces continuous values affinity absolute value  
Proteinprotein binding affinity was predicted by using twolayer SVR TLSVR 
Each input vector at first layer of TLSVR is 2209dimensional As depicted above 
each real value of a vector represents a score of an atom pair in interface within each 
of 71 bins ie 18 Å 02 Å 16 Å of a protein complex Here the contact atom pairs 
with distance below 18 Å of atom clashes were disregarded 71 individual SVR mod
es were included at first layer As shown in Fig1 the predicted values from the  
individual SVR modes of the first layer were input into the second layer SVR the 
combiner The output of the combiner was the final predicted affinity Parameters 
were default in individual SVR models All the computational experiments are carried 
out with LIBSVM that is available at httpwwwcsientuedutwcjlinlibsvm 
148 
X Li et al 
 
 
Fig 1 Scheme of the proposed twolayer SVM prediction system 
For comparison purpose a onelayer SVR OLSVR model was also developed 
Different from TLSVR each input feature for OLSVR was the sum score of 2209 
interacting atom pairs within each bin Thus total 71 scores from different distance 
bins were input into onelayer SVR after discarding the contact atom pairs with dis
tance below 18 Å of atom clashes as described above We then compared the predic
tion results of the one with twolayer SVR  
24 
Evaluation of TLSVR Model 
The performance of SVR models was tested by the leaveoneout crossvalidation 
LOOCV by testing of each protein complex structure of the data set due to the small 
size of the data set The data set having n  complexes is broken in n  subsets each 
having one example The classifier was trained on 
1
n − subset and evaluated on nth 
subset The process was repeated n times using each subset as the testing set and the 
rest of complexes for training set The correlation coefficient and standard derivation 
between the predicted values of all samples and the real ones are calculated by com
bining overall estimate of training procedure 
3 
Results and Discussion 
31 
Results of OLSVR and TLSVR 
TLSVR models were generated by using RBF kernel type in this study without extra 
specification Prediction of binding affinities of protein complexes was done by using 
TLSVM The final correlation coefficient R result of the TLSVR was obtained from 
LOOCV evaluation which was recognized to be the most extreme and accurate type 
of crossvalidation test  
Table 1 Results of the one and twolayer radial base kernel SVM for proteinprotein binding 
affinity prediction 
SVR Models 
Features 
Rd 
SDe 
PMF 
 
018 
218 
Onelayer 
1 final score 
024 
216 
Onelayer 
71 grid scores 
050 
192 
Twolayer 
1 final score 
048 
194 
Twolayer 
2209 ×71 grids 
080 
132 
 
ProteinProtein Binding Affinity Prediction Based on an SVR Ensemble 
149 
 
 
 
 
A 
B 
Fig 2 Correlation of the predicted affinities and the observed ones A Prediction results of 
onelayer SVR OLSVR B prediction results of twolayer SVR ensemble TLSVR 
Table 1 shows the results of the OLSVR and TLSVR models with RBF kernel The 
best OLSVR prediction obtained R=052 and SD=190 Fig2a when input features 
were the sum over 2209 atom pair smoothed scores within each of 71 distance bins 
TLSVR prediction obtained R=080 and SD=132 for 71 distance bins Fig2b when 
input features were smoothed scores of each contact atom pair All prediction results 
demonstrate that TLSVR has satisfactory performance and greatly improves the pre
diction accuracy of PPBA compared with OLSVR  
Why did TLSVR improve the prediction accuracy so much In OLSVR at each in
terval 2208 atom pair scores were lineally summed up to one score value Since each 
sum score of a distance bin constituted one feature 71 features in total were input into 
OLSVR The lower prediction accuracy of OLSVR shows that simply linearly sum
mation of the scores of 2209 interacting atom pairs at each distance bin without  
consideration their nonlinear effects on binding affinity results in significant deteri
oration in prediction On the other hand twolayer radial basis SVR improves the 
accuracy greatly by the consideration of the nonlinear relationship between atom pair 
in different bins Thus the proposed TLSVR model leads to tremendous improvement 
in the prediction performance by filtering results of the first layer 
Table 2 Comparison of  the prediction performance of TLSVM on dataset with slightly 
different sequence similarity cut 
Heterogeneity 
R 
SD 
50Similarity Cut Culled on Entity plus  49 Su’s protein 180 
080 
132 
50Similarity Cut Culled on Entity 161 
084 
122 
40Similarity Cut Culled on Entity 91 
078 
133 
 
We notice that the nonlinear combination of 71 scores captured by using radial 
kernel basis OLSVR did not improve the prediction accuracy by comparing one final 
sum score with 71 scores from different distance bins as input into OLSVR As shown 
in Table 1 for OLSVR model accounting for the nonlinear effects of scores from 71 
150 
X Li et al 
 
bins obviously improved the prediction accuracy of OLSVR to R=05 compared with 
R=024 obtained with one final score as input Table 1 These results demonstrates 
that accounting for the nonlinear relationship of 2209 atom pairs may be the impor
tant factor that improves TLSVR prediction accuracy Using SVR combiner that cor
related the results of models generated in first layer also contributes the prediction 
improvement It can be concluded that the proposed TLSVR method highly improves 
the prediction accuracy and generalization ability 
Finally on dataset with different sequence similarity cut and heterogeneous protein 
complexes our results in Table 2 show that both heterogeneous and low similarity 
cutoff decrease the TLSVM prediction accuracy 
32 
Comparison with Other Stateof Art Methods 
To our best knowledge DFIRE and Su’s methods are two representative recently 
proposed PMFbased methods for PPBA prediction RFscore is recently developed 
method for proteinligand affinity prediction We compare our method with these 
three representative methods on our collected data set Comparison results are shown 
in Table 3 We can see that DFIRE obtained a correlation of 012 only Su’s method 
only obtained a correlation coefficient of 018 in predicting the generic and heteroge
neous PPBA TLSVR model obtained much a higher correlation coefficient of 080 
between the predicted and experimental affinities than both methods  
Table 3 Comparison of TLSVM with two state of art methods DFIRE and Su’s method with 
LOO crossvalidation on our data set 
On independent test set 
R 
SD 
DFIRE on training set 
012 
221 
Su’s Method on training set 
018 
218 
RFscore 
018 
218 
TLSVM 
080 
132 
 
Acknowledgements This work was supported by the Knowledge Innovation Pro
gram of Chinese Academy of Sciences No 0823A16121 Anhui Provincial Natural 
Science Foundation No 1208085MF96 and National Natural Science Foundation of 
China Nos 31071168 30900321 60973153 and 61133010 
References 
1 Kollman PA Massova I Reyes C Kuhn B Huo S Chong L Lee M Lee T 
Duan Y Wang W Donini O Cieplak P Srinivasan J Case DA Cheatham TE 
3rd Calculating Structures and Free Energies of Complex Molecules Combining Molecu
lar Mechanics and Continuum Models Acc Chem Res 33 889–897 2000 
2 Bohm HJ Prediction of Binding Constants of Protein Ligands a Fast Method for the 
Prioritization of Hits Obtained from De Novo Design or 3D Database Search Programs J 
Comput Aided Mol Des 12 309–323 1998 
 
ProteinProtein Binding Affinity Prediction Based on an SVR Ensemble 
151 
 
3 Melo F Feytmans E Novel Knowledgebased Mean force Potential at Atomic Level J 
Mol Biol 267 207–222 1997 
4 Su Y Zhou A Xia X Li W Sun Z Quantitative Prediction of Proteinprotein Bind
ing Affinity with a Potential of Mean Force Considering Volume Correction Protein 
Sci 18 2550–2558 2009 
5 Lu H Lu L Skolnick J Development of Unified Statistical Potentials Describing Pro
teinprotein Interactions Biophysical Journal 84 1895–1901 2003 
6 Muegge I PMF Scoring Revisited J Med Chem 49 5895–5902 2006 
7 Englebienne P Moitessier N Docking Ligands into Flexible and Solvated Macromole
cules 4 Are Popular Scoring Functions Accurate for this Class of Proteins Journal of 
Chemical Information and Modeling 49 1568–1580 2009 
8 Oda A Tsuchida K Takakura T Yamaotsu N Hirono S Comparison of Consensus 
Scoring Strategies for Evaluating Computational Models of Proteinligand Complexes 
Journal of Chemical Information and Modeling 46 380–391 2006 
9 Kastritis PL Bonvin AMJJ Are Scoring Functions in ProteinProtein Docking Ready 
To Predict Interactomes Clues from a Novel Binding Affinity Benchmark Journal of 
Proteome Research 9 2216–2225 2010 
10 Li XL Hou ML Wang SL A Residual Level Potential of Mean Force Based Ap
proach to Predict ProteinProtein Interaction Affinity In Huang DS Zhao Z Bevilac
qua V Figueroa JC eds ICIC 2010 LNCS vol 6215 pp 680–686 Springer Heidel
berg 2010 
11 Wolpert DH Stacked Generalization Neural Network 5 241–259 1992 
12 Xia JF Zhao XM Huang DS Predicting Proteinprotein Interactions from Protein 
Sequences Using Meta Predictor Amino Acids 39 1595–1599 
13 Teramoto R Kashima H Prediction of Proteinligand Binding Affinities Using Multiple 
Instance Learning Journal of Molecular Graphics and Modelling 29 492–497 
14 Ballester PJ Mitchell JBO A Machine Learning Approach to Predicting Protein
ligand Binding Affinity with Applications to Molecular Docking Bioinformatics 26 
1169–1175 2010 
15 Li XL Wang SL A Comparative Study on Feature Selection in Regression for Pre
dicting the Affinity of TAP Binding Peptides In Huang DS Zhang X Reyes García 
CA Zhang L eds ICIC 2010 LNCS vol 6216 pp 69–75 Springer Heidelberg 
2010 
16 Li XL Wang SL Hou ML Specificity of Transporter Associated with Antigen 
Processing Protein as Revealed by Feature Selection Method Protein and Peptide Let
ters 17 1129–1135 2010 
17 Wang RX Fang XL Lu YP Wang SM The PDBbind Database Collection of 
Binding Affinities for Proteinligand Complexes with Known Threedimensional Struc
tures Journal of Medicinal Chemistry 47 2977–2980 2004 
18 Wang RX Fang XL Lu YP Yang CY Wang SM The PDBbind Database Me
thodologies and Updates Journal of Medicinal Chemistry 48 4111–4119 2005 
19 Vapnik VN Statistical learning theory Springer New York 1998 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 152–159 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A Novel TwoStage Alignment Method for Liquid 
Chromatography Mass SpectrometryBased 
Metabolomics 
Xiaoli Wei1 Xue Shi1 Seongho Kim2 Craig McClain3456 and  Xiang Zhang1 
1 Departments of Chemistry University of Louisville Louisville KY 40292 
jujuxiaoxueshisxgmailcom xiangzhanglouisvilleedu 
2 Bioinformatics and Biostatistics University of Louisville Louisville KY 40292 
biostatisticiankimgmailcom 
3 Medicine 4 Pharmacology  Toxicology 5 Alcohol Research Center 6 Robley Rex Louisville 
VAMC University of Louisville Louisville KY 40292 
craigmcclainlouisvilleedu 
Abstract We report a novel twostage alignment algorithm that contains full 
alignment and partial alignment for the analysis of LCMS based metabolomics 
data The purpose of full alignment is to detect landmark peaks that present in 
all peak lists to be aligned These peaks were first selected based on mz value 
and isotopic peak profile matching After removing peaks with large Euclidian 
distance of retention time from the potential landmark peaks a mixture score 
was calculated to measure the matching quality of each landmark peak pair be
tween reference peak list and a test peak list After optimizing the weight factor 
in the mixture score the value of minimum mixture score of all landmark peaks 
was used as the threshold for peak matching in the partial alignment A local 
optimization based retention time correction method was used to correct the re
tention time changes between peak lists during partial alignment The twostage 
alignment method was used to analyze a spikedin experimental data and fur
ther compared with literature reported algorithm RANSAC implemented in 
MZmine 
Keywords LCMS twostage peak list alignment local optimization 
1 
Introduction 
Metabolomics is the study of low molecular weight molecules ie metabolites 
found within cells and biological systems  It aims to measure and interpret the com
plex timerelated concentration activity and flux of large sets of metabolites in  
biological samples  Several types of instruments have been utilized to analyze  
                                                           
  This work was supported by NIH grant 1RC2AA019385 through the National Institute on 
Alcohol Abuse and Alcoholism This work was also partially supported by National Institute 
of Health NIH grant 1RO1GM087735 through the National Institute of General Medical 
Sciences NIGMS 
 
A Novel TwoStage Alignment Method for Liquid Chromatography 
153 
 
metabolites including nuclear magnetic resonance NMR gas chromatographymass 
spectrometry GCMS and liquid chromatographymass spectrometry LCMS 
Each type of instrumental analysis provides limited coverage of the metabolites and 
therefore only generates a partial metabolite profile of each sample Currently signif
icant challenges remain in almost every aspect for the application of metabolomics to 
biomedical research Among these the lack of accurate and efficient bioinformatics 
tools for the processing of metabolomics data has become a critical bottleneck to the 
progress of metabolomics  Many data analysis steps are involved in deciphering the 
mass spectrometry data including data preprocessing metabolite identification quan
tification network and pathway analysis 
Peak alignment is a key step of data preprocessing in LCMS based metabolomics 
It recognizes peaks generated by the same metabolite occurring in different samples 
from the millions of peaks detected during the course of an experiment 1 2 A large 
volume of informationrich data can be generated in a LCMS based metabolomics 
study To carry out the alignment procedure several bioinformatics tools have been 
developed including XCMS2 3 centWave 4 MZmine2 5 MZedDB 6 OpenMS 
7 However the accuracy of peak alignment remains challenge in metabolomics  
The objective of this work was to develop a novel approach for high accuracy peak 
alignment For this reason we developed a twostage alignment method to align the 
peak lists generated from highresolution mass spectrometry for metabolomics study 
The developed method has been implemented in MetSign 8 and used to analyze a set 
of spikedin experimental data acquired on a LCMS system The performance of this 
method was compared with existing software packages MZmine 
2 
Experimental Section  
21 
Spikedin Samples  
A mixture of 30 compound standards was prepared at a concentration of 100 μgmL 
for each compound The standards included 11 fatty acid benhenic acid tricosanoic 
acid stearic acid myristic acid nonadecanoic acid heptadecanoic acid adipic acid 
heneicosanoic acid nonanoic acid butyric acid linoleic acid 5 triglycerides trilau
roylglycerol trimyristin tripalmitin tricaprylin tricaprin 9 phospholipids 
PC160160 PC160140 PC120120 PC6060 LysoPC16000 Ly
soPC100 
PC2045Z8Z11Z14Z160 
PC1829Z12Z1829Z12Z 
PC24115Z24115Z and 5 other small molecules caffeine Ltryptophan lido
caine creatine trans4hydroxylLproline 10 μL of the standard mixture was added 
to a 100 μL sample of metabolite extract of mouse liver and dichloromethane me
thanol vv = 21 was then added to the sample vials to make the total volume up to 
200 μL  
22 
LCMS Analysis  
A LECO Citius LCHRT high resolution mass spectrometer equipped with an Agilent 
1290 Infinity UHPLC with a Waters Acquity UPLC BEH hydrophilic interaction 
chromatography HILIC 17 μm 150 × 21 mm column was used in this work The 
154 
X Wei et al 
 
sample was loaded in H2O + 5 mM NH4OAc + 02 acetic acid buffer A and sepa
rated using a binary gradient consisting of buffer A and buffer B 9010 acetoni
trileH2O + 5 mM NH4OAc + 02 acetic acid Flow rate was set at 250 μLmin on 
the column with 100 B for 4 min 45 B at 12 min holding to 20 min 100 B at 
21 min and holding to 60 min for the gradient The Citius was operated with electro
spray ionization in positive ion mode The system was optimized in high resolution 
mode R = 50000 FWHM and was mass calibrated externally using Agilent Tune 
Mixture ATM The mass spectrometry was operated in both full mass analysis and 
tandem MSMS mode to acquire molecular mz value and the corresponding MSMS 
spectrum The spikedin sample was analyzed 6 times on the LCMS system 
3 
Theoretical Basis  
The peak alignment method was developed as a twostage algorithm full alignment 
and partial alignment The goal of full alignment is to recognize landmark peaks 
which are defined as a set of metabolite peaks present in every sample In the partial 
alignment stage the peaks in a test sample that are not recognized as the landmark 
peaks are aligned 
Let 
1
2
1





i
n
S
S
S
S
S +
=
be the sample set and 
1
n +  is the total number of 
samples to be aligned After selecting the reference peak list RPL in a random man
ner the rest of peak lists are considered as test samples which can be written as 
1
2

 

 
 
i
n
S
RPL t t
t
t
=
…
…
 Each of the test peak lists is aligned to the RPL re
spectively 
Considering two peak lists 
 
RPL ti
 all mz value matched peak pairs between 
these two peak lists can be selected using a user defined mz variation If a peak can 
be matched to multiple peaks in the other peak list the peak pair with the minimum 
retention time difference is selected as the most probable match and the other matches 
are discarded Therefore the mz matched peak pairs can be recorded as 

 



1
1
2
2








p
p
r s
r s
r s
…
 where r  is a peak from RPL  s  is a peak from 
it  and p  is the total number of the mz matched peak pairs  The mz matched peak 
pairs are further filtered based on the Euclidean distance of retention time between r  
and s  ie 
j
j
j
d
r
s
=
−
 with a confidence interval of 95 The peak pairs filtered 
by retention time are represented as 
 



1
1
2
2








m
m
r s
r s
r
s
…
 and m
≤ p
 
This process is iteratively operated on all the test samples respectively 
A mixture similarity score 
m
S  was developed to measure the matching quality be
tween two peaks as follows 




1

|
exp
16
1
1
i
min
m
i
i
med
min
i
d
d
S
d
w
w
w
d
d
−
Δ
=
∗
−
∗
+
−
∗
−
+ Δ






   
1
 
A Novel TwoStage Alignment Method for Liquid Chromatography 
155 
 
where 
id  is the Euclidean distance of retention time between the i th matched peak 
pair 
dmin
 and 
dmed
 are the minimum and median retention time distance among all 
matched peaks in the two peak lists respectively Δ is the absolute value of mz dif
ference between the i th matched peak pair and w  is a weight factor and 
0
1
≤ w
≤
  
The peaks that are present in every test peak list and are matched to the same peak in 
the RPL are used to optimize the value of weight factor w  for the alignment of a test 
peak list and the RPL by maximizing the value of 
1


|

k
m
i
i
i
S
d
w
=
Δ

 k  is the num
ber of matched peaks between the test peak list and the RPL and w  is set as 005 
01 02 03 04 05 06 07 08 09 and 095 respectively After optimizing the 
weight factor w  the value of 
m
S  is calculated for each matched peak pair between 
the test peak list and the RPL followed by an outlier detection in 
j
m
S  
1

j
k
=
…
 
By iteratively considering pair set 
 |
1
 
i
RPL t i
n
=
…
 the landmark peaks 




1
11
1
1










n
m
m
nm
r t
t
r t
t
…
…
…
 are obtained The minimum mixture score 
min
Sm
 among all the test peak lists is then used as a threshold value in the partial 
alignment 
To perform the partial alignment the retention time value of each landmark peak in 
the test peak list is assigned to the retention time value of the corresponding landmark 
peak in the RPL A local polynomial fitting method is employed to correct the reten
tion time of peaks present between two adjacent landmark peaks Because multiple 
landmark peaks can be detected in a set of experimental data adjusting retention time 
shifts using two adjacent landmark peaks can correct nonlinear retention time shifts 
To correct the retention time of peaks not present between two landmark peaks an 
iteratively optimization method is applied to the group of peaks eluted earlier than the 
firsteluted landmark peak and the group of peaks eluted later than the lasteluted 
landmark peaks respectively In each optimization process 30 of landmark peaks 
are randomly selected from 



1
11
1






m
m
r t
r t
…
 and a polynomial model fitting 
error is computed as follows 
 


1
|
|
N
o
f
R i
R i
i
t
t
ε
=
=
−

    
2
where 

o
tR i
 is the original retention time of the i th peak  

f
tR i
 is the fitted retention 
time of the i th peak N is the number of peaks in the test peak list at the region of 
interest This process is repeated 1000 times and the model with minimum error is 
selected and used for retention time correction  
After the retention time correction the partial alignment is applied to all the non
landmark peaks present in each of the test peak lists and aligns them to the peaks 
156 
X Wei et al 
 
present in the RPL where a mixture score 
m
S is calculated using equation 1 for 
each peak pair A peak pair is considered to be a match if its mixture score is larger 
than 
min
Sm
 It is possible that one peak in the test sample can be matched to multiple 
peaks in the RPL and vice versa In these cases the peak pair with the maximum mix
ture score is kept while the remaining matches are discarded If there is a peak in the 
test peak list that cannot be matched to any peaks in the RPL this peak is considered 
as a new peak to the RPL and is added to the RPL The updated RPL is then used to 
align the peaks in the next test peak list and this process is repeated until all the test 
peak lists are aligned 
4 
Results and Discussion 
The raw instrument data were first converted into mzML format and further reduced 
to peak lists using MetSign software 8 There are about 2300 peaks detected by Met
Sign software in each sample and about 1100 peaks assigned to a database compound 
Of the 30 spikedin compound standards most of the compounds were detected based 
on the match of mz values with a variation window of ≤ 5 ppm and the similarity of 
isotopic peak profile measured by Pearson’s correlation coefficient ≤ 075 Table 1 
shows the number of compound standards detected in each replicate injection The 
variation of the number of detected compound standards was generated by the expe
rimental variation After MetSign processing a total of six peak lists were generated 
and these peak lists were subjected for peak alignment  
Table 1 The number of compound standards detected in six replicate injections 
Index of replicate injection 
1 
2 
3 
4 
5 
6 
No of compound standards 
identified in each injection 
25 
23 
23 
24 
24 
24 
During the full alignment a total of 283 landmark peaks were detected with reten
tion time ranges from 9152 s to 64314 s Of the 283 landmark peaks 18 were the 
peaks generated by the spikedin compound standards while the remaining landmark 
peaks were generated by the metabolites extracted from mouse liver Figure 1 shows 
the effectiveness of retention time correction to the nonlandmark peaks during the 
partial alignment Even though the experiments of the six replicate injections were 
performed under the identical experimental conditions the retention time of each 
compound still drifted between injections and such a retention time drift is not linear 
Therefore the local polynomial fitting method is able to correct the retention time of 
peaks present between two adjacent landmark peaks 
To compare the alignment accuracy of the twostage alignment method the expe
riment data were also processed using publically available software MZmine MZmine 
software has two alignment methods Join aligner and RANSAC aligner Join aligner 
is a simple alignment method which aligns detected peaks in different samples  
 
 
A Novel TwoStage Alignment Method for Liquid Chromatography 
157 
 
500
520
540
560
580
600
620
640
480
500
520
540
560
580
600
620
640
Retention time before correction s
Retention time after correction s
 
Fig 1 Local optimization based retention time correction for compounds eluted between two 
landmark peaks The value of each red star in xaxis is the retention time of a compound before 
correction and the value in yaxis is after retention time correction The solid blue line is a 
guideline depicting a situation of no retention time correction 
 
3
4
5
6
0
5
10
15
20
25
Frequency
Number of molecula
 
 
Twostage alignment
Ransac alignment
 
Fig 2 Alignment results of spikedin compound standards by twostage method blue and 
RANSAC method of MZmine25 red 
 
 
 
158 
X Wei et al 
 
through a match score RANSAC aligner is an extension of the Join aligner It in
cludes a method of retention time correction to adjust the retention time shift in all 
peak lists Therefore we chose the RANSAC alignment in MZmine25 for compari
son Figure 2 depicts the alignment results of the twostage alignment method and the 
RANSAC method Based on the experimental design all of the spikedin compound 
standards should be correctly aligned In the twostage alignment a total 21 peaks of 
the spikedin standards are aligned in all six injections while RANSAC only fully 
aligned 16 compound standards Furthermore all the spikedin compound standards 
were aligned in at least 4 peak lists of the 6 replication injections by our method 
while RANSAC still had 2 compound standards aligned in only three injections 
 
0
2
4
6
8
10
12
0
01
02
03
04
05
06
07
08
09
1
RSD1tR 
Probability
 
 
Twostage alignment
Ransac alignment
 
Fig 3 The comparison of RSD value between twostage alignment we proposed and RANSAC 
alignment in MZmine25 
Figure 3 shows the distribution of relative standard deviation RSD of all aligned 
peaks by the two testing alignment algorithms The maximum RSD of aligned peaks 
in the twostage alignment method is 42 Manual validation shows that this align
ment is correct There are 12 compounds were aligned by RANSAC with a retention 
time RSD larger than 4 The maximum retention time RSD reached 107 which is 
much larger than the retention time variation caused by the experiments Such a large 
retention time variation was caused by the inaccuracy of peak alignment From the 
comparative analysis we conclude that the twostage alignment method outperforms 
the RANSAC alignment by providing high accuracy of peak alignment for the analy
sis of LCMS based metabolomics data 
5 
Conclusions 
A novel twostage alignment algorithm containing full alignment and partial  
alignment was developed for high accuracy peak list alignment for LCMS based 
 
A Novel TwoStage Alignment Method for Liquid Chromatography 
159 
 
metabolomics The full alignment detects landmark peaks that are present in all the 
peak lists During this process the potential landmark peaks were first selected based 
on the mz and isotopic peak profile matching After removing outliers based on the 
Euclidian distance of retention time from the potential landmark peaks a mixture 
score method was employed to evaluate the match quality of each landmark peak pair 
between the reference and the test sample peaks The value of minimum mixture 
score of all landmark peaks was used as the threshold of peak matching during partial 
alignment in which local optimization based retention time correction was employed 
to correct the retention time changes between peak lists The performance of the two
stage alignment method was tested by analyzing a spikedin experimental data and 
further compared with literature reported algorithm RANSAC implemented in 
MZmine25 The comparison demonstrates that our twostage alignment method out
performs the RANSAC algorithm for high accuracy of peak alignment  
References 
1 
Zhang X et al Data preprocessing in Liquid Chromatographymass Spectrometry
based Proteomics Bioinformatics 2121 4054–4059 2005 
2 
Wang B et al DISCO Distance and Spectrum Correlation Optimization Alignment for 
twodimensional Gas Chromatography timeofflight Mass Spectrometrybased Metabo
lomics Anal Chem 8212 5069–5081 2010 
3 
Benton HP et al XCMS2 Processing Tandem Mass Spectrometry Data for Metabolite 
Identification and Structural Characterization Anal Chem 8016 6382–6389 2008 
4 
Tautenhahn R Bottcher C Neumann S Highly Sensitive Feature Detection for High 
Resolution LCMS BMC Bioinformatics 9 504 2008 
5 
Pluskal T et al MZmine 2 Modular Framework for Processing Visualizing and Ana
lyzing Mass Spectrometrybased Molecular Profile Data BMC Bioinformatics 11 395 
2010 
6 
Draper J et al Metabolite Signal Identification in Accurate Mass Metabolomics Data 
with MZedDB an Interactive mz Annotation Tool Utilising Predicted Ionisation Beha
viour ’rules’ BMC Bioinformatics 10 227 2009 
7 
Sturm M et al OpenMSAn Opensource Software Framework for Mass Spectrometry 
BMC Bioinformatics 9 2008 
8 
Wei XL et al MetSign A Computational Platform for HighResolution Mass Spectro
metryBased Metabolomics Analytical Chemistry 8320 7668–7675 2011 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 160–167 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Reconstruction of Metabolic Association Networks  
Using Highthroughput Mass Spectrometry Data 
Imhoi Koo12 Xiang Zhang2 and Seongho Kim1 
1 Department of Bioinformatics and Biostatistics  
University of Louisville Louisville KY 40292 USA  
2 Department of Chemistry University of Louisville Louisville KY 40292 USA 
imhoikooxiangzhangs0kim023louisvilleedu 
Abstract Graphical Gaussian model GGM has been widely used in genomics 
and proteomics to infer biological association networks but the relative 
performances of various GGMbased methods are still unclear in metabolomics 
The association between two nodes of GGM is calculated by partial correlation 
as a measure of conditional independence To estimate the partial correlations 
with small sample size and large variables two approaches have been 
introduced which are arithmetic meanbased and geometric meanbased 
methods In this study we investigated the effects of these two approaches on 
constructing association metabolite networks and then compared their 
performances using partial least squares regression and principal component 
regression along with shrinkage covariance estimate as a reference These 
approaches then are applied to simulated data and real metabolomics data 
Keywords metabolomics graphical Gaussian model partial correlation partial 
least squares regression principal component regression false discovery rate 
1 
Introduction 
Metabolomics is a rapidly emerging field to systemically analyze smallmolecule 
metabolites in a biological organism 1 It is equally important in systems biology as 
other “omics” such as genomics transcriptomics and proteomics One of the 
important approaches to integrating the individual “omics” data for system level 
analysis is the reconstruction of cellular networks which is collection and 
visualization of all physiologically relevant cellular processes  
In metabolomics a relatively small number of studies have been reported for 
metabolic network construction For instance Arkin et al 2 predicted interactions 
within reaction networks over time for the glycolytic pathway where Pearsons 
correlation coefficient was used to construct the interaction networks A major 
drawback of Pearsons correlationbased networks is unable to distinguish between 
                                                           
  This work was also partially supported by National Institute of Health NIH grant 
1RO1GM087735 through the National Institute of General Medical Sciences NIGMS 
 
Reconstruction of Metabolic Association Networks 
161 
 
direct and indirect associations On the other hand graphical Gaussian models 
GGMs reveal direct associations with conditional independencesdependences 
among variables using partial correlation coefficients that are calculated by the 
correlation of two variables after removing affection of other variables 3 GGMs 
have been employed in metabolomics for several studies 4 5 Note that the size of 
samples experiments was larger than the number of variables metabolites for these 
studies so that network construction was straightforward 
If the number of samples is much smaller than number of variables it is difficult to 
directly estimate partial correlation due to singularity To overcome this difficulty 
several methods have been developed by either reducing the number of given 
variables or a regularized estimation 6 7 Another alternative is to use dimension
reduced regression such as partial least squares regression PLSR and principal 
component regression PCR approaches When calculating the partial correlations 
using regression coefficients arithmetic and geometric means of regression 
coefficients were employed in Kramer et al 8 and Pihur et al 9 respectively The 
partial correlation coefficients estimated by these two methods are not the same to 
each other and it is important to investigate the effects of the different calculation 
methods on network reconstruction Therefore we evaluated the performance of 
PLSR and PCR using shrinkage covariance estimate as a reference in terms of 
network construction  
2 
Methods and Materials 
The graphical Gaussian model GGM is a statistical multivariate analysis to infer the 
direct relationship among variables using nodes and edges 3 where the nodes 
correspond to the variables under consideration and the edges represent the 
conditional independence between two variables as measured by partial correlation 
coefficient  
Suppose a data matrix 
 consists of  observed samples and  metabolites with a 
mean of zero Then the partial correlation coefficient matrix 
 is calculated 
by the inverse of the covariance matrix 
 as follows  
 
1
where 
 
The covariance matrix  becomes singular when the sample size  is smaller than 
the number 
 of variables To deal with singularity several methods have been 
introduced 8 In this study the following three methods are considered 
21 
Shrinkage Covariance Estimation 
Schafer and Strimmer 10 introduced shrinkage covariance estimator SCE for the 
partial correlation estimation when the covariance matrix 
 is singular 
Under singularity of covariance matrix the SCE is to trade off the unbiased sample 
covariance  and low dimensional shrinkage target matrix 
 
162 
I Koo X Zhang and S Kim 
 
 
2
where 
 is shrinkage intensity The optimal value of the tuning parameter  is 
analytically determined and estimated from the data  
22 
Regression with Dimension Reduction 
Partial Least Squares Regression and Principal Component Regression The 
common property of both partial least squares regression PLSR and principal 
component regression PCR is to use dimension reduction method to avoid the 
singularity for the “small  large ” paradigm PLSR finds orthogonal vector 
 to 
maximize the covariance between 
 and dependent response variable 
 
while PCR searches for orthogonal vector 
 to maximize the variance of 
  
Consider linear regression of dependent variable  on data matrix 
 as follows 
 
3
where 
 is a vector of regression coefficients and  is error The estimation of 
coefficient  using PLSR consists of two steps 11 The first step is to extract a latent 
variable set 
 of orthogonal components 
 which 
maximizes a covariance with dependent variable  The second step is to estimate the 
coefficient of regression of  on the new latent variable set 
 and then to transform it 
into space spanned by data 
 The first PLSR component 
 is obtained by 
maximizing the covariance as follows 
 
4
The next components 
 are satisfied with maximizing the squared 
covariance to  and are mutually orthogonal to each other Consider the orthogonal 
part 
 of 
 on all components 
 
 
5
where 
 is the projection operator related to 
 The th latent 
variable 
 is then obtained by solving the optimization problem 
 
6
Using the following equations the vector of regression coefficients 
 is 
determined to predict the output of a model including  components 
 
7
For PCR the equations 4 and 6 are replaced with the following equations 
respectively 
 
Reconstruction of Metabolic Association Networks 
163 
 
 
8
Then the predicted output 
 and regression coefficients 
 can be calculated 
by  
 
9
Once the regression coefficients in equations 7 and 9 are computed the partial 
correlation coefficients are estimated by using either geometric or arithmetic mean of 
regression coefficients  
Method 1 Geometric mean approach In this approach the partial correlation 
coefficient 
 of 
 in the equation 1 is estimated by  
 
10
Method 2 Arithmetic mean approach Arithmetic mean approach of the 
associationinteraction scores was introduced by Pihur et al 9 The partial 
correlation is calculated by 
 
11
In this equation 11 the coefficients are obtained from 
 
12
where 
 is a th latent variable of PLSR and PCR  and  is the number of latent 
variables which is predetermined by user 
23 
False Discovery Rate 
After estimating partial correlation coefficients statistical hypothesis test is 
performed to select the significant edges indicating strong association between two 
variables To do this false discovery rate FDR is applied to control the expected 
proportion of incorrectly rejected null hypotheses by using the qvalue method in R 
software package fdrtool 12 
24 
Data 
Simulation Data The simulated data were generated using two conditions sample 
size and network complexity The number of variables  was always set to 100 We 
used three different densities 5 15 and 25 to describe the complexity of the 
network Given each density we considered five different sample sizes 25 50 100 
150 and 200 to generate simulated data For each case we generated 100 data sets 
164 
I Koo X Zhang and S Kim 
 
and then compared the performance of each method with their averages The R 
software package GeneNet was used to generate the simulated data 13  
 
Experimental Data We also investigated the performance of each method using 
experimental data of metabolites extracted from mouse liver The experimental data 
consist of all compounds detected from mouse samples on a linear trap quadruple
Fourier transform ion cyclotron resonance mass spectrometer LTQFTICR MS via 
direct infusion electrospray ionization DIESImass spectrometry For the 
association network study we used 99 compound peaks that were detected in all 40 
samples by MetSign software 14  
25 
Performance Evaluation 
We evaluated the five estimation methods shrinkage covariance estimation SCE 
geometricarithmetic meanbased partial least squares regression PLSRG and 
PLSRA respectively and geometricarithmetic meanbased principle component 
regression PCRG and PCRA respectively in this study In order to evaluate their 
performance the following three criteria were considered 
1 The true positive rate is the proportion of true positives which are correctly 
predicted 
  
2 The positive predictive value is the proportion of subjects with positive output 
results which are correctly predicted 
  
3 F1 score is a measure of accuracy which is the harmonic average of TPR and PPV 
 
3 
Results and Discussion 
Fig 1 ac show the F1 scores of each method in terms of network construction 
based on simulated data It can be seen that the performance of geometric meanbased 
and arithmetic meanbased approaches relies on the estimation methods PLSR and 
PCR As for PCR geometric meanbased approach performs better than arithmetic 
meanbased approach when the network is complex regardless of the sample size 
However arithmetic meanbased approach has the larger F1 score than geometric 
meanbased approach with PLSR PLSRG when the sample size is large In 
particular when the sample size is less than 50 PLSRG performs the best with 
density of 5 while PCRG is the best method if the density is 15 or 25 based on 
F1 score as shown in the figure  
Interestingly in case of PPV as shown in Fig 1 df arithmetic meanbased 
approach with PCR outperforms geometric meanbased approach regardless of 
sample size and network complexity On the other hand as for TPR in Fig 1 gi 
geometric meanbased approach performs better than arithmetic meanbased approach 
when PCR is applied while arithmetic meanbased approach with PLSR PLSRA is 
better when the density is 15 or 20 
 
Reconstruction of Metabolic Association Networks 
165 
 
 
Fig 1 Performance plots a b and c show the F1 scores d e and f show the positive 
predictive value g h and i show the true positive rate a d and g correspond to 
density 5 b e and h correspond to density 15 c f and i correspond to density 
25 SCE PLSRG PLSRA PCRG and PCRA stand for shrinkage covariance estimate 
geometric meanbased partial least squared regression arithmetic meanbased partial least 
squared regression geometric meanbased principle component regression and arithmetic 
meanbased principle component regression respectively Error bar stands for average value 
and 95 confidence interval of F1 score PPV and TPR over 100 runs 
Table 1 shows the numbers of empty network estimated by SCE PLSRG and 
PLSRA out of 100 independent runs Note that PCR methods generated no empty 
network When the true network becomes more complex those methods generated 
more estimated empty network Furthermore the number of estimated empty 
networks is decreased as the sample size goes to 200 However the trend of PLSRG 
is different with other methods For example the empty network for PLSRG with 
density of 25 is increased as the sample size is increased  
The results of network construction using real experimental data are shown in 
Table 2 The number of significant edges and the number of intersection of edges of 
pair of two methods are reported The geometric meanbased approaches PLSRG 
and PCRG generate larger significant edges than arithmetic meanbased approaches 
Namely PLSRA and PCRA selected at least 54 times more edges Most edges 
90 and 89 of PLSRA and PCRA are overlapped with these of PLSRG and 
PCRG respectively 
166 
I Koo X Zhang and S Kim 
 
The reason for the difference of the F1 score between two meanbased approaches 
for PLSR and PCR in complex density is likely due to the different statistical property 
of latent variables from them The difficulty of regression using PLSR and PCR under 
complex network can also be another reason for disagreement of performance pattern 
of them Furthermore since the output of arithmetic mean is larger than that of 
geometric mean for the same input discrimination power of arithmetic meanbased 
approach to estimating significant edges combined with FDR method increases when 
 = 100 150 200 and the density is 5 This makes the trend of PPV and TPR for 
the final approaches consistent in density of 5 For the real experimental data the 
condition seems similar to the case that density is 5 and sample size is 50 or 100 in 
terms of the number of significant edges  
Table 1 Number of empty networks for SCE and PLSR with geometric G and arithmetic 
A approaches out of 100 independent simulations 
 
5 
15 
25 
n 
SCE PLSRG PLSRA SCE PLSRG PLSRA SCE PLSRG PLSRA 
25 
92 
0 
0 
100 
6 
8 
100 
5 
7 
50 
0 
0 
0 
97 
8 
1 
100 
12 
2 
100
0 
0 
0 
85 
39 
0 
93 
59 
1 
150
0 
0 
0 
41 
26 
0 
86 
69 
0 
200
0 
0 
0 
24 
11 
0 
82 
72 
0 
Table 2 Number of significant edges for SCE PLSR and PCR with geometric G and arithmetic 
A approaches for real experimental results and number of intersection of two methods 
 
SCE 
PLSRG 
PLSRA 
PCRG 
PCRA 
SCE 
259 
150 
131 
137 
85 
PLSRG 
 
1228 
172 
608 
136 
PLSRA 
 
 
191 
133 
79 
PCRG 
 
 
 
1292 
188 
PCRA 
 
 
 
 
211 
4 
Conclusion 
We evaluated the performance of two estimation methods arithmetic meanbased and 
geometric meanbased approaches using regression coefficients to construct association 
networks We observed that the performances of geometric meanbased and arithmetic 
meanbased approaches are dependent on the dimensionreduced regression methods 
PLSR and PCR and simulation settings such as sample size and density Arithmetic 
estimation outperforms geometric mean when it is incorporated with PLSR and the 
sample size is larger while the geometric meanbased approach performs better when the 
true network is complex and it is used with PCR in terms of F1 score  
References 
1 Watkins SM German JB Hammock BD Metabolomics Building on a Century of 
Biochemistry to Guide Human Health Metabolomics 11 3–9 2005 
2 Arkin A Shen PD Ross J A Test Case of Correlation Metric Construction of a 
Reaction Pathway from Measurements Science 2775330 1275–1279 1997 
 
Reconstruction of Metabolic Association Networks 
167 
 
3 Whittaker J Graphical Models in Applied Multivariate Statistics Wiley series in 
probability and mathematical statistics Wiley Chichester 1990 
4 Theis FJ Krumsiek J Suhre K Illig T Adamski J Gaussian Graphical Modeling 
Reconstructs Pathway Reactions from Highthroughput Metabolomics data BMC Systems 
Biology 5 2011 
5 Chan E Rowe H Hansen B Kliebenstein D The Complex Genetic Architecture of 
the Metabolome PLoS Genetics 611 e1001198 2010 
6 Dobra A Hans C Jones B Nevins JR Yao G West M Sparse Graphical Models 
for Exploring Gene Expression Data Journal of Multivariate Analysis 901 196–212 
2004 
7 de la Fuente A Bing N Hoeschele I Mendes P Discovery of Meaningful Associations in 
Genomic Data using Partial Correlation Coefficients Bioinformatics 2018 3565–3574 
2004 
8 Kramer N Schafer J Boulesteix AL Regularized Estimation of Largescale Gene 
association Networks Using Graphical Gaussian Models BMC Bioinformatics 10 2009 
9 Pihur V Datta S Datta S Reconstruction of Genetic Association Networks from 
Microarray Data a Partial Least Squares Approach Bioinformatics 244 561–568 2008 
10 Schafer J Strimmer K A Shrinkage Approach to Largescale Covariance Matrix 
Estimation and Implications for Functional Genomics Statistical Applications in Genetics 
and Molecular Biology 4 2005 
11 Houskuldsson A Pls Regression Methods Journal of Chemometrics 23 211–228 
1988 
12 Strimmer K fdrtool a Versatile r Package for Estimating Local and Tail Areabased 
False Discovery Rates Bioinformatics 2412 1461–1462 2008 
13 Schafer J Strimmer K An Empirical Bayes Approach to Inferring Largescale Gene 
Association Aetworks Bioinformatics 216 754–764 2005 
14 Wei X Sun W Shi X Koo I Wang B Zhang J Yin X Tang Y Bogdanov B Kim 
S Zhou Z McClain C Zhang X Metsign A Computational Platform for Highresolution 
Mass Spectrometrybased Metabolomics Analytical Chemistry 8320 7668–7675 2011 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 168–173 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Predicting Protein Subcellular Localization by Fusing 
Binary Tree and ErrorCorrecting Output Coding 
Lili Guo12 and Yuehui Chen12 
1 Computational Intelligence Lab School of Information Science and Engineering  
University of Jinan 106 Jiwei Road 250022 Jinan PRChina 
2 Shandong Provincial Key Laboratory of Network Based Intelligent Computing 
gfcguoguo163com 
Abstract In this paper a new method was applied to predict the protein subcel
lular localization The features used in the paper were the Distance frequency 
DF the Physical and chemical composition PCC and the Pseudo Amino Ac
id composition PseAA The classifier was integrated by Binary tree and Error
Correcting Output Coding ECOC based six Artifical neural networks ANN 
The prediction ability was evaluated by 5jackknife crossvalidation By com
paring its results with other methods such as LeiSVM and ESVM the experi
mental result demonstrated that our method outperformed their predictions and 
indicate the new approach is feasible and effective  
Keywords subcellular localization feature extraction Binary tree ECOC 
ANN ensemble classifier 
1 
Introduction 
We can realize from the biology that the proteins play a vital function on cell survival 
The cells are highly ordered structure We usually divide intracellular regions into 
different organelles or cellareas depending on different functions and space distribu
tions as nucleus Golgi body endoplasmic reticulum chondriosome   endochylema 
and cytomembrane etc which are called subcellular organelles After protein synthe
sis in ribosome they are transported to specific position  
Precise knowledge of a protein’s function requires appropriate subcellular localiza
tion 1 as it has been observed that a protein may lose its functions if not properly 
localized 2 In addition the information about subcellular localization may provide 
understanding about the study of proteins’ functions also about protein interaction 
evolutionary analysis 
2 
Materials and Methods 
21 
Dataset 
We choose the SNL6 3 dataset to validate the availability of our classifier This 
dataset is founded by Lei and Dai and more commonly used in subcellular  
 
Predicting Protein Subcellular Localization by Fusing Binary Tree and ECOC 
169 
 
localization SNL6 contains 504 protein sequences and 6 subcellular positions 
Among the 504 sequences 61 chromatin 55 nuclear lamina 56 nuclear speckle 219 
nucleolus 75 nucleoplasm 38 PML body 
22 
Representation of Protein Sequence 
Distance Frequency DF 
Relevant researches indicate that the physical and chemical properties of amino acids 
are much consequence to the subcellular localization 4 so we think that the hydro
pathy  hydrophobic distribution of amino acids in sequence closely relate with sub
cellular localization So the 20 native amino acids 20 letters in alphabet are divided 
into 6 classes 4 according to the properties Table1 shows the result of classification 
Table 1 Amino acids hydration property classification 5 
Classification 
Abbreviation 
Amino acids 
hydrophily 
L 
RDENQKH 
hydrophobicity 
B 
LIVAMF 
neutral 
W 
STYW 
proline 
P 
P 
glycocoll 
G 
G 
cysteine 
C 
C 
 
After the classification any protein sequence can be represented by combination of 
the six letters 6 For every type of amino acids we separately calculate the distance
value’s occurrence number of two letters which belong to same type One sequence 
thus gets one vector Vi on the basis of distance frequency  












L
L
L
B
B
B
C
C
C
i
s
s
s
V
v
v
v
v
v
v
v
v
v
1
2
1
2
1
2


=
⋅ ⋅ ⋅
⋅ ⋅ ⋅
⋅ ⋅ ⋅
⋅ ⋅ ⋅

  
1
The value of s is a key problem If it is too small we can not extract enough feature 
information conversely it can produce lots of noise data Experimental results show 
that we can get sufficient information when s is 11 So this method consists of 66 
descriptor values 









jv
j
s
L B W P G C
ξ
ξ
=1 2⋅⋅⋅
=
 has two parts 1 when js 
jvξ is occurrence number of distance value j of two letters which belong to ξ type 2 
when j=s 
jvξ  is occurrence number of distance value H which equal or greater  
than s 
Physical and Chemical Composition PCC  
The 20 native amino acids are divided into three groups for their physicochemical 
properties including seven types 7 of hydrophobicity normalized van der Vaals 
volume polarity polarizibility charge secondary structures and solvent accessibility 
For instance using hydrophobicity attribute all amino acids are divided into three 
groups polar neutral and hydrophobic A protein sequence is then transformed into a 
sequence of hydrophobicity attribute Therefore the composition descriptor consists 
of three values the global percent compositions of polar neutral and hydrophobic 
residues in the new sequence For seven types of attributes PCC consists of a total of 
3×7=21 descriptor values 
170 
L Guo and Y Chen 
 
Pseudo Amino Acid composition PseAA 
According to the concept of Chou’s PseAA composition 8 a sample of protein se
quence is a point in 20+λ D space 










T
X
x x
x
x
x
λ
λ
20+
1
2
20
21
20+
=
⋅⋅⋅
⋅⋅⋅
∈ℜ
 
2
          
 
          
i
j
j
j
j
i
i
j
j
j
j
f
i
f
w
P
x
w
i
f
w
P
λ
λ
μ
γ
2 0
= 1
= 1
2 0
= 1
= 1

1 ≤
≤ 2 0

+


= 

2 1 ≤
≤ 2 0 +

+





 
3
Where the


if
i
1≤ ≤ 20 in Ep 3 is the occurrence frequencies of 20 amino acids in 
sequence 


iP
i
λ
21 ≤
≤ 20 +
is the additional factors that incorporate some sort of 
sequence order information The parameter w is weight factors 
Features Fusion 
Putting the characteristic data attached to another and this is called the fusion of two 
features extraction methods DF consists of 66 descriptor values PCC consists of a 
total of 21 descriptor values and PseAA we used consists of 40 values so the fusion 
of DF PCC and PseAA consist of 127 values 
23 
Ensemble Classifier Prediction System 
The biggest problem of studying subcellular localization is data imbalance 9 as the 
sequences’s number of Nucleolus is 219 which is much greater than other types’ So 
we introduce the Binary tree to classify the Nucleolus and others firstly The follow
ing flowchart shows the prediction process 
 
Binary Tree Structure  
Actually the Binary tree is not a concrete classifier but a kind of classification struc
ture and it classifies the subcellular position Nucleolus and others in this paper Here 
we use the simplest Artifical neural network to classify two categories 
ECOC Framework  
ECOC 10 trains single classifier respectively according to encoding matrix In test
ing process every single classifier outputs a predicted value which forms a output 
vector
 
  
 

n  
H x
h x h x
h x
1
2
=
⋅⋅⋅
 It uses Hamming distance function or Eucli
dean distance function to calculate the distance between the output vector Hx and 
each row of encoding matrix the corresponding class label of the shortest coding is 
the output of the test sample 11 
 
 
Predicting Protein Subcellular Localization by Fusing Binary Tree and ECOC 
171 
 
 
Fig 1 A flowchart to show the prediction process of ensemble classifier 
The encoding matrix is defined as MK×n and each element of the matrix is 0 1 K 
refers to class number of the dataset and n stands for the number of the single classifi
er Each row in M corresponds to one class while each column one single classifier 
For instance the all single classifiers are
 
 

n  
h x h x
h x
1
2
⋅⋅⋅
 if the Mij=0 that 
means that classifier j regards all the samples of class label i as positive samples or 
the samples are regarded as negative ones The encoding matrix takes many forms 
12 as onetomany matrix onetoone matrix random sparse coding matrix and 
dense random coding matrix etc In this paper our encoding matrix is 
000000101101011011110110000111 The five rows 
stands for the five categories except for Nucleolus and each column corresponds to 
one single classifier 
Artifical Neural Network ANN 
The single classifier of ECOC framework is ANN ANN has obtained very good ap
plication in many fields of pattern recognition and is such a algorithm which imitates 
the message processing of people’s neurons 13 it has strong robustness and toler
ance and can study uncertain system So the ANN has been applied to the subcellular 
location In this paper the Particle swarm optimization PSO is adopted to optimize 
the parameters weights and thresholds of the ANN 
3 
Experimental Results 
In the prediction and classification problems we often use the following three me
thods to examine the quality of a predictor independent dataset test subsampling 
test and jackknife test 14 But among the three crossvalidation methods the  
5classes 
output 
Input protein 
sequence 
Nucleolus 
Others 
others 
Nucleolus 
ECOC classifier 
Artifical neural 
network
Binary tree structure 
172 
L Guo and Y Chen 
 
jackknife test is deemed to the most objective that can always yield a unique result for 
a given benchmark dataset and hence has been increasingly and widely adopted to 
examine the power of various predictors 15 Therefore the 5jackknife cross
validation was adopted in our study to test the prediction quality In this paper three 
kinds of feature extraction methods were used which were mentioned above that DF 
PCC and PseAA ie the fusion of the three kinds of features  
Table 2 Comparison of accuracies between different methods for SNL6 
Subset Subcellular  location                   Accuracy  rate
LeiSVM 
ESVM 
This paper 1 
S1 
Chromatin 
1361=213 
1461=229 
3761=607 
S2 
NuclearLamina 
2055=364 
1855=327 
4055=727 
S3 
Nuclearspeckles 
1956=339 
1556=268 
3756=660 
S4 
Nucleolus 
182219=831 
198219=903 
147219=671 
S5 
Nucleoplasm 
2175=280 
3275=427 
5475=720 
S6 
PML body 
438=105 
738=184 
2538=658 
 
Overall 
259504=514 
284504=564 
340504=675 
Table 3 The comparison of the results with my prior research 
Compartments 
Different methods  
PseAA+PCC+ECOC  PseAA + PCC +HpAA2+ECOC  This paper 
Chromatin 
3561=573 
3261=525 
3761=607 
Nuclear lamina 
2555=454 
3155=563 
4055=727 
Nuclear speckles 
3056=535 
2756=482 
3756=660 
Nucleolus 
163219=744 
165219=753 
147219=671 
Nuclear diffuse 
3875=507 
3975=520 
5475=720 
PML body 
1338=342 
2038=526 
2538=658 
Overall 
304504=602 
314504=626 
340504=675 
 
As for how to calculate the overall success rate for a statistical system we count 
classification accuracy of every class and the overall accuracy Accuracy is refers to 
the ratio of the number of proteins which are correctly classified and the all proteins 
Listed in Table 2 are the results obtained respectively with other methods and ours 
on the benchmark dataset SNL6 The comparison of results with my prior research 
which also use the same dataset is also showed in Table 3 
4 
Conclusion 
In this paper we used the features fusion and integrated classifiers to predict the sub
cellular localization The overall accuracy rate achieved by this paper was 675 
which was much better than that by LeiSVM and ESVM The result was better than 
                                                           
1  The method of this paper is DF + PCC + PseAA + Binary tree + ECOC 
2  HpAA Notes that Amino acids hydration properties composition which is a method of feature 
extraction 
 
Predicting Protein Subcellular Localization by Fusing Binary Tree and ECOC 
173 
 
others and my prior work because we analyzed the imbalance of data so we first used 
the Binary tree to classify This was also an innovation of this paper A lot of people 
have studied this subject so the next step of research I hope to make innovation in 
feature extraction and classifier to improve the accuracy of classification 
Acknowledgments This research was partially supported by the Natural Science 
Foundation of China 61070130 the Key Project of Natural Science Foundation of 
Shandong Province ZR2011FZ001 the Key Subject Research Foundation of Shan
dong Province and the Shandong Provincial Key Laboratory of Network Based Intel
ligent Computing 
Reference 
1 Deng M Zhang K Mehta S Chen T Sun F Prediction of Protein Function Using 
Protein–protein Interaction Data Journal of Computational Biology 10 947–960 2003 
2 Boden M Teasdale RD Determining Nucleolar Association from sequence by Leve
raging Proteinprotein Interactions Journal of Computational Biology 15 291–304 2008 
3 Lei Z Dai Y An SVMbased System for Predicting Protein Subnuclear Localizations 
BMC Bioinformatics 6 291–298 2005 
4 Pánek J Eidhammer I Aasland R A New Method for Identification of Protein Sub 
Families in a Set of Proteins Based on Hydropathy Distribution in Proteins Proteins 
Struct Funct Bioinformatics 558 923–934 2005 
5 Chen YL Li QZ Prediction of the Subcellular Location of Apoptosis Proteins J 
Theor Biol 245 775–783 2007 
6 Zhang L Liao B Li DC Zhu W A Novel Representation for Apoptosis Protein 
Subcellular Localization Prediction Using Support Vector Machine J Theor Biol 259 
361–365 2009 
7 Shi JY Zhang SW Pan Q Cheng YM Xie J SVMbased Method for Subcellular 
Localization of Protein Using Multiscale Energy and Pseudo Amino Acid Composition 
Amino Acids 331 69–74 2007 
8 Chou KC Prediction of Protein Cellular Attributes Using Pseudoamino Acid Composi
tio Proteins Struct Funct Genet 433 246–255 2001 
9 Zhang S Huang B Xia X et al Bioinformatics Research in Subcellular Localization 
of Protein Prog Biochem Biophys 346 573–579 2007 
10 Huang Y Li YD Prediction of Protein Subcellular Locations Using Fuzzy KNN me
thod Bioinformatics 201 21–28 2004 
11 Dietterich TG Bakiri G Solving Multiclass Learning Problems via ErrorCorrecting 
Output Codes Artificial Intelligence Research 2 263–286 1995 
12 Luo D Xiong R Distance Function Learning in ErrorCorrecting Output Coding 
Framework In King I Wang J Chan LW Wang D eds ICONIP 2006 Part II 
LNCS vol 4233 pp 1–10 Springer Heidelberg 2006 
13 Masulli F Valentini G Effectiveness of Error Correcting Output Codes in Multiclass 
Learning Problems In Kittler J Roli F eds MCS 2000 LNCS vol 1857 pp 107–
116 Springer Heidelberg 2000 
14 Chou KC Zhang CT Review Prediction of Protein Structural Classes Crit Rev Bio
chem Mol Biol 30 275–349 1995 
15 Chen C Chen L Zou X Cai P Prediction of Protein Secondary Structure Content by 
Using the Concept of Chou’s Pseudoamino Acid Composition and Support Vector Ma
chine Protein Pept Lett 16 27–31 2009 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 174–181 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Exponential Stability of a Class of HighOrder Hybrid 
Neural Networks 
Qian Ye Baotong Cui Xuyang Lou and Ke Lou 
Key Laboratory of Advanced Process Control for Light Industry Ministry of Education 
Jiangnan University Wuxi 214122 China 
yeqian85gmailcom 
Abstract This paper considers a generalized model of highorder hybrid neural 
networks with timevarying delays and impulsive effects is considered By 
establishing an impulsive delay differential inequality and using the method of 
Lyapunov functions we investigate the global exponential stability of highorder 
dynamical neural networks with timevarying delays and impulsive effects Our 
sufficient conditions ensuring the stability are dependent on delays and impulses 
and show delay and impulsive effects on the stability of neural networks 
Keywords Exponential stability highorder hybrid neural networks Lyapunov 
function impulse effects 
1 
Introduction 
Dynamical neural networks are often used to describe dynamic systems due to its 
practical importance and wide applications in many areas such as optimization 
industry biology economics and so on In such applications it is of prime importance 
to ensure the stability of the equilibrium points of the designed network Recently the 
stability of delayed neural networks has also been studied extensively see eg 13 
Since the existence of delays is frequently a source of instability there has been a 
considerable attention given in the literature on Hopfieldtype neural networks with 
time delays Highorder neural networks have been investigated recently in 45 It is 
known that highorder neural networks have stronger approximation property greater 
storage capacity and higher fault tolerance than lowerorder neural networks 
On the other hand most neural networks can be classified as either continuous or 
discrete However there are many realworld systems and natural processes that behave 
in a piecewise continuous style interlaced with instantaneous and abrupt changes 
impulses 6 Correspondingly there is not much work dedicated to investigate the 
stability of impulsive neural networks 5 79 In particular Li and Hu 8 gave a 
criterion for the existence and global exponential stability of a periodic solution in a class 
of Hopfield neural networks with impulses but without delays As far as we know few 
results have been reported in literature on the exponential stability of highorder 
dynamical neural networks with both impulsive effects and timevarying delays 
                                                           
  This work is partially supported by National Natural Science Foundation of China 
No61174021 No61104155 and the 111 Project B12018 
 
Exponential Stability of a Class of HighOrder Hybrid Neural Networks 
175 
 
In this paper we introduce a new class of highorder dynamical neural networks 
with impulsive effects By establishing an impulsive delay differential inequality we 
obtain the sufficient conditions ensuring the global exponential stability of impulsive 
highorder dynamical neural networks with timevarying delay The conditions are 
dependent on the greatest delay value and the strength values of the impulsive effects 
and are less restrictive Our condition is widely applicable because it does not need 
the differentiability or the monotonicity of the activation functions An example is 
given to demonstrate the effectiveness of the results 
Notation For 
 we denote  
 
For 
 denote  
 
 
 is bounded variation function and righthand continuous on 
any subinterval 
  Let 
 denote the set of real numbers 
 denotes the 
set of nonnegative real numbers Denote 
 an identity matrix The notation 
 
respectively  means that is symmetric and positive definite respectively negative 
definite matrix We use 
 and 
 to denote respectively the transpose 
of the inverse of the smallest and the largest eigenvalues of a square matrix 
 The 
norm 
 is either the Euclidean vector norm or the induced matrix norm  
2 
Preliminaries 
Consider an impulsive highorder dynamical neural network with timevarying delays 
described by  
 
                      1 
where 
 
 
 is the neuron state 
 is positive constant it 
denotes the rate with which the cell 
 resets its potential to the resting state 
 
are the firstorder synaptic weights of the neural networks 
 is the secondorder 
synaptic weights of the neural networks 
 
 is the transmission 
delay of the 
th neuron such that 
 where 
 is a constant the 
activation function 
 is continuous on 
 
 is the external input the 
operator 
 represents the distribution derivative bounded variation functions 
 
 are right continuous on any compact subinterval of 
×
n
n n
x
R
A
R
∈

∈

1
1
1
1
|
| |
|
|
| |
| |
|

|
|
max
|
| 
n
n
T
n
ij
n n
i
ij
j n
i
i
x
x
x
A
a
x
x
A
a
×
≤ ≤
=
=
=
=
=
=


L
‖‖
‖ ‖
φ R
R

→

0
0
φ

lim φ
 φ

lim φ

s
s
t
t
s
t
t
s
+
−
+
−
→
→
=
+

=
+

τ
τ
τ
0
τ
0
φ 
sup φ
 φ 
sup φ

s
s
t
t
s
t
t
s
−
− ≤ ≤
− ≤ 
=
+

=
+

ψ ψ
n
PC
R
R
=
|

→


t
τ t
− 

R
R+
E
0
P 
1 λ
T
m
P
 P−


λM
P

1
1
 
 

 


τ  
n
n
i
i
i
ij
j
j
j
ij
j
j
j
j
j
j
D y t
c y t
A g
y t DU
B g
y t
t
DV
+
=
=
= −
+
+
−


1
1


τ  


τ  
n
n
ijl
j
j
j
l
l
l
jl
i
j
l
T g
y t
t
g
y t
t
DW
I
=
=
+
−
−
+


1 2

i
n
∈  


L
0
t
≥ t

 
iy t
ic
i
ij
ij
A
B

ijl
T
τ  
j t

1 2

j
n
=  

L
j
0
τ  
σ
 j t
≤ 
σ
g j
 0
σ

t − +∞ 
iI
D
j
U Vj


 0

Wjl
t
R

+∞ →
176 
Q Ye et al  
 
 
 
 and 
 depict the impulsive effects of neural networks 
 
 
 We assume that  
 
 
 
where the fixed impulsive moments 
 satisfy 
 and 
 
 is the 
Dirac function 
 
 
 represent the strength of impulsive effects of the 
th 
neuron at time 
 
 and 
 respectively  
In order to obtain our results we need establishing the following definitions and 
lemmas   
Definition 1 The function 
 is called a solution of Eq1 
with the initial condition given by  
                      
2 
if 
 is continuous at 
 and 
 
 and 
 exists 
 
satisfies Eq1 for 
 under the initial condition Especially a point 
 is 
called an equilibrium point of 1 if 
 is a solution of 1   
Lemma 1 7 Suppose 
 and 
 satisfies scalar impulsive differential 
inequality  
                     3 
where 
 is continuous at 
 
 and 
 exists 
 
with 
 Then  
 
where 
 and 
 is a solution of the inequality 
   
Lemma 2 9 Let 
 Then  
1 
 where 
 denotes the spectral radius  
2 
 if 
  
 0

t +∞ 
j
DU 
DVj
jl
DW
1 2

i
m
∈  
L
1 2

j
n
∈
 


L
1 2

l
n
∈  


L
1
1
δ

1 2
1 2

j
jk
k
k
DU
u
t
t
j
n k
N
∞
=
= +
−

=  
  ∈
≡
 


L
L
1
1
δ

1 2
1 2

j
jk
k
k
DV
v
t
t
j
n k
N
∞
=
= +
−

=  
  ∈
≡
 


L
L
1
1
δ

1 2
1 2

jl
jk
k
k
DW
w
t
t
j
n k
N
∞
=
= +
−

=  
  ∈
≡
 


L
L
kt
k 1
k
t
−  t
lim
k
k
t
→∞
= ∞
δ t
jk
u 
vjk
wjk
j
kt 
k
t
−t
k
t
−t

    0
σ

n
y t
t
R

− +∞ →
0
0
 
φ 

σ
 φ
y t
t t
t
t
PC
=
 ∈
− 

∈

 
y t
k
t
≠ t
0
t
≥ t

 


k
k
y t
= y t +

k 
y t−
 
y t
0
t
≥ t
n
y
R
∗ ∈
 
y t
y∗
=
0
p
 q
≥
 
V t
τ
0
τ
0
0
 
 
  




  
 
 

τ

k
k
k
k
k
D V t
pV t
q V t
t
t t
t
V t
b V t
d V t
k
N
V t
t t
t
t
φ
−
+
+
−

≤ −
+
 ≠
 ≥


≤
+
 ∈



=
 ∈
− 

 
V t
0
t
≠ t

 


k
k
V t
= V t +

k 
V t −
PC
φ ∈
1
n = 
0
0
λ

0
τ
0
 

δ 
 
k
t t
k
t
t
t
V t
e
t
t
t






 


0
τ
0
 
0
 
 

 

τ
0
 
0
t
t
τ t
 
0

λτ
δ
max1

k
k
k
b
d
e
=
|
| + |
|
λ
0

λτ
λ
0
p
qe
−
+
≤ 
×
n n
B
∈ R

ρ 
B
	 B

B 
B
ρ ⋅
1
1


1

E
B
B





1
1


1

 1
1

1
 1
1

1
1
1
B 	 
1
B 	 
1
 
Exponential Stability of a Class of HighOrder Hybrid Neural Networks 
177 
 
3 
 exists and 
 if 
 and 
 where 
 
means 
 is a nonnegative matrix  
4 
 for any 
 if 
 is a symmetric matrix 
where 
 and 
 denote the minimum eigenvalue of the matrix and the 
maximum one respectively   
Assume that the neuron activation function 
 is continuously differentiable and 
satisfies the following conditions  
 
Notice that the activation function 
 satisfy Lipschitz condition are not necessarily 
linear and differentiable Define 
 For any 
 we assume that there exists at least one solution of 1 with the initial 
condition 2 Let 
 be an equilibrium point of 1 
 be any solution of 1 and 
  
Set
 
 
Then for each 
 
and 
 
Substituting them into 1 we get  
           4 
where 
 
 is between 
 and 
  
3 
Main Results 
It is clear that the stability of the zero solution of Eq4 is equivalent to the stability 
of the equilibrium point 
 of Eq1 Therefore we mainly discuss the stability of 
the zero of Eq4 For convenience we denote  
 
 
  
  
 
  
 
 
  
 
  
1


E
− B −
1


0
E
− B −
≥
ρ 
B 1
0
B ≥ 
B ≥ 0
B
λ
λ  
T
T
T
m
M
x x
x Bx
B x x
≤
≤
n
x
∈ R
B
λ  
m ⋅
λ  
M ⋅
ig
1


 


|

 |
0




12
 
i
i
i
i
i
i
i
i
i
i
i
i
i
i
g u
g v
H
g u
L
u
v u v
R i
n
u
v
χ
−
≤
≤
≤
∀
≠
∈
=
−
L
ig
1
1
χ
χ
χ 


T
n
n
L
diag L
L
=



=



L
L
φ
∈ PC

y∗
 
y t
 
 
x t
y t
y∗
=
−


 

 


j
j
j
j
j
j
f
x t
g
y t
g
y∗
=
−



τ  


τ  


j
j
j
j
j
j
j
j
f
x t
t
g
y t
t
g
y∗
−
=
−
−

1 2
i
n
=  
L
|
  |
|
|
j
j
f
z
L
z
≤
 
0
zfj
z
z
R
≥
∀ ∈
1
1
1
1
 
 

 


 




 



n
n
i
i
i
ij
j
j
j
ij
j
j
j
j
j
j
n
n
ijl
ilj
l
j
j
j
jl
j
l
D x t
c x t
A f
x t DU
B f
x t
t
DV
T
T
f
x t
t
DW
n
τ
ζ
τ
+
=
=
=
=
= −
+
+
−
+
+
−


 
1 2
i
n
=  
L 
ζl


τ  
l
l
l
g
y t
t
−


l
l
g y∗ 
y∗
1
2

n 
C
diag c c
c
=




L
×

ij 
n n
A
= A

×

ij 
n n
B
B
=
if
ˆ
 ˆ


 2
if
j
j
jj
ij
ij
j
ji
i
c
L A
j
i
A
A
A
L
A
L
j
i
+

−

= 

=
= − |
|
+ |
|
 
≠ 

0
0
T
PL
S
LP


=





0
0
T
k
k
k
T
k
k
P Q
D
Q P


=





max0

jj
jj
A
A
+ =


 
×
×




k
k
ij
n n
ij
jk
n n
A
A
A u
=
=

 
×
×




k
k
ij
n n
ij
jk
n n
B
B
B v
=
=

1
2
ζ
ζ ζ
ζ T
n
=




L
Γ
ζ ζ
ζT
= diag
 


L
178 
Q Ye et al  
 
 
  
  
 
  
 
 
  
  
 
   
Theorem 1 Assume that in addition to 
 the following conditions are satisfied 
for 
  
 
  
 
  
 let 
 satisfy 
 
 
 where 
   
Then the zero solution of 4 is globally exponentially stable if 
  
Proof From 
 the inequality 
 has at least one 
solution 
 Consider the Lyapunov functional  
 
 5
From 3 and 
 and Lemma 2 for 
 we can get  
   6 
×


i
ijl
n n
T
= T

1
1
2
2

 



T
T
T
T
H
n
n
T
T
T
T
T
T
T
=
+

+


+

L
1
1
1
2
2
2







T
T
T
T
Hk
k
k
n
n
nk
T
T
T
w
T
T
w
T
T
w
=
+

+


+

L
1


k
k
P
E
A
L −
=
− |
|

1

 
χ

k
k
k
Hk
Q
E
A
L
B
L
T
L
−
=
− |
|
|
|
+ |
||
|

min1


j n
j
j
j
a
c
s L
≤ ≤
=
−
1
1
1
1
max
|
|
|
|
|



n
n
n
j n
ij
j
ijl
ilj
l
j
i
i
l
b
B
L
T
T
χ L
≤ ≤
=
=
=
=
+
+


max0
|
|
n
j
jj
ij
j i
s
A
A
≠
=
+
×


ij
n n
Q
= | A
|

1
|
|
|
|




n
ij
ij
ijl
ilj
l
ij
n n
l
p
B
T
T
P
p
χ
×
=
=
+
+
=

×

ij 
n n
P
= p

ξ
λ 

λ 

T
k
M
k
k
M
k
P P
D
=
+

λ 

λ 

T
k
M
k
k
M
k
Q Q
D
ς =
+

1


H 
k
N
∈
2

H 
ρ

1
kA
L
|
|
 
3

H 
ˆ
μ
λ  
 m A

4

H 
λ
 0
λσ
ˆ
λ
λ  
μ
0
m A
e
−
+
≤ 
λσ
θ
max1 ξ
ζ

k
k
ke
=

+

1
 sup ln

k
k N
k
k
t
t
θ
θ
∈
−
=
−
μ
λ  
M S
=

θ
λ
 
3
4




H
− H

λσ
ˆ
λ
λ  
μ
0
m A
e
−
+
≤
λ
0
 
2
1
1
 
 
2
n
i
i
V t
x
t
=
= 
 1
H 
k
t
t
≠
1
2
2
1
1
1
1
1
1
1
 
  |
  |
 
 
|
|
|
  ||
  |
|
||
  ||

  |
|
|
|
  ||

  |
| 
1
ˆ
|   |
|   |
2


n
i
i
i
n
n
n
i
i
ii
i
i
ij
j
i
j
i
i
j i
n
n
j
ij
i
j
j
i
j
n
n
n
ijl
ilj
l
j
i
j
j
i
j
l
T
D V t
x t
x t
c x t
A L x t
A
L
x t
x t
L
B
x t
x t
t
T
T
L
x t
x t
t
x t
x t
A x t
τ
χ
τ
+
=
+
=
=
≠
=
=
=
=
=
′
=
≤ −
+
+
+
−
+
+
−
= −
+






 |
0
|   |
| 
  |
0
| 
  |
ˆ
2
   
 

 
ˆ
2
 
  
 
T
T
m
m
ML
x t
x t
t
LM
x t
t
A V t
V t
V t
t
A
V t
V t
τ
τ
λ
μ
μ
τ
λ
μ
μ

 
 


 
 

−
−

 
 

≤ −
+
+
−
≤ −
−
+
 
Exponential Stability of a Class of HighOrder Hybrid Neural Networks 
179 
 
where  
  
   
On the other hand by using the properties of Dirac measure we have  
 
            
                   7 
that is  
 
From 
 and Lemma 2 
 Then  
 
 8 
yielding  
        
 
 
 
                               9 
Employing Lemma 1 from 6 9 
 and 
 we have  
                     10 
So for all 
  
 
Then the conclusion holds and the proof is complete   
Theorem 2 Assume that in addition to 
 and 
 the following conditions 
are satisfied for 
  
 
  
 let 
 be a solution of 
 and  
 
sup
 
 
t
s t
V t
V s

 


1
2
 

 
 
  T
n
x t
x t
x t
x t

 



 
 n
 
n
1
1
2
2

τ 


τ  

τ  

τ   T
n
n
x t
t
x t
t
x t
t
x t
t
|
−
|= |
−
||
−
|
|
−
| 
L
1
1
 



 


 
n
n
i
k
i
k
ij
j
j
k
jk
ij
j
j
k
j
k
jk
j
j
x t
x t
A f
x t
u
B f
x t
t
v
τ
−
=
=
−
=
+
−


1
1




 



n
n
ijl
ilj
l
j
j
k
j
k
jk
j
l
T
T
f
x t
t
w
ζ
τ
=
=
+
+
−
 
 


  
 
τ 
Γ
 
τ 
T
k
k
k
k
k
k
k
Hk
k
k
x t
x t
A f x t
B f x t
t
T
f x t
t
−
=
+
+
−
+
−

2


H
1


0
k
E
A
L −
− |
|
≥ 
 


 

τ 
χ

τ 
k
k
k
k
k
k
k
Hk
k
k
x t
x t
A
L x t
B
L x t
t
T
L x t
t
−
|
|=|
| + |
|
|
| + |
|
|
−
| + |
||
|
|
−
|
1
1





 
χ


τ 
k
k
k
k
Hk
k
k
E
A
L
x t
E
A
L
B
L
T
L
x t
t
−
−
−
≤
− |
|
|
| +
− |
|
|
|
+ |
||
|
|
−
|
1
1
 
 
 




τ   



τ  
2
2
T
T
k
k
k
k
k
k
k
k
k
k
k
k
k
V t
x t
x t
P
x t
Q
x t
t
P
x t
Q
x t
t
−
−
=
|
| |
|≤
|
| +
|
−
|
|
| +
|
−
|
1
1





τ 

τ 
2
2
T
T
T
T
k
k
k
k
k
k
k
k
k
k
x t
P P
x t
x t
t
Q Q
x t
t
−
−
=
|
|
|
| +
|
−
|
|
−
|

 10
0
11

 12
1
2

τ 
0

τ 
T
T
k
k
k
k
T
k
k
k
k
k
k
x t
P Q
x t
x t
t
Q P
x t
t
−
−

 
 

|
|
|
|
+

 
 

|
−
|
|
−
|

 
 

σ
λ 

λ 
 

λ 

λ 
  
T
T
M
k
k
M
k
k
M
k
k
M
k
k
P P
D
V t
Q Q
D
V t
−
−
≤
+
+
+
σ
ξ


  
k
k
k
k
V t
V t
ς
−
−
=
+

3

H 
4


H 
0
1
0
1
2
0
0
0
0


1
1
0






0




0



0
1
 
 
 
 
 



k
k
t t
k
t
t
t
t
t t
t t
t t
t t
k
k
V t
e
V t
e
e
e
V t
e
e
V t
e
V t
t
t
t
k
N
λ
σ
θ
θ
λ
σ
θ
λ
σ
λ θ
σ
θ
θ
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
≤
…
≤
…
≤
=
≤ 
∈
‖
‖
‖
‖
‖
‖
‖
‖
0
t
≥ t
0



0
 
  
t t
V t
e
V t
λ θ
σ
−
−
−
≤
‖
‖
 1
H 
2


H 
k
N
∈
 1 

H
′
b
a
 
 
2


H
′
λ
≥ 0
λσ
λ
0
a
be
−
+
≤
1
ln
 max1

 sup



k
k
k
k
k N
k
k
P
Q
e
t
t
λσ
η
η
η
∈
−
=
+
=
−
‖ ‖‖ ‖
180 
Q Ye et al  
 
Then the zero solution of 4 is globally exponentially stable if 
 
   
Proof Since 
 the inequality 
 has at least one solution 
 
Consider the Lyapunov functional  
                          11 
Calculating the upper right derivative 
 along the solutions of 4 from the 
conditions 
 and 
 we have  
           12 
On the other hand by 
 and Lemma 2 then the inequality 8 holds which 
implies 
From this inequality 13 
 and 
 we can get by Lemma 1 
 
Therefore the conclusion holds and the proof is complete  
4 
Example 
Consider 
the 
hybrid 
neural 
network 
1 
with 
parameters
 
 
 
 
 
 
 
 
 for 
 
  
 
 
In terms of the parameters defined in the section 3 we have  
 
 
 
 
 
 
 
 
 
 
 and 
 
  
η
 λ


η
≤ λ

b
a
 
λσ
λ
0
a
be
−
+
≤
λ
0
 
1
 
|   |
  
n
i
i
V t
x t
x t
=
=
=

‖
‖
D V
+
 1
H 
2


H 
1
1
1
1
1
1
1
 
sgn
  |
  |

 |
  |
|
|
|

  |
|
|
|

  |
 
  
n
i
i
i
n
n
n
j
j
j
j
ij
j
j
j
j
j
i
n
n
n
ijl
ilj
l
j
j
j
j
i
l
D V t
x t
x t
c
s L
x t
B
L
x t
t
T
T
L
x t
t
aV t
b V σ
τ
χ
τ
+
=
=
=
=
=
=
=
′
=
≤ −
−
+
−
+
+
−
≤ −
+




2


H
 
 


   
k
k
k
k
k
k
V t
x t
P
V t
Q
V t
σ −
−
=
≤
+
‖
‖ ‖ ‖
‖ ‖
 1 

H
′
 
2


H
′ 
0



0
0
 
 


t t
V t
e
V t
t
t
λ η
σ
−
−
−
≤
≥
‖
‖
1 2
i =  
1
1
1


tanh0 53

g
y
y
=


2
2
2


tanh0 67

g
y
y
=


τ  
0 5
1 2
t
j t
e
j
−
= 

=  
0
0
t = 
1
0 6
k
k
t
t
k
−
=
+ 

0 2 1k
ujk
= 
−

0 2
0 2
k
vjk
=  e 

0 1 0 2
k
wjk
=  e 

1 2
j l =  
k
∈ N

1 90
0
0 05
0 14
0 09
0 25
0
1 89
0 20
0 31
0 21
0 45
C
A
B











=

=

=


















1
2
3
0 05
0 14
0 29
0 10
0 23
0 07
0 06
0 05
0 23
0 14
0 09
0 02
T
T
T




− 







=

=

=







− 


− 

− 






σ
0 5
=  
χ
1
i = 
1 85
0 17
ˆ
0 17
1 58
A

− 


=



− 



ˆ
λ  
1 4979
m A = 

μ
0 5837
= 

10410
kP
=
‖ ‖
01190 02
k 
Qk
e
‖ ‖=
1 45
a = 

0 7
b =  
ξ
1 2350
k = 

0 1725
kς = 

1
0 6
k
k
t
t
k
− −
= 
ρ

0 0784
1
kA
L
|
|
= 
 
ˆ
μ
0 5837
1 4979
λ  
m A
= 
 
=

 
Exponential Stability of a Class of HighOrder Hybrid Neural Networks 
181 
 
 where 
 is an unique solution of equation 
 It follows from Theorem 1 that the equilibrium point 
 is 
globally exponentially stable and the exponentially convergent rate is approximately 
equal to 00283 To use Theorem 2 we note that 
 and 
where 
 is the unique solution of 
the equation 
 Thus from Theorem 2 that the equilibrium point 
 
is globally exponentially stable Moreover when 
 the system 1 reduces to 
the model studied in 7 it is easy to check all the conditions of Theorem 1 in 7 are 
satisfied and 
 where 
 is an unique 
solution of equation 
 Thus the example demonstrate the 
effectiveness of the results  
5 
Conclusions 
By means of an impulsive delay differential inequality and several Lyapunov 
functions we have analyzed the global exponential stability of highorder dynamical 
neural networks with timevarying delays and impulsive effects Several delay
dependent sufficient conditions ensuring the stability have been proposed which is 
illustrated in the given numerical example 
References 
1 Ozcan N A new sufficient condition for global robust stability of delayed neural networks 
Neural Processing Letters 343 305–316 2011 
2 Di Marco M Grazzini M Pancioni L Global robust stability criteria for interval 
delayed fullrange cellular neural networks IEEE Transactions on Neural Networks 224 
666–671 2011 
3 Faydasicok O Arik S Equilibrium and stability analysis of delayed neural networks 
under parameter uncertainties Applied Mathematics and Computation 21812 6716–6726 
2012 
4 Liao XX Liao Y Stability of Hopfieldtype neural networks II Sci China Series 
A 408 813–816 1997 
5 Liu XZ Teo KL Exponential stability of impulsive highorder Hopfieldtype neural 
networks with timevarying delays IEEE Transactions on Neural Networks 166 1329–1339 
2005 
6 Liu XZ Ballinger G Uniform asymptotic stability of impulsive delay differential 
equations Comput Math Applicat 41 903–915 2001 
7 Yang Z Xu D Stability analysis of delay neural networks with impulsive effects IEEE 
Transactions on Circuits and Systems II Express Briefs 528 517–521 2005 
8 Li YK Hu LH Global exponential stability and existence of periodic solution of 
Hopfieldtype neural networks with impulses Phys Lett A 333 62–71 2004 
9 Berman A Plemmons RJ Nonnegative Matrices in Mathematical Sciences Academic 
Press New York 1979 
1
ln
06502
067

8 
 
5
k
k
k
t
t
θ
λ
− −
≤

=
λ
λσ
ˆ
λ
λ  
μ
0
m A
e
−
+
= 
0 0T

η
1 2731
1 45
0 7
k
a
b
= 

= 
  = 
1
 

0 402
ln
4
λ
0 5352
k
k
k
t
t
θ
−
≤ 

= 
−

λ
0 5352
= 
λσ
λ
a
be
=
−

0 0T

0
ijl
T ≡ 
1
 

0 642
ln
9
λ
0 6794
k
k
k
t
t
θ
−
≤ 

= 
−

λ
λσ
ˆ
λ
λ  
λ  
0
m
M
A
S e
−
+
= 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 182–189 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Mining Google Scholar Citations An Exploratory Study 
Ze Huang and Bo Yuan 
Intelligent Computing Lab Division of Informatics  
Graduate School at Shenzhen Tsinghua University Shenzhen 518055 PR China 
workthyhotmailcom yuanbsztsinghuaeducn 
Abstract The official launch of Google Scholar Citations in 2011 opens a new 
horizon for analyzing the citations of individual researchers with unprecedented 
convenience and accuracy This paper presents one of the first exploratory stu
dies based on the data provided by Google Scholar Citations More specifically 
we conduct a series of investigations on i the overall citation patterns across 
different disciplines ii the correlation among various index metrics iii the 
personal citation patterns of researchers iv the transformation of research top
ics over time Our results suggest that Google Scholar Citations is a powerful 
data source for citation analysis and provides a solid basis for performing more 
sophisticated data mining research in the future 
Keywords Google Scholar Citations Citation Analysis Tag Cloud  
Clustering  
1 
Introduction 
Citation analysis refers to the investigation of the frequency and patterns of citation 
records ie references to published or unpublished sources in scholarly literature It 
has been widely used as a method of bibliometrics to evaluate the quality of journals 
and to establish the links among works and authors For example it is possible to 
identify groups of people that collaborate frequently or how a piece of work is related 
to existing studies 1 It is also important for researchers to choose the right journals 
as well as track the development of specific research topics 2 In many academic 
institutes citation record is also being used as one of the major selection criteria in the 
process of recruiting and promotion  
There have been some critics 3 4 on the potential misinterpretation of citation in 
evaluating journals and researchers i the number of citations can be manipulated to 
deliberately increase the impact of a journal ii the influence of selfcitation and neg
ative citation needs to be taken into account iii it may be difficult to find a good 
tradeoff between the number of citations and the number of publications iv different 
research disciplines may have significantly different typical citation numbers Never
theless citation record provides a practical and quantitative performance measure 
which has been accepted widely in academia  
Online bibliographic databases such as Web of Science published by Thomson 
ISI SciVerse Scopus published by Elsevier and Google Scholar a freely accessible 
 
Mining Google Scholar Citations An Exploratory Study 
183 
 
web search engine released in 2004 by Google have brought tremendous benefits to 
researchers across different disciplines 5 In nowadays Web of Science covers over 
12000 journals and 150000 conference proceedings but its most famous citation 
index Science Citation Index Expanded only covers around 8000 journals In the 
meantime SciVerse Scopus contains nearly 19500 titles from 5000 publishers 
worldwide with 46 million records most of which are journal articles  
Different from subscriptionbased commercial databases Google Scholar retrieves 
bibliographic data of academic literature by automatically crawling over the Web and 
uses a ranking algorithm which relies heavily on citation counts to display the search 
results Although its exact coverage is not known to the public some publishers may 
not allow Google Scholar to crawl their databases some studies show that Google 
Scholar usually returns the highest number of citations compared to other similar 
services 6 7 Note that conference papers are extensively indexed in Google Scholar 
and their citation counts are calculated in combination with journal articles This is 
particularly important for disciplines such as computer science where many high qual
ity research outcomes are published in conferences  
Although most bibliographic databases provide comprehensive search functions 
there is an inherent issue making the accurate evaluation of individual researchers a 
very challenging task the disambiguation of authors In many occasions different 
researchers share exactly the same name eg even different names may appear to be 
identical in terms of spelling when translated into English and it is necessary to 
group the publications corresponding to the same author before doing any further 
analysis Despite of some recent progress in this area it is still not a fully reliable 
procedure 8 After all a researcher may have different affiliations and collaborate 
with different people and publish papers in seemingly irrelevant disciplines 
The official launch of a new service in the name of Google Scholar Citations1 
GSC in late 2011 provides a different solution to the above issue Built on the top of 
Google Scholar it allows researchers to create personal accounts and add manually 
or automatically papers published by them By doing so each registered researcher 
has a minihomepage similar to a blog with a list of papers and citation counts In 
other words GSC gives researchers the freedom to maintain the list of publications to 
ensure the highest accuracy and integrity of data For example the bibliographic in
formation of each paper is usereditable which means that errors accidentally intro
duced during the crawling of web pages can be corrected in a straightforward manner 
Also similar to many social networking services researchers can follow new articles 
and citations of any registered researchers 
In this paper we present one of the first exploratory studies on the analysis of the 
structured data in GSC We will investigate i the overall citation patterns across 
different disciplines ii the correlation among various index metrics iii the personal 
citation patterns of researchers iv the transformation of research topics over time 
Section 2 describes the data collection procedure including the content structure of 
GSC Section 3 presents the major results of citation analysis while Section 4 focuses 
on the analysis of topic trends This paper is concluded in Section 5 with some discus
sion and directions for future work 
                                                           
1  httpscholargooglecomcitations 
184 
Z Huang and B Yuan 
 
2 
Data in GSC 
In GSC researchers can manually label their research disciplines In many cases each 
registered researcher has three to four discipline labels To explain how we collected 
the data all web pages in GSC are divided into two types in this paper Discipline 
Level DL pages and Author Level AL pages 
21 
Web Page Description 
The major information in GSC is shown in Table 1 DL pages display authors in a 
certain discipline eg data mining For example with “label datamining” as the 
keyword GSC returns a name list of 10 authors who have identified themselves as in 
the data mining discipline sorted based on the total citation number in descending 
order By clicking the “Next” link the next 10 authors if any will be displayed  
Each author in the list has an URL linking to hisher personal page AL page 
which shows detailed publication information The AL page can be divided into 3 
sections from top to bottom i author profile ii citation indexes table iii a list of 
papers that the author has published sorted by each paper’s citation number in des
cending order The paper list shows maximally 20 or 100 papers and additional papers 
if any may be accessed by clicking the “Next” link This action was simulated in our 
program to gather the information of all papers corresponding to an author 
Table 1 The content description of two types of web pages 
Page Type 
Content 
Details 
DL Page 
Author List 
URL links to each author’s personal page 
AL Page 
Author Profile 
Citation Index Table 
Paper List 
Affiliation Disciplines Home Page 
Citations hindex i10index All  Recent 
Title Author Year Citation Number 
 
Note that the citation indexes table has two columns The first column consists of 
statistical values based on an author’s entire publication records and the other one is 
based on recent papers published within 5 years eg since 2007 as of 2012 
22 
Data Collection 
Since GSC does not provide APIs to the public we collected the data by analyzing 
the web page source code with a crawler program The crawler extracted required 
information through pattern match using regular expression  
For DL pages we focused on 6 related disciplines Data Mining DM Artificial 
Intelligence AI Bioinformatics Bio Information Retrieval IR Machine Learning 
ML and Pattern Recognition PR So far many disciplines in GSC did not have 
sufficient number of registered authors eg 200 authors in Sociology Note that the 
same author may appear in multiple disciplines and authors whose papers had zero 
 
Mining Google Scholar Citations An Exploratory Study 
185 
 
citation were not counted For AL pages the 6 indexes in the citation index table and 
publication information title year of publication and citation count were retrieved 
Papers without any citation were excluded 
Totally we collected up to 1000 authors in each discipline and no more than 100 
papers for each author It took less than 30 minutes for the crawler to collect the data 
and the dataset used in the following experiments was collected on March 11 2012 
3 
Citation Analysis 
31 
Index Metrics 
The number of total citations is a most commonly used index metric to quantify the 
impact of an individual’s research output However it is far from sufficient to com
pare and evaluate research work comprehensively Recently many new metrics were 
designed and enhanced such as the hindex 9 and the gindex 10 In GSC only 
three metrics are adopted the total number of citations TC the hindex and the i10
index The hindex attempts to address both the productivity and the impact factors 
and is defined as the maximum number h so that there are h papers each with citation 
number ≥ h The i10index was introduced in July 2011 which indicates the number 
of academic papers of an author that have received at least 10 citations  
Fig 1 shows the TC values of the top 30 scholars in each of the 6 disciplines It is 
clear that researchers especially the most eminent ones in AI tend to have much larger 
TC values compared to disciplines such as IR and PR This fact suggests that TC is 
not a reliable metric for evaluating the impact of individuals in different disciplines 
0
5
10
15
20
25
30
0
2
4
6
8
10
12
14
16
18
x 10
4
Authors
Total Number of Citations
 
 
AI
Bio
ML
DM
PR
IR
 
Fig 1 A comparison of the total number of citations of researchers in different disciplines 
186 
Z Huang and B Yuan 
 
As mentioned above currently three index metrics are calculated in GSC which 
are divided into two groups based on all publications and recent publications respec
tively The Pearson Correlation Coefficient was used to measure the dependences 
among these metrics Fig 2 shows a scatter plot based on the values of the hindex 
and the i10index over the entire dataset ie all disciplines and all publications 
Intuitively there was strong linear correlation between the two index metrics 
0
20
40
60
80
100
120
140
0
50
100
150
200
250
300
350
400
450
500
hindex
i10index
 
Fig 2 The relationship between the hindex and the i10index 
In fact the correlation coefficient CC between the hindex and the i10index was 
around 095 very strong correlation In the meantime the CC value between TC and 
the hindex was around 077 which was similar to the CC value between TC and the 
i10index around 075 Table 2 shows the CC values among the three index metrics 
in each discipline the number of authors is shown in parentheses In all disciplines 
the CC values were more than 07 and some disciplines eg IR had stronger CC 
values between TC and the hindexi10index than others eg AI  ML  
Table 1 The coefficients among index metrics in different disciplines 
Discipline 
All Publications 
Recent Publications Since 2007 
    TC vs h 
 TC vs 
i10 
   h vs 
i10 
   TC vs 
h 
TC vs i10 
   h vs 
i10 
IR 511 
08495 
09004 
09389 
08583 
09185 
09289 
AI 1000 
07829 
07738 
09710 
08276 
08366 
09636 
Bio 999 
08019 
07282 
09374 
07752 
07205 
09180 
DM 879 
07862 
07669 
09327 
08052 
08225 
09306 
ML 1000 
07690 
07240 
09512 
07604 
07498 
09511 
PR 539 
08434 
08583 
09306 
08404 
08613 
09334 
 
Mining Google Scholar Citations An Exploratory Study 
187 
 
32 
Personal Citation Pattern 
One of the major benefits of GSC is that it provides the most accurate citation profiles 
for individual researchers Among many potential research questions that can be ad
dressed we focused on the personal citation patterns at this stage Regardless of the 
TC value of a researcher it is interesting to investigate how the citations are distri
buted among hisher publications For example some authors may have a small num
ber of highly cited documents eg review papers and books are often cited heavily 
while other authors may have citations distributed relatively evenly 
To testify this hypothesis cluster analysis was conducted as follows Firstly only 
researchers with more than 200 citations and at least 10 papers were selected 3539 
authors in total Secondly the papers of each researcher were sorted based on the 
citation numbers in descending order Thirdly a 10D vector was created with the first 
element corresponding to the proportion of the citations of the top 10 papers among 
all citations of the specific researcher Similarly the second element corresponded to 
the citation proportion of the second 10 papers and so on Note that the actual num
ber of papers was rounded to the nearest integer towards minus infinity for the first 9 
subsets and all rest papers were assigned to the 10th subset the number of papers in it 
may be higher than average  
As a result each researcher was represented by a data point in the 10D space spe
cified by the distribution of citations Fig 3 shows the results of clustering using the 
KMeans method K=2 and the Euclidean distance as the similarity measure 
 
 
Fig 3 Personal citation pattern cluster centroids left clustering evaluation right 
Fig 3 left shows the two cluster centroids after clustering It is evident that au
thors in cluster 1 tended to have the majority of citations concentrated on the few very 
best papers In fact the top 10 papers contributed around 60 of the total citations 
By contrast papers of authors in cluster 2 received citations in a more uniform man
ner Fig 3 right demonstrates the quality of clustering using the Silhouette method 
There were more authors in cluster 2 2225 authors or 6287 than in cluster 1 
1314 or 3713 in cluster 2 Moreover only few data points had negative Silhouette 
values the average Silhouette value was 06717 and it is clear that the two clusters 
were reasonably well structured  
188 
Z Huang and B Yuan 
 
4 
Topic Trend 
The titles of papers may provide some vital clues on how topics or keywords evolved 
in a discipline One solution is to show the title texts in the form of tag cloud using 
IBM Word Cloud Generator2 where the font size of a word in the cloud is proportion
al to its frequency in the text We divided the papers into 2 subsets before 2007 and 
since 2007 to observe the change over time Fig 4 shows an example of the keywords 
in the field of IR For better visual effect some common title words such as based 
approach systems analysis using were ignored as these words appeared frequently 
across all disciplines Additionally we filtered out information retrieval and search 
due to their large number of occurrences in both clouds  
 
Fig 4 Tag clouds of titles in IR top before 2007 bottom since 2007 
By comparing the two clouds corresponding to two consecutive time periods it is 
possible to find some interesting clues For example the font size of text dropped 
down while the font size of semantic scaled up which may suggest that semantic 
retrieval has been growing rapidly as the mainstream research topic in IR Meanwhile 
the word social appeared only in the bottom cloud indicating that a new research 
direction related to social networks has emerged in recent years 
                                                           
2 httpwwwwordlenet 
 
Mining Google Scholar Citations An Exploratory Study 
189 
 
5 
Conclusion 
Citation analysis has attracted significant attentions from researchers in all aspects 
With the availability of online searchable citation databases such as Web of Science 
and Google Scholar it is now possible to conduct decent analysis using data mining 
and knowledge discovery techniques In this paper we presented one of the first stu
dies on Google Scholar Citations which provides well organized citation records in 
terms of discipline and authorship We found that different disciplines had different 
numbers of typical citations and there were strong correlations among the hindex the 
i10index and the total number of citations For individual researchers we identified 
two distinct groups of authors in terms of the distribution of citations Finally we 
demonstrated the effectiveness of tag cloud in discovering the topic trend in certain 
research field In the future with the increasing number of registered researchers we 
will be able to collect more comprehensive data sets and conduct more thorough anal
ysis It would also be interesting to combine the domain knowledge with the results of 
data analysis to provide more insights into each discipline 
Acknowledgements This work was supported by the National Natural Science 
Foundation of China No 60905030 
References 
1 White HD McCain KW Visualizing a Discipline An Author CoCitation Analysis of 
Information Science 19721995 Journal of the American Society for Information Science 
and Technology 494 327–355 1998 
2 Chen C CiteSpace II Detecting and Visualizing Emerging Trends and Transient Patterns 
in Scientific Literature Journal of the American Society for Information Science and 
Technology 573 359–377 2006 
3 Gisvold SE Citation Analysis and Journal Impact Factors – Is the Tail Wagging the 
Dog Acta Anaesthesiol Scand 4310 971–973 1999 
4 MacRoberts MH MacRoberts BR Problems of Citation Analysis A Critical Review 
Journal of the American Society for Information Science and Technology 405 342–349 
1989 
5 Bakkalbasi N Bauer K Glover J Wang L Three Options for Citation Tracking 
Google Scholar Scopus and Web of Science Biomedical Digital Libraries 37 2006 
6 Harzing A Wal R Google Scholar as a New Source for Citation Analysis Ethics in 
Science and Environmental Politics 81 61–73 2008 
7 Meho LI Yang K Impact of Data Sources on Citation Counts and Rankings of LIS Fa
culty Web of Science versus Scopus and Google Scholar Journal of The American Socie
ty for Information Science and Technology 5813 2105–2125 2007 
8 Torvik V Smalheiser N Author Name Disambiguation in MEDLINE ACM Transac
tions on Knowledge Discovery from Data 33 Article 11 2009 
9 Hirsch J An Index to Quantify an Individual’s Scientific Research Output Proceedings 
of the National Academy of Sciences 10246 16569–16572 2005 
10 Egghe L Theory and Practise of the gindex Scientometrics 691 137–152 2006 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 190–197 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Knowledge Acquisition of Multiple Information Sources 
Based on Aircraft Assembly Design 
Liang Xia 1 Lizhi Zhang 2 and Zhenguo Yan2 
1 Xian University of Science and Technology Xi’an China 
Zlz365126com  
2 The Ministry of Education Key Laboratory of Contemporary Design and Integrated  
Manufacturing Technology Northwestern Polytechnical University Xi’an China 
593658826qqcom  
Abstract This paper analyzes the importance of knowledge acquisition in air
craft assembly design process and introduces briefly how to create an assembly 
knowledge base The knowledge involved in the aircraft assembly design 
knowledgebased system is classified into three types fact knowledge  
rule knowledge and instance knowledge The acquisition methods and the  
corresponding examples of these three kinds of knowledge are described in  
detail 
Keywords aircraft assembly design fact knowledge acquisition rule 
knowledge acquisition  instance knowledge acquisition 
1 
Introduction 
Aircraft assembly design is a complex and long period work need to carry out a lot of 
researches and tests meanwhile a large number of formulas must be used and produce 
a mass of data With the development of Elearning and network technique human 
will possess of more data But it is difficult that designers find useful knowledge from 
the resource library To solve the problem we must transform the resource informa
tion to useful knowledge to help people make a decision 1 The aircraft assembly 
project is one of the CoPSComplex Product Systems and it needs interunit cooper
ation so aircraft assembly design is actually a problem solving process under the 
guidance of multitype multidisciplinary expertise But the current state of the vari
ous information points scattered in the different sources of information lacking of 
effective organization management and guidance we need to consider how know
ledge extraction and form a unified description Now there are some methods about 
knowledge acquisition for example by intraining exams 2 based on scenario 
model and repository grid and attribute ordering table technology 3 distributed 
knowledge acquisition based on semantic grid 4 and so on Because the different 
applications the different knowledge characteristics we can not copy the other know
ledge acquisition methods 
 
Knowledge Acquisition of Multiple Information Sources 
191 
 
2 
Building Process of Assembly Knowledge Base 
Assembly rapid design knowledge is the basis of the assembly design the whole as
sembly design process revolves around the assembly design knowledge Divided by 
locations and functions in conceptual design process the assembly knowledge can be 
divided into assembly metaknowledge and assembly instanceknowledge Meta
knowledge can be divided into domain knowledge knowledge of demand domain 
knowledge of functional domain knowledge of process domain and knowledge of 
physical domain mapping rules rules of interpretation evaluation rules 5  
Building processes of assembly knowledge base include the introduction of assem
bly design knowledge knowledge classification acquisition representation creating 
a database and knowledge base maintenance techniques It can be summarized in 
four stages conceptualization stage formalization stage implementation stage and 
testing phase These four stages contact closely but indivisible The build process is 
shown in Fig 1 
 
Fig 1 The building process of Knowledge Base 
Conceptualization stage is the knowledge acquisition phase the task at this stage is 
to identify the overall mission and relationship Such as” How these are formed”” 
what is the relationship between the child objects  and” what is the relationship 
between the child object and parent object” These questions as a starting point to 
obtain the knowledge so as to achieve a comprehensive analysis 
3 
Knowledge Acquisition 
Knowledge acquisition is the most important and difficult part of assembly know
ledge base construction it is directly related to the quality and quantity of knowledge 
base Knowledge acquisition methods can be divided into two types which are direct 
access to knowledge and indirect access to the knowledge Based on the phenomenon 
and the data direct access to knowledge summed up the knowledge of the conclusion 
or decision into the knowledge base This method attempts to solve the difficulties of 
automatic acquisition of knowledge Due to induction is an integrated process so it is 
more difficult than the interpretation   
At present there is no such knowledge of the program This paper discusses the 
method of indirect access and imparts the knowledge to knowledge processing  
192 
L Xia L Zhang and Z Yan 
 
systems with the knowledge editor and other tools Aircraft assembly design know
ledge base involves three types of knowledge namely the fact knowledge rule know
ledge and instance knowledge The knowledge acquisition mode is shown in Fig 2 
 
Fig 2 Knowledge acquisition mode 
31 
Acquisition of Rule Knowledge 
The rule knowledge are some experiences standards and so on they can be used in 
the rulebased reasoning process derived from practical experience of the experts and 
books theory Assembly design is the realization of the design requirements such as 
selection and design layout design as well as to determine the performance require
ments Thus the main task of rule knowledge acquisition in assembly design is to 
extract the related knowledge of selection and design the related knowledge of the 
layout design and the relevant knowledge to determine the performance requirements 
The acquisition problem of rule knowledge is to solve the design process in which 
knowledge is required For example when we select the rules for the strengthening of 
the molding structure in the type design of the assembly design the crucial question is 
how to determine the strengthening of the molding structure Answers to the 
questions First identify ways of strengthening the structure according to the molding 
shape curvature The second is to identify ways of strengthening the structure 
according to the relative size of profile molding appearance Then according to the 
 
Knowledge Acquisition of Multiple Information Sources 
193 
 
characteristics of rule knowledge of the assembly design to further refine the 
knowledge of rules 
①For the acquisition of unity knowledge as to whether the use of flanging streng
thening structure of assembly design as an example we can analyze from two aspects 
According to the first aspect from the forming shape curvature defining reinforcing 
structure and gets a rule A If the molding shape with big curvature radius will 
choose flanging strengthening structure According to the second aspect from the 
molding shape profile relative to the size of the angle consideration and gets a rule B 
If the molding shape outline is relatively small select flanging strengthening struc
ture 
②For the acquisition of overall knowledge knowledge due to its large capaci
ty first is to arrangement the experience knowledge into that form and then trans
form into rules from the assertion 
For example the following paragraph is expert knowledge about the fixture sample 
together in the form selection model 
“For the choice of fixture model together form model appearance and involution 
accuracy requirement is that the decisive condition of the fixture model together way 
In general simple shape and coerror model using the pair together in the form 
involution accuracy higher assembly component model using triangle pairs together 
in the form high accuracy of the cotemplate hole involution forms if it is the block 
structure of the larger plane model the fixture sample is used to dovetail together in 
the form”  
Organized into assertions as 
“If the model shape is simple and the coerror using the pair together in the form” 
“If the template used for assembly components and higher on the coaccuracy the 
use of triangular form of involution” 
“If the model is large and the high accuracy the cohole pairs together in the 
form” 
“If the model plane larger and subblock structure the dovetail together in the 
form” 
According to the above assertion can create the following fixture model selected 
set of rules together in the form 
RS Fixture model selection together in the form 
Features：involutive forms 
Rules 
Rules1 Using the pair together in the form 
IF simple shape AND coaccurate low 
THEN Object name by the line together to form 
Rules2 Using triangle pairs together in the form 
IF the model shape for assembly parts AND higher accuracy 
THEN Object name by triangle to form 
Rules3 Using holes together in the form  
IF model of larger size AND high accuracy 
THEN Object name by hole to form 
194 
L Xia L Zhang and Z Yan 
 
Rules4 Using dovetail together in the form 
IF larger model plane and block shape 
THEN Object name by dovetail to form 
32 
Acquisition of Fact Knowledge 
In objectoriented knowledge base the fact knowledge can be obtained by the human
computer interaction characteristics inheritance rulebased reasoning The characte
ristics inheritance obtains fact value from the parent class as the fact property value 
Based on existing conditions the rulebased reasoning invoked the rules set of object 
class the reasoning results as the eigenvalue of the fact knowledge All the three me
thods to achieve the knowledge acquisition is recording the ask inherit deduce 
on the corresponding position of characteristic value 
  The following is an acquisition example of fact knowledge The vacuum fixture 
is typically used on a CNC Computerized Numerical Control milling machine it can 
machining the large area parts such as the aircraft integral panel the overall beam the 
overall rib and honeycomb core   
 
Object vacuum fixture classIntensify force 
calculations 
Sealing coefficient Ask 
Residual pressure of Vacuum chamber Ask 
Effective vacuum area Ask 
Seal bar compression rebound per meter length Ask 
The length of Seal bar Ask 
Atmospheric pressure Inherit 
Pressure difference Deduce 
The logic algorithm processes of fact knowledge acquisition are shown in Fig 3 
33 
Acquisition of Instance Knowledge 
Instances as the name suggests are the classic typical examples methods or solutions 
used in the assembly design The task to obtain instance knowledge is extracted 
success stories from the previous assembly design as an example stored in instance 
library after the knowledge engineer finish work 
Understanding stage in the knowledge acquisition of instance the first step is to 
understand where the problem is in assembly design instance The main features in
clude those elements how to communicate with assembly design experts and re
search 
Instance knowledge of assembly design includes the asking of instance problem 
the solution to the problem the evaluation of instance The asking of instance prob
lem is the demand analysis for assembly design According to the design needs the 
result obtained is the solution to the problem The evaluation of instance is the evalua
tion of the practicality and reliability of instance Then according to the feature of the  
 
 
Knowledge Acquisition of Multiple Information Sources 
195 
 
 
Fig 3 Logic algorithm processes of the fact knowledge acquisition 
characteristics problem of the assembly design instance to further refine the content 
of the instance knowledge Generally the assembly design example includes the fol
lowing aspects  
1 Instance number the number of technology and equipment in actual production 
2 The problem of instance requirements to solve ie design requirements as
sembly type purpose structure strength functional requirements user requirements 
and development costs  
3 Solution to the problem ie specific design 
Vacuum molding for example its design includes the following four aspects  
196 
L Xia L Zhang and Z Yan 
 
①Application mainly used for the honeycomb sandwich structure and laminated 
pieces of molding of singlesided assembly requirements Honeycomb sandwich in
cludes the prepreg honeycomb sandwich structure such as cowlings etc and alu
minum skin honeycomb sandwich structure such as flooring tail boom etc  
②Principle The mold only has a punch or the die side The vacuum bag film with 
putty bar along the mold surrounding sticky motif to form a sealed bag the product is 
sealed between the mold and the vacuum bag a vacuum filter tipped on the vacuum 
bag vacuum tipped out of the air from the vacuum filter Under negative pressure the 
prepreg plies are tightly pressed to the mold Keeping pressure for some time look for 
leaks and put positive pressure in the autoclave and heat it to complete the curing 
③Mold structure The mold without pressure side bar is mainly used for the pre
preg honeycomb sandwich structure Prepreg honeycomb sandwich structure of the 
cellular is 45 degrees pressure can not be collapsed and therefore the pressure side 
bar does not be needed The mold with pressure side bar is mainly used for the alumi
num skin honeycomb sandwich structure Due to the honeycomb of aluminum skin 
honeycomb sandwich structure is generally 90 degrees pressure easily collapsed so 
the pressure side bar does be needed 
④The basic design requirements methods and processes Mold shape is the 
product shape Mold surface needs to be marked the product edge line the cellular 
line the center line of the holes Product edge line or the outside of pressure side bar 
must to be remained at least 80 mm area for the production of vacuum bag The sur
face of the products outside the edge line is extended along the product shape The 
mold should ensure air tightness can not have a throughhole in the region of the 
vacuum bag Mold thickness is as even as possible mold should have carrying han
dles or rings for the relatively large size of the pieces mold design should consider 
the reduction ratio 
4 Program evaluation accuracy specifically to achieve targets reliability 
4 
Conclusion 
Assembly rapid design knowledge is the basis of the assembly design the whole 
assembly design process revolves around the assembly design knowledge This paper 
analyzes the importance of knowledge acquisition in aircraft assembly design process 
Introduce briefly how to create an assembly knowledge base On this basis the 
acquisition methods of fact knowledge rule knowledge and instance knowledge are 
introduced in detail Assembly design knowledgebased systems full use of the above 
three kinds of knowledge representation When the user query the system can display 
the corresponding knowledge according to the multiple types of feature information 
given by the user 
 
Acknowledgment Project supported by the Defense Industrial Technology Devel
opment Program Grant No A0520110003 and the Shaanxi Science and Technology 
projects Grant No 2010K0907 The author would also like to gratefully acknowl
edge the useful remarks and kind support of Dr Yu jianfeng and Dongliang 
 
Knowledge Acquisition of Multiple Information Sources 
197 
 
References 
1 
Romero PC Ventura PS Educational data mining A Survey from 1995 to 2005 Ex
pert Systems with Applications an International Journal 331 135–146 2007 
2 
Furman SM Scott LZ Joseph CK Factors Associated with Medical Knowledge Ac
quisition During Internal Medicine Residency Journal of General Internal Medicine 227 
962–968 2007 
3 
Tai L Guo H Zhong T Li D Journal of Harbin Institute of Technology Journal of 
General Internal Medicine 5 2007 
4 
Wang H Nie G Fu K Distributed Knowledge Acquisition Based on Semantic Grid 
In Proceedings of 2009 AsiaPacific Conference on Information Processing vol 1 2009 
5 
Lin J Liao W Li Y Objectorientedbased Knowledge Representation for Aircraft As
sembly Assembly Conceptual Design Machine Building Automation 385 80–82 2009 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 198–205 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Generalizing Sufficient Conditions and Traceable Graphs 
Kewen Zhao  
Institute of Information Science and Mathematics Qiongzhou University 
 Sanya Hainan 572022 PR China 
kwzqzuyahoocn 
Abstract In 2005 Rahman and Kaykobad proved that if G is a 2connected 
graph with n vertices and du+dv+δuv≥n+1 for each pair of distinct 
nonadjacent vertices uv in G then G is traceable Information Processing 
Letters 94 2005 1 3741 In 2006 Li proved that if G is a 2connected graph 
with n vertices and du+dv+δuv≥n+3 for each pair of distinct nonadjacent 
vertices uv in G then G is Hamiltonianconnected Information Processing 
Letters 98 2006 4 159161 In this present paper we prove that if G is a 
2connected graph with n vertices and du+dv+δuv≥n for each pair of 
distinct nonadjacent vertices uv in G then G has a Hamiltonian path or G 
belongs to a class of exceptional graphs We also prove that if G is a 2connected 
graph with n vertices and du+dv+δuv≥n+2 for each pair of distinct 
nonadjacent vertices uv in G then G is Hamiltonianconnected or G belongs to 
a classes of exceptional graphs Thus our the two results generalize the above 
two results by Rahman et al and Li respectively 
Keywords rahmankaykobad condition new sufficient condition traceable 
graphs hamiltonian graphs Hamiltonianconnected  
1 
Introduction 
We consider only finite undirected graphs without loops or multiple edges For a graph 
G let V G be the vertex set of G and EG the edge set of G The complete graph of 
order n is denoted by Kn For two vertices u and v let δuv be the length of a shortest 
path between vertices u and v in G that is δu v is the distance between u and v If H 
and S are subgraphs of G or subsets of VG let NHS be the set of vertices in H that are 
adjacent to some vertex in S and let the cardinality of NHS be |NHS| =dHS In 
particular if H = G and S is a vertex u then NGS is the neighborhood of u in G 
Furthermore let GH and GS denote the subgraphs of G induced by V GV H and 
S respectively For each integer m≥3 let Pmx1 xm=x1x2…xm denote a path of order m 
whose two endvertices are x1xm and define 
N+
Pmu=xi+1∈VPmxi∈NPmu 
N
Pmu=xi1∈VPmxi∈NPmu 
 
Generalizing Sufficient Conditions and Traceable Graphs 
199 
 
N±
Pmu=N+
Pmu∪N
Pmu 
where subscripts are expressed as integers modulo m 
A path in a graph G that contains every vertex of G is called a Hamiltonian path A 
graph G is said to be Hamiltonianconnected if its any two vertices can be connected by 
a Hamiltonian path Hamiltonian path and Hamiltonianconnected are important 
subjects in interconnection networks  
If no ambiguity can arise we sometimes write Nu instead of NGu V instead of 
VG etc We refer to 35 for graph theory notation and terminology not described in 
this paper 
It is wellknown that the Hamiltonian graph problem is NPcomplete 1 In 2005 
Rahman and Kaykobad 4 established a sufficient condition for Hamiltonian path 
graphs 
Theorem 11Rahman and Kaykobad 4 If G is a connected graph with n vertices and 
du+dv+δuv≥n+1 for each pair of distinct nonadjacent vertices uv in G then G 
has a Hamiltonian path  
In this present paper we study further condition du+dv+δuv≥n which 
improves the above condition of Theorem 11 
Theorem 12 If G is a 2connected graph with n vertices and du+dv+δuv≥n for 
each pair of distinct nonadjacent vertices uv in G then G has a Hamiltonian path or 
G∈Gn22∨K−
n+22 
Where Gn22 is graphs of order n22 and n is even K−
n+22 is empty graph of 
order n+22 For graphs A and B the join operator “A∨B” of A and B is the graph 
constructed from A and B by adding all edge joining the vertices of A and the vertices 
of B  
Theorem 13 If G is a 2connected graph with n vertices and du+dv≥n2 for each 
pair of distinct nonadjacent vertices uv of δuv=2 in G then G has a Hamiltonian 
path or G∈Gn22∨K−
n+22 
In 5 we studied the pancyclic with condition du+dv+δuv≥n+1 and in 6 we 
studied Hamiltonianconnected which is also the following topic 
In 2006 Li 3 investigated Hamiltonianconnected with the following 
RahmanKaykobad condition dx+dy+δxy≥n+3  
Theorem 14Li3 If G is a 3connected graph with n vertices and 
dx+dy+δxy≥n+3 for each pair of nonadjacent vertices xy in G then G is 
Hamiltonianconnected   
In this paper we also present the following two results which improve the above 
Theorem 14 
Theorem 15 If G is a 2connected graph of order n≥3 such that dx+dy+δxy≥n+2 
for each pair of nonadjacent vertices xy in G then G is Hamiltonianconnected or 
G∈Gn2∨K−
n2 
200 
K Zhao 
 
Theorem 16 If G is a 2connected graph of order n≥3 such that dx+dy≥n for each 
pair of nonadjacent vertices xy with dxy=2 in G then G is Hamiltonianconnected or 
G∈Gn2∨K−
n2 
2 
Proof of Main Results 
Clearly Theorem 12 can be obtained easily from Theorem 13 so in this section first 
we will prove Theorem 13 
Proof of Theorem 13 Suppose G satisfies the hypothesis of Theorem 13 and contains 
a longest path Pm=x1x2…xm that is not a Hamiltonian path Let H be a component of 
GPm First we consider the following two claims 
Claim 1 dxi+1xj+1=2 for each pair xi+1 xj+1∈N+
PmH  
Proof of Claim 1 Suppose to the contrary that dxi+1xj+1≠2 Then if dxi+1xj+1=1 let 
P be a path in H which two endvertices adjacent to xixj respectively Without loss of 
generality assume ij then x1x2…xiPxjxj1…xi+1xj+1xj+2…xm is a path longer than Pm a 
contradiction Thus under the hypothesis dxi+1xj+1≠2 we have dxi+1xj+1≥3 Without 
loss of generality assume u∈VH and xi∈NPmu since Pm is a longest path so clearly 
both u and xi+1 do not have any common neighbor in GPm Hence we have 
|NGPmu|+|NGPmxi+1|≤|VG–Pm–u| 
1
Also since Pm is a longest path so clearly xi+1 is not adjacent to any of N+
Pmu thus 
|NPmxi+1|≤|VPm||N+
Pmu| and since dxi+1xj+1≥3 so xi+1 is also not adjacent to xj 
xj+2 and 
clearly 
xjxj+2∉N+
Pmu 
Hence 
we 
further 
have 
inequality 
i 
|NPmxi+1|≤|VPm||N+
Pmu||xjxj+2| together with that Pm is a longest path then u is 
not 
adjacent 
to 
xm 
so 
|N+
Pmu|=|NPmu| 
then 
from 
inequality 
i 
|NPmxi+1|≤|VPm||N+
Pmu||xjxj+2| we have 
|NPmu|+|NPmxi+1|≤|VPm||xjxj+2| 
2
Combining inequalities 1 and 2 we have  
du+dxi+1≤|VG–Pm–u|+|VPm||xjxj+2|=n3 
a contradiction Therefore the above Claim 1 that dxi+1xj+1=2 holds  
Claim 2 dxi+1+dxj+1≤n|VH|  
Proof of Claim 2 Without loss of generality assume ij and let S1 denote the path 
x1x2…xi that is a section of Pm and S2 denote the path xi+1xi+2…xjxjand S3 denote the 
path xj+1xj+2…xm Since Pm is a longest path of G so clearly none of N−
S1xj+1 are 
adjacent to xi+1 Otherwise if xh∈N−
S1xj+1 is adjacent to xi+1 Let P be a path in H 
which 
two 
endvertices 
adjacent 
to 
xixj 
respectively 
then 
x1x2… 
xhxi+1xi+2…xjPxixi2…xh+1xj+1xj+2…xm is a path longer than Pm a contradiction 
Similarly none of N+
S2xj+1 are adjacent to xi+1 and none of N−
S3xj+1 are adjacent to 
xi+1 Also clearly N−
S1xj+1∩N+
S2xj+1=∅ and N+
S2xj+1∩N−
S3xj+1=∅ and 
xi+1∉N−
S1xj+1∪N+
S2xj+1∪N−
S3xj+1 Hence we have  
 
Generalizing Sufficient Conditions and Traceable Graphs 
201 
 
|NPmxi+1|≤|VPm||N−
S1xj+1|+|N+
S2xj+1|+|N−
S3xj+1||xi+1|≤|VPm||NPmxj+1|
|xj||xi+1|=|VPm||NPmxj+1| this implies  
|NPmxi+1|+|NPmxj+1|≤|VPm|    
3
Also both xi+1 xj+1 do not have any common neighbor in GPmH and both xi+1 xj+1 all 
are not adjacent to any vertex of H hence 
|NGPmxi+1|+|NGPmxj+1|≤|VGPmH|   
4
Combining inequalities 3 and 4 we have  
dxi+1+dxj+1=Nxi+1|+|Nxj+1|≤|VPm|+|VGPmH|≤n|VH| 
5
Therefore the above Claim 2 that dxi+1+dxj+1≤n|VH| holds  
Then together with the assumption of Theorem that dxi+1+dxj+1≥n2 we have 
|VH|≤2 Now we consider the following cases on |VH|≤2 
Case 1 |VH|=2 
In this case since Pm is a longest path and |VH|=2 so clearly |xi+1xi+2…xj1≥2 for 
each pairs xi∈NPmuxj∈NPmvwhere uv=VH Also since |VH|=2 so u and v all 
are 
not 
adjacent 
to 
x1x2xm1xm 
thus 
we 
can 
check 
mindudv≤|VPm||x1x2xm1xm|3+1+|VHu| ≤n63+2=n3 Without loss 
of generality assume du=mindudv then by the assumption of Theorem that 
du+dxi+1≥n2 we have dxi+1≥n2n3=2n32  
i If there exist two distinct vertices xixj∈NPmu Then we have 
dxi+1+dxj+1 ≥4n34 
6
When n≥7 inequality 6 contradicts inequality 5 
When n≤6 Since u is not adjacent to x1x2xm1xm then we can obtain a path that is 
longer than Pm a contradiction 
ii If xi=NPmu ie |NPmu|=1 so we have du=2 By the assumption of 
Theorem du+dxi+1≥n2 we have dxi+1≥n4 Since G is 2connected so |NPmv|≥1 
If there exist xhxj∈NPmvxi by dxh+1+dxj+1≥n2 maxdxh+1dxj+1≥n22 so 
we can check dxi+1+maxdxh+1dxj+1n|VH| which contradicts the inequality 
5 
If 
there 
only 
exist 
xj=NPmvxi 
then 
we 
also 
can 
check 
dxi+1+dxj+1n|VH| which contradicts the inequality 5 
Case 2 |VH|=1  
In this case we claim du≥n32 Otherwise if dun32 by that G is 
2connected let xixj∈NPmu and by assumption of Theorem that du+dxi+1≥n2 and 
du+dxj+1≥n2 dxi+1n12 and dxj+1n12 so dxi+1+dxj+1n1 which 
contradicts inequality 5  
202 
K Zhao 
 
Thus du≥n32 holds  
When n is even Since du is integer by du≥n32 so du≥n22  
When n is odd we claim du≥n22  
Otherwise suppose to the contrary that dun22 together with the above claim 
du≥n32 this implies du=n32 Consider the following 
When |VGPm|≥2 let uv∈VGPm Since |VH|=1 for each component H of 
GPm and by the above consequence that du=n32 and dv=n32 so both uv 
have at least a common neighbor in Pm this implies du+dv≤2n62n2 which 
contradicts the condition of Theorem  
 When |VGPm|=1 iIf du≥3 since du=n32 so there must exist two 
vertices xixi+2 of Pm that are adjacent to u Otherwise if there does not exist this case 
then we can check du≤|VPm||x1xm|3+1=n3 which contradicts du=n32 
then by du=n32 |xi+1xi+2…xj1≤2 for any two consecutive neighbor vertices 
each pairs xixj∈NPmu so clearly xi+1 is not adjacent to every vertex of VPmNPmu 
Otherwise we easily obtain a path longer than Pm a contradiction so dxi+1≤|NPmu| 
Hence we have du+dxi+1≤n32+n32=n3 a contradiction ii If du=2 Since 
du=n32 so n=7 When Nu=xixi+2 then clearly Nxi+1=xixi+2 hence 
du+dxi+1=4≤n3 a contradiction When Nu=x2x5 then x1x3x4x6∉EG 
Otherwise x1x3x2ux5x4x6 is Hamiltonian path a contradiction Without loss of 
generality assume x1x3∉EG then clearly dx1≤2 hence du+dx1≤n3 a 
contradiction 
Therefore du≥n22 holds for all even and odd n in Case 2 
Then since Pm is a longest path of G so u is not adjacent to x1xm and u is at most 
adjacent to a vertex of xixi+1 for each pair of consecutive vertices xixi+1 on Pm By 
du≥n22 clearly we have Nu=x2x4…xm1 Then also since Pm is a longest path 
of G so clearly vertex set x1x3…xmu is a independent set By the assumption of 
theorem that du+dv≥n2 for any two uv∈x1x3…xmu this implies du=n22 
for each u∈x1x3…xmu Thus we easily obtain G∈Gn22∨K−
n+22 where 
VGn22=Gx2x4…xm1 K−
n+22=Gx1x3…xmu and “∨” is the join operator 
Therefore this completes the proof of Theorem 13              
The Proof of Theorem 12 Clearly if graph G satisfies the condition of Theorem 12 
then G also satisfies the condition of Theorem 13 And clearly Gn22∨K−
n+22 
satisfies the condition of Theorem 12 so Theorem 12 can be obtained from Theorem 
13 immediately 
Similarly Theorem 15 can be obtained easily from Theorem 16 so in this 
following we only need to prove Theorem 16 
Proof of Theorem 16 Assume that G is not Hamiltonianconnected with satisfying 
the condition of Theorem Then there exist two distinct vertices xy such that the 
longest path Pmxy is not Hamiltonian path Let H be a component of GPm Since G is 
 
Generalizing Sufficient Conditions and Traceable Graphs 
203 
 
3connected so there exist two vertices uv∈VH satisfying xi+1∈N+
Pmu and 
xj+1∈N+
Pmv Then we claim dxi+1xj+1=2 Otherwise if dxi+1xj+1≠2 by Pmxy is a 
longest xypath so xi+1xj+1∉EG this implies dxi+1xj+1≥3 Then also since Pmxy 
is a longest xypath so both u and xi+1 do not have any common neighbor in GPm 
Hence we have  
|NGPmu|+|NGPmxi+1|≤|VG–Pm–u| 
2
Let Pm1=Pmxm so |NPmu|≤|N+
Pm1u|+1 Clearly xi+1 is not adjacent to any of 
N+
Pm1u and since dxi+1xj+1≥3 then xi+1 is not adjacent to xjxj+2 and xjxj+2∉N+
Pmu 
Thus when xj+1≠xm we can check |NPmxi+1|≤|VPm||N+
Pm1u|+|xjxj+2| so we 
have 
|NPmu|+|NPmxi+1|≤|NPmu|+|VPm||N+
Pm1u||xjxj+2|≤|VPm|1 
2 
When xj+1=xm By Pm is a longest xypath so u is not adjacent to xm1 and dxi+1xj+1≥3 
so 
xi+1 
is 
not 
adjacent 
to 
xm 
So 
similarly 
we 
can 
check 
|NPmxi+1|≤|VPm||N+
Pm1u|+|xjxm| so we have 
|NPmu|+|NPmxi+1|≤|NPmu|+|VPm||N+
Pmu||xjxm|≤|VPm|1 
2 
Combining 
inequality 
1 
and 
one 
of 
2 
2 
we 
have 
du+dxi+1≤|VG–Pm–u|+|VPm|1=n2 a contradictionThus dxi+1xj+1=2 holds 
Then we let path x1x2…xi=P1 path xi+1xi+2…xjxj=P2 and xj+1xj+2…xm= P3 Since 
Pmxy is a longest xypath so clearly none of N−
P1xj+1 are adjacent to xi+1 none of 
N+
P2xj+1 are adjacent to xi+1 and none of N−
P3xj+1 are adjacent to xi+1 and xi+1 is not 
adjacent to itself with |N−
P1xj+1|+|N+
P2xj+1|+|N−
P3xj+1|≥ |NPmxj+1|−|xj|−|x1| 
hence we have  
|NPmi+1|≤|m−|N−
P1xj+1|−|N+
P2xj+1|−|N−
P3xj+1|−|xi+1|≤m−|NPmxj+1|−|xj|−|
x1|−|xi+1|= m−|NPmxj+1|+1 then by plus |NPmxj+1| in two side of the inequality 
we have 
|NPmi+1|+|NPmxj+1|≤ m−|NPmxj+1|+|NPmxj+1|=m+1 
3
Also by Pmxy is a longest xypath so both xi+1 xj+1 do not have any common 
neighbor vertex in GPmH and xi+1 xj+1 are not adjacent to any vertex of H Hence we 
can check 
|NGPmi+1|+|NGPmxj+1|≤ |VG−Pm−H| 
4
 
204 
K Zhao 
 
Combining 3 and 4 we have 
dxi+1+dxj+1≤n−|VH|+1 
5
Since dxi+1+dxj+1≥n this implies |VH|≤1 Then we consider the following cases 
In this case we claim dun12  Otherwise if du≤n12 by condition of 
Theorem that du+dxi+1≥n we have dxi+1≥n+12 and dxj+1≥n+12 then we 
have 
dxi+1+dxj+1≥n+1 
this 
contradicts 
the 
inequality 
5 
that 
dxi+1+dxj+1≤n|VH|+1   
We also claim that there exist at most two components H=u and R=v in GPm 
Otherwise if there at least exist three components in GPm and let H R T are three 
components of GPm by Case 2 we let H=u R=v and T=w since G is 
3connected we may let xi+1xj+1∈N+
PmH Since Pmxy is a longest xypath then 
xi+1 or xj+1 is not adjacent to vertex VROtherwise if xi+1 and xj+1 are all adjacent to 
vertex VR then we obtain a xypath x1x2…xiuxjxj1…xi+1vxj+1xj+2…xm is a longer 
xy path a contradiction Without loss of generality assume xi+1 is not adjacent to 
vertex VR Since |VH|=1 for each component H of GPm we claim that vertex 
VH=u and VR=v must be adjacent to x1 and xmOtherwise for example if u is 
not adjacent to xm then we have |N+
Pmu|=|NPmu| du=|NPmu| and clearly 
dxi+1≤|VPm||N+
Pmu|+|VGPmHT| 
Hence 
we 
have 
du+dxi+1≤|NPm 
u|+|VPm||N+
Pmu|+|VGPmHT|=n2 a contradiction Similarly we can prove 
that u is adjacent to x1 and VR=v is adjacent to x1 and xm Thus duv=2 then 
clearly du≤m+12 and dv≤m+12 so we have du+dv≤m+1≤n2 a 
contradiction The contradiction shows that there at most exist two components in 
GPm By du≥n12 then we consider the following two cases 
In this case there only exist a components H=u in GPm Otherwise if there exist 
two components H=u and R=v in GPm then m ≤n2 so there must exist two xi and 
xi+1 that are adjacent u so we obtain a xypath longer than Pmxy a contradiction  
  By dun12 and Pmxy is a longest xypath we have du=n2 and 
Nu=x1x3…x2r1x2r+1…xm and clearly x2x4…x2r x2r+2…xm1∪u is an 
independent set Thus we have G∈Gn2∨K−
n2 
3 
Conclusion 
Recently Hasan Kaykobad Lee et al2 presented a detailed analysis of the recent 
works on Hamiltonian graphs under RahmanKaykobad conditions They showed each 
classes of exceptional graphs under each distances of graphs RahmanKaykobad 
condition du+dv+δuv≥n+1 is sufficient to make a graph Hamiltonian In this 
present paper we study further condition du+dv+δuv≥n for the existence of 
Hamiltonian paths Naturally it is interesting that there are what classes of 
nonHamiltonian graphs under the condition du+dv+δuv≥n  
Acknowledgements The authors are very grateful to the anonymous reviewer for his 
very helpful remarks and commentsThe work of the first author was supported by the 
NSF of Hainan Province no 10501 
 
Generalizing Sufficient Conditions and Traceable Graphs 
205 
 
References 
1 
Garey MR Johnson DS Computers and Intractability A Guide to the Theory of 
NPCompleteness W H Freeman and Company New York 1979 
2 
Kamrul M Hasan M Kaykobad YK Lee SY Note A Comprehensive Analysis of 
Degree Based Condition for Hamiltonian Cycles Theoretical Computer Science 4111 
285–287 2010 
3 
Li R A New Sufficient Condition for Hamiltonicity of Graphs Information Processing 
Letters 984 159–161 2006 
4 
Rahman M Kaykobad M On Hamiltonian Cycles and Hamiltonian Paths Inform 
Process Lett 941 37–41 2005 
5 
Zhao KW Lin Y Zhang P A Sufficient Condition for Pancyclic Graphs Information 
Processing Letters 10916 991–996 2009 
6 
Zhao KW Lao HJ Zhou J Hamiltonianconnected Graphs Computers Math 5512 
2707–2714 2008 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 206–213 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Note on the Minimal Energy Ordering  
of Conjugated Trees 
Yulan Xiao1 and Bofeng Huo12 
1 Department of Mathematics Qinghai Normal University 
2 Key Lab of Tibetan Information Processing Qinghai Normal University  
Ministry of Education and Qinghai Province 
Xining 810008 PR China 
hbfqhnueducn 
Abstract For a simple graph 
 the energy 
 is defined as the sum of 
the absolute values of all eigenvalues of its adjacency matrix 
 Gutman 
proposed two conjectures on the minimal energy of the class of conjugated trees 
trees having a perfect matching Zhang and Li determined the trees in the 
class with the minimal and secondminimal energies which confirms the con
jectures Zhang and Li also found that the conjugated tree with the third
minimal energy is one of the two graphs which are quasiorder incomparable 
Recently Huo Li and Shi found there exists a fixed positive integer
 such 
that for all
 the energy of the graphs with the thirdminimal through the 
sixthminimal are determined In this paper the 
 is fixed by a recursive me
thod and the problem is solved completely 
Keywords extremal graph minimal energy quasiorder incomparable conju
gated tree 
1 
Introduction 
For a given simple graph 
 of order
 denote by 
the adjacency matrix  
of 
 The characteristic polynomial of 
is usually called the characteristic 
polynomial of 
 It is wellknown 3 that the characteristic polynomial of a tree 
can be expressed as  
 
 
  
where 
denotes the number of the 
matchings of  
  
The energy is one graph parameter stemming from the Hückel molecular orbital 
HMO approximation for the total 
electron energy see 9 If 
 de
note the eigenvalues of adjacency matrix 
 the energy of 
 is defined as 
                                                           
  Supported by NSFC and Youth Innovation foundation of Qinghai Normal University 
G
EG
G
A
0
N
N0
n 
0
N
G
n
AG
G
AG
G
T





=
−
−
=
2
0
2
1
n
k
k
n
k
m T k x
T x
  


  
φ
  
m T k
k
T
π
λn
λ λ




2
1
AG
G
 
Note on the Minimal Energy Ordering of Conjugated Trees 
207 
 
 
 
Coulson 2 obtained 
 
1
In particular the energy of a tree 
 6 is 
 
 
where 
is the number of the 
matchings of 
 Now we show one well
known result due to Gutman 9 
Lemma 1 If 
 and 
 are two graphs with the same number of vertices then  
 
 
If 
 and 
 are two trees with the same number of vertices it is clear that 
 if 
 for all
 So there exists a quasi
order 
 in the set of trees with the same order For two trees 
 and 
 with 
vertices if 
 holds for all 
 then we define
 Thus 
 implies 
 5 7 18 Similarly a quasiorder can be defined in 
bipartite graphs 16 and unicyclic graphs11 these relations have been established 
for numerous pairs of graphs 4 5 8 11 12 14–19 We will compare the energy of 
trees of order 
having a perfect matching and determine the third  forth fifth 
and sixthminimal energy in this class For more results on graph energy we refer to 
9 10 and for terminology and notation not defined here we refer to Bondy and 
Murty 1 
2 
Preliminaries 
Denote by 
the star
 
 the tree obtained by attaching a pendent edge to a 
pendent vertex of the star
 
 by attaching two pendent edges to a pendent 
vertex of  
 and 
 by attaching a 
 to a pendent vertex of
 In 5 
Gutman gave the following  
Lemma 2 For any tree 
of order
 if
 then  
 
Denoted by 
the class of trees with 
 vertices which have a perfect matching For 
the minimal energy tree in
 Gutman proposed two conjectures in 8 Zhang and Li 
17 confirmed that both conjectures are true by using quasiordering relation 
  

=
=
n
i
i
G
E
1
λ


x
G x
G ix
ix
n
E G
d
1 
+∞
−∞




′
−
=
 






φ
φ
π
T
x
m T k x
x
T
E
n
k
k d
1
π
2
0
2
1
2
2


+ ∞




=
−










+
=



log


  
m T k
k
T
G1
2
G
x
ix
G
ix
G
E G
E G
d
1
2
1
2
1

+∞
−∞
=
−






log




φ
φ
π
1T
2T




2
1
E T
E T
≤
 

  
m T k
m T k
2
1
≤
 
2
1
n
k

=

1T
1T
n
 

  
m T k
m T k
2
1
≤
k ≥ 0
2
1
T
T 
2
1
T  T




2
1
E T
E T
≤
n
Xn
1
1
K n−
nY
2
1
K n−
n
Z
3
1
K n−
Wn
3P
3
1
n−
K 
T
n
n
n
n
n
Y Z W
X
T



≠
T
W
Z
Y
X
n
n
n
n




Φn
n
Φn

208 
Y Xiao and B Huo 
 
Lemma 3 17   In the class
 
 is minimal for the graph
 and 
 if and only if  
 where 
 is obtained by attaching a pendent 
edge to each vertex of the star 
 
For the trees of the second third and forthminimal energies in
 they obtained 
the following  
Lemma 4 17 In the class
 the graph attained the secondminimal energy is
 
where 
is the graph obtained from 
 by attaching a 
 to the 2degree vertex 
of a pendent edge and 
 if and only if
 
Lemma 5 17 In the class
 the graphs attained the third and forthminimal ener
gy are in 
 where 
 is the graph obtained from 
by attaching two 
’s 
to the 2degree vertex of a pendent edge and 
 is obtained from 
 by attaching 
a 
 to a 1degree vertex to form a path of length 6 Furthermore 
and 
are 
quasiorder incomparable Denote by 
 the graph obtained by attaching a 
 to a 
1degree vertex of 
and attaching a pendent edge to each other 1degree vertex of  
 
 the graph obtained by attaching a pendent edge to each vertex of 
 
 Li and Li 15 proved the following results 
Lemma 6  In the class
 the graph of the third forth fifth and sixthminimal 
energy are in 
 Furthermore 
and
  but 
 and 
 
are quasiorder incomparable  
 
3 
Main Results 
In 13 Huo et al gave the following consequence 
 
Theorem 7  There exists a fixed positive integer
 for all 
 
 
Φn
ET
nF




nF
E
E T
=
nF
T =
nF
1   2 1
n −
K

n
Φ
Φn
n
B
Bn
nF −2
3P




E Bn
E T
=
Bn
T =
n
Φ
n Mn
L 
nL
nF −4
3P
Mn
−2
nF
3P
nL
n
M
nI
3P
n
X
 1 
2 −
n
X

2
n
W
2
n
W
n
Φ




2
n
n
n
n
I W
M
L
n
n
I
M 

2
n
n
L  W
nI
n
L
1
1
1
1
2
2
2
2
Ln
Mn
In
W ∗
n
2
n
2 − 3
n
2 − 2
n
2 − 1
n−6
2
N0
n  N0




n
n
E L
E I

 
Note on the Minimal Energy Ordering of Conjugated Trees 
209 
 
Corollary 8  There exists a fixed positive integer
 for all 
  
 
In the following theorem by a recursive method we will give the accurate value of 
 in Theorem 7 and Corollary 8 
Theorem 9 For all even integers
 
  
Proof By using Lemma 1 it is easy to get 
 
 
In the proof of Theorem 7 the uniformly convergence of the sequence 
 has been proved Denote by 
 the integrand Since for 
all
  
   
2
we can express 
 as
  
For all real number
and all positive integer
 
 
 
 
  
It follows that 
 
 
 
 
Notice that 
is a pointwise convergent sequence since 
exists and 
is a piecewise continuous function  
 
 
 
 
Theorem 7 has proved that
converge uniformly to 
 in a interval 
 and d  will be introduced later For
 
 
N0
N0
n 









2
n
n
n
n
E W
E L
E I
E M



0
N
n ≥ 84








∗



2
E Wn
E L
E I
M
E
n
n
n
x
x
x
n
x
x
x
n
x
n
x
x
E L
E I
n
n
n
n
n
1d
3
4
2
3
1
1
2
3
π
2
0
2
4
6
8
2
4
6
8
2
2
2
3
2

+∞
+
+
−
+
+
+
+
+
+
+
=
−
+
+
+
+












log










n
n
E L
E I
g n
−
=
  
f x n
≥ −1
X
X
X
X
X
≤
+
≤
+

log1
1
  
f x n
1
3
4
2
3
2
6
1
2
4
6
8
2
4
2
2
2
2
+
+
−
+
+
−
+
−
+
=
+
+
+
x
x
n
x
x
x
x
x n
f
n
n
n
n










 

x
n
1
3
4
2
3
2
6
2
4
6
8
2
4
2
2
2
2
+
+
−
+
+
−
+
−
≤
+
+
+
x
x
n
x
x
x
x
x n
f
n
n
n
n










 

1
1
2
3
2
6
2
4
6
8
2
4
2
3
2
2
2
+
+
+
+
+
−
+
−
≥
+
+
+
x
n
x
n
x
x
x
x
x n
f
n
n
n










 

12
4
1  if x
3
4
2
3
2
6
2
4
6
8
2
4
2
2
2
2
−
−
≤
+
+
−
+
+
−
+
−
≤
+
+
+
n
n
x
x
n
x
x
x
x
x n
f
n
n
n
n











 

12
4
1  if x
1
2
3
2
6
2
4
6
8
2
4
2
3
2
2
2
−
−
≥
+
+
+
+
+
−
+
−
≥
+
+
+
n
n
x
n
x
n
x
x
x
x
x n
f
n
n
n











 

  
f x n
  
lim
x n
f
n
∞
→



=
≠
+
+
+
+
=
0
0
0
2
2
2
1
4
6
2
1
2
4
2
3
6
2
1
x
x
x
x
x
x
x
x
x
log
 
ϕ
  
f x n
x
ϕ
I = δ +∞ ⊂ 0 +∞
≠ 0
x











log
 
 









+
+
+
+
+
+
−
−
+
+
+
+
=
−
+
+
−
+
+
2
9
2
3
7
6
1
4
5
15
8
1
2
4
6
8
10
12
2
4
6
8
10
2
11
2
15
2
7
2
x
n
x
n
x
n
x
n
x
x
x
x
x
x
x
x
x n
f
n
ϕ
210 
Y Xiao and B Huo 
 
If 
 
 
if  
 
It is not hard to verify that

 
 
where 
 is 
the only 
positive 
root of polynomial 
 and
 For
 we have
 Therefore in the interval 
 
 
 
That is for arbitrarily small 
 there exists positive integer
 such that for 
 
 
Consequently 
 
 
 
Since
 if take
 there exists 
 such that for 
 
 
 
For the integrand fx n we have 
 
 
 
where 
 and  
 
 
As the only positive root of the polynomial
 the 
number 
is also the only positive root of 
 Thus for 
 
 
3
and for 
 
0
1
4
5
15
8
2
4
6
8
10
− ≥
−
+
+
+
x
x
x
x
x











 
  
2
9
2
3
7
6
1
4
5
15
8
2
4
6
8
10
12
2
4
6
8
10
2
11
2
15
2
7
2
+
+
+
+
+
+
−
−
+
+
+
≤
−
+
+
−
+
+
x
n
x
n
x
n
x
n
x
x
x
x
x
x
x
x
x n
f
n
ϕ
0
1
4
5
15
8
2
4
6
8
10
− ≤
−
+
+
+
x
x
x
x
x













 
  
1
5
7
12
15
7
1
4
5
15
8
2
4
6
8
10
12
2
4
6
8
10
2
11
2
15
2
7
2
+
+
+
+
+
+
+
−
−
+
+
+
−
≤
−
+
+
+
+
x
n
x
n
x
n
x
n
x
x
x
x
x
x
x
x
x n
f
n
ϕ
0
1
4
5
15
8
2
4
6
8
10
− ≥
−
+
+
+
x
x
x
x
x
for n ≥ 3 and
any x ∈ δ +∞
δ
1
4
5
15
8
2
4
6
8
10
−
−
+
+
+
x
x
x
x
x


 
δ ∈ 0 6750  06751
  
x ∈ 0 δ 
 
  
x
f x n
≤ ϕ

 
+ ∞
δ



+∞
+∞
+∞
→
+∞
→+∞
=
=
δ
δ
δ
ϕ

 
  
lim
  
lim
x
x
x
f x n
x
x n
f
n
n
d
d
d
ε  0
N
N
n 
³
³
³
+∞
+∞
+∞


−
δ
δ
δ
ε
ϕ
ε
ϕ

 
  
 
x
x
x
f x n
x
x
d
d
d
+






+∞
∞
+
+∞
+∞
+
=
+
+

+
=
0
0
0
0
d
d
d
d
d
d

 
 



 

 



ε
ϕ
ε
ϕ
ϕ
δ
δ
δ
δ
x
x
x
x
x
x
x
x n
f
x
x n
f
x
x n
f

+∞
≈ −
=
0
0 118023
d

 
α
ϕ
x
x
ε = α
0
N
n  N0


+∞
+∞
=
+

0
0
0
d
d

 
  
ε
ϕ
x
x
x
x n
f





 −
=
−
+
  
 
log
  

 
g x n
h x
f x n
f x n
1
1

1
4
5
15
8
2
4
6
8
10
2
2
1
−
−
+
+
+
=
x
x
x
x
x x
h x

















 

1
12
2
4
2
6
2
8
2
10
2
12
2
14
16
2
7
2
3
2
7
2
15
2
1
2
23
4
29
4
11
4
33
4
15
2
11
4
51
4
7
2
21
4
27
4
1
2
13
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
=
x
n
x
n
n
x
n
n
x
n
n
x
n
n
x
n
n
x
n
x
x n
g
1
4
5
15
8
2
4
6
8
10
−
−
+
+
+
x
x
x
x
x
δ
hx
  
∈ 0 δ 
x
 

 
  
x
f x n
f x n
≤ ϕ
+
≤
1

 
+ ∞
∈
δ  
x
 
Note on the Minimal Energy Ordering of Conjugated Trees 
211 
 
 
4
Naturally it follows that for
 
is monotonically increasing on 
 
while for
 
 is monotonically decreasing on 
 
Notice that 
 for 
 and 
 for 
  
Moreover 
 and 
 for
 Let
 
 we deduce that for any 
 
 
 and 
 are 
all positive According to the geometrical interpretation on the integral of realvalue 
function the following inequalities are plain 
 
5
Similarly since 
 for 
and 
 for 
 according to 
the geometrical interpretation on the integral of realvalue function we have 
 
6
Let 
 be a positive integer by inequalities from 3 to 6 the following in
equalities hold for 
 
 
 
We find the critical point of 
 at which the expression 2 switches from negative to 
positive The point is 
 In fact by running a computer with Maple program 
we get 
 
7
It implies that for 
 
 
For 
 we use a recursive method that evaluate the integral piecewise 
Concretely let 
 and 
 be positive integers with 
 By means of 
the inequalities from 3 to 6 for positive integer 
 with 
 we have 
 
8
 
  

 
 
f x n
f x n
x
≤
+
≤
1
ϕ
  
x ∈ 0 δ 
  
f x n
n

 
+ ∞
∈
δ  
x
  
f x n
n
    0
f x n

12 
4
0  
−
−
∈
n
n
x

    0
f x n

+ ∞
−
−
∈
12  
4 
n
n
x


 
δ ∈ 0 6750  06751

1 3 
12
4

∈
−
−
n
n
n ≥13
0 6750
1
= 
d
0 6751
2
d = 
n ≥13
 

d n
f
1
 

n
d
f
2
  
n
f δ
13
d     n
d
d
1
2
≥





+∞
+∞
+∞

  
  
 

d
d
x
f x n
x
f x n
x
x n
f
δ
ϕx  0

x ∈ 0 1 
ϕx  0

+ ∞
∈
1 
x
d     
d
d
2
0
0
0
1

 
 
 





d
d
x
x
x
x
x
x
ϕ
ϕ
ϕ
δ
1 ≥13
N
N1
n ≥


 
 

 


  
  
 








+∞
∞
+
+∞
+∞
+

+
≤
+
=
1
2
d
d
d
d
d
d
d
1
0
1
0
0
0
d
d
x
f x N
x
x
x
f x N
x
x
x
f x n
x
f x n
x
x n
f
ϕ
ϕ
δ
δ
δ
δ
N1
1 = 377
N
0 0001
377 d
d
1
2
0


 
 
= −
+ 

+∞
d
d
x
f x
x
x
ϕ
n ≥ 377
0
d
0


+∞
x
f x n
 

377
94
≤
≤ n
1n
2n
12
13   
=
≥
i
ni

n
2
1
n
n
n
≤
≤


 

 

 



  
  
 








+∞
∞
+
+∞
+∞
+

+
≤
+
=
1
2
d
d
d
d
d
d
d
1
0
2
1
0
2
0
0
d
d
x
f x n
x
f x n
x
f x n
x
x n
f
x
f x n
x
f x n
x
x n
f
δ
δ
δ
δ
212 
Y Xiao and B Huo 
 
By running a computer with Maple program we get 
 
9
According to inequalities 8 and 9 we deduce 
 for 
 
It implies 
 for 
 For 
 we give Table 1 which 
is also attained by employing Maple program From the table one can see that 
 for the even integers 
 from 
 to
 while 
 
for even integers 
 from 
 to
 The theorem is a summary of the discus
sion above in which the conjugated trees with the thirdminimal through the sixth
minimal energies for 
are determined  
Table 1 Values of 
 for even integers n with 
 
 
 
 
 
n=8 
0154368 
n=10 
0133281 
n=12 
0117005 
n=14 
0103962 
n=16 
0093213 
n=18 
0084161 
n=20 
0076404 
n=22 
0069664 
n=24 
0063737 
n=26 
0058475 
n=28 
0053762 
n=30 
0049510 
n=32 
0045649 
n=34 
0042123 
n=36 
0038887 
n=38 
0035904 
n=40 
0033143
n=42 
0030577 
n=44 
0028185
n=46 
0025949 
n=48 
0023852
n=50 
0021881 
n=52 
0020023
n=54 
0018269 
n=56 
0016609
n=58 
0015035 
n=60 
0013541
n=62 
0012118 
n=64 
0010763
n=66 
0009470 
n=68 
0008235
n=70 
0007052 
n=72 
0005920
n=74 
0004834 
n=76 
0003791
n=78 
0002788 
n=80 
0001824
n=82 
0000895 
n=84 
−06 × 10−8 
n=86 
−0000864 
n=88 
−0001697 
n=90 
−0002503 
n=92 
−0003282 
n=94 
−0004036 
 



 






 






 



0 0004
 94 d
108 d
 
0 0001
 108 d
152 d
 
0 0004
 152 d
377 d
 
1
2
1
2
1
2
0
0
0
≈ −
+
≈ −
+
≈ −
+






∞
+
∞
+
+∞
d
d
d
d
d
d
x
f x
x
x
f
x
f x
x
x
f
x
f x
x
x
f
0
d
0


+∞
x
f x n
  
377
94
≤
≤ n




n
n
E L
E I

377
94
≤ n ≤
94
8
≤
≤ n




n
n
E L
E I

n
n = 8
82




n
n
E L
E I

n
n = 84
94
≥ 84
n




n
n
E L
E I
−
94
8
≤
≤ n
n




n
n
E L
E I
−
n




n
n
E L
E I
−
 
Note on the Minimal Energy Ordering of Conjugated Trees 
213 
 
References 
1 Bondy JA Murty USR Graph Theory Springer Berlin 2008 
2 Coulson CA On the Calculation of the Energy in Unsaturated Hydrocarbon Molecules 
Proc Cambridge Phil Soc 36 201–203 1940 
3 Cvetković DM Doob M Sachs H Spectra of GraphsTheory and Application Aca
demic Press New York 1980 
4 Chen A Chang A Shiu WC Energy Ordering of Unicyclic Graphs MATCH Com
mun Math Comput Chem 55 95–102 2006 
5 Gutman I Acylclic Systems with Extremal Hückel πelectron energy Theor Chim Ac
ta 45 79–87 1977 
6 Gutman I Polansky OE Mathematical Concepts in Organic Chemistry Springer Ber
lin 1986 
7 Gutman I Zhang F On the Ordering of Graphs with Respect to Their Matching Num
bers Discrete Appl Math 15 25–33 1986 
8 Gutman I Acylclic Conjugated Molecules Trees and Their Energies J Math Chem 1 
123–143 1987 
9 Gutman I The Energy of a Graph Old and New Results In Betten A Kohnert A 
Laue R Wassermann A eds Algebraic Combinatorics and Applications pp 196–211 
Springer Berlin 2001 
10 Gutman I Li X Zhang J Graph Energy In Dehmer M EmmertStreib F eds 
Analysis of Complex Networks From Biology to Linguistics pp 145–174 WileyVCH 
Verlag Weinheim 2009 
11 Hou Y Unicyclic Graphs with Minimal Energy J Math Chem 29 163–168 2001 
12 Hou Y On Trees with the Least Energy and a Given Size of Matching J Syst Sci Math 
Sci 23 491–494 2003 
13 Huo B Li X Shi Y Wang L Determining the Conjugated Trees with the Third
through the Sixthminimal Energies MATCH Commun Math Comput Chem 65 521–
532 2011 
14 Lin W Guo X Li H On the Extremal Energies of Trees with a Given Maximum De
gree MATCH Commun Math Comput Chem 54 363–378 2005 
15 Li S Li N On Minimal Energies of Acyclic Conjugated Molecules MATCH Commun 
Math Comput Chem 61 341–349 2009 
16 Li X Zhang J Wang L On Bipartite Graphs with Minimal Energy Discrete Appl 
Math 157 869–873 2009 
17 Zhang F Li H On acyclic Conjugated Molecules with Minimal Energies Discrete 
Appl Math 92 71–84 1999 
18 Zhang F Lai Z Three Theorems of Comparison of Trees by Their Energy Science Ex
ploration 3 12–19 1983 
19 Zhou B Li F On Minimal Energies of Trees of a Prescribed Diameter J Math 
Chem 39 465–473 2006 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 214–220 2012 
© SpringerVerlag Berlin Heidelberg 2012 
An Ensemble Method Based on Confidence Probability 
for Multidomain Sentiment Classification  
Quan Zhou Yuhong Zhang and Xuegang Hu 
School of Computer  Information Hefei University of Technology 
Hefei 230009 Anhui China 
yuanzhouquan163com  
zhangyhjsjxhuxghfuteducn 
Abstract Multidomain sentiment classification methods based on ensemble 
decision attracts more and more attention These methods avoid collecting a large 
amount of new training data in target domain and expand aspect of deploying 
source domain systems However these methods face some important issues the 
quantity of incorrect prelabeled data remains high and the fixed weights limit 
accuracy of the ensemble classifier Thus we propose a novel method named 
CEC which integrates the ideas of selftraining and cotraining into mul
tidomain sentiment classification Classification confidence is used to prelabel 
the data in the target domain Meanwhile CEC combines the base classifiers 
according to classification confidence probabilities when taking a vote for pre
diction The experiments show the accuracy of the proposed algorithm has highly 
improved compared with the baseline algorithms 
Keywords ensemble multidomain sentiment classification cotraining 
1 
Introduction 
Nowadays expressing ones’ ideas and feelings on the Web has become a trend 
Product reviews blogs and forum posts are everywhere How to identify ones’ emotion 
expressed in these texts is of growing importance for practical applications eg 
ecommerce business intelligence information monitoring et al Thus there are 
increasingly interests in the study of sentiment classification 
However with the differences among domains the ways to show affection are rich 
and varied This affects and limits sentimental classification For example “durable 
and delicate” is often used to express positive sentiment for electronics review but 
hardly used for movie remark When a traditional classification scenario is carried over 
into a new sentimental domain a mass of labeled training data in the new domain are 
needed Obviously it is costly and timeconsuming Thus multidomain sentiment 
classification is proposed Matthew Whitehead et al 1 and Li 2 proposed mul
tidomain sentiment classification methods based on the ensemble classification using 
the labeled data in source domains Nevertheless there are some issues needing to be 
promoted The quantity of incorrect prelabeled data remains high and the fixed 
weights limit accuracy of the ensemble classifier 
 
An Ensemble Method Based on Confidence Probability 
215 
 
To address these issues we introduce the classification confidence into the ensemble 
strategy and propose CEC Confident Ensemble Classifier in which classification 
confidence is used to prelabel data in the target domain and to weight the voting of 
ensemble classification It successfully decreases the differences between the source 
domains and the target domain and gets a more accurate classifier for the target domain 
The rest of this paper is organized as follows Section 2 describes the proposed al
gorithm Section 3 presents experimental results Finally section 4 concludes this 
paper 
2 
An Ensemble Method Based on Confidence Probability 
This paper attempts to address the problem of sentiment classification from multiple 
sources Assume that there are k source domains in which data are labeled and each 
domain is denoted as Si Let T be the target domain and Li be the data prelabeled from 
Si Labels are denoted as 
y ∈1 1
−
 where “1” represents a positive review and “1” 
represents a negative review Our objective is to maximize the accuracy of assigning a 
label in y to the data in T utilizing the training data Si 
21 
The CEC Description 
The formal description of the proposed algorithm is as follows 
Input source domains S1S2…Sk target domain T confidence 
proportion r Maximum number of loop m 
Output TL=x1y1x2y2…xn…yn 
1 Set the initial number of iteration t=1 Ti= where 
i∈12…k 
2 Train MaxEnt classifier on Si∪Ti  and predict T to get piy︱x 
respectively  
3 If t=1 
 
Sort all the piy︱x by descending order and choose the αth ones 
as the threshold where α=|rn| 
4 Add instance x to Ti if it satisfies 
 
Pjy|x≥threshlod where j=1…i1i+1…k and i∈12…k 
5 If avgpiy|x decreases or t==m 
 
go to 6 
   else go to 2 
6 Predict each instance in the target domain T  according to the 
Formula as follows 
1
1

|


|

k
i
i
p y
x
p
y
x
k
=
=

 
1 The first step is initialization where t is the initial number of loop 2 Get base 
classifiers with Maxent model 3 Confidence probability threshold is determined 
according to the parameter r 4 Pick out the prelabeled data whose confidence 
probability is higher than threshold and add them to the training sets of other base 
216 
Q Zhou Y Zhang and X Hu 
 
classifiers 5 Judge whether the average of 


p i y x
 declines If the average falls 
down turn to 6 Otherwise go to 2 and enter the next loop 6 Predict each data in the 
target domain T through combining the base classifiers 
22 
The Prelabeling Method Based on Confidence Probability 
Avrim3 Cardie 4 et al tended to set the number of prelabeled data in advance for 
each base classifier However because of the different data distributions most base 
classifiers can not achieve good classification performance 1 Setting the number of 
prelabeled data blindly can not guarantee prelabeled data with high confidence 
probabilities Meanwhile as the similarities between source and target domains are 
different setting uniform number is bad for the more similar sources and setting spe
cific number for each source domain is difficult 
To solve these problems unlike traditional methods we combine and sort the clas
sification results of base classifiers and get the threshold of classification confidence 
probability utilizing r and n where r is a parameter and n is the number of data in the 
target domain The threshold is calculated as follows 

α = r n
 
Only the data of which classification confidence probability is above the αth top con
fidence probability can be prelabeled and added into the training data sets of other 
base classifiers This strategy ensures the accuracy of prelabeling data and allows the 
source domain which is more similar to the target domain to prelabel more data Thus 
it can reduce the interferences caused by less similar source domains 
23 
The Number of Iteration 
Avrim3 Stephen5 et al utilized a simple method setting the maximum number of 
iteration to end their algorithms Hence their methods can not exit the loop in a flex
ible way 
The confidence probability is also used to define the number of iteration When the 
average confidence probability of the prelabeled data declines the loop is ended It can 
be understood intuitively Through the experiments this criterion of ending the loop 
not only ensures prelabeling data adequately and efficiently but voids a mount of 
incorrect prelabeled data by exiting the loop timely 
24 
The Ensemble Method 
The ensemble classifiers can be arranged into two categories majority voting and 
weighted voting Majority voting methods predict the labels of data simply utilizing 
the principle of that the minority is subordinate to the majority They can achieve great 
performance when the accuracy of base classifiers are roughly equal6 However 
because the base classifiers in this paper come from different source domains and the 
similarities between these source domains and the target domain are different the 
accuracies of base classifiers are different and majority voting methods are ineffective  
 
 
An Ensemble Method Based on Confidence Probability 
217 
 
Weighted voting methods7 adjust the weights of training data with the error of each 
base classifier on T They are not applicable too As data distributions of target domain 
and source domain are different it is useless to calculate errors on source domains 
Furthermore because of the lack of labels in the target domain it is an impossible 
mission to calculate the error on target domain in advance Thus the CEC demonstrates 
it in a different path Summarize the classification confidence probabilities from all 
base classifiers to label the instances in target domain T The formula is shown as 
follows 
1
1
 | 
 | 
k
i
i
p y x
p y x
k
=
=

 
Here y corresponds to the label of instance x k is the number of base classifiers 

| 
ip y x denotes the confidence probability of base classifier ci labeling y to instance x 
The classification results are influenced by the probability distribution of labels 
through each base classifier This method predicts the data according to ensemble 
probability distribution rather than labels predicted by base classifier directly 
3 
Experimental Evaluation 
In order to determine how well the proposed algorithm performs we investigate per
formance over a variety of reviewplusrating English data sets1 and Chinese data sets2 
The English data sets include 11 domains such as camera camp and so on The Chi
nese data sets come from three domains including hotel electronics and stock  
The proposed framework is compared with two algorithms one combines the base 
classifiers directly and uses a simple majority vote of its component models for each 
new classification recorded as SECSimple Ensemble Classifier2 the other is the 
weighted ensemble method mentioned above recorded as CWEC1 For all of our 
experiments involving MaxEnt model we used the NLTK library which could be 
download at wwwsourceforgenet 
Parameter r represents the share of classification results accounted for by the con
vinced correct results Because of different data distributions base classifiers could not 
get high accuracy and it is not appropriate to set a high value for r Fig 1 illustrates the 
accuracy results as a function of r on the tv and stock data sets It suggests that we can 
get the best accuracy when r falls in 005 02 
In order to evaluate how the proposed ensemble algorithm performs on different 
feature spaces we utilized four feature selection criteria – OR1 MI8 DB29 and 
LLRTF Table 1 shows experimental results of the proposed CEC and the baselines 
CWEC1 SEC2 on 11 English datasets In the experiments we set r=02 and 
maximum number of loop m=10 Furthermore we also added some constraints to the 
classifiers to avoid the case of prelabeling data be unbalanced 
                                                           
1  The English data sets are available online httpwwwcsindianaedu˜mewhiteh 
htmlopinionmininghtml  
2  The Chinese data sets download address httpwwwsearchforumorgcntansongbosenticor 
pusjsp 
218 
Q Zhou Y Zhang and X Hu 
 
 
Fig 1 Accuracy curves of CEC for different parameter r 
Table 1 Accuracy comparison of different methods on English data sets  
Dataset 
OR 
MI 
DB2 
LLRTF 
SEC CWEC CEC SEC CWEC CEC SEC CWEC CEC SEC CWEC CEC 
camp 
51 
52 
55 
55 
55 
56 
55 
55 
56 
80 
81 
87 
camera 
53 
59 
63 
63 
63 
64 
63 
63 
64 
75 
76 
81 
doctor 
54 
58 
50 
50 
54 
50 
50 
54 
50 
82 
82 
86 
drug 
51 
51 
51 
51 
50 
52 
51 
50 
52 
60 
59 
66 
lawyer 
52 
52 
57 
57 
58 
57 
57 
58 
57 
80 
81 
86 
radio 
50 
50 
50 
50 
50 
54 
50 
50 
54 
70 
72 
76 
restaurant 51 
57 
58 
55 
58 
60 
63 
66 
66 
84 
83 
88 
music 
53 
56 
58 
55 
56 
59 
61 
65 
65 
69 
69 
72 
laptop 
53 
52 
55 
55 
55 
60 
52 
51 
54 
70 
68 
79 
tv 
52 
53 
63 
63 
63 
57 
61 
61 
58 
76 
77 
82 
movie 
50 
50 
50 
50 
50 
50 
50 
50 
50 
57 
56 
74 
 
Tab 1 reveals several observations 1 the proposed algorithm outperforms base
lines in all different feature spaces 2 Especially in LLRTF the classification accuracy 
of camera data set can achieve to 80 Compared to baselines the accuracy can be 
improved as much as 17 Meanwhile on all of data sets CEC can improve the ac
curacy by almost 7 and achieve to 80 on average 3 It is noticed that the method 
and the baselines give low performance on the drug data set The accuracy is low 
which is due to the similarities between drug domain and other domains are low  
4 Meanwhile we note that the proposed algorithm and baselines can not get pretty 
high accuracies in other three feature spaces including OR MI and DB2 This suggests 
that the method of feature selection is an important factor in influencing crossdomain 
classification accuracy 
 
An Ensemble Method Based on Confidence Probability 
219 
 
Table 1 Accuracy comparison of different methods on Chinese data sets  
Dataset 
OR 
MI 
DB2 
LLRTF 
SEC CWEC CEC SEC CWEC CEC SEC CWEC CEC SEC CWEC CEC 
hotel 
50 
50 
57 
51 
51 
54 
50 
51 
54 
73 
73 
75 
electronics 54 
46 
54 
50 
50 
50 
52 
50 
52 
67 
67 
77 
stock 
50 
60 
50 
57 
57 
55 
50 
54 
55 
68 
69 
82 
 
Tab2 presents the comparison results of different methods on three Chinese data
sets It is illustrated that CEC can get better performance than the baselines even 
though the source domain datasets is scarce 
4 
Conclusions and Future Work 
In this paper a novel ensemble method is proposed for transferring knowledge from 
multiple sources to the target domain in sentiment classification The basic idea is 
providing believable label for unlabeled data in the target domain and combining the 
base classifiers by the confidence probability to solve the problem of domain adapta
tion However the method trains base classifiers on all source domains without au
tomatically choosing the useful ones It makes the method spacetime consuming and 
lacking efficiency In the future we will extend relational theory to measure the simi
larity between domains to address this issue 
Acknowledgments This research is supported by the National Natural Science 
Foundation of China NSFC under grants 60975034 the Fundamental Research Funds 
for the Central Universities under grant 2011HGBZ1329 and 2011HGQC1013 
References 
1 
Whitehead M Yaeger L Building a General Purpose CrossDomain Sentiment Mining 
Model In Proceedings of the 2009 WRI World Congress on Computer Science and In
formation Engineering pp 472–476 2009 
2 
Li S Zong C Multidomain Sentiment Classification In Proceedings of ACL 2008 
HTL Short Papers Companion Volume pp 257–260 2008 
3 
Avrim B Tom M Combining Labeled and Unlabeled Data with CoTraining In Pro
ceeding of The Eleventh Annual Conference on Computational Learning Theory pp 
92–100 1998 
4 
Ng V Cardie C Weakly Supervised Natural Language Learning without Redundant 
Views In Proceedings of the 2003 Conference of the North American Chapter of the As
sociation for Computational Linguistics on Human Language Technology Edmonton 
Canada pp 94–101 2003 
5 
Clark S Curran J Osborne M Bootstrapping Pos Taggers Using Unlabelled Data In 
Proceedings of the Seventh Conference on Natural Language Learning at HLTNAACL 
2003 Edmonton Canada pp 49–55 2003 
220 
Q Zhou Y Zhang and X Hu 
 
6 
Dietterich TG Ensemble Methods in Machine Learning In Kittler J Roli F eds 
MCS 2000 LNCS vol 1857 pp 1–15 Springer Heidelberg 2000 
7 
Freund Y Schapire RE A Decision Theoretic Generalization of Online Learning and an 
Application to Boosting Journal of Computer and System Sciences 551 119–139 1997 
8 
Yang Y Pedersen J A Comparative Study on Feature Selection in Text Categorization 
In ICML pp 412–420 1997 
9 
Tan S Wang Y Cheng X An Efficient Feature Ranking Measure for Text Categoriza
tion In SAC 2008 Fortaleza Ceará Brazil pp 407–413 2008 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 221–229 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Robust ISOMAP Based on Neighbor Ranking Metric 
Chun Du Shilin Zhou Jixiang Sun and Jingjing Zhao 
School of Electronic Science and Engineering National University of Defense Technology 
Changsha Hunan PR China 
yxduchungmailcom 
Abstract ISOMAP is one of classical manifold learning methods that can 
discover the lowdimensional nonlinear structure automatically in a high
dimensional data space However it is very sensitive to the outlier which is a 
great disadvantage to its applications To solve the noisy manifold learning 
problem this paper proposes a robust ISOMAP based on neighbor ranking 
metric NRM Firstly NRM is applied to remove outliers partially then a two
step strategy is adopted to select suitable neighbors for each point to construct 
neighborhood graph The experimental results indicate that the method can 
effectively improve robustness in noisy manifold learning both on synthetic and 
realworld data  
Keywords ISOMAP noisy manifold learning neighbor  ranking metric 
1 
Introduction 
Recently there has been growing concern about manifold learning which is effective 
for nonlinear dimensionality reduction Some manifold learning algorithms such as 
isometric mapping ISOMAP 1 locally linear embedding LLE 2 and local 
tangent space alignment LTSA 3 have been successfully used in pattern 
recognition and data visualization Most existing manifold learning algorithms try to 
explore the lowdimensional nonlinear manifold structure of highdimensional data 
However they are not usually robust in presence of noisy data The noisy data lie 
outside the manifold outliers and can easily change the local linear structure of the 
manifold  
In order to resolve the noisy manifold learning problem several extended 
algorithms have been proposed For example Choi et al 4 presented robust kernel 
ISOMAP which could handling with critical outliers which result in shortcircuit 
edges to improve the topological stability But it may be not effective when the noise 
is uniform or gaussian distribution Chang and Yeung 5 proposed Robust locally 
linear embedding RLLE based on the robust PCA RLLE can reduce the undesirable 
effect of outliers largely Nevertheless the computational requirement of this method 
is significantly higher than LLE Moreover the difficulty of parameter selection is 
another impediment for its wide application 
In this paper we address the noisy manifold learning problem in the context of 
ISOMAP First of all a new and intuitive neighborhood similarity measure called 
222 
C Du et al 
neighbor ranking metric NRM is presented Secondly NRM is respectively used for 
removing outliers and selecting suitable neighbors of each point to construct 
neighborhood graph Lastly we apply above techniques to make ISOMAP robust  
2 
A Brief Review of ISOMAP 
ISOMAP works by assuming isometry of geodesic distances in the manifold It  can 
be considered as a special case of the classical multidimensional scalingMDS6 
where dissimilarities is measured by geodesic distance instead of Euclidean distance            
Given a m dimensional data set
1
  2


m N
N
X
x x
x
R
×
=
∈
 sampled from a d 
dimensional manifold M  d
 m
 the signal model relevant to noisy manifold 
learning is given as follows 
 
i
i
i
x
f τ
ε
=
+
 
1
where f  is mapping function

d
m
f
R
R
Ω ⊂
→
  Ω  is an open subset and 
iε  
denotes noise ISOMAP assumes that intrinsic dimension d  is known and proceeds 
in the following steps 
1 
Set Neighborhood Two simple methods are commonly used one is to select the 
k nearest neighbors k − NN the other is to choose all neighbors within a fixed 
radiusε − hypersphere  
2 
Estimate the geodesic distances Supposed 
ix  and 
jx  are a pair of samples the 
geodesic distances 
 

G
i
j
d
x x
 can be approximated by the shortest path 
distances between node 
ix′  and 
jx ′  corresponding to data point  
ix  and
jx  
in the graph G 
3 
Compute the lowdimensional coordinates One can use the classical MDS 
algorithm with the matrix of graph distances 

 

G
G
i
j
D
d
x x
=
 to map the data 
set into a lowdimensional embedded space 
 
Unlike the linear dimensionality reduction techniques such as Principal Component 
Analysis PCA ISOMAP can discover the nonlinear degrees of freedom that 
underlie complex natural observations However it still has some shortcomings 1 
ISOMAP is not robust in presence of outliers 2 the resulting output is extremely 
sensitive to parameters that affect the selection of neighbors  
3 
Robust ISOMAP Method 
31 
Neighbor Ranking Metric 
At the first step of the classical ISOMAP algorithms we often select the 
neighborhood of each point using k − NN method But in some cases the k  nearest 
neighbors based on Euclidean may not be that in the sense of manifold For example 
 
Robust ISOMAP Based on Neighbor Ranking Metric 
223 
as Figure 1 depicts the 4 nearest neighbors of data ‘H’ according to the ranking by 
Euclidean distance is G I F J and that of data ‘A’ is H I G J Obviously ‘A’ is 
an outlier near to dataset based on Euclidean distance but far from the manifold based 
on geodesic distance If ‘A’ is inserted into graph G as a node while ‘AH’ as an edge 
the structure of manifold will be changed So it is necessary to find a measure on how 
likely a data is a neighbor of another data 
 
A
G
I
J
B C D E F
H
L
N O
K
M
 
Fig 1 Schematic plot of suitable neighborhood selection 
The core idea of our method is inspired from the practical experiences that 
neighbors should be admitted each other Similar to the relationship between two 
individuals in real world person A considers B as his best friend but B may not think 
so Only if A and B all regard each other as his best friend they are real best friends 
Motivated by the ideas mentioned above we present a new neighborhood 
similarity measure called neighbor ranking metricNRM For each sample
ix  let 
1
2




i
N
x
i
i
i
U
x
x
x
=
 be the neighborhood set of 
ix  sorted by Euclidean distance in 
ascending order Then NRM is defined as follows 
 

i
j
ij
ji
NRM x x
O
O
=
−
 
2
where || is the absolute value operator
ij
O  denotes the order  where 
jx lies 
in
U ix

ji
O  denotes the order  where 
ix lies in
Uxj
 It can be clearly seen from 
equation 2 that if 
ix  and 
jx  have small NRM value the possibility of 
jx to be a 
neighbor of 
ix  is very high So let η be the threshold to judge the value of NRM 
 

i
NRM x xj
η
 indicates that 
jx
 belongs to the neighborhood of 
ix  and  
vice versa 
32 
Outlier Removing 
NRM can be easily applied to outlier removing which is important to noisy manifold 
learning Supposed 
ix  belongs to the k nearest neighborhood of some other samples 
whether 
ix is an outlier or not largely depends on the number of samples who 
consider 
ix as their k nearest neighbor Specifically we can detect outliers by the 
criterion 
ixS
 T
 where
ixS is the number of neighbors belongs to
ix and T is the 
threshold  In this paper we set 
T = 3
 by experience and remove the samples whose 
neighborhood size is less than 3 
224 
C Du et al 
33 
Neighborhood Selection 
After removing outliers neighborhood selection is needed The 

k NN method is 
widely used because it can guarantee the sparseness of resulting structures7 
However this method is not without its shortcomings To begin with the choice of an 
appropriate k is difficult If k is too small the local neighborhoods may estimate the 
correlation between the points and their neighbors falsely even divided the manifold 
into several disconnected submanifolds In contrast too large k will make the local 
neighborhoods include points in different patches of data manifold and result in short 
circuits Furthermore the 

k NN method sets parameter the same value for all points 
This strategy is not reasonable because local structures of each point are usually 
different In general the selection of neighborhood size should be datadriven and 
depend on such factors as density curvature and noise 
As indicated in some previous works89 there are several requisitions for 
neighborhood selection 1 the selected neighbors for each sample should reflect the 
local geometric structure of the manifold 2 the number of sample in each 
neighborhood should be large enough to make sure the neighborhood graph is 
connected 
In this paper we mainly focus on neighborhood selection in presence of noisy data 
and further extend the k − NN method based on NRM The basic idea of our method 
is to expand each neighborhood while removing its nonneighbor elements The detail 
of our method is described as follows 
 
 Input data set X  the initial parameter k  the tolerable thresholdη  
 Output the neighborhood size 
ixk  the neighborhood 
ix
N  
Step1 compute Euclidean distance matrix 
X
D between any pair wise points then 
determine the initial k − NN neighborhood 
1
2




i
k
x
i
i
i
N
x x
x
=
  
Step2 expand neighborhood
N ix
 For each element of
N ix
 obtain its k nearest 
neighborhood
N ixl

12
l
k
=
 then  expand 
l
i
i
i
x
x
x
N
N
N
=
∪
 and delete the 
repetitive elements At last sort the updated 
ix
N  by Euclidean distance to 
ix in 
ascending order and set


i
i
x
x
k
= S N
 where 

S N ix 
denotes the number of the 
elements in
ix
N  
Step3 remove nonneighbor elements of
N ix
 For each element
l
ix 
12
ix
l
k
=
 
compute the 
 

l
i
NRM x xi
 using Equation 2 then judge whether
l
ix is a suitable 
neighbor or not If 
 

l
i
NRM x xi
η
and min


il
O Oli
 k
 retain
l
ix in
ix
N  otherwise 
consider it as a nonneighbor and remove it from
ix
N  then let
=
1
i
i
x
x
k
k
 
Step4 For each 
ix
∈ X
that has not been processed repeat step2 to step3  
Obviously the method described above is a twostep strategy In order to construct 
a connected neighborhood graph the method firstly expands initial k − NN 
 
Robust ISOMAP Based on Neighbor Ranking Metric 
225 
neighborhood by merging the set of 
ix
N and
N ixl
 The expansion process is easy to 
implement and can make 
ix
N comprise candidate neighbors as large as possible 
However not all elements in expanding
ix
N are suitable neighbors in the sense of 
manifold So in the second step of our method NRM is applied to detect and delete 
these nonneighbor elements The remaining elements would be considered as the 
suitable neighbors of 
ix  in neighborhood graph G  
34 
Robust ISOMAP Algorithms 
Based on the discussion mentioned above the outline of the proposed algorithm can 
be concluded as follows 
 
Input data set X  the initial parameter k  the tolerable thresholdη   
Output the lowdimensional embedding result Y  
Step1 Identify and remove the outliers from the original data X  by performing our 
outlier removing method in section 32 
Step2 Construct the neighborhood graph G based on our neighborhood selection 
method in section 33 
Step3 Carry out the classical ISOMAP algorithm to G and build the low
dimensional embedding Y that best preserves the manifold’s intrinsic structure 
4 
Experiment 
Two examples are presented to illustrate the performance of the robust ISOMAP 
method The test data sets include Swiss roll and USCSIPI dataset In order to 
evaluate the embedding result we use the residual variance which is defined as 
below 
2

1



X
Y
Residual
variance
D
D
= − ρ
 
3
where 
X
D  and 
Y
D  are two vectors which express the geodesic distances in the high
dimensional data space and Euclidean distances in the lowdimensional embedded 
space respectively ρ is the standard linear correlation coefficient The lower residual 
variance is the better the performance of the manifold learning method is 
41 
Experiments on Swiss Roll Data 
At first the robust ISOMAP method and classical ISOMAP method are applied to Swiss 
roll data without noises 2000 clean points are randomly sampled from the Swiss roll 
manifold The classical ISOMAP method uses k − NN method to select neighborhood 
and k is assigned within the range from 4 to 30 For the robust ISOMAP method we set 
the parameter 
η = k
 by experience The embedding results with 
k =10
are shown in 
Fig 2a and Fig 2b  The change of the residual variances with different k  is shown 
in Fig3 From Fig 2 and Fig 3 we can see that both methods achieve almost the same 
226 
C Du et al 
embedding results with 
k ∈715
 More importantly the robust ISOMAP method can 
attain lower residual variances than classical ISOMAP method with the increase of k  
That is to say the resulting output of the robust ISOMAP is not sensitive to parameter k  
and the robust ISOMAP method seems superior to classical ISOMAP method when the 
original Swiss roll data is clean 
 
a 
b 
Fig 2 Embedding results on 2000 clean Swiss roll data a by applying the classical ISOMAP 
algorithm b by applying the robust ISOMAP 
 
5
10
15
20
25
30
01
005
0
005
01
015
02
025
03
035
04
 
 
Classical ISOMAP
Robust ISOMAP
 
Fig 3 The change of the residual variances with different k  
Furthermore we test robust ISOMAP and classical ISOMAP on the noisy Swiss 
roll data As shown in Fig 4a 2000 clean points are randomly sampled from the 
Swiss roll manifold and 200 uniform distributed outliers are added as noises marked 
as ‘’ Shown in Fig4c is the outlier removing results We can find that the vast 
majority of clean points are preserved while almost half of the labeled outliers are 
removed The embedding result of classical ISOMAP is shown in Fig 4b As can be 
seen there is a strong distortion between the embedding result in presence of outliers 
and that in absence of outliers see Fig 2a The embedding result of robust 
ISOMAP is shown in Fig 4d Comparing Fig 4b with Fig 4d we find that the 
robust ISOMAP method is more topologically stable than the classical ISOMAP  
 
 
Robust ISOMAP Based on Neighbor Ranking Metric 
227 
 
 
 
a 
b 
c 
d 
Fig 4 a Noisy Swiss roll data 200 outliers b  Embedding results by classical ISOMAP 
c The result of outlier removing d Embedding results by robust ISOMAP 
 
 
Fig 5 The change of the residual variances with different k  
 
 
a 
b
Fig 6 Embedding results on wood texture data a by applying the classical ISOMAP 
algorithm b by applying the robust ISOMAP 
 
 
228 
C Du et al 
since it adopt an effective strategy to suppress the effect of noises and select suitable 
neighborhood for each point  Fig5 shown the change of the residual variances with 
different k Obviously the residual variances of robust ISOMAP method are far less 
than that of classical ISOMAP for every
k ∈430
 Thus it can be concluded that the 
robust ISOMAP can perform better than classical ISOMAP in presence of outliers 
42 
Experiments on USCSIPI Dataset 
To illustrate the effectiveness of robust ISOMAP on real data we perform 
experiments on wood texture data from the USCSIPI image database1 The wood 
texture data consists of rotated texture images of seven different rotation angles  The 
size of each image is 512 512
×
 We select three rotated texture images 

o0 
o
60  
o
90  and divide each original images into 400 partially overlapping blocks 
of size 32 32
×
 Thus our wood experimental dataset consists of 1200 images each of 
1024 dimensions Then we randomly select 120 images 40 images for each rotated 
angle and add ‘salt and pepper’ noise to each image where the noise density is 005 
Lastly the classical ISOMAP and robust ISOMAP are applied to the wood texture 
data set with noisy images added The embedding results are shown in Fig 6 It is 
obviously that the robust ISOMAP can improve the performance of classical 
ISOMAP on noisy data set With removing outliers partially and selecting suitable 
neighborhood for each data point the robust ISOMAP can restrain noise effect and 
preserve the separation between clusters well 
5 
Conclusion 
This paper investigates the noisy manifold learning problem and proposes a robust 
version of ISOMAP method The proposed method presents a new neighborhood 
similarity measure called NRM and makes ISOMAP more robust from two aspects 
1 removing outliers to suppress the effect of noises 2 selecting suitable 
neighborhood for each point to preserve topological stability Experiments on several 
data sets such as Swiss roll data and wood texture data verified the robustness of our 
proposed method However our method still has its shortcomings For example how 
to determinate parameters of the method is still a complicated work In the future 
work we will improve our method and extend it to other manifold learning methods 
References 
1 
Tenenbaum JB de Silva V Langford JC A Global Geometric Framework for 
Nonlinear Dimensionality Reduction Science 290 2319–2323 2000 
2 
Roweis ST Saul LK Nonlinear Dimensionality Reduction by Locally Linear 
Embedding Science 290 2323–2326 2000 
3 
Zhang Z Zha H Principal Manifolds and Nonlinear Dimension Reduction via Local 
Tangent Space Alignment SIAM J Scientific Computing 26 313–338 2005 
4 
Choi H Choi S Robust kernel Isomap Pattern Recognition 40 853–862 2007 
                                                           
1  httpsipiusceduservicesdatabase 
 
Robust ISOMAP Based on Neighbor Ranking Metric 
229 
5 
Chang H Yeung DY Robust locally Linear Embedding Pattern Recognition 39 1053–
1065 2006 
6 
Cox T Cox M Multidimensional Scaling Chapman and Hall Boca Raton 1994 
7 
Mekuz N Tsotsos JK Parameterless Isomap with Adaptive Neighborhood Selection 
In Franke K Müller KR Nickolay B Schäfer R eds DAGM 2006 LNCS 
vol 4174 pp 364–373 Springer Heidelberg 2006 
8 
Zhan Y Yin J Liu X Zhang G Adaptive Neighborhood Select Based on Local 
Linearity for Nonlinear Dimensionality Reduction In Cai Z Li Z Kang Z Liu Y 
eds ISICA 2009 LNCS vol 5821 pp 337–348 Springer Heidelberg 2009 
9 
Zhang Z Wang J Zha H Adaptive Manifold Learning IEEE Trans Pattern Anal 
Mach Intell 34 253–265 2012 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 230–235 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Modeling by Combining Dimension Reduction  
and L2Boosting 
Junlong Zhao 
School of Mathematics and System Science Beihang University LMIB of the Ministry  
of Education Beijing China 
zjlczh126com 
Abstract Dimension reduction techniques are widely used in high dimensional 
modeling The two stage approach first making dimension reduction and then 
applying existing regression or classification method is commonly used in 
practice However an important issue is that when two stage approach can lead 
to consistent estimate In this paper we focus on L2boosting and discuss the 
consistency of the two stage methoddimension reduction based L2boosting 
briey DRL2B We establish the conditions under which DRL2B method 
results in consistent estimate This theoretical finding provides some useful 
guideline for practical application In addition we propose an iterative DRL2B 
approach and make some simulation study Simulation results shows that 
iterative DRL2B method has good performance 
Keywords dimension reduction L2boosting consistency 
1 
Introduction 
Dimension reduction methods have been widely used in high dimensional modeling 
to extract the main information and to reduce the noise in the data Suppose that 
×1
∈
R p
X
 is pdimensional vector The aim of the many dimension reduction methods 
is to find the low dimension linear combination 
X
B
T
 where 
R p k
B
×
∈
 with 
k  p
 
There are many dimension reduction methods have been developed For example 
the Principal Component Analysis PCA Linear Discriminant Analysis LDA and 
Independent Component Analysis ICA are the most widely used methodsJolliffe 
2002 In regression setting the wellknown methods include the MAVE Xia et al 
2002 SIR Li 1991 SAVE Cook and Weisberg 1991 KDRFukumizu et al  
2009 etc 
The two stage approach applies dimension reduction first and then builds the 
model  by the existing regression or classification methods developed in low 
dimensional setting Two stage method is widely used in the different fields such as 
image processing and face recognitionZhao et al 2003 However for two stage 
method the error generated in estimating B will inevitably has impact on the 
following regression or classification procedure Therefore an important issue is that 
whether such impact is negligible or equivalently wether two stage method can lead 
 
Modeling by Combining Dimension Reduction and L2Boosting 
231 
to consistent estimate L2Boosting Bühlmann and Yu 2003 is an effective method 
for regression and classification and has been widely used in different settings Many 
authors had studied the theoretical and numerical property of L2Boosting Zhang and 
Yu 2005 Bühlmann and Horthorn 2007 Chang et al 2010 In this paper we focus 
on two stage method dimension reduction based L2Boosting method briefly DRL2B 
and obtain the conditions that guarantee the consistency of the DRL2B approach This 
condition suggests that CART as the most widely used base learner in boosting 
approach may lead to inconsistent estimate in DRL2B method This provides some 
useful guideline in the application 
The main contents of this paper are arranged as follows The DRL2B algorithm is 
reviewed in section 21 and its consistency is established in section 22 and the 
iterative DRL2B algorithm is proposed in section 23 Simulation results in section 3 
show that iterative DRL2B and DRL2B method have good performance 
2 
Dimension Reduction Based L2Boosting and Its Extension 
21 
Algorithm of Dimension Reduction Based L2Boosting 
Let S denote the set of the real value functions and define spanS = 
=
m
k
1ω k f k
 
k ∈ R
ω
+
∈
∈
Z
S m
f
k
 
 
being the linear function space spanned by S where 
+
Z  is the 
set of positive integer For any 
f ∈spanS
 define the 1norm as 
|| f ||1
= inf
|| ω 1||
 
=  =
m
i
k f k
f
1ω
+
∈
∈
Z
S m
f
k

 In high dimensional setting dimension reduction methods 
usually assume the true model has the low dimensional structure 
 

f B X ε
Y
T
=
 Li 
1991 where 
Rp k
B
×
∈
 and 
k
T
I
B B
=
and k is usually much small  Many dimension 
reduction method has been developed to estimate the subspace spanB such as 
LDA OLS SIR Li 1991 MAVE Xia et al 2002 etc 
Let Af =
2


f 2
E Y
−
 the 
2
L  loss of the function f Define the base function 
space 
B
S  = g
x
B
T
  g being some kinds of base learner Throughout the paper we 
further require that SB satisfies that for any k ×  k orthogonal matrix H  
SB = SBH 
1
that is for any 
B
T
S
g B x
∈

 we have 
B
T
S
x
g HB
∈


 Therefore it suffices to find 
one of the orthonormal base of spanB Suppose that 
n
i
Y
X
i
i

1




=
  are iid 
observations of X Y  Let 
n
B  be the estimate of B with 
k
n
T
n
I
B B
=
 
Define
  2


  2 




2
1
2
X
f
Y
E
X
f
Y
f
A
n
n
i
i
i
n
−
=
−
=  =
 
and 

 
g B x
S
T
n
Bn =
  
being some kinds of base learner Similar to greed boosting algorithm Zhang and 
Yu2005 the major steps of DRL2B algorithm is as follows  
1  Obtain the estimate Bn of some orthnormal basis B by dimension reduction 
methods 
2  Select 
f ∈ SBn
0
  
232 
J Zhao 
3  For 

2 1 0
k =
 select a closed subset 
Λk ⊂ R
 such that 
0∈Λk
 and 
k
k
Λ = −Λ
  
4 Find
k
k
αˆ ∈Λ
 and 
Bn
T
n
k
S
B x
g
∈
ˆ 
 which approximately minimize the loss function 
k
k
k
k
n
S
g
k
k
k
n
g
f
A
g
f
A
Bn
k
k k
ε
α
α
α
+
+
≤
+
∈
∈Λ

 ˆ
inf
ˆ ˆ 
 ˆ

where 
kε is a series of nonnegative real 
number towards 0 
5  Let 
k
k
k
k
g
f
f
ˆ ˆ
ˆ
ˆ
1
+α
+ =
  
22 
Consistency of DRL2B 
Our goal is to show the statistical asymptotic convergence that is as 
n → ∞
 ˆ 
An fk
inf  
A f
→
f ∈ SB
 in probability where

 

→ ∞
=
 k n
k 
 as
n → ∞
 Many 
dimension reduction such as SIR SAVE MAVE etc can obtain 
Bn → B
in 
probability 
as 
n → ∞
 
Without 
loss 
of 
generality 
we 
assume 
that 
1

 
sup
2 ≤
g X
g E
 Let 
kh  and
ks  satisfy 

−
=
+
=
∈ Λ
≤
1
0
0 ||1
||

|
ˆ
|
k
i
i
k
k
k
k
h
f
s
h
α
and 
suppose that 1 holds Furhtermore we make the following assumptions 
C1 Lipschiz condition There exists positive constant
L  0
 such that 
F
x
x
L
g x
g x
||
||
|



|
2
1
2
1
−
≤
−
   for any 
g ∈ S
 and any 
x1 x2
 
C2   There exists a basis B of 
 
span B  and 
s  0
 such that 


s
p
n
O n
B
B
−
=
−
 
C3  


1
 

  1
s
k n
o n
s
k n
=
+
+
as 

 
→ ∞
=
k n
k
 
C4  The covering numbers 
 

Ν SB ε
 of the space 

span
SB
 is finite 
C5 
0
|


|
M
Y
f X

−
 for all 
f ∈ SB
 for some 
0
M  almost surely 
C6  Assumption 31 of Zhang and Yu2005 holds 
 
Assumption 
 ∞
Ν
 

SB ε
 holds in many situations for example 

span
SB
 is finite 
dimensional function space Sobolev spaces or spaces associated to a kernelCucker 
and Smale 2001 Condition C5 holds as  Y  and  

∈


span
sup
SB
 
f  f
 are finite 
almost surely The following theorem  presents the statistical asymptotic convergence 
of the 
B
DR
2L
 algorithm 
Theorem 1 Under C1C6 as 
n → ∞
we have 
 
inf

 ˆ

pan
 
A f
f
A
SB
s
f
p
k n
n
⎯→ ∈
⎯
 
23 
Further Refinement Itrative DRL2B 
Borrowing the idea of iteration method such as the PLS which iteratively apply the 
dimension reduction and the linear regression we propose the following corrected 
algorithm called iterative DRL2B Taking the case of fixing step length as an 
example that is 
αk = r
 for some r we describe the iterative DRL2B  algorithm as 
follows 
1 
Initialize  
0ˆf  denoted as
f = Y
0ˆ
 Let k = 0 
2 
Increase k by 1 and compute the residuals 
 
21


1
 
n
i
X
f
Y
U
i
k
i
k
i

=
−
=
−
 
 
Modeling by Combining Dimension Reduction and L2Boosting 
233 
3 
Apply dimension reduction method to
U X
k
  to obtain the estimate 

k
Bn
 Let         
 
 
X
V
k
n
k
B
=
 
4 
Fit 
Uk
 by 
k
V  using the base learner to obtain the estimate 
 
gˆ k ⋅  
5 
Update 
k
k 1
ˆ
ˆ
ˆ
rg
f
fk
+
=
 where 
1
0
 r ≤
 is the step length 
6 
Repeat step 2–5 until 
k = mstop
 
3 
Simulations and Real Data Analysis 
31 
Simulations 
In this section we conduct simulations to compare DRL2B iterative DRL2B and the 
componentwise L2Boosting Bülhmann and Yu 2003 briefly denoted as CL2B 
Define the testing error as 
|
|

 ˆ
2
testset
Y
Y
error
−
=
 Take at random 80 data as 
training set and the rest as testing set We select optimal stopping number mopt by 
fivefold CV method based on the training data and compute the corresponding 
prediction error on the test data Repeat the procedure for 100 times and compute the 
mean and standard deviation of prediction error under the corresponding mopt 
denoted respectively as MEAN and STD in the following table 1 of this section Let 
Mopt  denote the mean of mopt over 100 replicas  We consider three setting of the 
values of n and p Case I n = 200 p = 10 Case II  n = 100 p = 50 Case III n = 100 
p = 100 As generally suggested in the literature the small value of r generally 
leading to better results Friedman 2001 is preferred especially for large p So we 
take take r = 01 for three cases   
For Case II and III in both DRL2B and iterative DRL2B partial SIR method Li 
et al 2007 which is computationally simple is used to estimate the index in Case II 
and III For case I of small p but large n we take MAVE to infer the index for DRL2B 
and SIR for iterative DRL2B In the following models we always assume that ε  
N01  Let 
p
T
R
p
b
∈
=
−
1 2 1  1
1
 
p T
b
 
21
2 =
 
p
T
R
b
∈
=
 1
11
3
 Consider 
the following nonlinear models 
 
• Model 1 
2
1
2
2

1
2sin
 ||
||
T
T
Y
b X
b X
b
ε
=
+
+
+  where X ∼ N0 Ip 
• Model 2  
+ε
+
=
2
3
3
2
2
||
 ||
||1
 ||
2sign
b
b X
b
b X
Y
T
T
  where XN0 Ip4Σ where Σ is 
the matrix with the i j elements 
|
|30
j
i
ij
−
σ =
  
Here 



B = b1 b2
 and 
 

B= b2 b3
 for model 2 and 3 respectively And the dimB=2 
We compare simulation results of four methods CL2B with spline base learner 
denoted as SplL2 DRL2B with dimB=2 that is we use the true dimension of B and 
thinplate spline base learner denoted as TstpDR iterative DRL2B method with  
dim
 
B k
=1 and spline base learner denoted as ItsplDR iterative DRL2B with  
dim
 
B k
=2 and thinplate spline base learner denoted as IttpDR  
Comparison of CL2B with DRL2B and iterative DRL2B From Table 1 it follows 
that TstpDR ItsplDR and IttpDR are much much better than splL2 in all three cases 
234 
J Zhao 
Comparison between DRL2B with iterative DRL2B From Table 1 three methods Ts
tpDR ItsplDR and IttpDR have the nearly the same prediction performance for Case 
I where p is small but n is large and they also have the similar performance for Case 
II where p is moderate p = n2 The performances of TstpDR in case I and II 
coincide with our theoretical finding in section 2 And for Case III where p is large p 
= n the performance ItsplDR and IttpDR are better than TstpDR Therefore for 
large p but small or moderate n iterative DRL2B is a good choice 
32 
Real Data Analysis 
We also compare these methods on two real data sets Ozone data and Boston 
Housing data Simulation results confirm  the better performance  of DRL2B and 
iterative DRL2B than SplL2  Simulations are presented in table 1 
Table 1 Simulation results for model 1 and 2 and real data 
 
              
 
SplL2 
ItsplDR   TstpDR  
ItpDR 
 
 
 
 
CASE  I 
MEAN 
3542 
1617 
1739 
1541 
STD 
1195 
0428 
0492 
0431 
Mopt 
202600 
76800 
22650 
38700 
 
model 1 
 
CASE II 
 
MEAN 
12034 
9359 
9450 
8901 
STD 
5175 
5886 
6450 
5966 
Mopt 
241550 
86300 
42600 
28350 
 
 
 
 
CASE III 
 
MEAN 
15564 
11748 
11871 
10542 
STD 
5206 
4595 
5519 
4131 
Mopt 
134600 
148100 
40650 
18650 
 
 
 
 
 
CASE  I 
 
MEAN 
10612 
3685 
2959 
3077 
STD 
3175 
1061 
1092 
0805 
Mopt 
179200 
115800 
36600 
45500 
 
model 2 
 
CASE II 
MEAN 
18867 
13973 
14327 
13129 
STD 
6165 
5358 
5395 
5512 
Mopt 
22700 
124200 
18800 
47100 
 
 
CASE III 
MEAN 
19044 
12615 
12436 
10735 
STD 
5536 
4411 
4474 
3597 
Mopt 
220300 
130 
57900 
72600 
Ozone data  
MEAN 
20134 
18837 
17077 
16864 
 
STD 
2521 
3326 
2252 
2433 
 
Mopt 
103450 
19700 
18450 
43100 
Boston 
Housing 
data 
 
MEAN 
19889 
17798 
17355 
14478 
 
STD 
4093 
4398 
4160 
3573 
 
Mopt 
133600 
119250 
23100 
149700 
 
Modeling by Combining Dimension Reduction and L2Boosting 
235 
Ozone data Breiman and Friedman 1985  This data contains n = 330 
observations and the number of the covariates p = 8  In each simulation 220 
observations are selected at random as training data and rest observations as testing 
data 
Boston Housing data  This data is about the price of the house of the suburb of 
Boston Cook and Weisberg 1994 including n = 506 observations each observation 
has p = 13 covariates Select at random  3n5 the integer part of 3n5 observations 
as training data and rest as testing data 
4 
Conclusions 
In this paper we prove the consistency of the dimension reduction based L2Boosting 
a simple  two stage method Moreover  we propose a refined algorithmIterative 
DRL2B Simulation results and real data analysis confirm the effective ness of 
DRL2B  
Acknowledgements  This work is supported by National Natural Science Foundation 
of China No 11026049 and No11101022 and Foundation of the Ministry of 
Education of China for Youths No 10YJC910013 
References 
1 Breiman L Friedman JH Estimating Optimal Transformations for Multiple Regression 
and Correlation J Amer Statist Assoc 80 580–598 1985 
2 Bühlmann P Hothorn T Boosting Algorithms Regularization Prediction and Model 
Fitting Statist Sci 224 477–505 2007 
3 Bühlmann P Yu B Boosting with the L2loss Regression and Classification J Amer 
Statist Assoc 98 324–339 2003 
4 Cook RD Weisberg S Sliced Inverse Regression for Dimension Reduction Comment 
J Amer Statist Assoc 86414 328–332 1991 
5 Cook RD Weisberg S An Introduction to Regression Graphics Wiley New York 
1994 
6 Chang Y Huang Y Huang Y Early Stopping in L2 Boosting Comp Stat  Data 
Anal 5411 2203–2213 2010 
7 Fukumizu K Bach FR Jordan MI Kernel Dimension Reduction in Regression Ann 
Statist 371 1871–1905 2009 
8 Friedman J Greedy Function Approximation A Gradient Boosting machine Ann 
Statist 29 1189–1232 2001 
9 Li KC Sliced Inverse Regression for Dimension Reduction with discussion J Amer 
Statist Assoc 86 316–327 1991 
10 Li L Cook RD Tsai CL Partial Inverse Regression Method Biometrika 94 615–625 
2007 
11 Jolliffe IT Principal Component Analysis 2nd edn Springer New York 2002 
12 Xia Y Tong H Li WK Zhu LX An Adaptive Estimation of Dimension Reduction 
Space J Roy Statist Soc Ser B 64 363–410 2002 
13 Zhang T Yu B Boosting with Early Stopping Convergence and Consistency Ann 
Statist 33 1538–1579 2005 
14 Zhao W Chellappa R Phillips PJ Rosenfeld A Face Recognition A Literature 
Survey ACM Computing Surveys 354 399–459 2003 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 236–243 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Geometric Linear Regression and Geometric Relation 
Kaijun Wang1 and Liying Yang2 
1 School of Mathematics  Computer Fujian Normal University Fuzhou 350108 PR China 
wkjwanggmailcom 
2 School of Computer Science and Technology Xidian University Xian 710071 PR China 
Abstract When a linear regression model is constructed by statistical calcula
tion all the data are treated without order even if they are order data We  
propose the Geometric regression and geometric relation method GR2 to util
ize the relation information inside the order of data The GR2 transforms the 
order data of each variable to a curve or geometric relation and uses the 
curves to establish a geometric regression model The prediction method using 
this geometric regression model is developed to give predictions Experimental 
results on simulated and real datasets show that the GR2 method is effective 
and has lower prediction errors than traditional linear regression 
Keywords relations between data geometric regression geometric relation 
1 
Introduction 
The regression analysis technique 14 is widely used to analyze dependency rela
tionship between a dependent or response variable and a set of independent or pre
dictor variables in multivariate data analysis and the regression model learned from 
training data may be used for a prediction task Let there be one dependent variable Y 
and q independent variables X1 X2 … Xq the linear regression model about these 
variables is written as 4 
0
1
1

q
q
Y
b
b X
b X
ε
=
+
+
+
+
 where bj j=01…q are 
regression coefficients random error ε is assumed to be normal distribution 
2
0

N
σ
 and
0
1
1



q
q
f X
b
b X
b X
=
+
+
+
 is the regression function Usually the 
coefficients bj can be found by the least squares estimates from given data 4 
When a regression model is constructed with data all the data are treated without 
order by statistical calculation even if the data are time series However there might 
be relation information inside the order of data and this information may be helpful to 
establish a regression model with better predictive ability for some applications 
To utilize the relation information inside data order we propose Geometric regres
sion and geometric relation method GR2 GR2 transforms the order data of each 
variable to a curve and then GR2 uses the curves instead of original data to establish 
a geometric regression model For a prediction task when some new order data of 
each independent variable are available GR2 makes a curve with the new data for 
each independent variable and these curves are sent to the geometric regression mod
el to produce batch prediction values of dependent variable Y 
 
Geometric Linear Regression and Geometric Relation 
237 
The GR2 method is proposed in Section 2 and the experimental results are in Sec
tion 3 Finally Section 4 is the discussion and Section 5 gives the conclusion 
2 
Geometric Regression and Geometric Relation 
We first discuss the projection of the data of each variable to a 2dimensional curve 
manifold according to relation T The geometric regression is designed to solve a 
regression model F that fits curves Finally the prediction by model F is discussed 
21 
Making a Curve with Order Data 
Let 1dimensional continuous dependent variable Y have n data yi in order y1 y2 
y3… yn Typically order relation T is the time order eg T=t1 t2 … tn where 
the time t1t2…tn when the n data are time series The order relation is trans
formed to geometric relation when T is represented by a geometric curve  
It is ideal to make a curve manifold one 2dimensional smooth curve to represent 
the n data and their order relations Here we approximate the ideal curve manifold via 
piecewise curves that fit local data This work includes the data division the making 
of a piecewise curve fitting a data group and the connection of piecewise curves The 
following is a simple introduction more detail refers to 5 
The work will be discussed within the framework of manifold theory 6 Let n da
ta be divided equally into k groups along T then there is m=nk data for each data 
group called local data eg the first data group is y1 y2 … ym and the second 
ym+1 ym+2 … ym+m Let there be a curve manifold M in Cartesian coordinate system 
tOy and local region Uj of M correspond to the jth data group The quadratic poly
nomial is selected on account of easy computation and then the fitting curve 
2
y
at
bt
c
=
+
+
 may be found by leastsquares fit 7 
Every piecewise curve is made under local Cartesian coordinate system uOy 
where coordinates u represents local T In detail a piece of quadratic polynomial 
curve Cj y=ƒju u∈1m is made under uOy with the jth group of data  m pairs 
of uiyi ie the jth data group is projected to a 2dimensional local curve 

1
1
1
P   

 
j
j
j
i
i
j
m
j
m m
y
C
y
f u
=
−
+
−
+ →
=
 
1
Then geometric properties of Ci are taken as geometric properties of corresponding 
local region Ui of M Thus all the k local regions corresponding to k data groups are 
ready A 2dimensional curve manifold is constructed when k local regions are joined 
along T ie the n data are projected to M in plane tOy 
P    1

j
i
i
n
y
M M
U
=
→
= 
 
2
For a better approximation to a smooth curve manifold we take the central 13 part of 
each local curve to form local region Ui and use a same length for all local regions It 
is similar to construct a curve manifold for each variable Xi by using above method 
238 
K Wang and L Yang 
22 
Geometric Regression Method 
A geometric regression method using integral calculation will be designed to fit a 
regression model to curve manifolds Without losing generality let there be linear 
regression equation
0
y
x
φ
φ
=
+
between variables X and Y and let the same local 
coordinate system uOx be used for local regions of curve manifold
X
M and the same 
uOy for
Y
M  Then any point on
X
M is ux and any point on
Y
M is uy The least
squaresfit method 7 added with the integral computation of curves for establishing 
a regression equation between curve manifolds is called geometric regression method  
Theorem 1 Geometric regression Let
0
y
x
φ
φ
=
+
be the linear regression equation 
between variables X and Y their curve manifolds are
X
M and
Y
M respectively Let 
every curve manifold comprise k local regions and the local region of 
X
M  be 
lg  
x
u
=
and the one of
Y
M be
l 
y
= f u
 where
1
  2

u
∈ u u
and l=1k The means of x 
values of 
X
M and the means of y values of 
Y
M are set as follows 
  
2
1
2
1
1
1
1
 
l
k
l
u
u g u
g
u
u
k
du
=
−
=

  
2
1
2
1
1
1
1
 
l
k
l
u
u
f u
f
u
u
k
du
=
−
=

 
The coefficients are found by the leastsquares fit method with
X
M and
Y
M  
fg
gg
L
L
φ =
 
3
0
f
g
φ
φ
=
−
 
4
where 
2
1
2
1
1
 
 


l
l
k
fg
l
u
u
u g
u
f
f g
du
L
k u
u
=
=
−
−

 
5
2
1
2
2
2
1
1
 


l
k
gg
l
u
u g u
g
du
L
k u
u
=
=
−
−

 
6
2
1
2
2
2
1
1
 


l
k
ff
l
u
u f u
f
du
L
k u
u
=
=
−
−

 
7
23 
Prediction by Geometric Regression Model 
Regression model F is first constructed with the geometric regression method from a 
training dataset and then model F is used for predictions The prediction performance 
of F is usually tested with a test dataset The GR2 procedure is in the following 
1 Geometric treatment of training data 
Sort training data by order relation T and make one curve manifold with training 
data under relation T for each variable eg Y and X by the method in subsection 21 
 
Geometric Linear Regression and Geometric Relation 
239 
2 Constructing geometric regression model F with curve manifolds 
Construct geometric regression model F eg 
0
y
x
φ
φ
=
+
 with curve manifolds 
by the method in subsection 22 
3 Geometric treatment of new observed data for independent variables 
When some new data in order T of each independent variable eg X are availa
ble eg x1 x2 … xp make a curve manifold with these new data under order 
relation T for each independent variable 
If there are fewer new data the new data and training data are combined under or
der relation T and then the combined data of each independent variable are trans
formed to a curve manifold eg 
X
M  under T 
Cut every curve manifold into parts each part with a unit length of T along coor
dinates t and then take the starting points of each unit part as projection points eg 
point ux on 
X
M  and the projection points corresponding to the new data are kept 
eg x1 x2 … xp but ui is useless and discarded 
4 Prediction by geometric regression model F 
The kept data eg x1 x2 … xp from step 3 are as input of model F to pro
duce prediction values in batch of dependent variable Y eg y1 y2 … yp 
Note when the geometric regression process is regarded as a projection ux → 
uy the projection u → u is an invariable projection so the u is meaningless and 
discarded for a prediction task 
24 
Parameters of Curve Manifolds 
Different parameter m will lead to different curve manifolds and different regression 
models  The optimal m or mo may be obtained while we find the geometric regres
sion model 
0
y
x
φ
φ
=
+
 by minimizing residual sums of squares 
 

RSS
φ m
 
2
 

2
f f
fg
gg
RSS
m
L
L
L
φ
φ
φ
=
−
+
 
8
Generally m=5 to 12 for n ≥42 is the proper searching scope of
o
m according to our 
experiences m=m1 to m2 for n 42 and n ≥15 where m1 and m2 are the rounding 
down of n1 and n3 respectively and m=2 to m2 for n 15 
3 
Experimental Results 
This section illustrates the prediction performance of GR2 which is compared with 
traditional linear regression LR 14 Here the prediction performance is evaluated 
by prediction error PE the mean squared error between prediction values of a re
gression model on test data and true values of test data For artificial data the training 
and test data sets are randomly generated from same functions For real data 10fold 
cross validation is used ie the data are first divided into 10 nonoverlapping subsets 
and each subset is used as a test set in turn and the remainder as a training set This 
procedure is repeated 20 times for artificial data and 10 times for real data resulting 
in an average PE as the final performance 
240 
K Wang and L Yang 
The artificial dataset is generated from the linear function Zt=2Xt+Yt+2 
where Xt= 4sin008t +10 and Yt= 4cos004t+4+10 300 noisy data of every 
variable are generated at t=123…300 and then random white Gaussian noise is 
added to the data In the experiments the order relation T=123 …300 the linear 
regression model is F z=ax+by+c and the optimal parameter mo is searched from 
m=56…12 The test process is under mo the 300 test data xi of X are transformed 
to curve manifold
X
M and the 300 test data yi of Y to
Y
M  for xi and yi their corres
ponding points on
X
M and
Y
M are uixi and uiyi respectively the xi and yi are 
taken as inputs of model F and output zi of model F is the prediction finally predic
tion errors are calculated for performance evaluation 
The real data sets the servo autoprice cpu and autompg are from the UCI data
base 8 These data are scaled to zero mean and unit variance to prevent possible 
numeric problems and the nominate attributes in these data sets are replaced by in
tegers eg 123… 9 The linear regression model is F 
0
1
1

q
q
Y
b
b X
b X
=
+
+
+
 
and the optimal parameter mo is searched from m=56…12 in the experiments 
As the data relation information of the four real datasets is not given we mine po
tential smalltobig order as order relation T from data The mining method is find a 
regression model Ftrain with training data by LR method use Ftrain to yield predictions 
of Y with training data of X prediction set Sptrain based on the training set and use 
Ftrain to yield predictions of Y with test data of X prediction set Sptest based on the test 
set and then sort the combination of Sptrain and Sptest by values to give smalltobig 
order relation T Note it is reasonable to use test data of X here since independent 
variable X is observed in real prediction tasks while unobserved dependent variable Y 
needs to be predicted 
The results versus different noise levels for artificial data are shown in Table 1 It 
can be seen that GR2 reduces much the prediction errors about 44  69 PE reduc
tion compared with LR The results for real data are shown in Table 2 One can find 
that GR2 reduces the prediction errors by about 11  21 compared with LR 
Table 1 Prediction errors PE in mean squared errors of LR and GR2 for artificial data sets 
PR denotes proportional reduction of PE by GR2 compared with LR 
Noise 
va
riance 
LR 
GR2 
PR 
02 
119 
066 
448 
06 
347 
123 
647 
10 
555 
178 
680 
14 
742 
238 
680 
18 
908 
280 
692 
22 
1088 
346 
682 
 
Geometric Linear Regression and Geometric Relation 
241 
Table 2 Prediction errors PE in mean squared errors of LR and GR2 for real data sets PR 
denotes proportional reduction of PE by GR2 compared with LR 
Data sets 
LR 
GR2 
PR 
servo 
132 
103 
216 
autoprice 
1188×107 
1056×107 
111 
cpu 
6297 
5402 
142 
autompg 
1248 
1066 
146 
4 
Discussion 
When order relations of data hold latent varying information of data called profitable 
case it would be helpful to mine and utilize this information for a regression
prediction task by GR2 Stronger the relations among data more benefit the GR2 can 
obtain from data relations eg the artificial data have strong relations of data in the 
experiments In contrast the traditional linear regression does not utilize this infor
mation Therefore when GR2 gains additional information about relations of data it 
would have better prediction performance on regression models than the traditional 
regression technique more analyses refer to 10 This also implies that GR2 method 
usually needs prior knowledge about data relations 
The differences between GR2 and traditional linear regression method LR in
clude sum of n data eg
2
1
n
i
iy
 =
 is performed in LR while the integral technique 
eg
2
2
1
l 
u
u f u du
 is used in GR2 LR uses data in regression directly while GR2 
uses curves transformed from data of each variable It is the geometric curves that 
reflect additional information about data varying so GR2 prefers more training data 
and more new data of independent variables to uncover the law of data varying 
The experimental results suggest that the data relation information is helpful for 
improving the predictive ability of linear regression models 
5 
Conclusion 
The relations between data are helpful for the regression analysis and predictions We 
propose the GR2 method to mine and utilize the relations of data for the linear regres
sion GR2 will improve the prediction ability of linear regression methods when rela
tively strong relations of data are obtained and the predictions by traditional linear 
regression methods are bad  
Acknowledgements This work is supported in part by Natural Science Found of 
China No 61175123 and a Key Project of Fujian Provincial Universities  Informa
tion Technology Research Based on Mathematics 
242 
K Wang and L Yang 
References 
1 Wagner M Adamczak R Porollo A Meller J Linear Regression Models for Solvent 
Accessibility Prediction in Proteins Journal of Computational Biology 123 355–369 
2005 
2 Hashimoto EM Ortega EMM Paula GA Barreto ML Regression Models for 
Grouped Survival Data Estimation and Sensitivity Analysis Computational Statistics  
Data Analysis 552 993–1007 2011 
3 Cogger KO Nonlinear Multiple Regression Methods a Survey and Extensions Intelli
gent Systems in Accounting Finance and Management 171 19–39 2010 
4 Yuan ZF Song SD Multivariate Statistical Analysis Science Press Beijing 2009 
5 Wang K Zhang J Shen F Shi L Adaptive Learning of Dynamic Bayesian Networks 
with Changing Structures by Detecting Geometric Structures of Time series Knowledge 
and Information Systems 171 121–133 2008 
6 Chen W An Introduction to Differential Manifold High Education Press Beijing 2001 
7 Sampaio Jr JHB An Iterative Procedure for Perpendicular Offsets Linear Least Squares 
Fitting with Extension to Multiple Linear Regression Applied Mathematics and Computa
tion 1761 91–98 2006 
8 Blake CL Merz CJ UCI Repository of Machine Learning Databases University of 
California Irvine 1998 httpmlearnicsucieduMLRepositoryhtml 
9 Meyer D Leisch F Hornik K The Support Vector Machine under Test Neurocomput
ing 55 169–186 2003 
10 Wang K Zhang J Guo L Tu C Linear and Support Vector Regressions based on 
Geometrical Correlation of Data Data Science Journal 6 99–106 2007 
Appendices 
Proof of Theorem 1 
 
Proof The coefficients a linear regression equation may be found by the least
squaresfit method that minimizes the sum of squared residuals A curve is regarded 
as infinite data and then the integral technique is used to deal with infinite data 
These are known every curve manifold has k local regions any point of a local re
gion of
X
M is ux 
1
  2

u
∈ u u
 under local coordinate system uOx and any point of 
a local region of
Y
M is uy under uOy 
For regression equation
0
y
x
φ
φ
=
+
 we sum and average its two sides for all the 
infinite points of curve manifolds where
l 
i
xli
=g u
and
l 
i
yli
= f u
are used 
 
0
1
1
1
1
1
1
1
1
lim
lim
i
i
N
N
l
l
l
i
l
i
k
N
k
N
y
x
k
N
k
N
φ
φ
→∞
→∞
=
=
=
=




=
+
×












 
This work is done through the integral to local regions
lg  
x
u
=
and
l 
y
= f u
 
 
2
2
1
1
0
2
1
2
1
1
1
1
1
1
1
 
 
l
l
k
k
l
l
u
u
u
u
u
g u
f
u
u
u
u
k
k
du
du
φ
φ
=
=
−
−
=
+ ×




 
 
Geometric Linear Regression and Geometric Relation 
243 
When 
2
1
2
1
1
1
1
 
l
k
l
u
u g u
g
u
u
k
du
=
−
=

 and 
2
1
2
1
1
1
1
 
l
k
l
u
u
f u
f
u
u
k
du
=
−
=

are set and 
0
f
g
φ
φ
=
+
 we can find out
0
φ  
0
f
g
φ
φ
=
−
  
The sum of squared residuals
 
E φ of 
0
y
x
φ
φ
=
+
fitting
X
M and
Y
M is the sum of 
errors between 
l 
i
yli
= f u
and
0
ˆ li
li
y
x
φ
φ
=
+
 i=1∞ in k local regions so 
 
E φ is 




0
2
2
1
1
1
1
 
lim
lim
i
i
i
i
N
N
l
l
l
l
l
i
l
i
k
N
k
N
f
g
E
y
x
y
x
φ
φ
φ
φ
φ
→∞
→∞
=
=
=
=




=
−
−
=
−
−
+












 







2
2
2
1
1
lim
2
i
i
i
i
N
l
l
l
l
l
i
k
N
f
f
g
g
y
y
x
x
φ
φ
→∞
=
=




=
−
−
−
−
+
−








 
When the integral is applied to local regions
lg  
x
u
=
and
l 
y
= f u
l=1k under 
local coordinate system uOx and uOy we have 
 
2
 
2
f f
fg
gg
E
L
L
L
φ
φ
φ
=
−
+
 
where 




2
2
2
1
1
1
2
2
2
2
1
1
1
lim
2
2
l
l
li
li
N
k
gg
l
l
i
k
N
u
u
u
u
u
u
g
g
g
g
du
du
du
x
x
x
x
L
→∞
=
=
=

 =
+
−




=
+
−






 


2
2
2
1
1
1
2
2
1
 
 
2
l
l
k
l
u
u
u
u
u
u
g
g
u
u
g
g
du
du
du
=
=
+
−
 


 
2
1
2
2
2
1
1
 


l
k
l
u
u g u
g
du
k u
u
=
=
−
−

 
and similarly 


2
1
2
2
2
2
2
1
1
1
1
 
lim
2


l
li
li
N
k
f f
l
l
i
k
N
u
u f u
f
f
f
du
y
y
k u
u
L
→∞
=
=
=

 =
−
−




=
+
−




 
2
1
2
1
1
 
 


l
l
k
fg
l
u
u
u g
u
f
f g
du
L
k u
u
=
=
−
−

  
For minimal fitting error set
 
0
E φ 
φ
∂
∂
=
 and then the φ  is 
 
fg
gg
L
L
φ =
  
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 244–251 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Optimal Control Strategies of a Tuberculosis Model  
with Exogenous Reinfection 
Yali Yang12 Xiuchao Song2 Yuzhou Wang2 and Guoyun Luo3 
1 College of Mathematics and Information Science Shaanxi Normal University  
Xian 710062 China 
2 College of Science Air Force Engineering University Xian 710051 China 
3 Unit 94170 of the PLA Xian 710082 China 
yylhgr126com xiuchaosong163com  
575884777799513324qqcom 
Abstract For the tuberculosis TB model with exogenous reinfection we 
study the impact of two control strategies chemoprophylaxis and treatment We 
focus primarily on controlling the disease using an objective function based on 
a combination of minimizing the number of TB infections and minimizing the 
cost of control strategies By using Pontryagin’s Maximum Principle we derive 
the optimal levels of the two controls Numerical simulations of the optimal 
system indicate that the strategies should be improved as the exogenous reinfec
tion transmission coefficient increasing 
Keywords tuberculosis model exogenous reinfection optimal control pon
tryagin’s Maximum Principle 
1 
Introduction 
Tuberculosis TB is an infectious disease caused by the bacillus Mycobacterium tu
berculosis Mtb which is spread in the air when people who are sick with pulmonary 
TB expel bacteria for example by coughing 1 Two billion peoples about onethird 
of the world’s population are infected by it 2 Fortunately the vast majority 90 of 
people infected with Mtb do not develop TB disease 5 of people infected progress 
rapidly to active infection and die without treatment while 5 progress slowly over 
their lifetime 3 It is shown that infected individuals may remain in this latent stage 
for long and variable periods of time in fact many die without ever developing active 
TB and latent individuals progression towards active TB may accelerate with re
exposure to TB bacilli through repeated contacts with individuals with active TB 4 
Hence in our TB model we must not only look at TB infection as the progression 
from primary infection but also include the possibility of exogenous reinfection 
Treatment for the infectious TB is the basis control strategy 5 moreover it has been 
reported that TB latent who with chemoprophylaxis are only 130 to 140 fraction less 
likely to develop active TB than those without chemoprophylaxis6 Therefore in this 
paper for the TB model with exogenous reinfection we consider two optimal control 
strategies chemoprophylaxis for the latent infections and treatment for the infectious 
 
Optimal Control Strategies of a Tuberculosis Model with Exogenous Reinfection 
245 
 
Our objective function balances the effect of minimizing the cases of infected TB and 
minimizing the cost of implementing the control strategies  
This paper is organized as follows In Section 2 a TB model with exogenous rein
fection and two control strategies is formulated and our objective function is intro
duced In Section 3 we derive the optimality system by using Pontryagins Maximum 
Principle At last we use the RungeKutta fourth order scheme to numerically simu
late the optimal control model 
2 
ATB Model with Exogenous Reinfection 
The total population is divided into five epidemiological compartments susceptible 
individuals S  latent individuals infected with Mtb but not infectious L  infectious 
with Mtb that is TB active cases I  being treated for infectious with Mtb T  
chemoprophylaxis for latent with Mtb V  Then N
S
L
I
T
V
=
+
+
+
+
 
We assume that an individual may be infected only through contacting with infec
tious individuals Furthermore we formulate an SLIT
 V
 epidemic model with 
exogenous reinfection seeking to reduce the latent and infectious TB groups we use 
chemoprophylaxis for the latent infection and treatment for the infectious respective
ly Furthmore system 1 parameters’ definitions are tabulated in Table 1 
Then the model is given by the following system of differential equations 








+
+
−
′ =
+
+
−
+
′ =
+
+
−
+
−
+
+
′ =
−
+
+
−
−
+
′ =
−
−
=
′
 



 

 

1

 





 


1



3
2
1
2
2
0
2
2
0
1
2
1
2
1
1
1
1
a V
e
m
u t L
V
a T
d
m
g u t I
g
T
b IL
g u t I
g
a I
m
kdT
e V
e L
I
b IL
e L
u t
m
k dT
b IS
L
b IS
S
m A
S
                             
In system 1 
1
 
 0

1
1
≤
≤
u t
u t
is the fraction which an individual leaves the la
tent 
compartment 
and 
enters 
the 
chemoprophylaxis 
compartment 
1
 
 0

2
2
≤
≤
u t
t
u
 is the fraction which an individual leaves the infectious com
partment and enters the treatment compartment during the treatment is strengthened 
The control functions 
 
iu t i =12
 are bounded Lebesgue Integrable functions The 
“chemoprophylaxis for latent infection” control 
1 
u t  represents the fraction of TB 
latent individuals that is identified and will be put under chemoprophylaxis to reduce 
the number of individuals that may be infectious When the rate 
1 
u t  is higher 
there is less latent and higher implementation costs Correspondingly the “treatment 
for infectious infection” control 
2 
u t  represents the strengthened fraction of TB 
infectious individuals that is identified and will be treated When the rate 
2 
u t  is 
higher there is less infectious and higher implementation costs 
 
246 
Y Yang et al 
 
Table 1 Definitions of used symbols 
Parameter 
Explanation 
m  
Natural death rate 
mA
 
Recruitment rate of individuals 
1
b  
Transmission coefficient of the infectious cases for the susceptible individuals 
2
b  
Transmission coefficient of the infectious cases for the latent individuals 
1e  
Rate of a latent individual becoming infectious 
2e  
Rate of an individual from chemoprophylaxis to infectious compartment 
g  
Rate of  an infective individual be treated before the treatment policy is streng
thened 
0
g  
The maximal added rate of  an infective individual be treated for during the 
treatment policy is strengthened 
d  
Rate of  an individual complete the treatment 
k  
Fraction of  diseaserelapsed 
1
a  
Diseaseinduced death rate of infectious individuals 
a 2
 
Diseaseinduced death rate of treatment individuals 
3
a  
Diseaseinduced death rate of chemoprophylaxis individuals 
 
Our objective functional to be minimized is 
2
 
 
 
 
 
 



0
2
2
2
2
1 1
4
3
2
1

+
+
+
+
+
=
ft
t dt
C u
t
C u
B V t
B T t
B I t
B L t
J u
        
subject to the state system given by 1 The control goal is to minimize the infected 
populations include the being treated populations and the cost of implementing the 
control measures The cost result from a variety of sources we assume that the cost of 
chemoprophylaxis is nonlinear we take quadratic form here7 The coefficients 
1
2
3
4



B B B B  and 
1
 2
C C  represent the corresponding weight constant respectively 
and these weights are balancing cost factors due to size and importance of the parts of 
the objective functional 
We seek to find an optimal control function 


1
2

 
 
u t u t
 such that 
3



 


min



2
1
2
1

2
1
∈ Ω
=
u u
J u u
u
J u
：
 
where  
21 


 0



1
2
1
=
≤
≤
∈
Ω =
i
u
t
L
u
u
u
i
i
i
f
： i
α
β
             4 
and 


12
i
α βi i =
 are fixed positive constants 
3 
Analysis of Optimal Controls 
The necessary conditions that an optimal pair must satisfy come from Pontryagin’s 
Maximum Principle8 This principle converts 1 2 and 3 into a problem of mi
nimizing pointwise a Hamiltonian H  with respect to 
1
2

 
 
u t u t
 
 
Optimal Control Strategies of a Tuberculosis Model with Exogenous Reinfection 
247 
 
2
2
1
2
3
4
1 1
2
2
1
1
2
1
1
1
2
3
1
2
1
0
2
2
4
0
2
2
5
1
2
3
 
 
 
 
 
 
 



1


 






 


 

 

 

 
H
B L t
B I t
B T t
B V t
C u
t
C u
t
A
S
IS
IS
k
T
u t
L
IL
L
V
k T
I
u t I
IL
u t I
T
u t L
V
l
m
b
l
b
d
m
e
b
l
e
e
d
m a
g
g
b
l
g
g
m d
a
l
m e
a
=
+
+
+
+
+
+


+
+


+
+

+
+
+

+

+
+
+
+

+
+
+

+
+
      
  5
where 

1
5
l i i
=

are the adjoint variables By applying Pontryagin’s Maximum 
Principle and the existence result for the optimal control pairs from9 we obtain the 
following result 
Theorem There exists an optimal control 


1
2


u u 
and corresponding solution 







S
L I T and 

V that minimizes 
1
  2
J u u 
over Ω  Furthermore there exists 
adjoint functions 
   
5
1
l t
l t
 such that 









+
+
+
−
′ = −
+
+
+
−
−
−
′ = −
−
+
+
+
+
−
+
−
+
= −
′
−
−
−
+
+
+
+
= −
′
+
−
=
′





1
6

 







 



 




3
2
5
3 2
4
5
2
4
3
2
3
4
4
2
0
3
1
3

2
3
2

1
2
1
2
3
5 1
3 1

2
3
2
1
2
1
2
1

1
2
1
1
a
m
l e
l e
B
l
a
d
l m
l kd
k d
l
B
l
l g
g u t
l g
a
l m
l b L
l
l b S
l
B
l
l u t
l e
l b I
l
e
l m u t
B
l
l m
l b I
l
l
  
with transversality conditions 
15
0 


=
=
i
t
l
f
i
7
The following characterization holds 

2


min
max

2


min
max
2

0
4
3
2
2

2
1

5
2
1
1

1
C
g I
l
l
u
C
l L
l
u
−
=
−
=
α
α
β
β
8
Proof Corollary 419 gives the existence of an optimal control due to the convexity 
of integrand of J  with respect to 
1
2

 
 
u t u t
 a priori boundedness of the state 
solutions and the Lipschitz property of the state system with respect to the state va
riables 
Applying 
Pontryagin’s 
Maximum 
Principle 
8 
we 
obtain 
V
H
l
L
H
S l
H
l
∂
′ = − ∂
∂
′ = − ∂
∂
′ = − ∂
5
2
1


and 
15
0 
 
=
=
i
t
l
f
i
 evaluated at the optimal 
control and corresponding states which results in the stated adjoint system 6 with 
transversality conditions 7 
By considering the optimality conditions 
= 0
∂
∂
iu
H
and solving for 
 
iu t i =12
 
subject to the constraints the characterizations 8 can be derived  
248 
Y Yang et al 
 
To illustrate the characterization of 
 
iu t i =12
  we have 
9
0
 
2
0
 
2
0
4
0
3
2
2
2
5
2
1 1
1
=
+
−
=
∂
∂
=
+
−
=
∂
∂
l g L
l g L
C u t
u
H
l L
l L
C u t
u
H
           
at 
 
iu t  on the set 
21 


=
≤
≤
i
u
t
i
i
iα
β
 On this set 
2



2


2

0
4
3

2
1

5
2

1
C
g I
l
l
u
C
l L
l
u
−
=
−
=
 
10
Taking into account the bounds on 
 
iu t i =12
 we obtain the characterization of 


1
2

 
 
u t u t
in 8 
The optimality system consists of the state system 1 with the initial conditions 
the adjoint system with the terminal transversality conditions 6 7 and the control 
characterization 8 The uniqueness of solutions to the optimality system can be ob
tained by standard results Thus the uniqueness of the optimal control follows from 
the uniqueness of the optimality system 
4 
Numerical Simulation 
In this section we discuss the numerical solutions of the optimality system and the 
interpretations from various cases 
we study numerically two optimal control strategies of the TB model The optimal 
control strategies are obtained by solving the optimality system consisting of 10 ordinary 
differential equations from the state and adjoint equations coupled with the control cha
racterization An iterative method is used for solving the optimality system Given initial 
guess for the controls and initial conditions for the states then the state differential equa
tions are solved forward in time using a fourthorder RungeKutta scheme Using the 
current iteration solution of the state equations and the given transversality conditions 7 
the adjoint system is solved backward in time again employing a fourthorder Runge
Kutta scheme Both state and adjoint values are used to update the control using the cha
racterizations given by 8 and then the iterative process is repeated This iterative 
process terminates when current state adjoint and control values converge sufficiently 
in other words the iteration is stopped if the values of unknowns at the previous iteration 
are very close to the ones at the present iteration 
In our simulation we assume 
A =100000
 thus 
170 100000
×
L = mA =
 
The bound on the control coming from estimating about the effectiveness of the inter
vention strategy In choosing lower and upper bounds for the controls for the control 
1 
u t  it is reasonable to assume that upper bound is 05 a reasonable lower bound is 
 
Optimal Control Strategies of a Tuberculosis Model with Exogenous Reinfection 
249 
 
0 for the control 
2 
u t  it is reasonable to assume that upper bound is 09 a reasona
ble lower bound is 0 The initial values for the states are given by 
0
95000 0
2000 0
2000 0
1000
0
0
S
L
I
T
V
=
=
=
=
=
 respectively  
Table 2 Parameter  values used in the Fig 1 
Parameter 
Value 
Source 
Parameter 
Value 
Source 
m
1700 
10 
1
b  
30100000 
Assumed 
1e   
000368 
11 
2e  
0001 
6 
g
050 
12 
d
15 
13 
k  
001 
1 
1
a  
03 
14 
a 2
  
005 
15 
3
a  
003 
Assumed 
0
g   
050 
Assumed 
2
s 1
b
b
=
 
s = 03
 
Assumed 
 
In Fig 1 parameter values as the Table 2 list Due to lack of data some parameter 
values are assumed within realistic ranges for the purpose of simulation The units 
where applicable are per year The weights in the objective function are 
1
2
3
4
1
100
10
05
B
B
B
B
=
=
=
=
 which mean that minimization of the number of 
latent infectious treated and Chemoprophylaxis has different importance Fig 1 
shows the optimal treatment strategy for the case of 
1
400 2
2000
C
C
=
=
 The popu
lations with optimal control actions are shown in dashed lines to compare with the 
populations without control actions Obviously we can know that from Fig 1 the 
infected individuals are decreased sharply when to use the optimal control strategies 
comparing with the case without control actions Then the infected will be eliminated 
if we adopt the optimal control measures 
 
Fig 1 Simulations of the model 1 Dashed line Populations with control Solid line without 
control 
250 
Y Yang et al 
 
In Fig 2 we show how the control strategy 
2 
u t  depends on the exogenous rein
fection transmission coefficient 
2
b  which is the exogenous reinfection transmission 
coefficient of the infectious cases for the latent individuals We suppose 
2
s 1
b
b
=
 
then let s  choose 0051152  respectively s is shown that there doesn’t exist 
exogenous reinfection for the TB 
s = 05
 is shown that the exogenous reinfection 
transmission coefficient 
2
b  is smaller than 
1
b  
s =15
 is shown that the exogenous 
reinfection transmission coefficient 
2
b  is bigger than 
1
b  and so on From Fig 2 we 
find that the control strategy 
2 
u t  should be increased as 
2
b  increasing 
 
Fig 2 The control strategies 
2 
u t  for different values of 
2
b  
We have identified optimal control strategies for several scenarios Numerical si
mulations of the control model indicate that the chemoprophylaxis and treatment 
strategies are effective in reducing TB disease transmission And our optimal control 
results also show that the costeffective combination of control may depend on the 
exogenous reinfection transmission coefficient In conclusion control programs that 
follow the chemoprophylaxis and treatment controls can effectively reduce the num
ber of infected for TB But when 
2
b  increasing disease control should be increased 
even when
b2 ≥ 2
 the disease seems can’t be control The studies show that we 
shouldn’t ignore the exogenous reinfection for TB disease control 
Acknowledgements This work was partially supported by the Nature Science Foun
dation of Shaanxi Province of China No2012JQ1019 the Research Fund of Shaanxi 
Key Laboratory of Electronic Information System IntegrationNo201113Y12 and the 
Nature Science Foundation of China No11071256 
 
Optimal Control Strategies of a Tuberculosis Model with Exogenous Reinfection 
251 
 
References 
1 WHO Global Tuberculosis Control WHO Report 2011 Switzerland 2011 
2 WHO Global Tuberculosis Control WHO Report 2006 Geneva 2006 
3 Blower SM Small PM Howell PC Control Strategies for Tuberculosis Epidemics 
New Models for Old Problems Science 273 497–500 1996 
4 Feng ZL CastilloChavez C Capurro AF A Model for Tuberculosis with Exogenous 
Reinfection Theor Popul Biol 57 235–247 2000 
5 Russell DG Barry CE Flynn JL Tuberculosis What We Don’t Know Can and 
Does Hurt Us Science 328 852–856 2010 
6 Ziv E Daley CL Blower SM Early Therapy for Latent Tuberculosis Infection Am 
J Epidemiol 153 381–385 2001 
7 Tchuenche J Khamis S Agusto F et al Optimal Control and Sensitivity Analysis of 
an Influenza Model with Treatment and Vaccination Acta Biotheor 2010 
8 Pontryagin LS Boltyanskii VG Gamkrelidze RV et al The Mathematical Theory 
of Optimal Processes Wiley New York 1962 
9 Fleming WH Rishel RW Deterministic and Stochastic Optimal Control Springer 
New York 1975 
10 Castillochavez C Song BJ Dynamical Models of Tuberculosis and Their Applica
tions Math Biosci Eng 1 361–404 2004 
11 Blower SM McLean AR Porco TC et al The Intrinsic Transmission Dynamics of 
Tuberculosis Epidemics Nature Med 1 815–821 1995 
12 Bhunu C Garira W Tuberculosis Transmission Model with Chemoprophylaxis and 
Treatment Bulletin of Mathematical Biology 70 1163–1191 2008 
13 Rodriguesa P Gomes MGM Rebelo C Drug Resistance in Tuberculosisa Reinfec
tion Model Theor Pop Biol 71 196–212 2007 
14 Ted C Megan M Modeling Epidemics of MultidrugResisitant Mtuberculosis of Hete
rogeneous Fitness Nature Medicine 10 1117–1121 2004 
15 Sanchez MA Blower SM Uncertainty and Sensitivity Analysis of the Basic Repro
ductive Rate Tuberculosis as an Example American Journal of Epidemiology 145 1127–
1137 1997 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 252–259 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Method of Smartphone Users’ Information Protection 
Based on Composite Behavior Monitor 
Hua Zha and Chunlin Peng 
Research Institute of Electronic Science and Technology University of Electronic Science and 
Technology of China ChengduSi Chuan Province PRChina 
zhahuaorphenpengchunlin163com 
Abstract For the users’ information of the Smartphone disclosure issues a 
method of users’ information secure for the Smartphone has been researched 
Based on hidden Markov model and fuzzy pattern recognition model the paper 
proposes a compound monitoring approach which use the heuristic scanning
behavior block monitoring to discriminate the behavior of the program whether 
is malicious Finally the paper introduces a method for testing Smartphone 
users’ information protection The results show that this method is able to well 
implement to monitor and block the malicious behavior of the program 
Keywords heuristic scanning behavior blocking malicious behavior 
Smartphones Information Protection 
1 
Introduction 
Today according to market research agency IDC recently released 1 at the end of 
the fourth quarter of 2011 every sold three mobile phones in the global there is a 
smartphone It is becoming the people second data processing center However the 
Smartphone users’ information data is more and more unsafe 
On the Smartphone the users’ information disclosure is particularly dangerous 
The literature 2 indicates  that malicious program will collect the Smartphone 
users’ privacy information after these implant in the Smartphone Then the 
Smartphone users’ privacy information will be sent out As these programs have 
hidden cell firewall is also difficult to intercept Therefore for Smartphone users’ 
information security we need to find a suitable method of protection for the 
Smartphone characteristics 
Combining the characteristics of Smartphone in this paper based on hidden 
Markov model and fuzzy pattern recognition model we put forward the heuristic 
scanbehavior blocker composite monitoring methods to achieve realtime security 
protection on mobile users’ information 
2 
Related Research 
Today Smartphone monitoring method is on mature there is feature code detection 
method heuristic analysis method and behavioral blocking method 3 4 
 
Method of Smartphone Users’ Information Protection 
253 
 
21 
Feature Code Detection Method 
This detection technology is single which is not affective for the unknown malicious 
program The literature 5 indicates that as the security threatened on Smartphones 
will become diverse the malicious programs will render polymorphism for the 
detection capability of a single feature code technology will gradually be replaced by 
other ways 
22 
Heuristic Analysis Method 
The literature 6 7 indicates that the heuristic analysis is a priori program behavior 
prediction methods through knowledge and experience it analysis the intent of the 
program and determine the behavior of the program By differences from the features 
of the traditional detection methods for detection of program code it detects 
suspicious logic of the program so that detecting unknown malicious programs is 
functional In the specific implementation the heuristic analysis method exists to 
identify suspicious code instruction sequence requires timeconsuming and false 
negatives and false positives problem  
23 
Behavior Blocking Method 
The literature 8 indicates that the behavior blocking methods will define security 
policy initially While the program is running realtime monitor the program 
behavior When the program is illegally the security policy blocks it There are two 
types of security policy one is a smart approach according to certain algorithm set 
the factors and analyzes and judges the behavior of the program another one is user 
direct involvement determine whether the program is malicious behavior by the user 
The literature 4 indicates “The behavior of the different types of viruses can widely 
vary and may cause the system behavior patterns have many kinds of infection” this 
will lead to the monitoring and analysis of the behavior of the program will take a lot 
of time and resources 
Today Smartphone computing power and storage capacity increase substantially 
the calculation time and resource consumption on heuristic analysis and behavior 
blocking constraints of the both of these two methods are no longer obvious In this 
paper we combine the heuristic analysis method and behavior blocking method and 
propose a Smartphone users information protection method based on heuristic 
scanning  behavior blocking composite monitoring 
3 
Method of Composite Monitoring 
31 
Malicious Behavior Feature 
According to feature analysis of the program of the malicious behavior for users’ 
information we mine the malicious behavior factors as shown in Table 1 
254 
H Zha and C Peng 
 
Table 1 The program of malicious behavior feature 
Number   
Behavior Feature 
Threat Degree 
1 
program selfdelete 
high  
2 
service auto start 
high 
3 
copying in the system directory 
high 
4 
process hidden 
high 
5 
SMS interception 
high 
6 
auto start of the memory card 
high 
7 
reading address book 
middle 
8 
reading SMS 
middle 
9 
reading call records 
middle 
10 
voice recording 
high 
11 
mobile locating 
high 
12 
abnormally calling 
high 
13 
sending SMS 
low 
14 
sending Email 
middle 
15 
sending by network 
high 
32 
Heuristic Scanning 
We construct a model for the procedure focused using the double stochastic process 
of hidden Markov model λ as a 5tuple SVABΠ 
Monitoring the behavior of procedures can be considered as a binary classification 
problem S as a group of state set define the set of procedure state S = 0 1 0 is  
nonemalice 1 is  malice the state number N = 2 s as a behavior detected has s∈S 
V is the set of procedures for malicious behavior as shown in Table 1 
V=V1V2V3…VM VM is the malicious behavior of M M is the total number of 
currently known malicious behavior 
A is the state transition matrix 
00
01
10
11
ij
N N
a
a
A
a
a
a
×




=
= 





aij = Pqt+1=j | qt=i1≤i  
j≤N where qt is the current program state and qt+1is the next program state 
B is the malicious programs probability distribution B = bjk bjk is the prob
ability distribution for procedure state j bj k = P  Vk | j 1≤k≤M1≤j≤N 
Π is the initial state probability distribution The state of the system is normal or 
abnormal so Π = π1 π2 πi = Pq1=iis  the probability of choosing i when time 1 
B is the probability distribution of malicious programs B = bjk bjk express 
the probability of malicious behavior while the program is in state j bj k = P  Vk | j 
1≤k≤M1≤j≤N。 
Π is the initial state probability distribution System state in a normal state or mali
cious state so Π = π1 π2  πi = Pq1=i πi is the probability of selecting state i in 
initial time 
HMM model as the malicious behavior of the generating device according to a 
certain step generate malicious behavior series O t = O1O2 O3 OT ∈V T is 
the number of malicious behavior monitoring to program 
 
Method of Smartphone Users’ Information Protection 
255 
 
Use the forward  backward algorithm 
Forward variable αt i = PO1O2O3…Ot  qt=i | λ αti refers to the probability of 
malicious behavior of the program sequence O1O2O3…Ot at the given model λ in time 
t under state i 
1 Initialization α1 i = πi bi o1 1≤i≤N 
2 Iterative calculation
1
1
1
 
 


N
t
t
ij
j
t
i
j
i a
b O
α
α
+
+
=


= 


 1≤t≤T–1 1≤i j≤N 
3 Distinguishing malicious behavior 


1
|
 
N
T
i
P O
i
λ
= = α
 
Backward variable βti = POtOt+1Ot+2…OT | qt = i  λ βti refers to the probability 
of malicious behavior of the program sequence OtOt+1Ot+2…OT at the given model λ 
in time t under state i  
1 Initialization βTi = 1 1≤i≤N 
2 Iterative calculation 
1
1
1
 


 
N
t
ij
j
t
t
i
i
a b O
j
β
β
+
+
= =
   1≤t≤T – 1 1≤j≤N 
3 Distinguishing malicious behavior 


1
1
1
|


 
N
i
i
i
P O
b O
i
λ
π
β
= =
 
We derive the value PO | λby forward and backward algorithm and set the value by 
weighted average to arrive at a final predictive value PO | λ Where weight is sim
plified we subject to the average 
We calculate the malicious behavior programs probability value PO | λ and set 
the threshold ε When calculating the probability value is greater than the threshold ε 
we can be submitted to user determine whether the program is malicious behavior 
33 
Behavior Blocking Monitor 
Behavior blocking monitors focus on suspicious malicious behavior such as listed in 
Table 1 a series of malicious behavior and monitor the type of malicious behavior of 
the corresponding system API functions We construct a model for the method of 
fuzzy pattern recognition First we use fuzzy sets to the program behavior is divided 
into the normal behavior and malicious behavior 9 then use the Optional Near prin
ciple for the classification monitoring program 
Fuzzy Pattern Recognition Definitions of sets the behavior of the program is di
vided into two categories normal and malicious Select a sample program from the 
sample space and extract the sample programs in a set of feature set A which related 
to user information security Each program has a corresponding feature that can be 
defined a fuzzy set on the feature class to describe an object or object class Based on 
the domain Q=q1q2…qn it composed by n feature An executable program or 
class can be used to define a fuzzy set on the domain of Q to describe that is M  The 
program has the feature qi membership is defined as the real number
iμ 
iμ ∈01 
Training  
1 
According to Table 1 corresponds to the access to the system API function 
select t feature set of API functions to form a fuzzy feature set t=15 feature set can 
increase with the increase of malicious behavior in the future  
256 
H Zha and C Peng 
 
2 
According to the feature set we establish the membership function of fuzzy 
set V  of malicious behavior 
2
2
 

0
 

0
 

 

0
1
V
i
V
V
i
V
i
V
E A
i
E A
E A
E A
e
σ
μ
−

≤

= 
≥
 −
 
In the Formula the 

E AiV 
 is the frequency average of the system API functions 
used in malicious behavior 


1
2

 
 
 3
V
V
tV
Max E A
E A
E A
σ =
 t is number of feature 
quantities With membership function
 
V 
V
E Ai
μ
 evaluate the fuzzy set 


1
1
2
2


V
V
V
t
t
V
A
A
A
μ
μ
μ
 =
 
3 
Establishment membership functions of the fuzzy set N in the API function 
of the normal behavior 
2
2
 

0
 

0
 

 

0
1
N
i
N
N
i
V
i
N
E A
i
E A
E A
E A
e
σ
μ
−

≤

=
≥
 −
 
In the Formula the 

E AiN 
 is the frequency average of the system API functions 
used in normal behavior With membership function
 
N 
V
E Ai
μ
 evaluate the fuzzy 
set


1
1
2
2


N
N
N
t
t
N
A
A
A
μ
μ
μ
 =
 
Classification  
1 
When the program calls a system API functions related to Smartphone users’ 
information  get the API function information  statistics on its frequency of feature 
obtain the set A1 A2…Att is feature space dimension 
2 
Membership function of the program is 
2
2


0

0



0
1
i
i
M
i
A
i
A
A
A
e
σ
μ
−

≤

= 
≥
 −
 
With 
membership 
function


M
iA
μ
 
evaluate 
the 
program 
fuzzy 
set 


1
1
2
2 
t
t
M
A
A
A
μ
μ
μ
 =
 
3 
Get the fuzzy set V and N from the training separately calculate the Euclid 
close degree 



Ψ M V

 and



Ψ M N

  
The formula of the Euclid close degree 






1
2
2
1
1

1
t
A
B
i
i
i
A B
t
μ
μ
=
Ψ
= −
−

 
 
4 
Use the optional near principles to categorize detected program The prin
ciple is for the fuzzy sets 

iA B
 i= 1 2…nif 
i
∃ has 




1


i
j
j n
A B
A B
≤ ≤
Ψ
=
∨ Ψ



 that 
consider the 
iA and B closest classify the both as one class 
Based on the above two methods we propose the composite model The model for the 
Smartphone users’ information protection system Use heuristic scanning methods 
scan the program determine whether the program behavior threat users’ information 
Use behavior blocking method monitor these system API functions related to users’ 
information in realtime when the program use these system API functions 
determine whether the program behavior threat users’ information The entire 
interception program is processed by user Then we construct the confidence list of 
this complementary measure store the users’ selection in the list 
 
Method of Smartphone Users’ Information Protection 
257 
 
4 
A Protection Model of Users’ Information Based on 
Composite Monitoring 
Based on composite monitor method this paper design users’ information protection 
model which of architecture as shown in Figure 1 
Heuristic analyzer
Heuristic scanning
User interface
Realtime behavior 
monitoring
Behavior analyzer
Trust 
list
Update
State 
Lib
Feature 
Lib 
 
Fig 1 The architecture of users’ information protection model 
Heuristic scanning module it’s initiated by user scans the Smartphone system 
program which not in the trust list detects the program behavior whether threat the 
users’ information For malicious behavior program it submits user to select blocking 
or releasing the program and adds the program into the trust list  
Heuristic analyzer it uses hidden Markov rule analyzes the program behavior 
which is submitted by the scanning module determines the program whether has the 
malicious behavior and backs the result to the scanning module 
Realtime behavior monitoring module it monitors system API functions which is 
related to users’ information If the program which is not in the trust list uses these 
system API functions it determines the program behavior whether threat the users’ 
information For the malicious behavior of program it submits user to select blocking 
or releasing the program and adds the program into the trust list 
Behavior analyzer it uses fuzzy pattern recognition module classifies the program 
behavior which is submitted by realtime behavior monitoring module and backs the 
classified results to monitoring module 
User interface the interface of user and module It submits the malicious behavior 
of program which monitor from scanning module and realtime monitoring module 
prompts the user to select block or release Finally it returns the result to their se
lected module and records into the trust list 
Trust list it establishes the black and white list stores separately the behavior pro
gram of normal and malicious 
5 
Experimental Analyses 
The experiment uses Windows Mobile Smartphone platform for the spy software 
WinCEFlexispyA sample10 writes a malicious behavior program named  
Alfredexe The malicious program enters a latent state after implantation When the 
258 
H Zha and C Peng 
 
malicious program receives command turn on the running state It’ll steal contacts 
SMS messages call records and phone physical geographic information via text mes
saging email and networking send out It’s also as detectaphone monitors user talk 
After the malicious behavior program Alfredexe implanted in the phone start the 
information program model We run the heuristic scanning to detect the phone in the 
program When it detects the program Alfredexe which is not in the trust list analyz
es the behavior of program draws a conclusion which Alfredexe is a malicious pro
gram and submits user to handle 
Open realtime behavior monitoring and run the malicious behavior program Al
fredexe When the program get the contacts to access to the relevant API function 
the realtime monitoring block and analyze its behavior While the program behavior 
classifies monitoring behavior it submits user to handle As shown in Figure 2 
 
Fig 2 A tip from realtime monitoring of the behavior blocking 
Table 2 User information monitor list 
 
Heuristic scanning  
Realtime behavior monitoring 
Contacts  
√ 
√ 
SMS 
√ 
√ 
Call records 
√ 
√ 
Calling 
√ 
√ 
Voice recording 
√ 
√ 
geographic locating 
√ 
√ 
SMS sending 
 
√ 
Email sending 
 
√ 
networking 
 
√ 
 
In this experiment we select Allow and continue to test Then the program will 
continue to get SMS messages call records geographic information and calling to 
access to the relevant system API function the realtime monitoring module can be 
blocked and  submit user to handle 
As the program Alfredexe covers most of current malicious behavior which is re
lated to Smartphone users’ information disclosure from this experiment can be seen 
that the heuristic scanning and realtime behavior monitoring can be good find the 
malicious behavior thereby demonstrated the method of Smartphone users’ informa
tion protection based on composite behavior monitor is effective 
Table 2 shows the model can protect the Smartphone users’ information 
 
Method of Smartphone Users’ Information Protection 
259 
 
6 
Conclusion 
This paper analyzes the malicious behavior feature of the program which threatens the 
security of Smartphone users’ information and studies the method of the information 
protection Based on Hidden Markov Models and fuzzy pattern recognition model the 
paper proposes a compound monitoring approach which uses the heuristic scanning
behavior block monitoring It designs the Smartphone users’ information protection 
model based on the method mentioned above Experiments show that the model can 
be good to monitor the program which threatens the users information security and 
verify the correctness of the compound monitoring approach which uses the heuristic 
scanningbehavior block monitoring achieved the expected goals In the furniture we 
need to further improve the malicious behavior library reduce omissions and false 
and increase the accuracy of the blocking of malicious behavior program 
References 
1 Statistical report of the global mobile phone OL 2012 
httpwww199itcomarchives23609html 
2 Cheng Z Mobile Malware Threats and Prevention McAfee Avert Labs 2008 
3 Yap TS Ewe HT A Mobile Phone Malicious Software Detection Model with Beha
vior Checker In Proc of HIS 2005 pp 57–65 2005 
4 Szor P The Art of Computer Virus Research and Defense Addison Wesley Professional 
2005 
5 Wang ZH Wang HF Study on AntiVirus Engine Based on Heuristic Search of Poly
morphic Virus Behavior Research and Exploration in Laboratory 259 1089–1091 
2006 
6 Wu JJ Fang MW Zhang XF Research of Mobile Phone Virus Defense Based on 
Heuristic BehaviorChecking Computer Engineering and Science 321 35–38 2010 
7 Cui P Application of Formal Semantics to Heuristic Antivirus Engine Journal of Liao
dong University Natural Sciences 153 167–171 2008 
8 Hu YT Chen G Zheng N Guo YH Malicious Executable Detection Based on Run
time Behavior Computer Engineering and Applications 4517 64–66 2009 
9 Zhang BY Yin JP Tang WS Study and Implementation Intelligent Detection Sys
tem to Recognize Unknown Computer Virus Computer Engineering and Design 2711 
1936–1938 2006 
10 Liu XL Liu K Principle of Malware Analysis and Study of Protective Measures on 
Windows Mobile Phone Network Security Technology  Application 9 41–43 2008 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 260–268 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Improved Digital Chaotic Sequence Generator Utilized  
in Encryption Communication 
Xiaoyuan Li1 Bin Qi2 and Lu Wang3 
1 Electronic Engineering Department Harbin Vocational Technical College Harbin China 
2 College of Information and Communication Engineering College  
Harbin Engineering University Harbin China 
3 School of Overseas Education Northeast Normal University Changchun China 
qibinwinterwangludailygmailcom 
 hzylxywl163com  
Abstract Chaos has good statistical characteristics and is sensitive with initial 
value which is appropriate for encryption system Since digital chaos is 
conducted under the condition of finite precision which will lead to the 
degradation of the system and the output sequence will appear periodicity This 
paper proposed an improvement for the short period of digital chaotic sequence 
and designs a new type of digital chaotic sequence generator based on FPGA 
Experimental data show that this new cycle of the system output sequence can be 
set according to the encryption necessary which remedies the shortperiod 
phenomenon of digital chaos so that digital chaotic sequence generator is safe to 
be applied to the encryption system 
Keywords correlative peak interval finite precision correlation chaotic 
pseudorandom sequence 
1 
Introduction 
In recent years chaotic encryption is widely used in many research fields for its good 
characteristics that chaos is sensitive with initial value and is similar to random 
sequence 1 Since random sequence can be used in many situations the replacement 
of random sequence with chaos has become a hotpot and conducted by many 
researchers 2 3 However using digital system to realize chaos has many difficulties 
that chaos is restrained by the finite precision of the system and even small errors 
introduced in each iteration will have a big effect on the implement of chaos 4 
Consequently the accumulation of the error will result in the deviation of the orbit and 
greatly affect the characteristic of the system 5 Using analog circuits to realize 
chaotic sequence had been used for a long time 6 However due to the sensitivity of 
chaos to initial values and parameters chaotic key stream is easily affected by 
environment conditions 7 Moreover it is hard to set up the cooperation between the 
sending and receiving terminal 8 And most research just focus on the simulation 
experiment which can not discover the problems happened in the actual hardware 9 
10 Theoretically chaotic sequence in nonperiod but it will turn to be a period 
 
Improved Digital Chaotic Sequence Generator 
261 
 
sequence in the situation of finite precision In addition chaos will have strong 
correlation after discretization which will greatly affect its performance 11 12 So 
this paper did some research on the correlative peak interval and presents a new type of 
digital chaotic sequence generator 
2 
Chaos Theory 
Bifurcation chart is the description of state variant based on parameter space General 
mapping form is 
1
 

n
n
n
x
f
x
x
R
μ
+ =
∈
   
1
Where 
  
f ⋅ ⋅  is the differentiable function， μ  is a parameter If initial value is fixed 
the value of 
nx  repeats infinite cycle among 

p p ≥1
 states which is a periodic orbit 
According to the method of linear stability the conditions for stable periodic orbits is 
1
 

1
p
t
t
f
x
μ
=
′
′ ≤
∏
 
2
where the periodic orbit p  is called superstable In this paper the logistic map is 
defined as 




1
1

04
01
n
n
x
x
x
x
μ
μ
+ =
−
∈
∈
 
3
We get the fixed point 

0

1 1
O x
A x
μ
=
= −
The stability of fixed point is 
determined by 
f  
′ x
 which is 
 
2
f
x
x
μ
μ
′
=
−
 
4
Consequently the stability of fixed points depends on the parameter μ  From the 
behavior of iterative equation 3 it could obtained that when the parameter μ  
becomes larger from zero the iterative process has different dynamic behavior 
Nonlinear equations change the topology structure of system trajectory which will 
cause the overall shape of the system changes suddenly and produce the phenomenon of 
bifurcation However it is the necessary process of the generation of chaos 
3 
Logistic Sequence Generator and Key Selection 
Logistic mapping mathematical expression is shown as equation 3 when μ  is in the 
interval of 35699456 4 logistic map enters into chaotic state and presents complex 
dynamics We design logistic chaotic mapping digital circuit model according to the 
equation shown in Fig 1 where Input is the initial value Shift and Sampler are two 
binary output sequence and the other module is the logistic chaotic mapping operations 
unit Output is the output of the key sequence generator 
262 
X Li B Qi and L Wang 
 
 
Fig 1 Logistic chaotic map circuit based on FPGA 
Initial value is the first consideration in the analysis of chaos It can be seen that the 
statistical characteristics of the output sequence are affected by the computing 
accuracy equation parameters and initial values As the computing precision in the 
system is limited so it is not suitable if we use the key input in chaotic encryption 
algorithm According to the research of chaotic dynamics when parameter μ  is in the 
interval of 35699456 4 logistic map is in the chaotic state and the output sequence is 
nonperiodic and nonconvergent However it can be seen from Lapunov index curve 
that the interval is not always in chaos state and when μ =4 the map is a full shot in 
the unit interval 0 l that the chaotic sequence has the characteristic of periodicity 
Therefore μ  can not be the initial key input of chaotic encryption when the initial 
value has a tiny deviation the orbit will separate with exponential speed So it is 
impossible to have a longterm prediction on the behavior of the system Just as the 
chaotic system is sensitive with the initial value when the chaotic system is assigned 
with different initial value we can get a series different and not related chaotic 
sequence Therefore we choose the initial value of chaotic systems as the chaotic key 
inputWe select μ =4 precision is 38 and initial value X 0 as the initial key conditions 
in logistic equation under the condition of Matlab simulation environment With 
different selected initial keys autocorrelation tests were shown in Fig 2 
It can be seen that with the same accuracy and parameters when the initial value is 
different the output sequences are not the same 
6
10  groups of data were selected to 
test relevance in this paper When the initial value is 0216 it does not have any 
relevant peak interval in the whole operation region and when the initial value is 
01378 there are a few of relevant peak intervals After repeated test any initial value 
will produce an unpredictable relevant peak interval That is using initial value as the 
input key could not make the encryption system security when digital chaotic system is 
used for plaintext encryption 
Consequently if the ciphertext sequence has cyclical relevant peak interval it will 
directly threaten the safety of the key sequences But the discretization of the chaotic 
relevant peak interval is inevitable so how to get the ideal interval between the peak 
values becomes a key issue in the digital chaotic sequence encryption system After a 
lot of relevant test we can see that relevant peak interval is equal or approximately 
equal so in this paper we do the analysis on relevant interval which is equal to the peak 
period 
 
 
Improved Digital Chaotic Sequence Generator 
263 
 
 
 
a initial value is 0118 
 
 binitial value is 01378 
 
c initial value is 0216 
Fig 2 The relation between related periodic interval and the initial value 
 
264 
X Li B Qi and L Wang 
 
4 
Digital Logistic Hardware Realization and Its Output Sequence 
Test 
41 
Digital Logistic Hardware Realization 
Both the design and development process are completed in the chip of Altera 
development board EP2C8Q208C8N The digital chaotic key stream generator was 
designed with the operating platform of Quartus Ⅱ 81 based on logistic equation Fig 
3 is the logistic hardware circuit diagram which includes obtaining the initial value 
floating point unit the implementation of the standard doubleprecision addition and 
multiplication iteration fixed point floatingpoint conversion and changing 
quantitative modules into binary sequences 
In order to generate binary chaotic sequence original chaotic sequence   
x n
 was 
changed to binary sequence   
s n
 which subjects to  
  
s n
= T x n
，

10
n =
，where 
  
T x n
is an irreversible function defined as 
2
1
2
0
2
1
2
1
0
0





1


m
m
m
k
k
m
k
k
x n
I
T
x n
x n
I
−
=
−
=
=

∈

= 

∈



 
5
As chaotic signal   
x n
 has good random statistic characteristics so   
s n
 has 
balanced 01 ratio 
 
Fig 3 Logistic chaotic map hardware circuit connection diagram 
 
Improved Digital Chaotic Sequence Generator 
265 
 
According to the equation 3 when 
  |
012
  
01
X
x n
n
x n
=
=
∈

 the 
binary sequence after conversion is 
   |
012
  
01
S
s n
n
s n
=
=
∈

 In order to 
make the design simple and the easy to be realized equation 3 is done with the linear 
transformation 
0 
2


 2
 2
1  


 
1
2


  2
1 
 2
2  
m
m
x
n
k
k
T
x
n
x
n
k
k

∈
+

= 
∈
+
+
 ，
 
6
Where m is the arbitrary positive integer and 
012
2
1
m
k =
−

 Whether the 
output of sequence is 0 or 1 will be determined by the parity of bit Bit extractor will 
complete the judgment of the bit parity that barrel shift register and the bit selector can 
complete the function of equation 3 In order to improve the period of chaotic 
sequence digital chaotic key stream generator we designed consists of two parts The 
first part is the output of the uncertain chaotic sequence where μ =4 the precision is 
38 and the initial value could be any value The simulation results were shown in Fig 
4 The default system clock period is 10ns rising edge trigger sclrp is the circuit reset 
signal When sclrp is high the whole circuit is valid to reset initialization Sel is the 
initial key loading start signal When sclrp is low and sel is high the circuit will sends 
the initial key into the circuit however when sel changes from high to low the circuit 
uses the input key as the initial value for the iteration logistic equation This paper 
chooses 48 bits binary number as the initial key input and output is the sequence output 
of Logistic iteration equation 
 
 
Fig 4 Uncertain chaotic system output with arbitrary chaotic key 
The second part is used for fixing initial value μ =4 the reliable system simulation 
waveform under the precision of 38 Fig 5 
 
Fig 5 Reliable system output sequence with fixed initial values parameters and accuracy 
266 
X Li B Qi and L Wang 
 
The XOR output between the reliable systems with initial key chaotic sequence 
system is shown in Fig 6 The reset signal is sclrp and the high value is effective 
When sclrp is low and sel is high the system will load initial key 
AAAAAAAAAAAA N1 is the output sequence of arbitrary system When the third 
rising edge of the clock signal is triggered the system will do the iteration with initial 
key N is a reliable system and the output is the result after the operation with N1 
 
 
Fig 6 Simulation results of the digital chaotic sequence generator key 
42 
Digital Logistic Output Sequence Test and Analysis 
The length of the binary sequence is N then the correlation coefficient of the binary 
sequence is defined as following where m is the parameters for the step 
1
1
 
N
m
i
i m
i
R m
x
x
N
−
+
=
=
⋅
⋅

 
7
As a result the period of operation is 402114 Fig 7 a and it has no period in Fig 7 
b which proved that the practical digital chaotic key generator has good 
cryptographic properties It is inevitable to have correlation peaks in the output 
sequence which directly threaten the safety of the key sequences After the processing 
of operator XOR it has better performance Fig 7c So with the method proposed in 
this paper the output sequence used in digital communication encryption will ensure 
better autocorrelation 
 
 
Improved Digital Chaotic Sequence Generator 
267 
 
 
aLimited periodthe period is 402114 
 
  bthere is no period in 
6
10  
 
c there is no period after XOR operation 
Fig 7 the relevance of the digital chaotic sequence key generator 
268 
X Li B Qi and L Wang 
 
5 
Conclusion 
Discrete chaotic sequence will generate chaos degradation which will make the 
periodic and pseudorandom sequence worse and result in the encryption system 
insecurity This paper proposes a digital chaotic key sequence generator based on the 
shortperiod phenomena and digital chaotic practical applications It consists of fixed 
initial value equation parameters and accuracy and the chaotic sequences after good 
statistical properties test are called reliable system The initial values of the system 
were used as the key input of the uncertain chaotic system The new sequence after 
XOR operation of the two systems was used for encryption 
References 
1 Suneel M Electronic Circuit Realization of the Logistic Map Sadhana 31 69–78 2006 
2 Ding Q Zhu Y Zhang F Peng X Discrete Chaotic Circuit and the Property Analysis 
of Output Sequence In International Symposium on Communications and Information 
Technologies pp 1009–1012 IEEE Press Beijing 2005 
3 Li Q Yang X 2D Chaotic Signals Generator Design Acta Electronic Sinica 33 
1299–1302 2005 
4 Yang X Tang Y Horseshoes in Piecewise Continuous Maps Chaos Solutions and 
Fractals 19 841–845 2004 
5 Li K Soh Y Zhang C A Frequency Aliasing Approach to Chaosbased Cryptosystems 
IEEE Transactions on Circuits and Systems 51 2470–2475 2004 
6 Huang R Huang H Chaos and Its Applications Wuhan University Press 2005 
7 Lv J Lu J Analysis and Application of Chaos Series Wuhan University Press 2002 
8 Ding Q Pang J Fang J Peng X Designing of Chaotic System Output Sequence Circuit 
Based on FPGA and its Possible Applications in Network Encryption Cards International 
Journal of Innovative Computing Information and Control 3 449–456 2007 
9 Ding Q Pan J Wang L The Cipher Code Parameter Selection and its Impact on Output 
Cycles In 2009 International Workshop on ChaosFractals Theories and Applications pp 
143–147 2009 
10 Zhao G Fang J The Progress of Modern Information Security and Chaotic Secure 
Communication The Progress of Physics 23 212–225 2003 
11 Zhang X Logistic Chaotic Mapping Section and its Performance Analysis Electronic 
Journal 37 720–725 2009 
12 Chee C Xu D Chaotic Encryption Using Discretetime Synchronous Chaos Physics 
Letters 3 284–292 2006 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 269–276 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Modeling and Adaptive Control for FlappingWing 
Micro Aerial Vehicle 
Qingwei Li1 and Hongjun Duan2 
1 Department of Environmental Science and Engineering  
Northeastern University at Qinhuangdao Qinhuangdao China 
2 Department of Automation Engineering Northeastern University at Qinhuangdao  
Qinhuangdao China 
lqwday126com dhjmailneuqeducn 
Abstract Flight quality of flappingwing micro aerial vehicle FMAV de
pends much upon efficient control of flight attitude So an accurate model of 
flight attitude is of utmost importance The fly mechanism of birds and big in
sects especially the motion rule of wings were investigated to establish a com
plete dynamic model and mathematical model for flight attitude of FMAV The 
design of attitude controller is challenging due to the complexity of the flight 
process and the difficulty is system uncertainty nonlinearity multicoupled pa
rameters and all kinds of disturbances To control the attitude movement effec
tively a global adaptive H∞ control strategy was constructed that the controller 
synthesis was based on Lyapunov function instead of solving the Hamilton
JacobiIsaacs HJI partial differential equation The method overcomes the  
impact of timevarying parameters and unknown disturbances to the system 
Simulation results support the effectiveness of the dynamic model and the  
control strategy 
Keywords flappingwing micro aerial vehicle dynamic model nonlinearity 
adaptive H∞ control attitude control 
1 
Introduction 
Flappingwing Micro Aerial Vehicle FMAV has a great technological potential in 
remote surveillance missions inspection of infrastructures like dam walls and investi
gation of hazardous or dangerous environment 12 Safe autonomous flight is essen
tial for widespread acceptance of aircraft that must fly close to the ground and such 
capability is widely sought For example search and rescue operations in the setting 
of a natural disaster allow different vantage points at low altitude Likewise FMAV 
performing reconnaissance for the police news or the military must fly low enough 
that the environment presents obstacles Flying close to and among obstacles is diffi
cult because of the challenges in sensing small obstacles in three dimensions Some 
aspects of collision avoidance are easier for air vehicles than ground vehicles Any 
object close to the intended path of an air vehicle must be avoided as opposed to 
ground vehicles where deviations from the nominal ground plane indicate obstacles 
270 
Q Li and H Duan 
and are often not visible until they are close The use of helicopters rather than fixed 
wing aircraft also helps because in the worst case it is possible to come to a hover in 
front of an obstacle 3 Significant research interest and energy has been directed 
towards the development of autonomous helicopters due to their high payload to 
power ratio Helicopters however are extremely dangerous in practice due to the 
exposed rotor blades and are only suitable for autonomous applications where there is 
no chance of unintended humanrobot interaction The small size highly coupled 
dynamics and low cost implementation of indoor aerial robotic devices poses a num
ber of significant challenges in both construction and control 45  
The flight circumstances of FMAV are similar to that of birds or big insects fluid 
dynamics under low Reynolds numbers and unsteady aerodynamics Flapping flight is 
possible with only two degrees of freedom flapping “up and down” motion and 
feathering angular movement about the wing longitudinal axis 6 In flight process 
the propulsion of FMAV is aerodynamic force which is represented as the sum of 
instantaneous translational force and rotational force 7 The flight control is accom
plished by controlling the aerodynamic forces and moments generated by the airfoils 
during flapping However aerodynamic forces on airfoils are highly nonlinear and 
timevariable during a stroke cycle As a result the system dynamics cannot be ap
proximated by a linear timeinvariant model 9 Paper 10 describes the model iden
tification and attitude control system for a Micromechanical Flying Insect MFI In 
nonlinear optimal control theory nonlinear H∞ control method is robust and potential 
approach to the attitude control problem However the practical applications of the 
nonlinear H∞ control method still remain open due to the difficulty in solving the 
associated HamiltonJacobiIsaacs HJI partial differential equation There have been 
some replaceable attempts for it 1217 In this paper a global H∞ control strategy 
for nonlinear timevarying system is introduced where Lyapunov function is used to 
solve the H∞ problem instead of solving the HJI partial differential equation This 
method is applied to H∞ attitude control for FMAV The design process consists of 
two steps First without regard to the disturbances a global asymptotically stable 
adaptive controller is contrived Then a nonlinear H∞ controller synthesis is devel
oped to overcome the external disturbances 
2 
Dynamic Modeling of FMAV 
In flight process aerodynamic force gravity and propulsion from airfoils act normal 
to airframe According to the transform of inertia coordinate system and airframe 
coordinate frame we get 
sin
cos
 cos
sin
 cos
tan  cos
sin

q
r
q
r
p
q
r
ϑ
ψ
ψ
θ
ψ
ψ
ϑ
ψ
ϑ
ψ
ψ

=
+

=
−


=
−
−




 
1
whereϑ θ   ψ  are pitch yaw roll angles respectively p  q  r are angular ve
locities respectively In inertia coordinate system the dynamic equations of FMAV 
rotating around its centroid are as follows 
 
Modeling and Adaptive Control for FlappingWing Micro Aerial Vehicle 
271 


 


 



y
z
bx
wx
x
z
x
by
wy
y
x
y
bz
wz
z
p
J
J
rq
M
M
J
q
J
J
pr
M
M
J
r
J
J
qp
M
M
J

=
−
+
+
 =
−
+
+

 =
−
+
+




 
2
where
Mbx
 
Mby
 
Mbz
 are roll yaw pitch moments of airframe respectively 
Mwx
 
Mwy
 
Mwz
 are roll yaw pitch moments of airfoils respectively 
xJ  
y
J  
zJ  are the inertia moments of airframe We can simulate airfoils motion by 
 
sin2

 
sin2

 
cos2
 
cos2
l
L
r
R
l
L
l
r
R
r
t
ft
t
ft
t
ft
t
ft
ϕ
ϕ
π
λ
ϕ
ϕ
π
λ
φ
φ
π
φ
φ
φ
π
φ
=
−


= −
−

= −
− Δ


=
+ Δ

 
3
where 
 
ϕl t
 
 
ϕr t
 
 
φl t
 
 
φr t
are the realtime values of feathering angles and 
flapping angles respectively 
L
ϕ  
R
ϕ  
L
φ  
R
φ are the maximum of feathering an
gles and flapping angles respectively 
Δ lφ
 
Δ rφ
are the emendation values of flap
ping angles f is the wingbeat frequency λ is the phase shifting between feathering 
and flapping the subscripts l and L mean “left” r and R mean “right” The flight 
attitude control is accomplished by adjusting the angles
 
ϕl t
 
 
ϕr t
 
 
φl t
 
 
φr t
 
realtimely to change airfoil aerodynamic moments 
Mwz
 
Mwy
 
Mwx
 So the fol
lowing mathematical model can be elicited 
 
  
+
=
+


m x x
f x x
u
w  
4
where x is the state vector uis the input vector w is the unknown disturbance vec
tor 
 
m x is the positive definite coefficient matrix 
  
f x x is the system vector 
function and 
ϑ
θ
ψ
 
 
=  
 
 
x

1
2
3
f
f
f




= 





f




wz
z
wy
y
wx
x
M
J
M
J
M
J




= 





u

cos
cos sin
0
 
sin
cos cos
0
0
sin
1
ψ
ϑ
ψ
ψ
ϑ
ψ
ψ
−




= 





m x
 
2
1
sin
1 sin 
cos
cos
f
ϑψ
ψ
ϑ
ψ
ϑ
ψ
=
−
+






 sin  sin
cos
cos

x
y
J
J
θ
ϑ ϑ
ψ
θ
ϑ
ψ
+
−
+



 sin
cos
cos

bz
z
M
J
ψ ϑ
ψ
θ
ϑ
ψ
+
+
+



 
272 
Q Li and H Duan 
2
cos 
sin

sin
cos
f
ψ
ϑ ϑ
θ
ψ
ϑθ
ϑ
ψ
= −
+
−


 



cos
sin
 sin

z
x
J
J
ψ
ϑ
ψ θ
ϑ
ψ
+
−
−
+



cos
 sin

by
y
M
J
ϑ
ψ θ
ϑ
ψ
+
+
+



 
3
cos

 cos
 sin
y
z
f
J
J
ϑθ
ϑ
ϑ
ψ ϑ
ψ
= −
+
−
 


cos 
cos sin
sin
1
bz
z
M
J
θ
ϑ
ϑψ
ϑ
ψ
ψ
+
−
+
+

 
 
3 
Adaptive H∞ Control and Performance 
The design process of nonlinear H∞ control law consists of two steps 
Step1 the following adaptive controller is proposed 
 
  
r
D
P
=
+
+
+
+



u
m x x
f x x
k e
k e
v  
5
where
rx is the desired value of x 
r
=
−
e
x
x  
kD

kP
are symmetric positive 
definite matrices v is the new control input vector that will be designed 
When
w = 0
 the closed loop system 4 is global asymptotically stable 
with
v = 0
 When
w ≠ 0
 we will design a controllerv that still makes the system be 
global asymptotically stable 
Step2 an extended error vector is defined by 
s = 
e  
6
Substitute 56 into 4 the following closed loop system can be obtained 
  
    
    
v
w
t
t
t
t
t
=
+
+
s
g s
g s
v
g
s
w
 
7
where
1
  
  

D
P
t
−
= −
+
g s
m x
k s
k e  
1
  
  
 
v
w
t
t
−
=
= −
g s
g
s
m x
 
Define the output vector to be controlled as follows 
1
2
  
  
t
t
=
+
z
h s
h s
v  
8
where
T
1  
t =    
0
h s
e s

T
2  
   
h s t = 0 0
I
 
Define the following positive definite function 
1 T
  
 
2
V
s t =
s m x s  
9
Theorem 1 Given the attitude control system 4 if the controller parameters matrix 
kD
and
P
k are selected properly that the following conditions are satisfied 
T
 2

0
D
+
−

s
m
I
k
s≤
 
10
 
Modeling and Adaptive Control for FlappingWing Micro Aerial Vehicle 
273 
T
T
T
0
P
−
+

s ms
s k e
e e≤
 
11
Under the given control law 5 with 
 2
v =
s
 
12
The closed loop system is globally asymptotic stable Moreover given any positive 
scalar γ 1 
2
2
0
0
 
d
 
d
t
t
t
t
t
t
2γ


z
w
≤
 
13
is satisfied for all
t  0
and all piecewise continuous functions
w t
 
Proof Take the time derivative of
  
V s t  
d
 d




v
w
V
V
t
V
=
+ ∂
∂
+
+

s g
g v
g w  
14
Define 
2
2
2
     
 
 
H s v w t
V
t
t
=
+
− γ

z
w
 
15
Substitute 14 and 8 into 15 the following result can be obtained 
d
 d




v
w
H
V
t
V
=
+ ∂
∂
+
+
s g
g v
g w
T
T
2
T
1
1
+
+
− γ
h h
v v
w w  
16
Solve equations
∂H 
∂
w = 0
and
∂H 
∂ = 0
v
 the results can be obtained as fol
lows 
 
T
T
1
2
2
1
  
2
2
w
V
t
 ∂

=
= −


γ
∂
γ


s
w
g
s
s

T
T
1
1
  
2
2
v
V
t
 ∂

= −
=

 ∂ 

s
v
g
s
s
 
H    
s v w t
takes the maximum when
v = 1
v  
1
w =
w  
  
 
H
s v w t
≤
1
1
 

 
H
t
s v w
T
T
T
T
 2

P
D
=
−
+
+
+
−


s ms
s k e
e e
e
m
I
k
s
T
1 4 2
 1 4
+
γ
−
s
s  
17 
Substitute 1012 and γ 1into 17 we have 
  
 
0
H
s v w t
≤
 
18
It follows from 15 that 
274 
Q Li and H Duan 
2
2
2
 
 
0
V
t
t
+
− γ

z
w
≤
 
19
When the system has an undisturbed motion ie
w = 0
 the following result can be 
gotten 
  2
V
t
−

z
≤
 
20
It indicates that the closed loop system is globally asymptotically stable 
when
w = 0
and
v = 1
v  Integrate on both sides of the inequality 19 that 
2
2
2
0
0
0
 


 
d
 
d
t
t
V s
V s
t
t
t
t
−
−
+ γ


z
w
≤
 
21
The initial condition of the closed loop system 4 is
s0 = 0
 so
 0

0
V
=
s
 Moreo
ver it follows from 9 that
 
V s ≥ 0
 The result of 13 can be obtained and the 
proof is end 
4 
Simulation Study 
The dynamic model parameters of FMAV are
R =10cm

c = 5cm
 
ρ =120kg  m3
 
m =120g

V = 5m s

f =125Hz
 
60
L
R
φ
= φ
=
  
30
L
R
ϕ
= ϕ
=
  
20
l
r
φ
φ
Δ
= Δ
=
  
λ =10
  
α = 5
 
β = 0
  The initial control 
conditions are
0
ϑ
θ
ψ
=
=
=
 
0
= diag111
m

diag050402
Δ
m =
 


T
0
= 554520
f



Δ = 151505 T
f

10 sin 15 sin 05 sin T
t
t
t


= 




w
 
The 
controller 
parameters 
are
γ = 2

kD = diag255225165
 
kP = diag5554
 The desired attitude angles of are given with step shapes 
The simulation results are in Fig1Fig3 The real lines denote the tracking results 
of nominal system and the attitude angles are with subscript “1” the broken lines 
denote the tracking results of actual system with uncertainty and disturbance and the 
attitude angles are with subscript “2” From the simulation results we get to know 
that the desired attitude angles are tracked stably 
15 
0s
4s
0 
4s
8s
15  8s
12s
0 
12s
16s
r
t
t
t
t
ϑ





= −







≤
≤
≤
≤ ≤

0 
0s
4s
45 
4s
8s
45  8s
12s
0 
12s
16s
r
t
t
t
t
θ





= −







≤
≤
≤
≤
≤

10  0s
4s
0 
4s
8s
10  8s
12s
0  12s
16s
r
t
t
t
t
ψ
−




=







≤
≤
≤
≤ ≤
 
 
Modeling and Adaptive Control for FlappingWing Micro Aerial Vehicle 
275 
0
2 
4
6 
8 
10 
12 
14
16
20
15 
10 
5
0 
5 
10 
15 
20 
ts 
Pitch angleo 

1
ϑ  
…
2
ϑ  
0
2
4
6
8
10
12 
14 
16 
50
40
30
20
10
0
10
20
30
40
50
ts
Yaw angleo 

1θ  
…
θ2
 
 
Fig 1 The step response of pitch angle   
Fig 2 The step response of yaw angle 
0 
2 
4
6
8
10
12
14
16
15 
10 
5 
0 
5 
10 
15 
ts
Roll angleo 

1
ψ  
…
ψ 2
 
 
Fig 3 The step response of roll angle 
5 
Conclusion 
Through observing the wings motion of birds or insects in flight we investigated the 
airframe dynamics airfoil aerodynamic forces and moments of FMAV then its dy
namic model and mathematical model of flight attitude were developed The attitude 
control system of FMAV embodies nonlinearity parameter coupling uncertainty and 
all kinds of disturbances A novel control scheme — global adaptive H∞ control 
scheme was proposed Lyapunov function was used to solve the H∞ problem instead 
of solving the HJI partial differential equation Simulation results indicate that the 
tracking errors of attitude angles reach into an acceptant bound in finite time 
References 
1 Wzorek M Conte G Rudol P Merz T Duranti S Doherty P From Motion Plan
ning to Control – A Navigation Framework for an Autonomous Unmanned Aerial Vehicle 
In Proc of the 21st Bristol International UAV Systems Conference 2006 
2 Le BF Mahony R Hamel T Binetti P Adaptive Filtering and Image Based Visual 
Servo Control of a Ducted Fan Flying Robot In Proc of the 45th IEEE Conference on 
Decision  Control San Diego CA USA December 1315 pp 1751–1757 2006 
276 
Q Li and H Duan 
3 Scherer S Singh S Chamberlain LJ Saripalli S Flying Fast and Low Among Ob
stacles In Proc of the International Conference on Robotics  Automation Roma Italy 
April 1014 pp 2023–2029 2007 
4 La CM Papageorgiou G William CM Kanade T Integrated Modeling and Robust 
Control for Fullenvelope Flight of Robotic Helicopters In Proc of the 2003 IEEE Inter
national Conference on Robotics  Automation Taipei Taiwan September 1419 pp 
552–557 2003 
5 Pounds P Mahony R Hynes P Roberts J Design of a Four Rotor Aerial Robot In 
Australian Conference on Robotics  Automation Auckland November 2729 pp 145–
150 2002 
6 Lasek M Pietrucha J Zlocka M Sibilski K Analogies Between Rotary and Flapping 
Wings From Control Theory Point of View AIAA2001–4002 2001 
7 Sanjay PS Michael HD The Aerodynamic Effects of Wing Rotation and A Revised 
Quasisteady Model of Flapping Flight Journal of Experimental Biology 205 1087–1096 
2002 
8 Yan J Wood RJ Avadhanula S Sitti M Fearing RS Towards Flapping Wing 
Control for a Micromechanical Flying Insect In Proceedings of the 2001 IEEE Interna
tional Conference on Robotics  Automation Seoul Korea May 2126 pp 3901–3908 
2001 
9 Schenato L Campolo D Sastry S Controllability Issues in Flapping Flight for Biomi
metic Micro Aerial Vehicles MAVs In Proceedings of the 42nd IEEE International 
Conference on Decision  Control Maui Hawaii USA pp 6441–6447 December 2003 
10 Deng XY Schenato L Sastry SS Model Identification and Attitude Control for a Mi
cromechanical Flying Insect Including Thorax and Sensor Models In Proc of IEEE In
ternational Conference on Robotics  Automation Teipei Taiwan September 1419 pp 
1152–1157 2003 
11 Liu ZL Svobode J A New Control Scheme for Nonlinear Systems with Disturbances 
IEEE Transactions on Control Systems Technology 141 176–181 2006 
12 Magni L Nijmeijer H van der Schaft AJ A Recedinghorizon Approach to the Non
linear H∞ control problem Automatica 37 429–435 2001 
13 Aliyu MDS A Transformation Approach for Solving the HamiltonJacobiBellman eq
uation in H2 deterministic and stochastic optimal control of affine nonlinear systems Au
tomatica 39 1243–1249 2003 
14 Aliyu MDS An Approach for Solving the HamiltonJacobiIsaacs Equation HJIE in 
Nonlinear H∞ control Automatica 39 877–884 2003 
15 Aguilar LT Orlov Y Acho L Nonlinear H∞control of Nonsmooth Timevarying 
Systems with Application to Friction Mechanical Manipulators Automatica 39 1532–
1542 2003 
16 Chang YC An Adaptive H∞ Tracking Control for a Class of Nonlinear MultipleInput 
MultipleOutput MIMO systems IEEE Transactions on Automatic Control 469 1432–
1437 2001 
17 Su W Souza C Xie L H∞ Control for Asymptotically Stable Nonlinear Systems 
IEEE Transactions on Automatic Control 445 989–993 1999 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 277–283 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Distributed Staffs Integral Systems Design  
and Implementation 
Qing Xie GuoDong Liu ZhengHua Shu BingXin Wang and DengJi Zhao 
Key Laboratory for Optoelectronics  Communication of Jiangxi Province  
Jiangxi Science  Technology Normal University Jiangxi Nanchang PR China 
xqwyy163163com 
Abstract This article is based on the practical application of the communications 
industry to make fully use of the characteristic of distributed systems such as it can 
solve the dispersed organization and the data that needs to be communicated 
expand the organization with minimal impact balance the load to find the right 
system architecture So as to achieve the distribution application centralized 
management in the building characteristics of the management mechanism to 
achieve the different system needs of different parts 
Keywords communications industry distributed systems minimal impact 
distribution application centralized management 
1 
Introduction  
With the rapid development of information technology the enterprise will improve 
the timeliness requirements of the network system reliability and the cost in the 
future But for a large telecommunications company the principle of Centralized 
system is accounted for part of ratio the problem of centralized system is also a very 
significant and outstanding issue For example the host is leaded to an excessive 
burden on running the process There is often appearing the speed so slowly or the 
phenomenon of the collapse Although the centralized system was revaluating in the 
last century some enterprises is still using the centralized system Because they are 
using the original software and the software are often very expensive Therefore the 
principle of centralized systems has not been suited to the modern fast and efficient 
development of the communication 
So we bring up the distributed system applying the principle of a distributed sys
tem to create an economic benefit faster and stronger any extension of the processing 
system Data operations are scripting by Linux and interface browser is achieving 
through the web 
2 
The Concept 
The distributed system made up of more independent calculating devices and they can 
be able to communicate with each other Right users the systems like a computer 
278 
Q Xie et al 
 
This sentence contains two meaning Firstly the computer is autonomous in 
hardware Secondly users will make the entire system as a computer in software  
Generally speaking the goal of parallel processing uses all processors to perform a 
large task But each processor of the distributed systems usually executes the quasi
sovereign program sequence Due to resource sharing availability and fault tolerance 
for various reasons the processor needs to coordinate action with each other 
Distributed programming language is used to write the distributed program that is 
running on distributed computer The distributed program made up of a number of 
programs which can perform independently They are distributed in a distributed 
processing system and simultaneously executed on several computers With compared 
to the centralized programming language there are three characteristics Distribution 
Communication and Stability 
The integral system of distributed is the information management system which 
draw on the principles of distributed systems use cities as a unit design the clerk 
management subsystem integration individually for its own in order to meet the dif
ferent needs for various cities Using distributed systems realize distributed applica
tions and centralized management To establish the efficient and special management 
mechanism as the same time convenient for the system expansion without interven
tion with each other and coordinate each other  
3 
Design and Realization 
31 
The General Framework 
Supposing each city that is A B C D E F etc sees a separate computer and the 
province company is user All cities are combined to form a high operation without  
 
 
 
 
 
 
 
 
 
 
 
 
Fig 1 The general framework 
Distributed clerk 
 
 Integral system 
 
A city 
 
B city 
 
C city     
 
etc       
 
Distributed Staffs Integral Systems Design and Implementation 
279 
 
intervention each other So users that is the provinces through the system platform 
can directly observed the operation of each city Therefore we make a simple 
distributed system which is simple and general 
32 
The System Topology Schematic 
Distributed system structures are divided into two parts the surrounding system and 
the shop assistant integral system At the same time two parts share different function 
As is shown in the chart below 
 
Fig 2 The system topology schematic 
The hardware of Clerk integral system including double workstation database 
server business library server and Web server hardware performance according to 
the hardware performanceseveral severs can be merged into a computer Server 
shelves is seted in the support centerServer different user can use this system by the 
web different users can use the system by the web 
280 
Q Xie et al 
 
33 
System Integration Rules 
 
Fig 3 System integration rules 
 
Distributed Staffs Integral Systems Design and Implementation 
281 
 
34 
The Core Algorithm  
Integral templatebased operation is to make the rules and main points program 
separated No longer binding the complex points to the rules of the main program 
Instead it was singled out and formed some multiple templates This has the 
advantage of flexible fast actuating operation of integral rules and management of 
dictionary which reduces system maintenance costs and maintenance requirements 
reflects the userlevel data and the effective separation of systemlevel data 
 
The Integral Process Is as Follows First according to the local city market carry on 
the personalized modification of the template rules for the provinces uniform integral 
and submitted to the management system Second the integral management system 
process the submission of points around the city generate different Integral template 
Store in the system database Third processing the main points program regularly 
transferring to the relevant Integral template generating a temporary integral pro
gram processing the data from the boss by the program  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig 4 The integral process 
The Template Operation of Integration Disposal Has the Following Advantages 
First simplify the integration rule modification process business directors only need 
to submit by themselves then take effect no longer will need the intervention of  
system Maintenance personnel modify the main program code Second reduce 
maintenance costs Because of the integration rules modification there is no need to 
modify the main programs code thus significantly reducing the pressure of the 
system maintenance personnel while also reducing systemic risk Third make the 
integration rule configuration flexible because of the dictionary management of the 
integration rules business directors of every city can carry on all kinds of 
optimization configuration conveniently greatly enhance the market response speed 
The integral rule table 
Integral model  
Integral main program 
Integral subprogram  
282 
Q Xie et al 
 
The Total System Function Charts 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig 5 The total system function charts 
4 
Summary 
This system is using distributed systems under the direction of the principle it not 
only can achieve the independently in various prefectures and cities but also modify 
their template create its own characteristics conducive to head office update policy 
adjustments more nearly and timely for the different market conditions while 
significantly reducing the investment of resources and equipment At present 
distributed systems is everywhere commercial academic government and family 
These systems usually provide to share resources such as color printers or scanners 
and other special equipment means of sharing data It is extremely important for our 
informationbased economy It is an example of the distributed system to calculation 
at the same time it is more and more popular in providing computing resources and 
services Distributed systems is more challenging to solve subproblems through 
parallel computing to provide higher performance and they also provide greater 
availability of certain components to prevent failure 
Integral management system 
functions framework  
Staff Servic
es  
OA  
Statistical 
analysis  
System 
maintenance  
Employer 
Information 
 
 
 
Password 
modification  
Integral 
Statistics  
Number 
maintenance 
 
 
 
Authority 
maintenance  
Account 
registration  
 
Distributed Staffs Integral Systems Design and Implementation 
283 
 
References 
1 
George C Jean D Tim K Distributed Systems Concepts and Design 3rd edn Addi
sonWesley Pearson Education 2001 
2 
Garofalakis M Mining Sequential Patterns With Regular Expression Constraints IEEE 
Transactions on Knowledge and Data Engineering 143 530–552 2002 
3 
Franks RG Performance Analysis of Distributed Server Systems Carleton University 
Ottawa 1999 
4 
Reiser M Lavenburg S Mean Value Analysis of Closed Queuing Networks 
ACM 283 629 1981 
5 
Reiser M Mean Value Analysis of Queuing Networks A New Look at An Old Problem 
In Performance of Computer Systems pp 63–77 NorthHolland Amsterdam 1979 
6 
John D Rich F Tai J Web Server Performance Measurement and Modeling Tech
niques Performance Evaluation 33 5–26 1988 
7 
CaracaValente JP LopezChavarrias I Discovering Similar Patterns in Time Series 
In Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Dis
covery and Data Mining USA Boston Massachusetts pp 497–505 2000 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 284–292 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Research of QoCaware Service Adaptation in Pervasive 
Environment 
Di Zheng1 Qingwei Xu2 and Kerong Ben1 
1 Department of Computer Science Naval University of Engineering  
Wuhan Hubei China 430033 
2 Communication Institute Beijing China 100085 
Abstract Existing contextaware middleware are designed to automatically 
adapt its behavior to changing environment such as effectively selecting 
services for adaptation according to the user’s current context However with 
the increase of pervasive services and the kinds of sensors the importance of 
information quality becomes more urgent We should continuously 
monitorcapture and interpret the environment related information efficiently to 
assure efficient context awareness Many attentions have been paid to the 
research of the contextaware pervasive applications and contextaware service 
adaptation However little of them pay attention to the quality based service 
adaptation Therefore we propose a quality management middleware to support 
QoC management for service adaptation By using this framework we can 
configure different strategies to refinery raw context discard duplicate and 
inconsistent context so as to protect and provide QoSenriched context 
information of users to support contextaware service adaptation 
Keywords QoC middleware contextaware pervasive service adaptation 
1 
Introduction 
In recent years contextawareness has become one of the core technologies in 
pervasive computing environment gradually and been considered as the indispensable 
function for pervasive applications1Many research efforts have been done for 
gathering processing providing and using context information 2  
At the same time with the help of contextware middleware we can efficiently 
select services for adaptation according to the user’s current context And one service can 
dynamic reconfigure the software components of it or request the help of other 
services can be a efficient approach for addressing contextawareness of applications 
The service can add replace or remove the component of it and also change the 
value of its variables However with the increase of the scale of applications we can not 
make sure all the contexts from different sources are valid and certain So Buchholz et al 
3 defined QoC “as any information describing the quality of information that is used as 
context” firstly Then following researchers have characterized context information by 
certain welldefined QoC aspects such as precision accuracy completeness upto
dateness security and so on 48 But these studies evaluate quality only on some 
 
Research of QoCaware Service Adaptation in Pervasive Environment 
285 
 
aspects ie they do not consider complex and comprehensive applications which need pay 
attention to different context configuration and adapt to the user as wells as to the 
environment the devices and the complex relationships between them adaptively and 
efficiently  
In our previous works we have put forward a middleware for the contextaware 
servicebased applications so as to make these applications can be adapted more easily 
than traditional applications by simply adding and deleting services91011 Based on 
this middleware we use the agents to support the configuration of different quality factors 
Furthermore the strategies used by these agents can help us evaluate raw context discard 
duplicate and inconsistent context so as to protect and provide QoSenriched context 
information of users to support quality based service adaptation 
2 
Quality Based Service Adaptation 
21 
Agent Based Quality Management of Context 
As depicted info figure 2 we divide the entire context –aware process into five layers 
including sensor layer retriever layer deal layer distribution layer and application 
layer Different from existing methods we pay attention to the quality management of 
context through all these layers 
Firstly in the sensor layer we set agents supporting different threshold to 
implement auto context discarding The agents can be configured with one or more 
threshold by this way we can reduce the number of the raw contexts Secondly in the 
retriever layer we use the context quality index to describe the quality of the contexts 
All these factors are decided by user’s demand and they may be different at all in 
various applications Furthermore we use the agents to complete the computation of 
these factors  
 
 
Fig 1 Agent based Quality Management of Context 
Then in the deal layer we expand existing context dealing process with the 
duplicate context discarding and inconsistent context discarding to provide more 
286 
D Zheng Q Xu and Kr Ben 
 
accurate and more efficient contexts for the applications All the algorithms are 
configured in the duplicate and inconsistent dealing agent 
At last in the distribution layer and application layer we expand traditional 
contextaware componentservice adaptationdeployment algorithms with the help of 
the incoming contexts This process is also helped by the distribution agent 
22 
QoCaware Context Processing Procedure  
The detailed qualityaware context processing procedure is shown in Figure 2The 
first step is the raw context gathering in which raw contexts from various sensor 
sources are collected during a fixed short period In this step we will use one or 
multiple thresholds to refinery the raw context 
The second step is the duplicate and inconsistency resolution during context 
interpretation and aggregation If there are some contexts having the same identifier 
or the same namevalue pairs we will check the sources of context If they have 
different sources then some errors may occur and we should check the gathering of 
the contexts If these two context objects are from the same source then we check the 
time when these context objects are generated If they have the same timestamp then 
it means that they are the exact duplicate of each other and anyone of them can be 
discarded as well as keeping the other one If they have the different timestamps it 
means that these are the duplicate contexts and will be discarded  
Then we apply rulebased reasoning to generate highlevel contexts The user
defined rules are in the form of Jena generic rules without negation and “or” 
operation The two reasoners are configured as “traceable” in order to facilitate 
updating dependency graphs in context repository though more memory is required 
After that we use inferred highlevel contexts to update the context repository and 
notify applications which register context triggers 
 
 
Fig 2 Qualityaware Context Processing Procedure 
 
Research of QoCaware Service Adaptation in Pervasive Environment 
287 
 
23 
Typical Quality Factors Used for Adaptation 
In pervasive environment there may be lots of factors affecting the adaptation For 
example we can use precision to reflect the granularity how the context can describes 
the real world For numeric context information the value described with three 
significant figures eg 322 is more precise than with two significant figures ie 
32 There are many other factors like this and they can help us improve the efficiency 
of the middleware as well as make the applications be adaptive to the environment 
more accuracy As follows are several typical quality factors 
 
Precision It is used to reflect the granularity how the context can describes the 
real world For numeric context information the value described with three 
significant figures eg 322 is more precise than with two significant figures 
ie 32 
 
Completeness It is used to describe the relative ration of the available context 
versus the entire collected context 
 
Freshness It is a tempo definition and it is used to define the time that elapses 
between the determination of context information and its delivery to a requester 
 
Certainty It is described as the certainty of the context When we have similar 
contexts from different sensors we should choose the right context with the help 
of certainty 
 
Relativity It is used to describe the frequency of the context Usually the 
context will be reused in a short period So we think the context being reused is 
relative 
 
Usability Usability of context information will be measured by comparing the 
granularity of context information presented by the context object and the 
granularity of context object that is required by a context consumer 
 
Accuracy It is used to describe the degree of the realness that the context can 
reflect the real environment The lower accuracy may affected by many reasons 
such as sensors and wrong dealing of context 
 
Uptodateness This quality measure indicates the degree to use a context object for 
a specific application at a given time 
The services including in different applications may pay attention to different 
factor one or their combination For example Healthcare patient monitoring service 
This service is an important part of HIS and it pays more attention to the time and the 
accuracy Supposing the service get an input context that a patient having heart 
disease may be in trouble then the doctors and the nurses may come to help him 
soon If the context is wrong the dealing is useless But if the context is delayed then 
the life of the patient may be lost So we can use the combination of delay and 
accuracy to help the service complete the adaptation The percent of these two factors 
can be adjusted by the users according to their demand 
24   QoCaware Adaptation 
To different service we use the configuration file of different factors to help the 
adaptation firstly For example we can configure the file as follows 
288 
D Zheng Q Xu and Kr Ben 
 
1
1
2
3
1
2
3



 

 

 
1
A D A
S
U p
C x t O b j
W
A
C x t O b j
W
C
C x t O b j
W
W
W
W
=
+
+
+
+
=
 
These rules are used in the case the service is used to open the device automatically 
when it finds the user is walking near In this equation
1
W 
2
W  and 
3
W  represent 
the weight of these three factors and we can configure them as we need We choose 
the value as 03  04 04 for accuracy and certainty are more important 


Up CxtObj  represents age of the context object CxtObj  by taking the difference 
between the current time and the measurement time of that context object 




curr
m easu
Age C xtO bj
t
t
C xtO bj
=
−
 


1









0 
A g e C x tO b j
T im e P e rio d
C x tO b j
U p C x tO b j
if
A g e C x tO b j
T im e P e r io d C x tO b j
o th e rw is e

−


=
  





 
The value of uptodateness and hence the validity of context object CxtObj  
decrease as the age of that context object increases 
 


A CxtObj  represents accuracy as follows 


CorrectnessProbability
A CxtObj
= MinimumCorrectnessProbability
 
We use CorrectnessProbability  to represent current correctness probability of 
context and use MinimumCorrectnessProbability to represent the minimum 
correctness probability defined by user If the ratio is larger than 1 the freshness may 
be good According to the difference of user’s demand we can use different 


A CxtObj  to select sensor source 
 


C CxtObj  represents certainty as follows 
1


1



0





1
N u m b e ro fA n s w e r e d R e q u e s t
C O C x tO b j
N u m b e r o fR e q u e s t
if F
C x tO b j
a n d C x tO b j
n u ll
C
C x tO b j
N u m b e r o fA n s w e r e d R e q u e s t
C O
C x tO b j
o th e r w is e
N u m b e r o fR e q u e s t
+

×

+

 
≠
 
 
≠
= 

×
+


 


CurrentFreshInterval
F CxtObj
= MinimunFreshInterval
   
0
0






m
j
j
n
i
i
C O C xtO bj
ω
σ
ω
σ
=
=
= 

 
We use NumberofAnsweredRequest  to represent the number of the reply requests and 
use NumberofRequest to represent the sending requests Furthermore we use the ratio 
to represent the certainty with the help of completeness 


CO CxtObj and freshness 


F CxtObj  If the ratio is more close to 1 the resolution may be better According to 
the difference of user’s demand we can use different 


C CxtObj  to select sensor 
source 
By using the configuration like this the service will compute the adaptation value 
and decide to react to the circumstance by itself 
 
 
Research of QoCaware Service Adaptation in Pervasive Environment 
289 
 
Algorithm 1 Algorithm for QoC Adaptation 
INPUT New arrived context  
1 
Configure the service demanding for the quality factors 
2 
Configure the distribution Agent 
3 
Set the inconsistent context discarding algorithm 
4 
While true 
5 
get the identifier of contexts 
6 
if   There exists contexts with same ID  
7 
then 
8 
       if sourceID of both context objects match 
9 
      then 
10             if timestamp of both context objects match 
11            then 
12              Find duplicate contexts and discard anyone 
13         end if 
14       else 
15           Check the context gathering 
16      end if 
17 else if There exists contexts with same namevalue pairs 
18               Discard one according to the quality tuple 
19         end if 
20 end if 
21 get the new instance of context in the queue of matching patterns 
pat 
que  
22 To all the patterns 
1
2


n
pat pat
pat  
23 In the pat’s trigger tgr   
24   if exists 
1
ins   in 
1
pat 
que    
2
ins  in 
2
pat 
que    and  
n
ins  in  

n
pat
que  and  tgr satisfy the constraint of 
1
ins ，
2
ins  ，…，
n
ins   
25       then 
26             if  the constraint of tgr is satisfied 
27            then 
28              We get inconsistency 
29             delete all the inconsistent context instances 
30              add the  remaining  instances to the repository 
31            end if 
32 end if 
33 reasoning the lowlevel context and get the highlevel context 
34 if the context asks  the service to adapt the changing of the environment and act 
to user’s activities 
35           Computing the 
 1

ADA S  of the service 
36          if the 
 1

ADA S  satisfy out configuration 
37                 Adaptation 
38         end if 
39 end if 
40 goto 4 
290 
D Zheng Q Xu and Kr Ben 
 
3 
Performance Results 
In fact the adaptation may be more costly for using the QoC management They offer 
a quality measure ie ‘probability of correctness’ which aids in increasing the utility 
of the decision to changeadapt context source So we should balance the effect and 
the cost of the QoC based adaptation  
Firstly we compare the overhead of the common context processing procedure as 
well as the procedure with QoCaware dealing As depicted in figure 3 the curve with 
minimum time represents traditional context dealing This time is composed of the 
time of sensing transferring reasoning and distribution The second curve above the 
bottom represents the dealing with replicate context discarding and the process may 
exhaust more time And the third curve above the bottom represents the dealing with 
replicate context discarding and inconsistent context dealing We use all inconsistent 
contexts discarding algorithm and we can find it may exhaust more time The fourth 
curve above the bottom represents the dealing with replicate context discarding 
inconsistent context dealing and quality factors computing 
 
 
 
Fig 3 The Overhead  and true probability of the QoCaware Context Dealing 
From the figure above we can see the QoC dealing may exhaust more time and the 
time is lower than 30 percent of the normal dealing time of the context In fact 
comparing to the effect of the QoC this degree of extra time may be accepted 
Secondly we will discuss the effect of the using of QoC As the discussion above we 
set several duplicate and wrong contexts in the context stream which may lead to 
wrong adaptation decisions In fact in our testing example the inconsistent context is 
more important for duplicate contexts may just lead to more time be used to get the 
high level context but the inconsistent context may lead to wrong results We record 
the number of right or wrong adaptation and compute the true probability of them We 
use the all inconsistent contexts discarding algorithm for ease 
We set different bad contexts for testing 
 
A random percent between 10 and 30 wrong contexts 
 
A random percent between 10 and 30 inconsistent contexts at the same time by 
the sensor pair 
 
A random percent between 10 and 30 duplicate contexts at the same time by the 
sensor pair 
 
Research of QoCaware Service Adaptation in Pervasive Environment 
291 
 
From the above figure we can see in all the procession the service adaptation with the 
help of QoC may have higher true probability Most of the wrong adaptation for the 
reason of inconsistent contexts or duplicate contexts may be eliminated by the quality 
management framework At the same time though we can use the QoC mechanism to 
improve the true probability of adaptation the effect of the wrong contexts can not be 
avoided To resolve this problem we need more complex algorithms In fact in most 
of the applications the wrong contexts may come from the failure of the sensors and 
they may be inconsistent with the value of the sensors nearby  Therefore our 
framework can support the efficient adaptation in most circumstances 
4 
Conclusions 
The diversity of the sources of context information the characteristics of pervasive 
environments and the nature of collaborative tasks pose a stern challenge to the 
efficient management of context information by sensing a lot of redundant and 
conflicting information Most of existing research just use the raw context directly or 
take just some aspects of the Quality of Context QoC into account In this paper we 
have proposed a middleware based contextaware framework that support QoC based 
service adaptation By this framework we can evaluate raw context discard duplicate 
and inconsistent context so as to protect and provide QoSenriched context 
information of users for efficient service adaptation In future work we will complete 
more experiments to discuss more aspects of the framework 
References 
1 Dey K Understanding and Using Context Personal and Ubiquitous Computing 51 4–7 
2001 
2 Chen G Kotz D A Survey of Contextaware Mobile Computing Research Hanover 
NH USA Tech Rep 2000 
3 Buchholz T Küpper A Schiffers D Quality of Context What It Is and Why We Need 
It In HPOVUA 2003 Geneva 2003 
4 Kim Y Lee K A Quality Measurement Method of Context Information in Ubiquitous 
Environments In ICHIT 2006 pp 576–581 IEEE Computer Society Washington DC 
2006 
5 Razzaque PNM Dobson S Categorization and Modelling of Quality in Context 
Information In Proceedings of the IJCAI 2005 2005 
6 Preuveneers D Berbers B Quality Extensions and Uncertainty Handling for Context 
Ontologies In Shvaiko P Euzenat J Léger A McGuinness DL Wache H eds 
Proceedings of CO 2006 Riva del Garda Italy pp 62–64 August 2006  
httpwwwcskuleuvenbedavypublicationscando06pdf 
7 Sheikh K Wegdam M Sinderen M Qualityofcontext and Its Use for Protecting 
Privacy in Context Aware Systems JSW 33 83–93 2008 
8 Manzoor A Truong HL Dustdar S On the Evaluation of Quality of Context In 
Roggen D Lombriser C Tröster G Kortuem G Havinga P eds EuroSSC 2008 
LNCS vol 5279 pp 140–153 Springer Heidelberg 2008 
292 
D Zheng Q Xu and Kr Ben 
 
9 Zheng D Jia Y Zhou P Han WH ContextAware Middleware Support for 
Component Based Applications in Pervasive Computing In Xu M Zhan YW Cao J 
Liu Y eds APPT 2007 LNCS vol 4847 pp 161–171 Springer Heidelberg 2007 
10 Zheng D Yan J Wang J Research of the Middleware based Quality Management for 
Contextaware Pervasive Applications In 2011 International Conference on Computer 
and Management Wuhan May 2011 
11 Zheng D Yan J Wang J Research of the QoCaware Service Selection for 
Middleware based Pervasive Applications In The 2nd International Conference on 
Biomedical Engineering and Computer Science Wuhan April 2011 
12 Zheng D Yan J Wang J Research of the Middleware based Quality Management for 
Contextaware Pervasive Applications In 2011 International Conference on Computer 
and Management Wuhan 2011 
13 Zheng D Yan J Wang J Research of the QoCaware Service Selection for 
Middleware based Pervasive Applications In The 2nd International Conference on 
Biomedical Engineering and Computer Science Wuhan April 2011 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 293–300 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Energy Efficient Filtering Nodes Assignment Method  
for Sensor Networks Using Fuzzy Logic 
Soo Young Moon and Tae Ho Cho 
College of Information and Communication Engineering Sungkyunkwan University 
Suwon 440746 Republic of Korea 
moonmoustaechoeceskkuackr 
Abstract Wireless Sensor Network WSN can enable contextaware services 
through sensing processing and reporting event information Due to limited 
resources WSNs are vulnerable to various malicious attacks In one of these 
attacks false event reports are generated to compromise the integrity of the 
sensor data In most filtering schemes every sensor node on a path from an 
event source to a sink node operates as a filtering node which verifies received 
reports and determine whether they are valid or false Hence even the valid 
reports are verified multiple times as they are forwarded toward the sink node 
causing unnecessary energy consumption In this paper we propose a filtering 
nodes assignment method to reduce the energy consumption while verifying the 
event reports The proposed method partitions the network into several areas 
and assigns filtering nodes for each area according to a fuzzy output value 
derived from the three inputs  the number of valid event reports received from 
the area the elapsed time since the last valid event report received from the 
area and the average hop count from the nodes in the area to the sink node The 
experimental results show that the proposed method conserves sensor nodes 
energy and increases the network lifetime with similar security level 
Keywords false report injection attacks filtering scheme SEF fuzzy logic 
1 
Introduction 
A wireless sensor network WSN is composed of sensor nodes which are equipped 
with sensing computation and communication capabilities and one or more sink 
nodes that connects the WSN to another network 1 2 Sensor nodes detect 
interesting events and report them to the sink node A sink node refines the data and 
provides it to users WSNs are prone to many security attacks due to their resource 
constraints and wireless communication 3 4 In false report injection attacks 5 an 
attacker can capture a few sensor nodes in the field and inject forged event reports 
through them The false reports are forwarded from the compromised nodes to the 
sink node using multi hop routing causing the energy depletion at the intermediate 
nodes and unnecessary response for fake events 
                                                           
  This research was supported by Basic Science Research Program through the National 
Research Foundation of Korea NRF funded by the Ministry of Education Science and 
Technology No 20120002475 
294 
SY Moon and TH Cho 
 
Filtering schemes 57 have been proposed to defend against false report injection 
attacks Statistical en route filtering scheme SEF 5 was proposed by Ye et al and 
its goal is to detect and drop false reports early in their phase Multiple sensing nodes 
generate and authenticate event reports collaboratively using their authentication 
keys Although SEF can detect and remove false reports early in their phase energy 
inefficiency can occur since a valid report is verified enroute many times We 
propose an event report verification limiting method for energy efficiency in WSN 
using fuzzy logic In the proposed method a sensor network is divided into several 
areas The sink node determines the reliability level of each area based on 1 the 
number of valid reports received from the area 2 the elapsed time since the last valid 
report received from the area and 3 the average hop count from the nodes in the area 
to itself The sink node also controls the number of filtering nodes for event reports 
from each area based on its reliability level 
The remaining sections of this paper are as follows In section 2 we summarize the 
operation of SEF In section 3 we explain our method in detail Section 4 shows the 
simulation results Section 5 concludes the paper and plans future work 
2 
Statistical Enroute Filtering SEF 
Statistical Enroute Filtering SEF 5 defends against false report injection attacks 
especially for large sensor networks SEF achieves early detection and dropping of 
false reports by performing collective generation of reports and enroute filtering 
using globallyshared authentication keys There are four phases in the operation of 
SEF key assignment phase report generation phase enroute filtering phase and sink 
verification phase The key assignment phase is executed only once but the other 
three phases are performed repeatedly In the key assignment phase every sensor 
node is associated with a key partition of the global key pool and loaded with some 
portion of keys in the partition After the phase the nodes are deployed in the sensor 
field randomly or manually In the report generation phase multiple nodes sense an 
event elect the center of stimulus CoS node among themselves and collaboratively 
generate and authenticate the final event report The CoS collects message 
authentication codes MACs created by other sensing nodes of distinct partitions 
and attaches them to the event report In the enroute filtering phase a receiving node 
checks if one of its keys has been used to generate a MAC in the report If so the 
node verifies the corresponding MAC in the report using its own key and forwards or 
drops it depending on the verification result In the sink verification phase the sink 
node validates all the MACs in the received report since it knows all the keys in the 
global key pool It detects and removes the report if at least one MAC was forged 
SEF can detect and drop false reports if the number of compromised nodes is smaller 
than the security parameter value 5 Fig1 shows the operation of SEF 
Each node except the sink node in fig 1 is assigned two authentication keys from 
one of three key partitions in the global key pool Kij means jth key in ith key partition 
An event report includes three MACs created by using K12 K23 and K31 respectively 
Every node on the forwarding path tries to verify a MAC in the received event report 
However it can verify one of the MACs only when it stores the corresponding key in 
its memory For example N1 does not know any of the three keys and therefore it just  
 
 
Energy Efficient Filtering Nodes Assignment Method for Sensor Networks 
295 
 
 
Fig 1 SEF Operation 
passes the received report to the next node N2 On the other hand each of N2 and N3 
knows K23 and K31 respectively and verifies the corresponding MAC and 
conditionally forwards the event report to the next node based on the verification 
result  The sink node knows all the authentication keys in the global key pool and 
hence it verifies every MAC in the received report and detects a false report  
In SEF every node is assigned as a filtering node and performs verification of 
received reports without considering how reliable the source nodes are The one hop 
verification probability OVP at each node is the same regardless of the event 
sources As a result valid reports are verified enroute repeatedly and unnecessary 
energy consumption for verifying legitimate reports occurs 
3 
Proposed Method 
31 
System Model and Assumptions 
We target sensor networks for monitoring applications in which selected nodes 
periodically generate and send event reports to the sink node We assume that sensor 
nodes are distributed randomly and their locations do not change Sensor nodes are 
equipped with limited memory 8 and communicate via wireless links Node 
compromise attack can occur although the sink node is safe from the attack The sink 
node can also authenticate its message to the sensor nodes 9 
32 
Motivation 
In SEF every node on a forwarding path is a filtering node that verifies a MAC in a 
received report Therefore a valid report is verified many times as it travels from 
source to the sink node especially when the path is long The repetitive verification 
operations result in unnecessary energy consumption and shorten the lifetime of 
sensor networks The proposed method controls the number of filtering nodes for each 
area based on its reliability to limit the number of verification operations If an area is 
more reliable than the others then smaller numbers of filtering nodes are assigned to 
the event reports originating from the area 
33 
System Operation 
After key assignment and node deployment the sink node partitions the sensor field 
into several areas based on geographic region The sink node then manages a 
reliability level table for the areas Figure 2 illustrates the partitioned field and the 
reliability level table 
296 
SY Moon and TH Cho 
 
 
 
Fig 2 Field partitioning and Rlevel initialization 
There are nine areas in the sensor field Each entry of the table contains the area 
ID the number of valid event reports received from the area  events the elapsed 
time since the last valid report from the area the average hop count Hop from the 
sink node to the nodes in the area and the reliability level Rlevel of the area The R
level ranges from one initial value to five 
After field partitioning and Rlevel initialization sensor nodes report events 
Periodically sensor nodes elect one of them as the CoS node which gathers MACs 
from other sensing nodes creates event report and sends the report to the next node 
Every event report should include the information of event location 
When a node on the forwarding path receives an event report it check whether it is 
a filtering node for the area of event source and whether it knows a key for generating 
one of MACs in the report Only when both conditions are true the node verifies the 
corresponding MAC and drops it if the verification fails Each node manages the list 
of areas for which it is a filtering node At first every node is the filtering node for all 
the areas in the network The list at each node can change independently as the 
network operates  
The sink node verifies all the MACs in the event reports received from areas in the 
field If one of the MACs is incorrect the sink node detects a false report drops it and 
resets the reliability level of the corresponding area to the level one In addition the 
sink node periodically eg every 10 reports received computes the reliability levels 
of areas and updates the reliability level table Whenever the sink node receives an 
event report it records the factors to compute the reliability levels of areas The 
relationship between the three factors – the number of valid event reports from an 
area the elapsed time since the last valid report received from the area and the 
average hop count from the nodes in the area to the sink node  and the reliability 
level  is as follows If the sink node receives many valid reports from an area and the 
elapsed time since the last valid report is short it judges the area as reliable 
Conversely it considers an area unreliable if few valid reports have been received 
 
Energy Efficient Filtering Nodes Assignment Method for Sensor Networks 
297 
 
from the area and or the elapsed time is long In addition an area close to the sink 
node is assumed to be more reliable than another far from the sink node When the 
reliability level of an area is changed the sink node reassigns filtering nodes for 
event reports from the area by determining the number of filtering nodes on the path 
from itself to the area Figure 3 shows the reliability level update operation 
 
 
Fig 3 Rlevel update 
In figure 3 the reliability levels of three areas change The sink node does not 
receive a valid event report from area 2 for 20 timeunits Hence the reliability level 
Rlevel for area 2 decreases from 4 to 1 In area 5 20 event reports occur and they 
are all valid As a result its Rlevel increases from 1 to 4 Area 9 sends 30 false 
reports and one of them is detected by the sink node Then the sink node resets the 
reliability level of area 9 to 1 In addition the number of valid event reports from the 
area becomes zero In the proposed method the sink node controls the number of 
filtering nodes for each area based on its reliability level An area with a high 
reliability level does not require many filtering nodes and vice versa In the proposed 
method the sink node sends a control message to adjust the number of filtering nodes 
for an area when its reliability level changes A control message includes an area ID 
remaining hop count and the number of filtering nodes for event reports originating 
from the area When a node on the path receives a control message it reduces the 
remaining hop count by one Then it checks if the remaining hop count is no more 
than the number of filtering nodes for the area If so it inserts the area ID to its list of 
areas for which it is a filtering node  
34 
Fuzzy Logic Design 
The sink node generates the reliability level of the area based on fuzzy logic 10 from 
the three factors The three factors may contain errors due to node and link failure or 
topology change of the WSN Further the same value for a factor may be considered 
high or low depending on the application It is efficient to use fuzzy logic due to these 
298 
SY Moon and TH Cho 
 
uncertainties and ambiguity to reduce error and generate suboptimal output without 
complex equations All the fuzzy input factors are normalized EventNum is the 
number of valid event reports that the sink node received from an area If all event 
reports originating from an area are valid EventNum becomes 100 EventNum is 
defined by three overlapping fuzzy sets  ‘Few’ 050 ‘Normal’ 0100 and 
‘Many’ 50100 A given value of EventNum can be included in multiple fuzzy 
sets at the same time and all the corresponding fuzzy sets are used to calculate the 
output value For example if the value of EventNum is between 0 and 50 both 
‘Few’ and ‘Normal’ are used EventNum represents the probability of an event 
report originating from an area being valid ElapsedTime is the elapsed time since 
the sink node receives the last valid event report from an area If all the event reports 
which occur at an area are false then the value becomes 100 Three overlapping 
fuzzy sets  ‘Small’ 12 ‘Medium’ 23 and ‘Large’ 3100 – define 
ElapsedTime There can be multiple fuzzy sets corresponding to a given value of 
ElapsedTime and all the matching fuzzy sets are used ElapsedTime is used for 
detecting false event report in an area HopCount is the average hop count from 
nodes in an area to the sink node HopCount of the farthest area from the sink node 
is defined as 100 Two overlapping fuzzy sets  ‘Near’ 160 and ‘Far’ 
30100 – exist for the input factor If HopCount is between 30 and 60 both 
fuzzy sets are used to compute the output value Rlevel is the reliability level of an 
area for given three input factors Its value is between 1 lowest level and 5 highest 
level There are five fuzzy sets for Rlevel  ‘Very Low’ ‘Low’ ‘Medium’ ‘High’ 
and ‘Very High’ Fig 4 shows the fuzzy membership functions for the input and 
output variables 
 
 
Fig 4 Fuzzy Membership Functions 
Given input values to the system the sink node derives the output value RLevel 
using fuzzy ifthen rules Table 1 represents a portion of the ifthen rules of the 
proposed method 10 
Table 1 Fuzzy ifthen rules 
Rule  
EventNum 
ElapsedTime 
HopCount 
RLevel 
1 
F 
S 
N 
H 
4 
F 
M 
F 
L 
8 
N 
S 
F 
M 
13 
M 
S 
N 
VH 
18 
M 
L 
F 
L 
 
 
Energy Efficient Filtering Nodes Assignment Method for Sensor Networks 
299 
 
The strategy of the rules can be summarized as follows If EventNum is high 
ElapsedTime and HopCount are low the sink node sets the Rlevel of the area 
high On the other hand if EventNum is low ElapsedTime and HopCount are 
high it sets the Rlevel of the area low For example in rule 13 if EventNum is 
many ElapsedTime is small and HopCount is near then the RLevel is very high 
4 
Simulation Results 
Simulation parameters for the proposed method are as follows The sensor field of 
1200 m × 240 m is divided into 80 areas of 60 m × 60 m size Both the sensing and 
transmission range of each sensor node are 30 m and there are 1920 nodes in the field  
The global key pool comprises 100 keys organized with 10 partitions of 10 keys 
each Each node is loaded with five keys from one of 10 partitions Every event report 
contains 5 MACs 1625125 μJ is consumed to transmit  receive a byte and 75 μJ 
for one filtering operation 5 We generated 300 event reports in one simulation run 
Shortest pathbased routing 11 was assumed and free fuzzy logic library 12 was 
used to calculate reliability values Figure 5 shows the energy consumption and 
detection probabilities of SEF and the proposed method 
 
 
a Energy consumption 
b Detection probability 
Fig 5 Simulation results FTR = 5 hop count  50 
OVP is the probability of verifying a MAC at each filtering node and determines 
security strength It can be shown that the proposed method consumes less energy 
than SEF when OVP is  01 The amount of energy saved increases as the OVP 
increases The detection probability of false event reports of the proposed method is 
lower than SEF However the difference is very small when OVP  01 
5 
Conclusion 
In this paper we proposed the energyefficient filtering nodes assignment method for 
sensor networks using fuzzy logic Existing filtering schemes do not consider 
different reliability levels of areas in the sensor field and consume much energy to 
repeatedly verify valid reports In the proposed method the sink node manages the 
reliability levels of areas in the sensor field and adjusts the number of filtering nodes 
300 
SY Moon and TH Cho 
 
for the areas based on their reliability levels Simulation results show that our method 
reduces energy consumption for enroute filtering of existing schemes without losing 
false report detection capability Future work will enable each sensor node to manage 
the reliability levels of its neighbor nodes This will control the filtering probabilities 
at each node for event reports from different nodes based on their reliability levels 
References 
1 Akyildiz IF Su W Sankarasubramaniam Y Cayirci E A Survey on Sensor 
Networks IEEE Communications Magazine 408 102–116 2002 
2 AlKaraki JN Kamal AE Routing Techniques in Wireless Sensor Networks a Survey 
IEEE Wireless Communication Magazine 116 6–28 2004 
3 Djenouri D Khelladi L Badache N A Survey of Security Issues in Mobile AdHoc 
and Sensor Networks IEEE Communications Surveys 2005 
4 Karlof C Wagner D Secure Routing in Wireless Sensor Networks Attacks and 
Countermeasures Elsevier’s Ad Hoc Networks Journal Special Issue on Sensor Network 
Protocols and Applications 123 293–315 2003 
5 Ye F Luo H Lu S Zhang L Statistical Enroute Filtering of Injected False Data in 
Sensor Networks IEEE JSAC 234 839–850 2005 
6 Lee HY Cho TH Key InheritanceBased False Data Filtering Scheme in Wireless 
Sensor Networks In Madria SK Claypool KT Kannan R Uppuluri P Gore MM 
eds ICDCIT 2006 LNCS vol 4317 pp 116–127 Springer Heidelberg 2006 
7 Nghiem PT Cho TH A Fuzzybased Interleaved Multihop Authentication Scheme in 
Wireless Sensor Networks Journal of Parallel and Distributed Computing 695 441–450 
2009 
8 Xbow Sensor Networks httpwwwxbowcom 
9 Perrig A Szewczyk R Tygar JD Wen V Culler DE SPINS Security Protocols 
for Sensor Networks Wireless Networks 85 521–534 2002 
10 Yen J Langari R Fuzzy Logic Prentice Hall 1999 
11 Intanagonwiwat C Govindan R Estrin D Directed Diffusion A Scalable and Robust 
Communication Paradigm for Sensor Networks In Proc of MOBICOM pp 56–67 ACM 
2000 
12 FFLL httpffllsourceforgenet 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 301–308 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Sentiment Analysis with Multisource Product Reviews 
Hongwei Jin Minlie Huang and Xiaoyan Zhu 
State Key Laboratory of Intelligent Technology and Systems 
Tsinghua National Laboratory for Information Science and Technology 
Department of Computer Science and Technology Tsinghua University 
Beijing 100084 China 
csjinhongweigmailcomaihuangzxydcstsinghuaeducn 
Abstract More and more product reviews emerge on Ecommerce sites and 
microblog systems nowadays This information is useful for consumers to know 
the others opinion on the products before purchasing or companies who want 
to learn the public sentiment of their products In order to effectively utilize this 
information this paper has done some sentiment analysis on these multisource 
reviews For one thing a binary classification framework based on the aspects 
of product is proposed Both explicit and implicit aspect is considered and 
multiple kinds of feature weighing and classifiers are compared in our 
framework For another we use several machine learning algorithms to classify 
the product reviews in microblog systems into positive negative and neutral 
classes and find OVASVMs perform best Part of our work in this paper has 
been applied in a Chinese Product Review Mining System 
Keywords product review sentiment analysis microblog SVM 
1 
Introduction 
With the development of Internet more and more customers get used to purchasing 
products on Ecommerce sites such as 360buy1 and Newegg2 They also write reviews 
on the products after using them which produce a large number of reviews on the 
Internet In addition microblog a system that allows users to post messages of no 
more than 140 words and share information instantaneously based on the relationship 
between users is under rapid development such as Twitter3 Sina microblog4 and 
Tencent microblog5 A lot of microblogs contain latest product reviews 
Reviews from the above two large data sources contain much useful information 
for users and companies Users can make better purchasing decisions based on these 
reviews while companies can also analyze customers satisfaction according to these 
reviews and further improve the quality of their products Since there is a mass of 
                                                           
1  httpwww360buycom 
2  httpwwwneweggcomcn 
3  httpwwwtwittercom 
4  httpweibocom 
5  httptqqcn 
302 
H Jin M Huang and X Zhu 
 
product reviews and a single user cannot read all of them automatically mining the 
reviews from multiple sources is particularly important 
Most reviews in Chinese Ecommerce sites are labeled with advantage or 
disadvantage which is naturally suitable for binary classification The stateofart 
research in Chinese sentiment analysis mainly focuses on the whole review 
classification while customers often desire a more detailed understanding of 
products For example they want to know others opinion on the battery of a cell 
phone Therefore we propose a framework of sentiment classification at aspect level 
to solve this problem In our framework not only explicit but also implicit aspects are 
taken into account To our knowledge no implicit aspect discovery work of product 
review in Chinese language has been reported before For the reviews of each aspect 
the unigram features of words are used as text features We also compare the 
performance of three feature weighing strategies three reduction dimension and three 
classification approaches  
The sentiment analysis for reviews of products on microblogs is in its infancy 
Besides the microblogs that express opinion on the products some microblogs only 
give some statements relative to the products which contain no sentiment polarity or 
are neutral Therefore in this paper we exploit linear regression multiclass 
classification twostage classification and Mincut model optimization to classify the 
product related microblogs into three classes and compare the performance of these 
methods 
2 
Related Work 
Sentiment classification mainly includes two methods supervised learning and 
unsupervised learning Turney1 proposed a simple unsupervised method to classify 
reviews into positive and negative categories Pang2 used Naive Bayes NB Max 
entropy ME and SVM separately as supervised learning algorithms for binary 
classification of reviews Besides these Pang3 exploited regression and OneVsAll 
SVMs to predict the score for classification namely the multiclassification which 
can also be realized by combining binary classifiers in a twostage manner Pang4 
and Su5 both optimized their multiclassification results using the Mincut model 
For sentiment classification in microblogs Go6 was the first to classify Twitter 
data into two classes positive and negative Barbosa7 classified Twitter data into 
three classes in a twostage manner Jiang8 analyzed topic related sentiment for 
Twitter text Based on the twostage binary classification result and the forwarding 
relationship between users the performance was promoted using the method of graph 
But for Chinese microblogs systems only text can be obtained instead of the 
relationship between users and microblogs 
3 
Binary Sentiment Classification Based on Product Aspect 
The overall flow chart about polarity classification positive and negative based on 
product aspect is shown in Fig1 
 
Sentiment Analysis with Multisource Product Reviews 
303 
 
 
Fig 1 The polarity classification positive and negative based on product aspect framework 
Aspect Aspect Word Product aspect refers to some attribute component or 
function of the product The word or phrase used to represent the aspect of product is 
called the aspect word Aspect is actually the concept at semantic level and the aspect 
word is the external presentation form of the aspect Different categories of products 
generally have a different set of aspects Product aspect can be divided into explicit 
aspect and implicit aspect Explicit aspect refers to the aspect word which describes 
the product performance or function can be directly found in the product reviews 
Implicit aspect may be found after the sentence semantic understanding because there 
are no aspect words in review sentence  
31 
Preprocessing 
In order to get relatively clean text about product reviews messy code will be 
removed in the preprocessing stage according to a userdefined messy code list Next 
we use the word segmentation software tools to make the sentence processed There 
are some words that appear in almost all of the text which are called stop words The 
text content is represented more accurately after deleting stop words Some words in 
the traditional stop words list is retained because they have sentiment polarity which 
is useful information for sentiment classification 
32 
Build Aspect Words List 
We use a statisticsbased algorithm to automatically build the vocabulary about the 
product aspects Here is the algorithm 
1 Count occurrences number of all the nominal words 
We count the occurrences number of all the unigram whose part of speechPOS 
tagging is noun in the reviews of same product category 
2 Filter highfrequency words 
The final aspect words list is filtered by threshold of high frequency and product 
specifications 
304 
H Jin M Huang and X Zhu 
 
33 
Review Sentences Breakup 
First of all we break a review into sentences according to various sentence end 
punctuations each of which is further cut into short sentences SS with comma Then 
each SS is assigned to different sentiment categories of different aspects according to 
the aspect vocabulary made in advance Finally if an SS has implicit aspect its words 
should also be correctly distributed 
34 
Implicit Aspect Discovery 
For example if there is a word beautiful in the sentence and without any explicit 
aspect word appeared we also need to know beautiful is likely in the description of 
the products appearance which is just the implicit aspect of product 
The discovery of implicit aspects needs to balance the following factors 
1 Calculate mutual information MI scores between adjectives in the current SS 
and each aspect Adjectives with most relevant aspects will be calculated and the 
aspect of the entire SS is determined by voting 
2 If the above rule doesnt work the SS is just assigned to the previous SSs aspect 
35 
Text Feature Representation 
After the processing mentioned above we get all the review sentences in each aspect 
in each category and vector space model is employed to represent them  
Features of the review text using wordbased unigram but different word have the 
different ability and importance to represent the text which is called weight In this 
paper BOOL TF and TFIDF are used as feature weights BOOL weight means that 
if the count of the term appears in the current document greater than zero the weight 
is 1 otherwise 0 TF term frequency refers to the frequency of the words occurrence 
in the file IDF inverse document frequency measure the common importance of the 
word In addition we use chisquare statistics to reduce feature dimension 
36 
Classifier 
Naive Bayes Navie Bayes is a simple model which calculates posterior probability of 
each class based on the priori probability and the likelihood Class with the largest 
posterior probability is assigned as the class of the document 
KNN The basic idea of the KNN algorithm is considering the K nearest the most K 
similar texts in the training set of the new given text The label of the new text is 
determined by these K texts 
SVM SVM is a popular classification algorithm which compresses the raw data set to 
support vector set and learn a decision hyperplane in the vector space This 
hyperplane best splits the data points into the two classes  
In this paper NB KNN and SVM are implemented using data mining tool Weka6 
                                                           
6  httpwwwcswaikatoacnzmlweka 
 
Sentiment Analysis with Multisource Product Reviews 
305 
 
4 
Sentiment Analysis on Product Reviews in Microblog System 
41 
Text Feature Extraction 
Like mentioned in Section 31 stop words and messy code in the microblog will be 
removed and then the Chinese word segmentation and POS tagging will be done The 
vector space model is still employed to represent each microblog The features are 
unigrams and the weight is BOOL value However we can take the advantage of 
characteristic of microblog to portray it better and reduce the feature dimension 
Emoticons For example the text form of emoticon 
 in microblog text is ha ha 
All the positive emoticons will be converted into token POS and all negative into 
token NEG according to the manual defined emotions list 
Usernames Forwarding mechanism of microblog systems makes microblog amazing 
transmitted It is in the form of  + username such as ryanking1219 So we use 
string USERNAME to instead all words beginning with   
Links Users like to include the URL which usually begins with http when they 
share videos or news such as httptcnaCKddG All website links are normalized to 
the token URL 
Topics In microblog systems topic starts with  and also ends with  We will 
replace all the string in this form with an equivalence class string TOPIC 
42 
TwoStage SVM 
Step 1 Subjectivity Classifier 
Neutral samples in the training data is considered as positive examples while positive 
and negative samples as negative cases to train the classifier Do binary classification 
on the test data for subjective and objective detection 
 
Step 2 Polarity Classifier 
Positive samples in the training data is considered as positive examples while 
negative samples as negative cases to train the classifier Do binary classification on 
the test data which is correctly divided into subject class in Step 1 for positive and 
negative detection 
43 
Minimum Cut Model Optimization 
Inspired by work of Pang4 and Su5 we also use Minimum cut Mincut model to 
optimize the Twostage SVM result Binary classification with Mincut in graph is 
based on the idea that similar items should be split in the same cut We build an 
undirected graph G with vertices s t v1 v2 … vn s is the source and t is the sink 
and all items in the test data are seen as vertices vi Each vertex v is connected to s and 
t via a weighted edge The weight is the estimation of the probability converted from 
SVM classifier output Each edges vi vk with weight assocvi vk expresses the 
306 
H Jin M Huang and X Zhu 
 
similarity between vi and vk and how important they should be in the same class Then 
we remove a set of edges to divide graph into two disconnected subgraphs The 
vertices via s are positive instances and the vertices via t are negative instances We 
penalize when putting highly similar items into different classes so the best split is 
one which removes edges with lowest weights This is exactly the Mincut of graph 
Because the capacity of the Mincut equals the max flow value we can employ 
EdmondsKarp algorithm to compute the Mincut 
In experiment we set 






cos


cos


0

i
j
i
j
i
j
v v
if
v v
t
assoc v v
otherwise
α
α



= 

 
1
where cosvi vj is the text similarity between vi and vj Constant α  is the scale factor 
of association scores Only scores higher than threshold t will be taken into 
consideration 
44 
Regression 
Text classification is also a problem that utilizes text features to predict a category 
label and similar entries should have similar category labels The most classic model 
is the linear regression function which is actually a linear weighted sum of all the 
features The final score is rounded as a category label 
45 
OVASVMs 
Although the original SVM is used to solve binary classification problem we can 
combine several SVM classifiers to achieve multiclass classification such as one
versusall method for short of OVASVMs When Training each classifier we mark 
one category samples as positive examples and all the rest samples as negative 
examples So that k categories of samples can construct k SVM classifiers in turn 
Class label of the test sample assigned to the class which has the largest classification 
function value 
5 
Experiments 
51 
Sentiment Classification Based on Product Aspect 
We downloaded 821 cell phone products data from the 360buy and Newegg and 
finally get 663537 advantages reviews and 314529 disadvantage reviews 
CHI dimension reduction selects 5 20 and 100 words most relevant to each 
aspect of each product category represented as CHI5 CHI20 and CHI100 Of 
course from the whole view of each product category the most relevant words of 
each aspect will be overlapped with each other 
Performance evaluation methods commonly apply PRECISION RECALL and F
values but these only represent local significance ie the performance of each 
aspect in each product category If you want to evaluate the overall classification 
performance all the aspects of product category need to be taken into account to 
calculate the Fvalue of the MICRO AVERAGING and MACRO AVERAGING 
 
Sentiment Analysis with Multisource Product Reviews 
307 
 
Microaverage gives the same weight to each document in each product category 
while Macroaverage gives the same weight to each aspect in each product category 
Results are illustrated in Fig2 
 
Fig 2 Cell phone experiment result 17 aspects 
The following conclusions could be drawn from the experimental result 
First the best combination is BOOLSVM Second the consequent of TF is very 
close to the TFIDF but is slightly lower than BOOL In addition the SVM 
classification performance is better than NB and KNN Last but not least CHI5 
result is similar with CHI20 and CHI100 even sometimes CHI5 perform best so 
dimension reduction is proved effective 
52 
Sentiment Analysis on Product Reviews in Microblog System 
We select 5 popular items including Nokia iphone 4s E72i Lenovo and 
Canon For all of these items we downloaded 2100 microblogs through the API 
provided by Sina microblog and Tencent microblog from October 2011 to November 
2011 We manually labeled all the microblogs and finally obtain 729 positive 345 
negative and 1026 neutral microblogs respectively The results are listed in Table 1 
Table 1 Result for microblog classification 
 
Regression 
OVASVMs
Twostage SVM
Twostage SVM + MinCut 
Accuracy 
516 
645 
626
640
 
As shown in Table 1 the OVASVMs classifier is best Twostage SVM classifier 
after optimization with the minimum cut model when parameter α  = 02 t = 05 has 
14 increase in performance and reach almost the same effect of OVASVMs 
308 
H Jin M Huang and X Zhu 
 
6 
Conclusion 
This paper makes sentiment analysis on product reviews from multiple data sources 
Firstly a binary sentiment classification framework based on the aspects of the 
product is proposed On the granularity of aspect we use unigram as review feature 
and adopt BOOL weight and SVM classifier to get the best results Secondly we 
classify our product review in microblog systems into three classes OVASVMs 
method offers the optimal result Finally Work in Section 3 has been partially applied 
in a Chinese Product Review Mining System7 and Section 4 will be applied soon 
References 
1 
Turney PD Thumbs up or Thumbs down Semantic Orientation Applied to 
Unsupervised Classification of Reviews In Proceedings of the 40th Annual Meeting on 
Association for Computational Linguistics ACL 2002 pp 417–424 Association for 
Computational Linguistics Stroudsburg 2002 
2 
Pang B Lee L Vaithyanathan S Thumbs up Sentiment Classification Using 
Machine Learning Techniques In Proceedings of the ACL 2002 Conference on Empirical 
Methods in Natural Language Processing EMNLP 2002 vol 10 pp 79–86 Association 
for Computational Linguistics Stroudsburg 2002 
3 
Pang B Lee L Seeing Stars Exploiting Class Relationships for Sentiment 
Categorization with Respect to Rating Scales In Proceedings of the 43rd Annual Meeting 
on Association for Computational Linguistics ACL 2005 pp 115–124 Association for 
Computational Linguistics Stroudsburg 2005 
4 
Pang B Lee L A Sentimental Education Sentiment Analysis Using Subjectivity 
Summarization Based on Minimum Cuts In Proceedings of the 42nd Annual Meeting on 
Association for Computational Linguistics ACL 2004 Association for Computational 
Linguistics Stroudsburg 2004 
5 
Su F Markert K Subjectivity Recognition on Word Senses via Semisupervised 
Mincut In Proceedings of Human Language Technologies The 2009 Annual Conference 
of the North American Chapter of the Association for Computational Linguistics NAACL 
2009 pp 1–9 Association for Computational Linguistics Stroudsburg 2009 
6 
Go A Bhayani R Huang L Twitter Sentiment Classification Using Distant 
Supervision Technical report Stanford Digital Library Technologies Project 2009 
7 
Barbosa L Feng J Robust Sentiment Detection on Twitter from Biased and Noisy Data 
In Proceedings of the 23rd International Conference on Computational Linguistics 
Posters COLING 2010 pp 36–44 Association for Computational Linguistics 
Stroudsburg 2010 
8 
Jiang L Yu M Zhou M Liu X Zhao T Targetdependent Twitter Sentiment 
Classification In Proceedings of the 49th Annual Meeting of the Association for 
Computational Linguistics Human Language Technologies HLT 2011 vol 1 pp 151–
160 Association for Computational Linguistics Stroudsburg 2011 
                                                           
7  http16611113818cReviewMiner 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 309–318 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Identifying CpG Islands in Genome  
Using Conditional Random Fields 
Wei Liu 12 Hanwu Chen1 and Ling Chen 23 
1 Department of Computer Science and Engineering  
Southeast University Nanjing 210096 China 
2 Department of Computer Science Yangzhou University Yangzhou 225127 China 
3 National Key Lab of Novel Software Tech Nanjing University Nanjing 210093 China 
yzliuwei126com 
Abstract This paper presents a novel method for CpG islands location identifi
cation based on conditional random fields CRF model The method transforms 
CpG islands location identification into the problem of sequential data labeling 
Based on the nature of CpG islands location we design the methods of model 
constructing training and decoding in CRF accordingly Experimental results 
on benchmark data sets show that our algorithm is more practicable and effi
cient than the traditional methods 
Keywords conditional random fields model CpG islands sequential data  
labeling 
1 
Introduction 
During the large scale genome sequencing project we probably can discover a new 
gene by detecting a CpG island 12 In genetics CpG islands 3are genomic regions 
that contain a high frequency of CpG sites but to date objective definitions for CpG 
islands are limited Therefore the prediction of CpG islands 4is theoretically and 
practically important in the mining and identifying new gene and early diagnosis of 
tumor 
In the past twenty years extensive efforts have been devoted to developing algo
rithms and criterias for identifying CpG islands GardinerGarden and Frommer first 
presented a technical standard for CpG islands detecting in 1987 Later Takai et al 5 
made some amendments and proposed a new technical standard of CpG islands which 
is helpful to remove Alu repeats In 2006 Hachenberg et al presented a new CpG 
islands recognition method named CpGCluster 6 based on objective physical dis
tance But since the criterias of CpG islands mentioned above are all artificially de
fined the CpG islands identified have diminutive biological meaning and they  
require tremendous computational time to get the results Many biologists are actively 
searching for more perfect criteria of CpG islands 7 which are helpful to identify 
true CpG islands of biological significance Wang Jinlong et al 8advanced a method 
to predict CpG islands based on fuzzy theory Shi Ouyan et al 9and Jiang Hong
jing et al 10respectively used HMM which has strict independence assumption to 
identify the location of CpG islands  
310 
W Liu H Chen and L Chen  
 
In this paper we present a new method for identifying CpG islands based on the 
model of conditional random fields 11 The method transforms the problem of CpG 
islands location identifying into the problem of sequential data labeling Based on the 
nature of CpG islands location we design the methods of model constructing training 
and decoding in CRF accordingly Experimental results on benchmark data sets show 
that our algorithm is more practicable precision and efficient than some traditional 
methods 
2 
Conditional Random Fields 
Definition1 Let 


1
2


n
X
X X
X
=
 and 


1
2


n
Y
Y Y
Y
=
 be the observation 
and labeling sequences respectively Suppose the graph G = V E of Y is a tree or in 
the simplest case，a chain where every edge is a cut of the graph By the fundamen
tal theorem of random fields 12Hammersley  Clifford 1971  the joint distribu
tion over the label sequence Y given X has the form 





 










 + 





∅


∈
∈
v V K
K
K
e E K
k k
v y v x
u g
f e y e x
x
y


| 

| 

exp
|
λ
ρθ
 
1
where V and E are the sets of vertexes and edges in G respectfully f is a transi
tional feature function and g is a state feature function The parameter 


1
2
1
2




q
l l
m m
=
 is estimated from training data




1
 
 

N
i
i
i
D
x
y
=
=
 
In the CRFs model the parameters 
k
k
f g  in 1 can be unified as the form 
of


1  
k
i
i
f y
 y x i
 and the joint distribution of chainstructured CRFs is written as 











−
=

i
j
j
j
j j
x i
y
y
f
Z X
y x
p
 
1
exp
1
|
λ
λ
 
2
Here Zx is an instancespecific normalization function as follows 
 










−
=
y
i
j
i
i
i
j j
y x i
f y
Z X


1
exp
λ
 
3
In 2 


1  
j
i
i
f y
 y x i
is a feature function with value 1 or 0 And the parameter
j
l in 
2 which is obtained during the training procedure is used to index the weight cor
responding to each feature function 18 A CRF model is specified by a group of 
feature functions
if and the corresponding weights
il  
 
Identifying CpG Islands in Genome Using Conditional Random Fields 
311 
 
3 
Identifying CpG Islands Using CRFs       
31 
Feature Selection 
311  Defining the States of Each Position  
We can see that there are eight states in the whole CRFs model illustrated in fig1 We 
define that the state “X+” indicates “the character X is in the CpG island” otherwise 
the state “X” indicates “the character X is out of the CpG island” 
where



 
X
Î A C G T
We can use the CRFs model mentioned above to give the 
most suitable labels for an observation sequence 
state 
A+ 
C+ 
G+ 
T+ 
A 
C 
G 
T 
The Released character 
A 
C 
G 
T 
A 
C 
G 
T 
Fig 1 A CRFs Model for CpG Island Detecting 
312   Feature Functions 
Based on the above defined eight states and the nature of CpG islands we set eight 
feature functions as follows 



 
 

1
1
1


 
 C 


i
i
i
i
i
i
y
y
x
f y
y x


= +
= 
=
= I
I
I
 



 
 

1
2
1




 C 


i
i
i
i
i
i
y
y
x
f
y
y x


= +
= +
=
= I
I
I
 



 
 

1
3
1
 


 A 


i
i
i
i
i
i
y
y
x
f
y
y x


= 
= +
=
= I
I
I
 



 
 

1
4
1
 
 
 A 


i
i
i
i
i
i
y
y
x
f
y
y x


= 
= 
=
= I
I
I
 



 
 

1
5
1


 
 G 


i
i
i
i
i
i
y
y
x
f
y
y x


= +
= 
=
= I
I
I
 



 
 

1
6
1




 G 


i
i
i
i
i
i
y
y
x
f
y
y x


= +
= +
=
= I
I
I
 



 
 

1
7
1
 


 T 


i
i
i
i
i
i
y
y
x
f
y
y x


= 
= +
=
= I
I
I
 



 
 

1
8
1
 
 
 T 


i
i
i
i
i
i
y
y
x
f
y
y x


= 
= 
=
= I
I
I
 
32 
Parameter Training  
In the CRFs model the parameter
j
l in 2 indexes the weight corresponding to each 
feature function and its value can be obtained by training from the known samples of 
CpG In the CRFs model we use maximum likelihood to train the samples for para
meter estimation 
Given a DNA sequence set
  
x i 
 the corresponding label sequence 
set
  
 

 
1

y i
i
N
=

 and eight feature functions defined above firstly we need to  
 
312 
W Liu H Chen and L Chen  
 
get the parameters 


 
12
8
j
j
l
=

 of CRFs model so as to maximize the condi
tional 
probability 
of 
the 
log 
likelihood 
 
 
 


1
log
N
i
i
i
L
P
y
x
l
l
=
= å
 
where
 
 


i
i
P
y
x
l
is defined by 2 
Since Newton’s Method costs large amount of time to compute the inverse of Hes
sian matrix and requires the Hessian matrix always being positive definite we use 
QuasiNewton method in the parameter optimization for CRFs model 
321   QuasiNewton Method 
Supposed the optimization problem is 
 
max
    st  
n
f x
x
Î R
Assuming that fx 
has 
continuous 
second 
partial 
derivatives 
on
 
x k
 
we 
would 
use 






1
2
1
1
k
k
k
k
k
f x
f x
f x
x
x
+
+
+

 

=

as the iterative formula instead of Hessian matrix  
Let the iterative formula be 


1
k
k
k
k
x
x
H
f x
+ =


 
 4
supposed that the iterative formula is 
1
k
k
k
k
H
H
A
B

=
+
+
 
5
Then we can get the values of 
k
A and
k
B  Moreover 
k
A and
k
B satisfy 
1
1
n
k
k
A
H 
=
=
å

0
1
n
k
k
B
H
I
=
= 
= 
å
 
After substituting 
k
A and
k
B  into 5 we can get the expression of
k
H  And the itera
tive formula can be obtained after substituting 
k
H  into 4 
322   Parameter Estimate 
For the CRFs model mentioned above the object function
 
f x of QuasiNewton 
Method is  
 
 
 
 






=
=
=
=
−
−
=
N
i
i
N
i
T
t
K
k
i
t
i
t
i
t
k k
Z X
x
y
y
f
L
1
1
1
1
1
log


λ
λ
 
Its first derivative is written as 
 
Identifying CpG Islands in Genome Using Conditional Random Fields 
313 
 
 
 
 


 

 

i
N
i
T
t
K
k
k
t
k
i
t
i
t
i
t
N
i
T
t
k
K
p y y x
y y x
f
x
y
y
f
L
|

 


1
1
1
1
1
1


=
=
=
−
=
=
−
=
∂
∂
λ
 
6
The kdimensional vector
 






∂
Ι ∂
∂
∂
∂
∂
=
∇
K
L
L
L
L
λ
λ
λ
λ


2
1
consists of elements 
k
dL
dl ，
（
12
k
K
=

） which is just the 
 
f x
in the iterative formula 4 of Quasi
Newton Method 
To calculate
   |  
i 
p y y
x
 in 6 we can use a dynamic programming method 
which is similar to the forwardbackward algorithm for Hidden Markov Model  
33 
Decoding Problem 
The 
decoding 
problem 
is 
described 
as 
follows 
 
Given 
DNA 
se
quences
  

1

x i
i
N
=

 
feature 
functions
1
8
f
 f
the 
corresponding 
weights


 
12
8
j
j
l
=

 and input sequence
1
2

n 
x
x x
x
=

 our goal is to label 
each 
position 
of 
x 
namely 
finding 
out 
label 
se
quence


1
2


   

i
n
y
y
y
y
y
=
Î +  


iy
=
 indicates this character is out of the 
CPG island otherwise 
iy  is in the island 
Because the label sequence to be solved satisfies the condition of the optimal sub
structure Viterbi algorithm which is based on the dynamic programming technique 
can be applied to solve the decoding problem of CPG islands In the Viterbi algo
rithm a Viterbi variable 

F i y
is defined to denote the possibility of optimal label 
sequence y’ on the processed subsequence x’=
1
2



i 
x x
x
 of the observation se
quence x=
1
2



l 
x x
x
 Accordingly 

Y i y
is defined to denote the label se
quence with the maximal possibility The formal definitions of 

i y
F
 and 

Y i y
are described as follows 




1
2
1
2
1
2



max




i
i
i
y y
y
i y
P
y y
y x x
x
l
F
=
 
and 




1
2
1
2
1
2



arg max




i
i
i
y y
y
i y
P
y y
y x x
x
l
Y
=
 
The algorithm gradually gets the optimal label in each position based on

F i y
until 
the whole label sequence is obtained The framework of the algorithm is depicted as 
follows 
 
314 
W Liu H Chen and L Chen  
 
Algorithm  CpGViterbi
Input


1

k
i
i
i
f
y
 y x
  feature functions 
       
kl
k = 18
   the corresponding weights 
X=
1
2



l 
x x
x
  Sequence to be labeled 
OutputY=
1
2



l 
y y
y
 The label sequence over x 
Begin  
1、Initialization 
   For all labels y set 

1 
1
y
F
=
； 
2、For i=2 to l do 
For all labels y calculate 
     












∗
−
=

k
k
k
y
y y x i
f
y
i
i y
  
exp
1
max




λ
φ
φ
 
     












∗
−
=

k
k
k
y
f y y x i
y
i
i y
 

exp

1
arg max


λ
φ
ψ
 
Endfor 
Endfor 
3、Termination  



ly
= Y l y
 
4、Computing label sequence
1
l
y
y ： 
   For i=l1 downto 1 do    

1 
1
i
i
y
i
y +
= Y
+
    
Endfor 
End  
4 
Experimental Results and Analysis 
41 
Experimental Environment  
From the database of human genome CPG islands 13we select 72 sequences in
cluded 80 islands as the training data set  and the remainders are used to test the pre
cision of our method  We use CRF++054 14 as the tool to train CRFs model All 
the experiments were conducted on a 30GHzPentium with 1GB memory Windows 
XP operating system All codes were complied using Microsoft Visual C++ 60 
42 
Data Files and Test File Formats  
The training procedure of CRF++ needs several data files such as training files model 
files label files and a feature template file These structures and formats of the data 
files on CPG islands database are illustrated as follows 
421   Training Files Model Files and Label Files 
The model file which describes the CRFs model is a binary file built by the training 
process and is used in the decoding Based on model files we use the algorithm  
 
Identifying CpG Islands in Genome Using Conditional Random Fields 
315 
 
CpGViterbi to obtain the global optimal labeling sequences which is presented by the 
label files  
A training file is used to record the information of the training sample It comprises 
multiple tokens The data of the first column shown in Table 1 denotes the observa
tion characters to be labeled The data of the second column represents the basic 
attribute feature of the sequence For characters A and T the attribute feature values 
are 0 indicating that the two characters are out of CPG islands Similarly we assign 1 
to the attribute feature value of characters C and G indicating that they are in CPG 
islands The states in or out of the CPG islands corresponding to observation charac
ters are listed in the third column Table 1 is a part of a training file used in the expe
riment where the observation sequence is “ATCGGACGAT” It can be seen from the 
third column “CGGACG” is a CPG island  
Table 1 An example of training file 
Observation character 
Attribute feature 
Label 
A 
0 
A 
T 
0 
T 
C 
1 
C+ 
Gcurrent focusing token 
1 
G+ 
G 
1 
G+ 
A 
0 
A+ 
C 
1 
C+ 
G 
1 
G+ 
A 
0 
A 
T 
0 
T 
422   Feature Template Files 
A feature template file is used to describe feature function which can define unary 
features dualistic features ndimensional features and even compound features based 
on character level As shown in table 2 a group of feature template is practically used 
in our experiments  
Table 2 A feature template example 
Unigram 
U00x20 
U01x10 
U02x00 
U03x10 
U04x20 
U05x01 
U06x10x00 
U07x00x10 
Bigram 
B1 
316 
W Liu H Chen and L Chen  
 
Each line in the template file denotes one template Note also that there are two 
types of templates that is “Unigram” and “Bigram” The types are specified with the 
first character of templates In the above feature template file the first character of 
parameter Uxx is “U” which denotes a unigram template “xx” means the number of 
feature templates In each template special macro xrowcol will be used to speci
fy a token in the input data where x represents taking strings from the database row 
specifies the relative position from the current focusing token and col specifies the 
absolute position of the column To test our algorithm we use four groups of tem
plates as shown in Table 3 
Table 3 The feature templates 
Unigram 
Unigram 
Unigram 
Unigram 
U00x20 
U00x20 
U00x20 
U00x20 
U01x10 
U01x10 
U01x10 
U01x10 
U02x00 
U02x00 
U02x00 
U02x00 
U03x10 
U03x10 
U03x10 
U03x20 
U04x20 
U04 x20x10 
U04x20 
U04 x00x20 
U05x01 
U05 xl0x00 
U05 x00x10 
U05 x20x10 
U06x10x00 
U06 x00xl0 
U06x10x20 
U06 x10x00 
U07x00x10 
U07 x10x10 
U07x00x10 
x20 
U07x20x10 
x00 
Bigram 
Bigram 
Bigram 
Bigram 
B1 
B2 
B3 
B4 
43 
Experimental Results and Analysis  
We test our algorithm and compare its performance with other methods in terms of 
recall and precision which are defined as follows 
The number  of  CpG i sl ands ext r act ed cor r ect l y
The t ot al  number  of  t r ue CpG i sl ands i n t he t est  set
Recall=
 
and 
The number  of   C G i sl ands ext r act ed cor r ect l y
The t ot al  number  of  C G i sl ands ext r act ed
p
Precision=
p
 
To comprehensively evaluate the system performance we used the Fscore which is 
the weighted geometrical average of precision and recall  Fscore is defined as 


2
2


+1
=
R+

P R
F
P
b
b
 
Here we set β=1 in our experiments which indicates recall and precision have the 
same importance 
 
Identifying CpG Islands in Genome Using Conditional Random Fields 
317 
 
We also conduct experiments of CpG islands identification using our algorithm 
and other three methods We compare their performance in terms of recall and preci
sion and Fscore In our experiments four feature templates shown in Table3 are 
used and the results are listed in Table4 
Table 4 Comparison of the results among CRFs MEMM and HMM 
Model 
Feature template 
PrecisionP（） 
Recall®（） 
FscoreF（） 
CRFs 
Template1 
9314 
9121 
9216 
Template 2 
9020 
8995 
9007 
Template 3 
8733 
8820 
8776 
Template 4 
8636 
8796 
8680 
MEMM 
Template 1 
9010 
8970 
8990 
Template 2 
8779 
8740 
8770 
Template 3 
8427 
8540 
8483 
Template 4 
8262 
8451 
8347 
HMM 
Template 1 
9150 
8960 
9059 
Template 2 
8936 
8813 
8898 
Template 3 
8372 
8437 
8404 
Template 4 
7826 
8020 
7922 
 
From Table4 we can see that our algorithm using CRFs outperforms the other  
methods in the aspects of both recall precision and Fscore for CpG islands identifi
cation Our experimental results have fully highlighted the merit of CRFs in CpG 
islands identification  
5 
Conclusions 
In this paper we present a novel method for CpG islands location identification based 
on the model of conditional random fields In order to overcome the shortcomings 
such as the strong independence assumptions and the labelbias problem exhibited by 
other models our method transforms the problem of CpG islands location identifica
tion into sequential data labeling Based on the nature of CpG islands location we 
define the corresponding feature functions and design the methods of model  
constructing training and decoding in CRF accordingly Experimental results on 
benchmark data sets show that our algorithm is more practicable and efficient than the 
methods based on hidden Markov model HMM or the maximum entropy Markov 
model MEMM 
 
Acknowledgements This research was supported in part by the Chinese National 
Natural Science Foundation under grant Nos 61070047、61070133 and 61003180 
Natural Science Foundation of Jiangsu Province under contracts BK2010318 
BK21010134 and Natural Science Foundation of Education Department of Jiangsu 
Province under contract 09KJB20013   
 
318 
W Liu H Chen and L Chen  
 
References 
1 Li WJ Li M Xin RH Wei LH Promoter Recognition in Human Genome Based on 
KL Divergence and BP Neural Network Journal of Liaoning Normal University Natural 
Science Edition 333 42–45 2010 
2 Huang YK Promoter Recognition System Research from Gene Sequence Data The 
Master’s Thesis of Harbin Engineering University 2009 
3 Zhang CT The Current Status and The Prospect of Bioinformatics The Journal of 
Liaoning Science and Technology 08 25–26 2001 
4 Tong Q Zhen HR Ning Y A Geneprediction Algorithm Based on the Statistical 
Combination and the Classification in Terms of CpG Content Journal of Beijing Biomedi
cal Engineering 426 178–181 2007 
5 Takai D Jones PA Comprehensive Analysis of CpG Islands in Hhuman ChromoSomes 
21 and 22 Proc Natl Acad Sci USA 996 3740–3745 2002 
6 Hackenberg M Previtil C Luis LE et al CpG Cluster a Distancebased Algorithm 
for CpGisland Detection BMC Bioinformatics 7 446 2006 
7 Wang Y Leung FC An Evaluation of New Criteria for CpG Islands in the Human Ge
nome as Gene Markers Bioinformatics 207 1170–1177 2004 
8 Wang JL Su JZ Wang FC Ying ZY A New Method to Predict CpGislands 
Based on Fuzzy Theory China Journal of Bioinformatics 67 91–94 2009 
9 Shi OY Yang J Tian X Hidden Markov Model for CpG Islands Prediction Based on 
Matlab Computer Applications and Software 1125 214–215 2008 
10 Jiang HJ Zhang ZL Discrimination of CpG Islands Location Based on HMM Ma
thematical Theory and Applications 629 113–116 2009 
11 John L Andrew MC Fernando P Conditional Random Fields Probabilistic Models 
for Segmenting and Labeling Sequence Data In Proc of the 18th ICML pp 282–289 
Morgan Kaufmann San Francisco 2001 
12 Hammersley JM Clifford P Markov Field on Finite Graphs and Lattices 1971 un
published 
13 ftpftpebiacukpubdatabasescpgisle 
14 httpsourceforgenetprojectscrfppfiles 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 319–326 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A Novel Gene Selection Method  
for Multicatalog Cancer Data Classification 
Xuejiao Lei1 Yuehui Chen2 and Yaou Zhao3 
1 School of Information science and Engineering University of Jinan PR China 
aduosi126com 
2 School of Information science and Engineering University of Jinan PR China 
yhchenujneducn 
3 School of Information science and Engineering University of Jinan PR China 
isezhaoyoujneducn 
Abstract In this paper a novel gene selection method which was merging the 
relevance score BW ratio and the Flexible Neural Tree FNT together was 
proposed for the multiclass cancer data classification Firstly the BW ratio me
thod was adopted to select some informative genes and then the FNT method 
was used to extract more characteristic genes from the gene subsets FNT is a 
treestructured neural network with input variables selection overlayer connec
tions and different activation functions for different nodes  Based on the pre
defined instructionoperator sets a flexible neural tree model can be created and 
evolved The FNT structure is developed by using probabilistic incremental 
program evolution PIPE algorithm and the free parameters embedded in 
neural trees are optimized by particle swarm optimization PSO algorithm Ex
periment on two wellknown cancer datasets shows that the proposed method 
achieved better results compared with other methods  
Keywords gene selection BW ratio Flexible Neural Tree Probabilistic 
Incremental Program Evolution Particle Swarm Optimization 
1 
Introduction  
In addition reducing the number of genes can help to cut down the inputs for compu
tation so the classifiers are much more efficient for classification and run much  
faster For this socalled “highdimensional small sample” problem a suitable gene 
selection method is very important   
Feature selection techniques can be divided into three categories depending on 
how they interact with the classifiers 1 Filter methods directly operate on the data
set that is they are independent for classification models The output of these me
thods are also score–ranking values according to these ranking values the informative 
genes are selected There are many filter methods which are proposed by predeces
sors such as BW ratio 2 Bscatter3 MinMax3 and so on The advantage of these 
methods is fast but has inferior results Wrapper methods run a search in the space of 
feature subsets guided by the outcome of the model eg classification performance 
320 
X Lei Y Chen and Y Zhao 
 
on a crossvalidation of the training set There are many researchers using these me
thods to select feature subsets such as Particle Swarm Optimization PSO 4 genet
ic algorithm GA 5 and so on They often obtain better results than filter methods 
but have an increased computational cost 6 Finally embedded methods with inter
nal information of the classification model was used to perform feature selection for 
instance Genetic programming GP 7 and Flexible Neural Tree FNT8 They 
often provide a good tradeoff between performance and computational cost 9 
In this paper we proposed a novel gene selection method which was mixing the 
BW ratio and the flexible neural tree Firstly the BW ratio method was used to select 
feature subsets and then the FNT was utilized to extract more informative gene sub
sets from the selected feature subsets which got from the first step The FNT structure 
is developed by probabilistic incremental program evolution PIPE algorithm and the 
free parameters embedded in neural trees are optimized by particle swarm optimiza
tion PSO algorithm  
2 
Gene Selection Methods 
Generally the microarray data has very high dimensionality in thousands and small 
size of samples in dozens However only small parts of genes have great impact on 
classification and most of them are useless These irrelevant genes not only confuse 
learning algorithms but also degrade their performance and efficiency Moreover the 
prediction model induced from irrelevant genes may prone to overfitting So in this 
paper we applied mixing BW ratio method and FNT to select some informative genes  
21 
BW Ratio Gene Selection Method and Flexible Neural Tree 
The BW ratio 2 gene selection method is firstly introduced by Sandrine Dudoit et al 
It is a preliminary genes selection method based on the ratio of their betweengroup to 
withingroup sums of squares The highest BW ratio value is most informative 
The FNT8 is a treestructured neural network with input variables selection over
layer connections and different activation functions for different nodes  Based on the 
predefined instruction and operator sets a flexible neural tree model can be created 
and evolved 
22 
Mix Gene Selection Method 
Depending on the advantage of filter methods and the embedded methods we pro
posed a novel gene method with mixing the BW ratio method and the FNT 
In our paper the FNT was used for the multiclass cancer data to select feature 
genes Firstly the BW ratio was applied to select some informative genes And then 
according to the pairwise comparison classification strategy the selected subsets 
which were got from the first step were divided into twolabel subsets Finally the 
FNT was utilized for the twolabel subsets to choose more characteristic genes see 
Fig2 
 
A Novel Gene Selection Method for MultiCatalog Cancer Data Classification 
321 
 
 
Fig 1 BW indicates the feature sets which are generated by BW ratio method respectively C1 
C1 … Cn denote n FNTbase classifiers Each FNT selects the more informative genes 
3 
Classification Method 
In this paper the flexible neural tree FNT was also applied as the base classifier and 
the pairwise comparison classification strategy Depending on the extracted datasets 
the FNT model can be created and evolved The structure of FNT is developed by the 
Probabilistic Incremental Program Evolution PIPE 10 and the parameters are op
timized by the Particle Swarm Optimization PSO The PIPE programs are encoded 
in narray trees that are parsed depth first from left to right with n being the maximal 
number of function arguments 
31 
Procedure of the General Learning Algorithm 
The general learning procedure for constructing the FNT model can be described as 
follows 
1 Create an initial population randomly FNT trees and its corresponding parame
ters 
2 Structure optimization is achieved by using the PIPE algorithm 
3  If a better structure is found then go to step 4 otherwise go to step 2 
4 Parameter optimization is achieved by the PSO algorithm as described in subsec
tion chapter 32 In this stage the architecture of FNT model is fixed and it is the 
best tree developed during the end of run of the structure search The parameters 
weights and flexible activation function parameters encoded in the best tree for
mulate a particle 
5 If the maximum number of local search is reached or no better parameter vector is 
found for a significantly long time then go to step 6 otherwise go to step 4 
6 If the satisfactory solution is found the algorithm is stopped otherwise go to  
step 2 
4 
Cancer Classification Using FNT 
41 
Data Sets 
We performed extensive experiments on two benchmark cancer datasets namely the 
MLL and Brain tumor GLIOMA which were downloaded from the website given in 
11 
322 
X Lei Y Chen and Y Zhao 
 
The MLL dataset 12 contains total 72 samples in three classes acute lymphoblas
tic leukemia ALL acute myeloid leukemia AML and mixedlineage leukemia 
gene MLL which have 24 28 20 samples respectively  
The Brain tumor GLIOMA dataset 13 contains in total 50 samples in four 
classes cancer glioblastomas CG noncancer glioblastomas NG cancer oligoden
drogliomas CO and noncancer oligodendrogliomas NO which have 14 14 7 15 
samples respectively Each sample has 12625 genes  
42 
Experiment Result and Analysis 
In our experiment firstly the BW ratio method was used to select 35 informative 
genes and then the FNT method was employed for the selected gene subsets to 
choose more characteristic genes Meanwhile we utilized the pairwise comparison 
strategy for multiclass classification The FNT was employed to be the base classifier 
The parameter of each FNT was adjusted by the PSO and the structure of each FNT 
was optimized by the PIPE  
For comparing with the other peoples’ work the classification performance was 
measured by 5fold cross validation technique The 5fold cross validation technique 
the samples were randomly divided into 5 equally sized subsets each subset was used 
as a test set for a classifier and the remaining 4 subsets were for training The training 
data was used to select informative features This process was repeated for 10 times to 
obtain the average results 
• MLL cancer 
Firstly the BW ratio method was used to select 35 informative genes see Fig3 left 
It was shown that the curve dot of ALL was close to the curve dot of MLL in the left 
picture of Figure 3 This was disadvantageous for the classification But our method 
supplied the shortage of the BW ratio method The selected subset was divided into a 
training set with 58 samples and a testing set with 14 samples by 5fold cross valida
tion technique The MLL had three classes 0AML 1ALL 2MLL According to 
the pairwise comparison classification strategy there were three classifiers And then 
utilize the FNT to select genes and classify The best FNT trees obtained by the pro
posed method were shown from Figure 4 to Figure 5 left It should be noted that the 
informative features for constructing the FNT model are formulated in the light of the 
procedure mentioned in the previous section These final informative genes selected 
by FNT are shown in Table 2 
For comparison purpose the classification performances of a KPCSR 14 
SVM+Ftest14 SVM+Cho’s14 KNN+Ftest14 KNN+Cho’s14 and our me
thod are shown in Table 1 For MLL dataset our method which has 26–45 incre
ments gives the best classification accuracy It is observed that our method has and 
the FNT classification models with the proposed mix gene selection method are better 
than other models for classification of microarray dataset 
 
A Novel Gene Selection Method for MultiCatalog Cancer Data Classification 
323 
 
  
 
Fig 2 In the two figures the X axis indicates the number of selected genes and the Y axis is 
the mean value of the samples of each feature gene which is selected by the BW ratio method 
The left picture is shown the feature gene description of the MLL and the Brain’s is left 
 
Fig 3 An evolved best FNT for the 0 class and 1 class of MLL data classification left the 0 
class and 2 class of MLL data middle the 2 class and 1 class of MLL data right 
• Brain cancer 
Firstly the BW ratio method was applied to select 35 informative genes see Fig3 
right The selected subset was divided into a training set of 40 samples and testing set 
of 10 by 5fold cross validation technique The Brain cancer had four classes 0CG 
1CO 2NG 3NO According to the pairwise comparison classification strategy 
there were six classifiers And then utilize the FNT to select genes and classify The 
best FNT trees obtained by the proposed method are shown from Fig 5 to Fig 6 It 
should be noted that the informative features for constructing the FNT model are for
mulated in the light of the procedure mentioned in the previous section These final 
informative genes selected by FNT algorithm is shown in Table 4 
For comparison purpose the classification performances of a KNN+EGSIEE15 
KNN+EGSEE15 KNN+GS115 KNN+GS215 and our method are shown in 
Table 3 For Brain cancer our method which has 42–102 increments gives the best 
classification accuracy Also in our experiments the maximal number of selected 
genes is 19 it is smaller than others’ It is observed that the FNT classification models 
with the proposed mix gene selection method are better than other models for classifi
cation of microarray dataset 
Table 1 Relative works on MLL dataset 
method 
Test accuracy  
Our method 
993 
KPCSR14 
967 
SVM+Ftest14 
948 
SVM+Cho’s14 
955 
KNN+Ftest14 
954 
KNN+Cho’s14 
960 
324 
X Lei Y Chen and Y Zhao 
 
Table 2 The extracted informative genes in case of MLL dataset 
01 class1 
x0x32x14x27x6x5x2x10 
02 class 
x8x14x5x23x19x4x9x26x29x1 
21 class 
x4x15x32x1x22x23x2x19x3x8x6 
 
 
Fig 4 An evolved best FNT for the 0 class and 1 class of Brain data classification left the 0 
class and 2 class of Brain data middle the 0 class and 3 class of Brain data right 
 
 
 
Fig 5 an evolved best FNT for the 2 class and 1 class of Brain data classification left an the 
3 class and 1 class of Brain data middle the 2 class and 3 class of Brain data right 
 
Table 3 Relative works on Lymphoma dataset 
method 
Test accuracy  
Our method2 
90019 
KNN+EGSIEE15 
84019 
KNN+EGSEE15 
78010 
KNN+GS115 
84095 
KNN+GS215 
80092 
The number in the bracket indicates the genes which are selected 
                                                           
1  The MLL has three classes 0AML 1ALL 2MLL 01 class indicates the classifier 
which is trained by the samples with label 0 and label 1 similarly 02 class is label 0 and 
label 2 and 21 class label 2 and label 1 
2  The number in the bracket indicates the number of the final selected genes 
 
A Novel Gene Selection Method for MultiCatalog Cancer Data Classification 
325 
 
Table 4 The extracted informative genes in case of MLL dataset 
0  1 class3 
x4x14x5x10x3x1 
0  2 class 
x18x8x3x1x20x9x17x11x12x25x3x5x6 x31x12x30 
0  3 class 
x7x13 x16x4x18x6 
2  1 class 
x12x1x14x29x21x33x0x13x17x29x34x11 x7x31x24 
3  1 class 
x16x7x5x6x25x11x4x10x14x29x15x34x2 x31x9x0x8x24x20 
2  3 class 
x6x29x1x5x10x28x22x11x25x0x4x23 
5 
Conclusion 
In this paper we proposed the mix gene selection method which was merging the 
BW ratio and FNT method together The MLL and Brain cancer datasets were used 
for conducting all the experiments Feature genes were first extracted by the defined 
relevance score technique BW ratio method which greatly reduces dimensionality as 
well as maintains the informative features Then the FNTs were employed to classify 
and select more characteristic genes Compare the results with some advanced gene 
selection technology the proposed method produces the best recognition rates In the 
future we will choose more critical features to characterize the gene expression data 
We also believe that FNT can have a great contribution to this study 
 
Acknowledgments This research was partially supported by the Natural Science 
Foundation of China 61070130 the Key Project of Natural Science Foundation of 
Shandong Province ZR2011FZ001 the Key Subject Research Foundation of Shan
dong Province and the Shandong Provincial Key Laboratory of Network Based Intel
ligent Computing 
References 
1 Saeys Y Abeel T Van de Peer Y Robust Feature Selection Using Ensemble Feature 
Selection Techniques In Daelemans W Goethals B Morik K eds ECML PKDD 
2008 Part II LNCS LNAI vol 5212 pp 313–325 Springer Heidelberg 2008 
2 Sandrine D Jane F Terence PS Comparison of Discrimination Methods for the Clas
sification of Tumors Using Gene Expression Data The American Statistical Association 
97457 2002 
3 Hong C Carlotta D An Evaluation of Gene Selection Methods for Multiclass Microar
ray Data Classification In Proceedings of the Second European Workshop on Data Min
ing and Text Mining in Bioinformatics 
4 Yang CS Chuang LY Li JC Yang CH A Novel BPSO Approach for Gene Selec
tion and Classification of Microarray Data IEEE 2008  
5 Hrishikesh M Nitya S Krishna M Tapobrata L An ANNGA model based promoter 
prediction in Arabidopsis thaliana using tilling microarray data Bioinformation 66 240–
243 2011  
                                                           
3  The Brain cancer has four classes 0CG 1CO 2NG 3NO 01 class indicates the 
classifier which is trained by the samples with label 0 and label 1 similarly 02 class is label 
0 and label 2 03 class label 0 and label 3 21 class label 2 and label 1 31 class label 3 and 
label 1 23 class label 2 and label 3  
326 
X Lei Y Chen and Y Zhao 
 
6 Kohavi R John G Wrappers for feature subset selection Artif Intell 9712 273–324 
1997 
7 Liu KH Xu CG A genetic programmingbased approach to the classification of mul
ticlass microarray datasets Original Paper Bioinformaticsbtn644 253 331–337 2009 
8 Chen Y Peng L Abraham A Gene Expression Profiling Using Flexible Neural Trees 
In Corchado E Yin H Botti V Fyfe C eds IDEAL 2006 LNCS vol 4224 pp 
1121–1128 Springer Heidelberg 2006 
9 Saeys Y Inza I Larranaga P A review of feature selection techniques in bioinformat
ics Bioinformatics 2319 2507–2517 2007 
10 Salustowicz R Schmidhuber J Probabilistic incremental program evolution Evolutio
nary Computation 52 123–141 1997 
11 Yang K Cai Z Li J Lin G A stable gene selection in microarray data analysis BMC 
Bioinformatics 7228 2006 
12 Armstrong SA Staunton JE Silverman LB Pieters R den Boer ML Minden 
MD Sallan SE Lander ES Golub TR Korsmeyer SJ MLL translocations specify 
a distinct gene expression profile that distinguishes a unique leukemia Nature Genetics 30 
41–47 2002 
13 Nutt CL Mani DR Betensky RA Tamayo P Cairncross JG Ladd C Pohl U 
Hartmann C McLaughlin ME Batchelor TT Black PM von Deimling A Pome
roy SL Golub TR Louis DN Gene Expressionbased Classification of Malignant 
Gliomas Correlates Better with Survival than Histological Classification Cancer Re
search 63 1602–1607 2003 
14 Zhang BL Cancer Classification by Kernel Principal Component Selfregression In 
Sattar A Kang BH eds AI 2006 LNCS LNAI vol 4304 pp 719–728 Springer 
Heidelberg 2006 
15 Li GZ Meng HH Ni J Embedded Gene Selection for Imbalanced Microarray Data 
Analysis IEEE 2008  
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 327–333 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A Novel Discretization Method for MicroarrayBased 
Cancer Classification 
Ding Li12 Rui Li12 and HongQiang Wang2 
1 Department of Automation University of Science and Technology of China 
Hefei 230027 PR China 
2 Intelligent Computation Lab Hefei Institute of Intelligent Machines Chinese Academy of 
Science POBox 1130 Hefei Anhui 230031 PR China 
ld051014hqwang126126com 
Abstract In this paper we propose a gene expression diversitybased method 
for gene expression discretization By counting the numbers of samples of 
different classes in an open expression intervals，the method calculates class 
distribution diversity and then expression diversity for genes Based on the gene 
expression diversity three discretization criteria are established for discretizing 
gene expression levels We evaluate the proposed method on the publicly 
available leukemia dataset and compare it with several previous methods  
Keywords gene expression gene expression diversity gene regulation 
discretization  
1 
Introduction 
Advent of highthroughput technology makes it possible to simultaneously measure 
expression levels of ten of thousands of genes in cells 1 These data are often used 
to build models to predict the phenotype of cells An increasing number of 
computational approaches have been developed for gene expression data analysis 2 
In such analysis the discretization of continuous gene expression data is an important 
step for efficient cancer classification 
At present a number of methods have been applied to convert continuous data to 
discrete ones for pattern recognition 39 For example Fayyad et al proposed the 
mdlp method which is a typical nonparameter supervision discretization method 7 
Another commonly used method is the chiM method which discretizes data based on 
the χ2 distribution 10 These methods work well in most cases but they can not 
relate gene expression with gene regulation for the biological significance of gene 
expression discretization In this paper we propose a new method for gene expression 
discretization based on the putative 3state regulation viewpoint 11 
The rest of the paper is organized as follows In section Methods we first 
formulate three core rules of discretization based on gene expression diversity and 
describe the proposed discretization procedure in detail In section Experimental 
                                                           
 Corresponding author 
328 
D Li R Li and HQ Wang 
 
results we first discuss the choices of the two parameters of the proposed method in 
practice and then evaluate the proposed method on a realworld microarray data and 
compare with two previous methods chiM and mdlp Finally the paper is concluded 
2 
Methods 
Gene expression levels are continuous values However gene regulation levels are 
generally assumed to be in a regulation space of 3 discrete states ie down 
regulatory nonregulatory and up regulatory states 11 We argue that a reasonable 
discretization of gene expression values should consider relating gene regulation 
states to separated intervals of expression levels The proposed gene expression 
discretization method is based on the 3state regulation assumption and aims at 
accurate cancer classification For the convenience of presentation we only consider 
binary cancer classification in this study Assuming that the expression values of a 
gene in a cancer type or subtype are normally distributed in a particular expression 
interval relationships between the distributions of the two cancer classes and gene 
regulation states could be one of the following three types i the two distributions are 
completely overlapped within a regulation state as shown in Fig1A ii the two 
distributions fall in two different regulation states and are clearly separated as shown 
in Fig1B iii the two distributions are partially overlapped in a regulation state as 
shown in Fig1C  
It can be found that only genes with the two types of expression distributions in 
Figs 1B and C are useful for cancer classification For them the expression values 
can be divided into two and three discretization states respectively However the 
genes expressed like in Fig1A can be thought of being disabled for classification and 
their expression values all should be assigned into one discretization state as noise 
Based on the idea we develop a new discretization method in the following sections 
In order to clearly depict the proposed disretizitation criteria we first define two 
concepts associated with gene expression distribution Consider binary classification 
problem with class 1 and class 2 For a left halfopen area of gene expression we 
define the diversity of class distribution within it denoted by D as  
D=n1N1 n2N2     
   1
where the N1 and N2 represent the total numbers of samples of class 1 and class2 
respectively and n1 and n2 represent the numbers of samples of class 1 and class 2 
having expression values in the area Assume a gene whose expressions are divided 
into have n left halfopen areas Let Di i=12…n represents the corresponding n 
class distribution diversities we define the diversity of expression for the gene 
denoted by ∆ as  
∆ =DmaxDmin  
2
where Dmax and Dmin represent the maximum and minimum of the n class distribution 
diversity respectively For Dmax and Dmin we denote the right endpoints of their left 
halfopen areas by Lmin and Lmax respectively 
 
 
A Novel Discretization Method for MicroarrayBased Cancer Classification 
329 
 
 
Fig 1 The relationship between gene regulatory states and the distribution of sample category 
Given two constants 0α λ1 three criteria for discretizing gene expression 
values can be made based on gene expression diversity as follows 
Criterion 1 if ∆ =DmaxDminα all the expression values of the gene will be grouped 
into one discretization state 
Criterion 2 if ∆ =DmaxDmin= α and min|Dmax||Dmin|λ where min and || represent 
the minimum and absolute functions respectively the expression values of the gene 
will be discretized into two states 
Criterion 3 if ∆ =DmaxDmin= α and min|Dmax||Dmin|=λ the expression values of 
the gene will be discretized into three states 
The three criteria are associated with the three cases shown in Fig1 Generally it 
is assumed that a gene has three regulation states changeable to the response to 
external stimulus So for a binary classification problem it is reasonable to divide the 
expression of a gene into at most three discretization states The parameters α and λ 
are referred to as gene expression diversity threshold and class distribution diversity 
respectively The choices of the two parameters will be discussed in section Results  
Based on the three discrete criteria a gene expression discretization procedure 
especially for a binary problem can be described as follows 
Step 1 Setting α and λ 
Step 2 Dividing the whole range of expression values into n n = 50 intervals 
uniformly and defining n n = 50 or above left halfopen areas based on the n 
intervals   
330 
D Li R Li and HQ Wang 
 
Step 3 Calcualting the class distribution diversities by Eq1 for each area to 
obtain Dmax Dmin Lmax and Lmin 
Step 4 Calcualting the expression diversity ∆ across the n class distribution 
diversities by Eq2  
Step 5 If Criterion 1 holds taking the whole expression range as the only one 
discretiation state and ending the procedure  
Step 6 If Criterion 2 holds dividing the whole expression range into two intervals 
∞，L and L +∞ where L=Lmax if |Dmax||Dmin| and Lmin otherwise and ending the 
procedure 
Step 7 If Criterion 3 holds dividing the whole expression range into there 
intervals ∞，L1 L1 L2 and L2 +∞ where L1=minLmax  Lmin  and L2=maxLmin 
 Lmax  max represents the maximum function and ending the procedure 
3 
Experimental Results 
We evaluated the proposed discretization procedure on the publicly available 
leukemia data 12 The dataset contains 72 samples of which 47 are acute 
lymphoblastic leukemia ALL and 25 are acute myelogenous leukemia AML Each 
sample consists of the expression levels of 7129 genes 
31 
The Choice of α 
The proposed method uses the variable ∆ to measure the diversity of gene expression 
The threshold α is used to find genes whose diversities are large enough to distinguish 
the two classes A too small α will bring noise to cancer classification while a too 
large α will run a risk of missing the genes whose expressions actually differ between 
the two classes In essential ∆ means the proportion difference of the two classes and 
may vary from zero to 1 So a difference level of proportions larger than 04 should 
be statistically significant to class distinction In this study we prefer to use α=04 for 
implementing the dicertizion procedure For the justification we have observed the 
random distribution of ∆ on the leukemia data We randomly shuffled the labels of the 
72 samples ten times and calculated the ∆s of all the 7129 genes for each time As a 
result 71290 ∆s were obtained whose distribution is drawn in Fig2 From Fig2 it 
can be found that a random ∆ seems to follow a normal distribution centered at 023 
Further the distribution takes on the 95 percentile of 037 These observations 
suggest that the value of α=04 is acceptable for gene expression discretization 
32 
The Choice of λ 
The parameter λ is used to remove the discretization intervals in which two classes 
are distinguishable This means that the magnitude of the corresponding Dmax or Dmin 
is not significantly large to specify a discretization interval So the value of λ should 
be chosen to guarantee to ignore the intervals where different classes behave the same 
but retain those making different classes separated Recalling that α reflect a 
minimum acceptable expression diversity of a gene we suggest setting  λ = α2 =02 
as default in practice  
 
A Novel Discretization Method for MicroarrayBased Cancer Classification 
331 
 
 
Fig 2 Histogram A and cumulative probability distribution B of ∆ 
33 
Application to the Leukemia Data  
We next applied our method to the leukemia data for cancer classification In the 
application two wellknown classification methods support vector machines SVM 
and Knearest neighborhood KNN were used to build classifier on the discretization 
results for all genes For reliable validation we randomly selected 50 samples 32 
ALL and 18AML from the total 72 samples as a training set  for data discretization 
and learning classifier and the rest as a test set 15 ALL and 7 AML for independent 
validation  and the random split was repeated 100 times The following measures 
were used for classification evaluation the minimum maximum and average of 
classification accuracies the average false positive rates FPR and the average false 
negative rates FNR For comparison we also used two previous methods chiM and 
mdlp to analyze the data in the same setting Table 1 compared the classification 
results by the three methods From Table 1 it can be found that our method obtained 
the highest average classification accuracy irrespective of the classifiers used 
Table 1 Comparison of the classification results by the three discretization methods on the 
leukemia data 
Methods 
SVM 
KNN 
ACC 
max 
ACC 
min 
ACC 
FPR 
FNR 
ACC 
max 
ACC 
min  
ACC 
FPR 
FNR 
Our 
method 
9005 
100 
7727 
1274 
0 
9900 
100 
9545 
106 
087 
chiM 
7727 
8622 
6818 
2492 
0 
8786 
100 
7273 
904 
1885 
Mdlp 
8014 
9091 
6818 
2256 
0 
9895 
100 
9545 
080 
159 
Note ACCmean accuracy minACCmin accuracy maxACCmax accuracy FPR false positive rate 
FNP false negative rate 
332 
D Li R Li and HQ Wang 
 
To demonstrate the robustness of our method we randomly selected 100 genes 
from 7129 genes and for each gene applied each of the three methods to discretize its 
expression levels using 38 of the total 72 samples 100 times Fig 3 shows the 
boxplots of the standard deviations sd of the discretization results for each of the 
three methods From Fig 3 it can be found that our method outperforms the previous 
two methods with the least mean of sd The mdlp chooses minimum description 
length criterion MDL as the judgment basis of the discretestoping which obtained 
a sd mean slightly higher than our method’s Altogether the comparison of sds 
confirms the best stability of our method 
 
 
Fig 3 Boxplots of standard deviations of the discretization results for the three methods 
4 
Conclusion 
We have proposed a new method based on gene expression diversity for discretizing 
gene expression data We evaluated it on a realworld data set ie the publicly 
available leukemia data set and compared with two previous methods chiM and 
mdlp The experimental results showed that our method is more stable and more 
robust than the previous ones for the discretization of gene expression and can result 
in a better classification performance for the leukemia data Future work will focus on 
applications to more realworld data sets 
 
Acknowledgement This work was supported by the grants of the National Science 
Foundation of China Nos 30900321 31071168 60975005 61005010 60873012 
60973153 61133010 and 60905023 
 
A Novel Discretization Method for MicroarrayBased Cancer Classification 
333 
 
References 
1 Berns A Cancer gene expression diagnosis Nature 403 491–492 2000 
2 Borges HB Nievola JC Feature Selection as a Preprocessing Step for Classification in 
Gene Expression Data In Seventh International Conference on Intelligent Systems Design 
and Applications pp 157–162 2007 doi101109ISDA200780 
3 Boullé M MODL A Bayes Optimal Discretization Method for Continuous Attributes 
Machine Learning 131–165 2006 
4 Brijs T Vanhoof K Costsensitive Discretization of Numeric Attributes In Żytkow 
JM ed PKDD 1998 LNCS vol 1510 pp 102–110 Springer Heidelberg 1998 
5 Butterworth R Simovici DA Santos GS OhnoMachado L A Greedy Algorithm 
for Supervised Discretization Journal of Biomedical Informatics 285–292 2004 
6 Dougherty J Kohavi R Sahami M Supervised and Unsupervised Discretization of 
Continuous Features In Prieditis A Russell SJ eds Proceedings of the Twelfth 
International Conference on Machine Learning Tahoe City California pp 194–202 
1995 
7 Fayyad UM Irani KB Multiinterval Discretization of Continuousvalued Attributes 
for Classification Learning In Proceedings of the Thirteenth International Joint 
Conference on AI IJCAI 1993 Chamberry France pp 1022–1027 1993 
8 Kohavi R Sahami M Errorbased and Entropybased Discretization of Continuous 
Features In Proceedings of the Second International Conference on Knowledge 
Discovery and Data Mining pp 114–119 AAAI Press Portland 1996 
9 Liu H Hissain F Tan CL Dash M Discretization An Enabling Technique Data 
Mining and Knowledge Discovery 393–423 2002 
10 Kerber R ChiMerge Discretization of Numeric Attributes In Proceedings of the Tenth 
National Conference on Artificial Intelligence pp 123–128 1992 
11 Wang HQ Huang DS Regulation Probability Method for Gene Selection Pattern 
Recognition Letters 27 116–122 2006 
12 Golub TR et al Molecular Classification of Cancer Class Discovery and Class 
Prediction by Gene Expression Monitoring Science 2865439 531–537 1999 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 334–341 2012 
© SpringerVerlag Berlin Heidelberg 2012 
SequenceBased Prediction of ProteinProtein 
Interactions Using Random Tree and Genetic Algorithm  
Lei Zhang12  
1 College of life Science University of Science and Technology of China  
Hefei Anhui 230027 China  
2 Intelligent Computing Laboratory Institute of Intelligent Machines 
Chinese Academy of Sciences Hefei Anhui 230031 China 
ranyaoleiyahoocomcn 
Abstract Proteinprotein interactions play important roles in the course of cell 
functions such as metabolic pathways and genetic information processing 
There are many shortcomings of traditional experiments such as tediousness 
and laboriousness The machine learning methods have been developed to 
predict PPIs and preliminary results have demonstrated their feasibility Here 
we introduce a sequencebased random tree and GA to infer PPI Experimental 
results on Scerevisiae dataset from DIP show that our novel method performs 
well than rotation forest with higher accuracy sensitivity and precision Most 
importantly our method runs faster than rotation forest   
Keywords Proteinprotein interactions protein sequence autocorrelation 
descriptor random tree GA 
1 
Introduction 
Proteins are the physical base of the biological processes The research of protein 
attributes is a hot area in biological field Proteinprotein interactions contain multi
information about life helping reveal the exact function of intrinsic biological 
phenomenon With the development of experiments conditions many PPIs have been 
discovered over the years and several databases have been created to store the 
information of these interactions In particular more PPIs are available from high
throughput interaction detection methods It is a wellknown fact that using high
throughput methods is expensive and timeconsuming False positives and false 
negatives also exist in the process of experiments A novel method is demanded to 
help in the process of identifying real proteinprotein interactions Therefore a 
number of computational approaches have been explored recently 
These methods 12 which are based on different attributes such as protein 
sequence protein domain subcellular localization and genetic codon can be divided 
into four subtypes based on a genome information b amino acids sequences c 
protein domains d other protein property 
 
SequenceBased Prediction of PPIs Using Random Tree and GA 
335 
 
It is based on multiple data types including the gene fusion 3 the proteins 
coevolution method 4 and phylogenetic profile method 5 Sequencebased 
inference methods and structuresbased prediction approaches have been proposed in 
previous research findings As a matter of fact the latter is more difficult than the 
former under later studies The reason is that the feature means from protein 
structures which are complex diversity and mutative are difficult to exploit Here 
we provide the details of sequencebased inference methods The primary structure of 
protein which is arrangements of amino acids contains much useful information for 
basic research and application in biotechnology The machine learning algorithms are  
universal method now Support vector machine developed by Bock and Gough 6 
was trained to recognize PPI based on the primary structure and the associated 
physicochemical properties of the proteins BenHur and Noble 7 proposed various 
kernel methods such as motif kernel and Pfam kernel using different data sources to 
predict interactions problems Shen et al 8 developed a novel method that combines 
triplet assembled information encoding with SVM to infer PPI Xia et al 9 used 
rotation forest that is an ensemble learning approach and autocorrelation descriptor to 
predict yeast PPIs  
Many methods of domainbased prediction PPIs are also developed over the years 
Deng et al 10 used maximum likelihood estimationMLE plus domains to infer 
protein interaction Random decision forest framework was used by Chen et al 11 
in predicting PPIs Lqbal et al 12 used messagepassing algorithms for the 
prediction of protein domain interactions from proteinprotein interaction data 
In this study we propose a novel sequencebased approach to the prediction of 
proteinprotein interaction As a powerful optimization tools WEKA and GA are 
applied to random tree which is a sensitive decision tree 
2 
Materials and Methods 
21 
Data Sets 
Database of Interacting ProteinsDIP combines information from a variety of sources 
to create a single consistent set of proteinprotein interactions Xia et al 9 obtained 
the Scerevisiae interaction dataset from DIP The final data set consists of 5594 
protein pairs that comprise positive set and 5594 protein pairs that comprise negative 
interaction set 
22 
Extracting Sequence Features 
The first priority is to extract computational information from existing bio
information In our experiments the universal method that each amino acid of protein 
sequence is converted into numerical value was used to encode protein sequence 
Each residue in sequence was represented by six sequencederived physicochemical 
properties which include hydrophobicity volumes of side chains of amino acid 
336 
L Zhang 
 
polarity polarizability solventaccessible surface area and net charge index of side 
chains of amino acid 
We formulated the problem of PPI prediction as a binary classification task where 
the interaction class was represented by 1 and noninteraction class was 
expressed as 0 The key to the machine learning algorithm depends on that how to 
extract features from hidden protein sequence or structure information There are 
many methods of extracting sequence features developed by in previous work such as 
auto covariance descriptor 13 conjoint triad 8 local descriptor 14 and 
autocorrelation descriptor 15 16 Then unequallength vector of numerical values 
were obtained It is crucial stage that how to transform unequallength vector of 
protein vector into equallength vector In order to perform empirical studies for the 
PPI the amino acid sequence information of heterogeneous length needs to be 
transformed into the feature vector information of homogeneous length In previous 
application Moran autocorrelation descriptor 15 was proved to be a better method 
for encoding features of protein sequence It was defined as by Xia et al 9 





2
1
1
1
1
 


=
+
−
=
−
−
−
−
=
N
j
j
d
j
d
N
j
j
p
p
N
p
p p
p
d
N
I d
           
1
N
p
p
N
j
j

= =
 
2
where p is the average of the property of the 20 amino acids and N  is the length of 
the sequence d  which ranges from 1 to 30 is the distance between one amino acid 
residue and its neighbors 
jp  and 
d
jp +  are the properties of the amino acid at 
positions j  and 
j + d
 respectively 
Thus each protein pair is represented by a vector of 360 features We adopted the 
same feature method with Xia et al 9 to make the assessment effect more 
equitableness Here sixty percent of instances were chose as the training set The 
remaining instances comprised the test set 
23 
Random Tree and GA 
Ensemble learning is a hot research topic in the field of machine learning Integration 
of multiple weak classifiers should have higher classification accuracy than a single 
classifier But there are still some weak points in it ie the use of more basic 
classifier will lead to increase computational cost and storage cost It is becoming 
more and more difficult to obtain divergence from a base classifier and others with 
the increase of the number of individual classifier Many empirical studies show that 
it can yield better performance when we select a part of the base classifier from all 
Zhou et al 17 proposed GASEN which is an approach that neural networks chosen 
as the base classifiers are effectively selected by GA to apply to regression or 
classification task The algorithm is as follows 
 
 
SequenceBased Prediction of PPIs Using Random Tree and GA 
337 
 
Inputtraining set S learner L trials thresholdλ  
Procedure 
for
t =1
 to 
tS =
bootstrap sample from S 
 
t
T
N = L S
 
 
generate a population of weight vectors 
evolve the population where the fitness of a weight vector w is measured as  
 
EWV
f w
=1
 
∗
w = the evolved best weight vector 
Outputensemble
∗
N  
  
 
 
x
N
Ave
x
N
wt
T


∗
∗
=
λ
                    for regression 
   
 
 

=
∗
∈
∗
=
y
Nt x
wt
y Y
x
N

1
max
arg
λ
                  for classification 
 
As far as we know there is no report discussing the application of select ensemble to 
predict PPIs We presented an approach that the base classifiers of random tree were 
selected by GA Trees are mathematical objects that play an important role in several 
areas of learning community Decision tree uses tree model and builds training model 
from top and bottom according to the feature values Because of the different root 
nodes discrepant tree structures are generated it is quite unstable For the ensemble 
learning decision tree can be chose as the base classifiers There are many types of 
trees such as C45 J48 In these tree structures leaves represent class labels and 
branches represent conjunctions of features that lead to class labels 
Random tree on itself tends to be too weak Thus we want to integrate it into our 
method Class for constructing a tree considers K random features at each node 
Random tree does not prune Similar to the standard random decision forest algorithm 
19 training data will build a training model The random tree randomly selects a 
subspace of features to focus on at each splitting node 
WEKA written in java is the popular software in machine learning community 
There are many machine learning algorithms provided by WEKA We used the 
random tree as the base classifier from library Except the number of base classifier 
the remaining parameters were set to the default We used the training data to build all 
the basic classifiers Here bootstrap sampling was used to construct every random 
tree Thus all the classifiers had different with each other In order to achieve good 
experimental results a classifier which had better performance and was discrepant 
compared with others was chosen How to select a good classifier from ensemble 
system can be seen as optimized problem There are many algorithms developed in 
this research area such as PSOParticle Swarm Optimization GAGenetic 
Algorithm GA is a search heuristic algorithm premised on the evolutionary ideas of 
natural selection and genetic GA is widely applied to optimized community There 
are several GA tool boxes developed by different investigators  
338 
L Zhang 
 
Here we used GAOT toolbox provided by Houck et al 18 The genetic operators 
including select crossover and mutation and the system parameters including the 
crossover probability the mutation probability and the stopping criterion are all set to 
the default values of GAOT The threshold used by GA was set to 001 A typical GA 
needs to evaluate the solution domain In our experiment the predictive value of each 
base classifier was chosen as one initial individual of population So the number of 
classifiers was the size of initial population The true labels of the training instances 
were used to assess evolutionary individual Thus every base classifier would be set a 
weight through their predictive outputs The sum of weights was one The weights 
that ranged from 001 to 001 would set to zero Thus the remaining weights which 
were we wanted were set to one The base classifiers which weights were set to one 
would be used to predict test instances  
24 
Evaluation Methods 
Accuracy sensitivity precision are extensively used in classification problem It can 
be used to evaluate results of inferring PPIs For the reasonable comparison of the 
results we used the same training set and test set in different algorithms  Sensitivity 
measures the proportion of actual PPIs which are correctly identified Precision 
represents the proportion of positive inferences which are correct Accuracy is defined 
as the percentage of all inferences which are correct   
3 
Results and Discussion 
We compared our experimental results with the rotation forest of Xia et al 9 in 
Table1 The graph shows that our method has the better performance than ensemble 
method Xia et al 9 selected J48 as the base classifier and PCA as transformation 
method form WEKA software Only two parameters K and L the number of feature 
subsets and the ensemble size respectively need to be set in previous experiments 
So the remaining parameters would be set default values implemented in WEKA 
Here K was set to 12 which required the least computational cost According the 
earlier reports K which is not too big and too small has no great impact on our 
experimental results L ranges from 1 to 100 The results show that the accuracy is 
changed with the different values of L it is a steady increase from 1 to 43 and it 
remains unchanged above the values  
Table 1 Chosen the best accuracy sensitivity precision comparison 
 
Our method
Rotation forest 
Accuracy 
9469 
9352 
Precision 
9682 
951 
Sensitivity 
9231 
915 
times 
39448 s 
485353s 
 
 
SequenceBased Prediction of PPIs Using Random Tree and GA 
339 
 
0
50
100
150
200
250
300
350
400
450
500
088
089
09
091
092
093
094
095
096
097
ensemble size
percentage
 
 
acc
pe
sn
 
Fig 1 The performance of our method changed with L 
Our method belongs to the selective ensemble which reveals that ensemble of the 
available random tree may be better than ensemble of all learners Its also a hot 
research topic In our application the selective essence was that the base learners of 
high accuracy and difference with others would be chose The results clearly show 
that our method has the better performance than previous studies The accuracy is 
improved from 9352 to 9469 In addition the precision is 9682 while the 
rotation forest has a lower value of 951 The sensitivity is improved from 915 to 
9231 
In Figure1 we can conclude that the predictive performance is not stable with the 
increase of L Although it is implemented in the same conditions the results have 
difference with others according to the number of base classifiers selected When the 
L is 280 base classifiers chosen have the best performance than others We can also 
find that predictive power is not more powerful with increase of base classifiers 
chosen It results from the diverse difficult obtained with the increase of L On this 
point the selective ensemble and traditional ensemble methods are entirely at one 
Our method has an obvious advantage that the program runs more quickly than 
rotation forest implemented in WEKA software Here we chose the random tree as 
our base classifiers Its faster than J48 when building the training model In Table1 
we gave the comparison of results of time needed to generate best performance If we 
used the random tree as the base classifiers of rotation forest the bad performance 
was generated So J48 is a good choice for rotation forest    
4 
Conclusion 
The computational methods are used to solve the biological problems Various 
machine learning algorithms have been developed to resolve the difficult problems 
340 
L Zhang 
 
The crucial is that the represented and useful information must be exacted from the 
biological model Here the values of physicochemical properties of the amino acids 
were used to extract useful information from protein sequence Moran autocorrelation 
descriptor 15 which takes into account the effect of the neighboring residues was 
adopted to encode the sequence Random tree from WEKA utilized the feature 
vectors to predict PPIs Its a weak classifier that a component classifier has a lower 
accuracy of 8134 So its a suitable base classifier in our experiment The GA is 
also the powerful optimization tool The experimental results show that our method 
can be applied to infer PPIs with higher accuracy precision and sensitivity than 
rotation forest In addition our method is faster than it Furthermore this method can 
be used to study protein networks and determine protein complexes 
Acknowledgments The author wishes to thank DrHongJie Yu for his helpful 
suggestions about English writings and expressions This work was supported by the 
Grants of the National Science Foundation of China Nos 61133010 31071168 and 
Anhui Provincial Natural Science Foundation No 1208085MF96  
References 
1 Shi MG Xia JF Li XL Huang DS Predicting Proteinprotein Interactions from 
Sequence using Correlation Coefficient and Highquality Interaction Dataset Amino 
Acids 38 891–899 2010 
2 Zhu H Bilgin M Bangham R et al Global Analysis of Protein Activities Using 
Proteome Chips Science 293 2101–2105 2001 
3 Marcotte EM Pellegrini M Ng HL Rice DW Yeates TO Eisenberg D 
Detecting Proteinprotein Interactions from Genome Sequences Science 285 751–753 
1999 
4 Pazos F Valencia A Similarity of Phylogenetic Trees as Indicator of Proteinprotein 
Interaction Protein Eng 14 609–614 2001 
5 Pazos F HelmerCitterich M Ausiello G Valencia A Correlated Mutations Contain 
Information about Proteinprotein Interaction J Mol Biol 271 511–523 1997 
6 Bock J Gough D Prediticing Proteinprotein Interactions from Primary Structure 
Bioinformatics 17 455–460 2001 
7 BenHur A Noble WS Kernel Methods for Predicting Proteinprotein Interactions 
Bioinformatics 21 i38–i46 2005 
8 Shen JW Zhang J Luo XM Zhu WL Yu KQ Chen KX Li YX Jiang HL 
Predicting Proteinprotein Interactions based on Sequence Information Proceedings of the 
National Academy of Sciences 104 4337–4341 2007 
9 Xia JF Han K Huang DS Sequencebased Prediction of Proteinprotein Interactions 
by Means of Rotation Forest and Autocorrelation Descriptor Protein  Peptide Letters 17 
137–145 2010 
10 Deng M Mehta S Sun FZ Inferring Domaindomain Interactions from Protein
protein Interactions Genome Res 12 1540–1548 2002 
11 Chen XW Liu M Prediction of Proteinprotein Interactions using Random Decision 
Forest Framework Bioinformatics 21 4394–4400 2005 
12 Lqbal M Freitas AA Johnson CG Vergassola M Messagepassing Algorithms for 
the Prediction of Protein Domain Interactions from Proteinprotein Interaction Data 
Bioinformatics 24 2064–2070 2008 
 
SequenceBased Prediction of PPIs Using Random Tree and GA 
341 
 
13 Guo YZ Yu LZ Wen ZN Li ML Using Support Vector Machine Combined with 
Auto Covariance to Predict Proteinprotein Interactions from Protein Sequences Nucleic 
Acids Research 36 3025–3030 2008 
14 Lo S Cai C Chen Y Maxey CM Effect of Training Datasets on Support Vector 
Machine Prediction of Proteinprotein Interactions Proteomics 5 876–884 2005 
15 Moran PA Notes on Continuous Stochastic Phenomena Biometrika 37 17–23 1950 
16 Broto P Moreau G Vandicke C Molecular Structures Perception Autocorrelation 
Descriptor and Nonlocal Interactions Neurocomputing 68 66–70 1984 
17 Zhou ZH Wu JX Tang W Ensembling Neural Networks Many Could be Better 
than All Artificial Intelligence 137 239–263 2002 
18 Houck CR Joines JA Kay MG A Genetic Algorithm for Function Optimization a 
Matlab Implementation Technical Report NCSUIETR9509 North Carolina State 
University Raleigh NC 2005 
19 Breiman L Random forest Machine learning 45 5–32 2001 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 342–349 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A TwoStage Reduction Method Based on Rough Set  
and Factor Analysis 
Zheng Liu1 Liying Fang1 Mingwei Yu2 Pu Wang1 and Jianzhuo Yan1 
1 Beijing University of Technology Beijing China 
2 Beijing Hospital of Traditional Chinese Medicine Beijing China 
fangliyingwangpuyanjianzhuobjuteducn 
catherine5656yumingwei1120163com 
Abstract The performance of dimensionality reduction on multiple relevant 
and uncertain data is not satisfied by a single method Therefore in this paper we 
proposed a twostage reduction method based on rough set and factor analysis 
RSFA This method integrates the advantages of feature selection on treating 
relevant and the advantages of rough set RS reduction on maintaining classi
fication power At first a RS reduction is used to remove superfluous and  
interferential attributes Next a factor analysis is utilized to extract common 
factors to replace multidimension attributes Finally the RSFA is verified by 
using traditional Chinese medical clinical data to predict patients’ syndrome The 
result shows that less attributes and more accuracy can be expected with RSFA 
which is an appropriate reduction method for such problems 
Keywords reduction reduction factor analysis classification 
1 
Introduction 
The multiple correlative and uncertain data is ordinary in daily life Dimensionality 
reduction to this kind of data is a necessary pretreatment in the classification without 
enough samples 1 
Feature selection and feature extraction are two methods of dimensionality reduc
tion The principle component analysis PCA as a classical method of feature selec
tion can deal with the relevance of condition attributes 23 However classification 
power is decreased by losing information Rough set RS 4 reduction is one of fea
ture extraction methods 5 which can be used for reduction without scarifying the 
performance of classification especially for the uncertain data 6 Nevertheless the 
further discussion on RS reduction for relevant attributes is still in the stage of explo
ration as far as I know It is concluded that in a degree a single method is limited to 
solve reduction problem Winarski 7 integrated PCA and RS by using RS reduction to 
select the best principle component for PCA However the dependency between con
dition attributes and decision attributes is damaged by the linear transformation In 
order to overcome the shortcomings of existing methods a comprehensive dimensio
nality reduction method which integrates the advantages of feature selection and RS 
reduction is proposed in this paper 
 
A Twostage Reduction Method Based on Rough Set and Factor Analysis 
343 
 
Section 2 the introduction of basic concepts and a frame of the method and algo
rithm are discussed Experiments based on the Traditional Chinese Medical TCM 
clinical symptoms and results analysis are presented in section 3 Section 4 contains 
some conclusions and some ideas for further work 
2 
Framework and Algorithms of RSFA  
RS reduction can improve the clarity of knowledge system and give a better support to 
classification when there are uncertain data such as noisy inaccuracy and incomplete 
data 
21 
Basic Concepts 
A set
  
S
= U A V f
 is defined to represent an information system where U is object 
sets 

f U
A
V
×
→
is the information function If A
C
D
=
∪
and C
D
φ
∩
=
 the S is 
a decision table where C is conditions sets D decision sets  
Given a subset of attributes B
⊆ A
and a subset X
⊆ U
 the Blower approxima
tion


B
− X
of the set X is defined as follow 





 


B
X
x
U x
IND B
x
X
−
=
∈
∈
∧
∈
 
1
Where
 
IND B is an indiscernibility relation on subset B For S with A
C
D
=
∪
 and a 
subset P
⊆ C
 a positive region


POSP
D is defined as 






P
X
IND D
POS
D
P
X
∈
= ∪
 
2


POSP
D contains all objects in U that can be classified without an error into distinct 
classes defined by


IND D  RS theory determines a degree of attributes’ dependency 
based on


POSP
D  


 
 



P
P D
card POS
D
card U
γ
=
 
3
where
card
is a cardinality of a set The  
γ P D
is one of the most important index 
for rough sets It presents a ratio of the number of objects that can be classified by 
attributes P absolutely to the number of all objects It can be considered as a measure of 
dependency of condition subsets P to decision attributes 
22 
Framework of RSFA  
For the multiple correlative and uncertainly data one single method is limited to solve 
the complicated problem A twostage dimensionality reduction method based on RS 
attributes reduction and FA RSFA is proposed to obtain the reducts for the attributes 
sets with these characteristics in this paper At first RS attribute reduction is used to 
remove superfluous and interferential attributes without losing the power of  
344 
Z Liu et al 
 
classification After that FA is used to extract common factors CF for the correlative 
attributes to get the further reduction and stronger power of classification Figure 1 
shows the framework of the RSFA system 
 
 
Fig 1 Framework and process of the RSFA 
In the RSFA method a RS reduction based on genetic algorithms GA is chosen to 
implement the first stage reduction The solutions are represented in binary as strings of 
0s and 1s After generating a population the fitness of each individual is evaluated The 
better individuals are selected from the current population based on their fitness Then 
genetic operators crossover mutation and inversion are used to modify the individuals 
to form a new population The algorithm will be ended if the maximum amount of 
generations is produced or the average fitness is not improved over the predefined 
amount of generations The fitness function is defined in Eq 4 which is proposed by 
Dr Aleksander 8 use 
 
γ P D
 conception in RS 
 
1

  

s
N
L
F s
C s D
N
ρ
ρ
γ
−
=
−
+
×
 
4
The larger the
 
F s of an individual is the stronger its adaptive capacity of the indi
vidual has Where s is the string encode candidate reduct The
 
C s is a condition sets 
corresponding s N is the cardinality of C
sL is the cardinality of the selected condition 
subsets In Eq 4 the first term rewards less attributes and the second tried to ensure 
the classification capacity of attributes set and the ρ is a parameter used to revise the 
weighing Because the number of attributes and the classification quality are both 
considered in this fitness it not only removes superfluous and interferential attributes 
but also can avoid losing classification capacity due to overmuch reduction 
 
A Twostage Reduction Method Based on Rough Set and Factor Analysis 
345 
 
With the acceptable classification capability the number of attributes of the subset is 
various in the same generation of individuals The subset that contains the maximum 
attributes is chosen to the second stage reduction because it has stronger capacity and 
less relevant information 
After calculating the similarity coefficient a FA model is established by the relevant 
attributes subset 

1
2



k
X
X
 X
（ k
∈ m
） 
1
11
1
12
2
1
1
2
21
1
22
2
2
2
1
1
2
2
p
p
p
p
k
k
k
kp
p
k
X
a F
a F
a F
E
X
a F
a F
a
F
E
X
a F
a F
a F
E
=
+
+
+


=
+
+
+



=
+
+
+

…+
…+
…
…+
 
5
In Eq 5 


1
 2


T
p
F
F F
F
=
…
is a CF vector 

ij 
k
p
A
a
×
=
is a factor loading matrix 
that can be used to measure the properties of the X by a linear combination of the 
underlying factor F  
23 
Algorithms of RSFA  
There are two algorithms used to implement the RSFA method a RS reduction al
gorithm which consider the function in Eq 4 as the fitness function b Mining of 
relevant attributes algorithm based on the model represent in Eq 5  
 
Algorithm 1 RS reduction algorithm based on GA 
Input An original data of

 
S
U C
D V f
=
∪
  predefine the iteration N 
Output Reduction subset s  that has stronger ability and most attributes 
Step1 Set iteration t=0 and s
= φ
 
Step2 Generate an initial population
 
P t and set an individual solution
g
s
= s
 
Step3 Find all adaptable solutions 
 
Step31 Evaluate the fitness
 
F s of every individual in the population
 
P t  
 
Step32 Select the individual that has the maximal 
 
F s  value 
  
If 
 
 
F s
 F s
 then select this individual 

s
= s
 
  
If 
 
 
F s
 F s
 then discard this individual s
= s
 
  
If 
 
 
F s
= F s
 then add this individual 

s
s
s
=
∪
 
 Step33 Generate 

P t +1
through crossover and mutation operators 
 Step34 Judge the stop criteria If the maximum number of generations is pro
duced or the average fitness is not improved over the predefined number of generations 
then stop otherwise go back to step3 
Step4 Count the number of the attributes in each selected reduction subset and 
consider the subset which contains the most attributes as the reduction subset s 
 
346 
Z Liu et al 
 
Algorithm 2 Mining of relevant attributes algorithm based on FA 
Input The reduction subset s which is the output of Algorithm 1 
Output The finally result of the dimensionality reduction 
Step1 Calculate the proportion of missing values for each attribute If the proportion 
more than 5 then deletes this attribute that contain less information 
Step2 Standardize the remain attributes and calculate the similarity coefficient 
Step3 KMO test select relevant attributes

1
2



k
X
X
 X
 
Step4 Extract the CF by PCA and estimate the factor loading matrix 
Step5 Varimax orthogonal rotation is used on the loading matrix 
Step6 Calculated the scores on every CF by regression 
Step7 Use factor scores instead of relevant multivariables The finally subset result 
of the dimensionality reduction is consist of these factor scores and other irrelevant 
attributes  
3 
Results and Analysis 
31 
Experiments Data 
Symptom which is mainly obtained through observation and interrogation by doctors 
is relevant information to a disease Symptom is multiple correlative and uncertain 
data A decision system

 
S
U C
D V f
=
∪
 is established in Table 1 
In this experiment 610 samples collected from a hospital were contained in the S 
Each instance has 35 condition attributes The attribute set
1
C  contains 17 clinical 
symptoms and each symptom is divided into four grades marked as 012 and 3 ac
cording to the severity The attribute set
2
C contains 18 tongue and pulse attributes 
which are represented in binary 0 or 1 whether or not the attribute is appeared The 
syndrome is a decision attribute Ⅰ Ⅱ and Ⅲ are accordingly defined as the vacu
ity repletion and vacuityrepletion complication 
Table 1 Decision system S of TCM symptom and syndrome 
sam
ple 
Attributes 
1
2
C
C
C
=
∪
 
syn
drom
e 
C11
C12 
C1i 
C117 
C21 
C22 
C2j 
C218 
brea
thehard 
weari
ness 
… 
asthenic 
fever 
red 
tongue 
Thin 
fur 
… 
slow  
pulse 
1x  
1 
1 
… 
0 
1 
0 
… 
1 
Ⅰ 
2x  
2 
3 
… 
0 
1 
0 
… 
1 
Ⅱ 
nx  
… 
… 
… 
… 
… 
… 
… 
… 
… 
x610
2 
1 
… 
0 
0 
0 
… 
0 
Ⅲ 
 
A Twostage Reduction Method Based on Rough Set and Factor Analysis 
347 
 
32 
Experiments and Results 
The RSFA is used to reduce the dimensions At first the superfluous and inter ferential 
attributes are removed using the algorithm 1 mentioned in section 23 and the sub
set

 








1
2
1
1
2
2

C
C
C
C
C
C
C
=
∪
⊆
∧
⊆
can be obtained Set popsize=70 Pc=03 and 
Pm=005 Next a correlation test is performed for the
1
C  The result kmo=076 kmo  
06 indicated that it is necessary to treat the relativity to
1
C set using the algorithm 2 
The finally subset result is


 
result
ind
S
U C
M
D V f
=
∪
∪
  where the

Cind
is the union 
of the

2
C and the irrelevant attributes in
1
C  the M is the scores of the CF extracted from 
the relevant attributes  
In this experiment three methods are used to implement the reduction for the system 
S which include PCA RS reduction and the RSFA proposed by this paper Based on 
these reduction methods classification accuracy is calculated by LSSVM with the 
RBF kernel function sig2=3 and gam=40 To estimate how accurately a predictive 
model will perform in practice the 10crossvalidated is used in LSSVM and repeats 
the experiment 5 times It can be seen from these experiments the accuracy results of 
classification based on RSFA reduction are better than based on other reduction me
thods The dimension of the reduction subset and the average accuracy of classification 
based on different reduction methods are shown in Table 2 
Table 2 Dimension and average accuracy of classification based on different reduction 
 
unreduction 
PCA 
RS 
RSFA 
Dimension 
35 
28 
21 
21 
Accuracy rate 
8003 
7853 
7928 
8327 
 
Compared with the performance of the PCA higher accuracy and fewer dimensions 
can be got through the classification based on RS and RSFA reduction For the latter 
ones evaporative rate of the attributes 9 are 40 more than 30 which illustrate the 
reduction is satisfied according to the experience 10 Especially for the TCM 
symptoms data the accuracy of classification based on RSFA reduction is increased by 
5 compared with RS reduction in the case of getting same number of attributes 
Compared with the performance of PCA the evaporative rate of attributes of RSFA is 
increased by 20 and the accuracy of classification based on RSFA is increased by 6 
RSFA reduction improved the classification ability even higher than the original not 
only because it removed interferential attributes but also because it held information by 
using CFs 
To validate the RSFA on different LSSVM parameters another experiment com
pared RSFASVM with RSSVM is taken The 10crossvalidated is used on each 
group and the experiment is repeated 5 times Figure 2 shows the average accuracy 
From the Figure 2 it can be seen that RSFA has a better performance than RS on 
different parameters It illustrate that RSFA is more suitable to deal with the dimen
sionality reduction for relevant attributes data set 
348 
Z Liu et al 
 
050
055
060
065
070
075
080
085
090
095
100
3
4
5
30
40
50
gam
accuracy
RSSVM
RSFASVM
 
Fig 2 Result of the comparison between RSFA and RS 
Next we tried to use some UCI datasets to verify the performance of RSFA reduc
tion However the relevance between attributes tested by KMO P04 are not sta
tistically significant namely factor analysis is not appropriate for dealing with them 
so the extra results have not shown here 
4 
Conclusion 
This paper discusses the advantages and shortcomings of single approach to dimen
sionality reduction Therefore we propose a twostage synthetic method named 
RSFA to reduce the dimension for multiple relevant and uncertain data The RSFA 
integrates the different advantages of RS reduction and FA RSFA complete reduction 
by removing superfluous interferential attributes and extracting common factors to 
replace the multiattributes Experiments based on the data of TCM clinical symptoms 
are made to verify the effectiveness Results demonstrate better performance on di
mensions and accuracy of classification based on RSFA reduction than other methods 
This paper only focused on solving the numeric variable a correlation analysis 
method based on RS reduction for multitype of data could be explored in the next 
researching 
 
Acknowledgements This paper is supported by 2010 Program for Excellent Talents in 
Beijing Municipal Organization Department 2010D005015000001 the New Cen
taury National Hundred Thousand and Ten Thousand Talent Project and got the 
cooperation with Beijing Hospital of Traditional Chinese Medicine Affiliated to 
CPUMS Special thanks have been given there 
References 
1 Wang XY Yang J Teng XL Xia WJ Jensen R Feature Selection Based on Rough 
Sets and Particle Swarm Optimization Pattern Recogn Lett 28 459–471 2007 
2 Tsai CF Feature Selection in Bankruptcy Prediction KnowlBased Syst 22 120–127 
2009 
 
A Twostage Reduction Method Based on Rough Set and Factor Analysis 
349 
 
3 Ravi V Pramodh C Threshold Accepting Trained Principal Component Neural Network 
and Feature Subset Selection Application to Bankruptcy Prediction in Banks Applied Soft 
Computing 8 1539–1548 2008 
4 Pawlak Z Rough Sets Interantional Journal of Computer  Information Sciences 11 
341–356 1982 
5 Chouchoulas A Shen Q Rough Setaided Keyword Reduction for Text Categorization 
Appl Artif Intell 15 843–873 2001 
6 Slezak D Degrees of Conditional in Dependence A Framework for Approximate 
Bayesian Networks and Examples Related to the Rough Setbased Feature Selection In
form Sciences 179 197–209 2009 
7 Swiniarski RW Skowron A Rough Set Methods in Feature Selection and Recognition 
Pattern Recogn Lett 24 833–849 2003 
8 Vinterbo S Ohrn A Minimal Approximate Hitting Sets and Rule Templates Int J Ap
prox Reason 25 123–143 2000 
9 Shu HC Comparative Study of Classification Method in Traditional Chinese Medicine 
Differentiation vol PhD 2008 in Chinese 
10 Dietterich TG Machinelearning Research  Four Current Directions Ai Mag 18 
97–136 1997 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 350–356 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Eyebrow Segmentation Based on Binary Edge Image 
Jiatao Song12 Liang Wang1 and Wei Wang1 
1 School of Electronic and Information Engineering  
Ningbo University of Technology Ningbo 315016 China 
2 Zhejiang Provincial Key Laboratory of Information Network Technology  
Hangzhou 310027 China 
sjt6612163com 
Abstract Eyebrow is one of the most salient face features It has a lot of poten
tial applications in face recognition nonverbal communication and so on In this 
paper a novel eyebrow segmentation method based on binary edge image BEI 
is proposed Our method firstly extracts BEI from a grayscale face image and 
then connections between different face components in a BEI are removed using 
a specially designed algorithm After that some eyebrowanalogue segments are 
extracted from a BEI based on the geometrical property of eyebrows The fourth 
step is to locate eyebrows using integral projection approach Finally the peri
meter of an eyebrow block is extracted to finish the segmentation of an eyebrow 
Experimental results on a set of 517 AR images with different facial expression 
and illumination show that a correct eyebrow segmentation rate of 934 is 
achieved indicating that the proposed method is robust to facial expression and 
illumination changes 
Keywords Eyebrow segmentation Binary Edge Image BEI connection re
moval illumination change expression change 
1 
Introduction  
Eyebrow is the most salient and stable feature in a human face Studies show that 
eyebrows play important roles in emotional expression and nonverbal communication 
as well as in facial aesthetics and sexual dimorphism 1 Sadr’s research 1 suggests 
that for face recognition the eyebrows may be at least as influential as the eyes They 
found that the absence of eyebrows in familiar faces leads to a very large and signifi
cant disruption in recognition performance Sinha 2 also pointed out that of the dif
ferent facial features eyebrows are among the most important for recognition Besides 
for face recognition eyebrows are also used for gender classification 3  
In order to extract eyebrow feature eyebrows should be firstly segmented from a 
face image Ref 4 locates eyebrows by means of PCA based template matching 
while Ref 5 finds a rectangular region enclosing the eyebrow based on clustering in 
HIS domain Chen et al 6 proposed a spatial constrained subarea Kmeans  
clustering method for eyebrow segmentation Recently Ding et al 7 proposed an 
eyebrow segmentation method by using color and gradient information But their 
method requires eyes to be firstly located  
Although some progress has been made the problem of automatic eyebrow seg
mentation is still far from being fully solved owing to its complexity Particularly 
 
Eyebrow Segmentation Based on Binary Edge Image 
351 
 
when there is facial expression or illumination change in a face image the performance 
of the existing methods will decrease greatly The work described in this paper focuses 
on the study of eyebrow segmentation from human face images with different facial 
expression and lighting conditions A novel eyebrow segmentation method based on 
the binary edge image BEI 8 is presented  
The rest of this paper is organized as follows In section 2 the method for the ex
traction of BEI is described briefly Section 3 presents our eyebrow segmentation 
approach Experimental results are given in section 4 Section 5 concludes our work  
2 
Extraction of Binary Edge Image BEI 
In order to precisely locate eyes from a human face image Song et al 8 proposed an 
approach for the extraction of BEI from a grayscale face image Their method is based 
on the multiscale analysis of wavelet transform WT Some BEIs are shown in Fig 1 
From this figure we can see that for different facial expression and illumination face 
components especially eyes and eyebrows are clearly extracted in BEIs The shape 
and contour of these components are very clear This is very helpful for the segmenta
tion of face components 
One problem with BEIs is that sometimes pixels corresponding to different face 
components such as eyebrows eyes and nose are connected This hampers the seg
mentation of face components Thus in our work an algorithm for the removal of 
connection of pixels belonging to different face components is proposed in section 32  
 
 
 
a Smile 
b Anger 
c Scream 
d Left light 
e Right light 
f Both side lights 
Fig 1 Examples of BEIs with different facial expression and illumination 
3 
The Proposed Method  
31 
Overview of the Proposed Eyebrow Segmentation Method 
Fig 2 shows the diagram of our proposed method The first step is to remove some very 
large foreground segments from a BEI for these segments often correspond to hair and 
are unlikely to be eyebrow segments The second step is to fill some holes in the seg
ments of BEI After that an algorithm is designed to decrease the connection of dif
ferent face components The next step is to extract some eyebrowanalogue segments 
from a BEI according to some geometric properties of eyebrows The fifth step is to 
detect the eyebrow segments using integral projection method and finally the contour 
of an eyebrow segments is extracted to finish the segmentation of eyebrows 
352 
J Song L Wang and W Wang 
 
 
Fig 2 Diagram of the proposed eyebrow segmentation method 
 
 
Fig 3 BEIs after large segments and connection between different face components being 
removed upper row and BEIs after eyebrowanalogue segments being extracted lower row 
32 
Removal of Connections between Different Face Components 
From Fig 1 we can observe that the foreground pixels black pixels in BEIs may 
correspond to two kinds of pixels in grayscale images The first kind is the pixels in 
eyebrows and iris which often have low intensities The second kind is the pixels on 
image edges which reflect the contour of face components and have relatively larger 
intensities If we remove the second kind of foreground pixels from BEIs the connec
tion between different face components can be decreased greatly  
Let I denote a BEI J denote the corresponding grayscale image The steps for 
connection removing include 1 filling holes in I  2 marking all the 8connected 
components in I  and extracting the perimeter pixels of each component 3 calculating 
the mean intensity of the perimeter pixels of each component in J   and using it as 
threshold value to binarize all the pixels in this component in J  Pixels with larger 
intensities are segmented as background pixels  
BEIs after above operations are shown in the upper row of Fig 3 Obviously the 
connection between different face components especially that between eyebrow and nose 
and that between eyebrow and eye decreases greatly  
Remove large area 
segments 
Remove connection between 
different face components  
Fill holes in 
BEI
Detect eyebrow 
segments 
Extract the contour of 
eyebrow segments 
Extract eyebrow 
analogue segments  
 
Eyebrow Segmentation Based on Binary Edge Image 
353 
 
33 
Extraction of EyebrowAnalogue Segments 
Statistical analysis shows that the aspect ratio of the bounding rectangle of an eyebrow 
normally ranges from 15 to 75 when the rotationinplane of a face is less than 15 
degrees Thus in our method segments in BEIs with aspect ratios of bounding rectangle 
outside of this range are classified as noneyebrow blocks Besides segments with too 
small areas are viewed as noise By removing these two kinds of segments directly 
from BEIs BEIs with only eyebrowanalogue segments such as eyebrows eyes and 
mouth retained are achieved just as shown in the lower row of Fig 3 This greatly 
simplifies the segmentation of eyebrows 
34 
Detection of Eyebrow Segments 
In our method the integral projection method and the prior knowledge about the con
figuration of face components on a face are employed for the detection of eyebrow 
segments The prior knowledge used includes 1 there are two eyebrows on a face 2 
eyebrows are above eyes 3 distance between eyes and eyebrows is larger than that 
between eyes and mouth  
 
 
Fig 4 Horizontal b and vertical c integral projection curves of a BEI a 
Fig 4 b and Fig 4 c shows a typical horizontal and vertical integral projection curve 
of a BEI with only eyebrowanalogue segments remained respectively In the horizontal 
projection curve there always exist some strong local peaks at the positions of eyebrow 
eye and mouth Based on this some candidate horizontal eyebrows strips can be obtained 
Then by projecting this strips vertically and utilizing the prior knowledge discussed 
above eyebrow segments can be detected  
After an eyebrow block is segmented its perimeter is extracted as the contour of an 
eyebrow which means the finish of eyebrow segmentation 
a 
b 
c 
354 
J Song L Wang and W Wang 
 
 
 
 
 
 
 
Fig 5 Eyebrow segmentation results of AR images with different expression and illumination 
 ag Neutral smile anger scream left light on right light on and both side lights on 
4 
Experimental Results and Discussion  
517 AR images 9 with different facial expressions and illuminations are used for 
eyebrow segmentation experiments The four different facial expressions include 
neutral smile anger and scream and the three illumination conditions are left light on 
right light on and both side lights on Images with eyebrows being correctly segmented 
are shown in Fig 5 The white contour represents the segmented shape of an eyebrow 
which coincides well with the actual perimeter of an eyebrow Further analysis shows 
a 
b
c
d
e 
f
g 
 
Eyebrow Segmentation Based on Binary Edge Image 
355 
 
that of all the 517 images used our method achieves a correctly eyebrow segmentation 
rate of 934 indicating that our eyebrow segmentation approach is robust to expres
sion and illumination changes 
The success of the proposed eyebrow segmentation method can be contributed to 
two factors One is the high quality of BEIs Eyebrows in BEIs with different expres
sion and lighting are wholly extracted and noise is little which makes eyebrows seg
mentation possible The other factor is the connection removal algorithm proposed in 
this paper which greatly simplifies the segmentation of eyebrows  
5 
Conclusions 
Eyebrow is important for face recognition facial expression analysis nonverbal 
communication and so on Its segmentation is the key step for eyebrowbased face 
image analysis system In order to address the problem of eyebrow segmentation under 
varying expression and illumination conditions a novel eyebrow segmentation method 
is proposed in this paper Our method is based on the binary edge image BEI and 
includes a specially designed algorithm for the removal of connection between dif
ferent face components from a BEI Experimental results on 517 AR images with 
different expression and illumination show that our method achieves a successful 
eyebrow segmentation rate of 934 indicating that the proposed method is robust to 
facial expression and illumination changes Based on the extracted eyebrows we will 
construct a high performance face recognition system in the future 
Acknowledgements The work described in this paper is partially supported by a 
project from the National Natural Science Foundation of China Grant No60972163 a 
project from the Natural Science Foundation of Zhejiang Province of China Grant 
NoY1110086 a project from the Natural Science Foundation of Ningbo of China 
Grant No2009A610090 an open project of Zhejiang Provincial Most Important 
Subject of Information Processing and Automation Technology Grant No 
201100808 an open project of Zhejiang Provincial Key Laboratory of Information 
Network TechnologyGrant No 201109 and two projects from the Open Fund of 
Mobile Network Application Technology Key Laboratory of Zhejiang Province Grant 
No MNATKL2011001 MNATKL2011003 
References 
1 
Sadr J Jarudi L Sinha P The Role of Eyebrows in Face Recognition Perception 32 
285–293 2003 
2 
Sinha P Balas B Ostrovsky Y Russell R Face Recognition by Humans Nineteen 
Results All Computer Vision Researchers Should Know About Proceedings of the 
IEEE 9411 1948–1962 2006 
3 
Dong Y Woodard DL Eyebrow Shapebased Features for Biometric Recognition and 
Gender Classification A Feasibility Study In 2011 International Joint Conference on 
Biometrics pp 1–8 IEEE Press New York 2011 
4 
Kapoor A Picard RW Realtime Fully Automatic Upper Facial Feature Tracking In 
5th International Conference on Automatic Face and Gesture Recognition pp 8–13 IEEE 
Press New York 2002 
356 
J Song L Wang and W Wang 
 
5 
Lee CH Kim JS Park KH Automatic Human Face Location in a Complex Back
ground Using Motion and Color Information Pattern Recognition 2911 1877–1889 
1996 
6 
Chen Q Cham W Lee K Extracting Eyebrow Contour and Chin Contour for Face 
Recognition Pattern Recognition 40 2292–2300 2007 
7 
Ding L Martinez AM Features versus Context An Approach for Precise and Detailed 
Detection and Delineation of Faces and Facial Features IEEE Transactions on 
PAMI 3211 2022–2038 2010 
8 
Song J Chi Z Liu J A Robust Eye Detection Method Using Combined Binary Edge and 
Intensity Information Pattern Recognition 396 1110–1125 2006 
9 
Martinez AM Benavente R The AR Face Database CVC Technical Report 24 1998 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 357–364 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Xray Image Contrast Enhancement Using the Second 
Generation Curvelet Transform 
Hao Li and Guanying Huo 
 School of Computer and Information Technology Beijing Jiaotong University  
Beijing 100044 People’s Republic of China 
lihaobju163com 
Abstract In this paper a novel Xray image contrast enhancement method 
using the second generation curvelet transform is proposed in order to better 
enhance contrast and edges while remove noise First source images are 
decomposed in the curvelet transform domain A nonlinear  enhancement 
operator is applied to low frequency subbands to enhance global contrast 
Combining with threshold denoising the nonlinear  enhancement operator is 
also applied to high frequency subbands to enhance edges and reduce noise 
Finally the processed coefficients are reconstructed to obtain enhanced images 
Experimental results on Xray images show that compared with histogram 
equalization and wavelet based contrast enhancement the proposed method can 
effectively enhance contrast and edges of Xray images while better reducing 
noise thus has better visual effect 
Keywords Xray image contrast enhancement curvelet transform nonlinear 
enhancement operator denoising 
1 
Introduction 
Xray is used to capture the internal body structure images which help a lot to the 
radiologists in recognizing the internal problems It is the most useful imaging 
modality to check for the bone fractures and other related anomalies Though there 
are numerous advantages of Xray technology but it generates low contrast images 
1 In addition to the Xray penetration characteristics Xray image contrast is 
significantly affected by scattered radiation and the contrast characteristics of the 
receptor and display system One can increase the power of Xrays for capturing 
images but it may harm human body Moreover the contrast of small objects within 
the body and anatomical detail is reduced by image blurring As the Xray images are 
being used for diagnostic purposes some contrast enhancement techniques should be 
implemented in manual or autodiagnose system to make the Xray images more 
visual and explanatory Image enhancement is a significant part for Xray inspection 
systems 
As the reasons stated above contrast enhancement is commonly required for the 
captured Xray images Various spatial and frequencybased techniques have been 
developed for image enhancement Commonly used spatial techniques are linear 
stretch histogram equalization 2 convolution mask enhancement 3 adaptive 
358 
H Li and G Huo 
 
histogram equalization 4 etc These conventional spatial techniques are usually 
simple fast and useful for noiseless images but they may cause noise amplification 
when the images have more noise Noise not only lowers visual quality but can cause 
feature extraction analysis and recognition algorithms to be unreliable  
Wavelet based contrast enhancement is a representative of frequencybased 
enhancement methods In 5 denoising and feature enhancement are achieved by 
simultaneously lowering noise energy and raising feature energy in the wavelet 
transform domain Compared with spatial enhancement techniques wavelet based 
contrast enhancement can better remove noise while effectively enhancing contrast 
and edges Despite the fact that wavelets have had a wide impact in image processing 
they fail to efficiently represent objects with highly anisotropic elements such as lines 
or curvilinear structures eg edges The reason is that wavelets are nongeometrical 
and do not exploit the regularity of the edge curve 6 In addition separable wavelets 
can capture only limited directional information For these reasons wavelet based 
contrast enhancement often cause distortion at edges  
The curvelet 6 transform was developed as an answer to the weakness of the 
separable wavelet transform in sparsely representing lines curves and edges 
Curvelets take the form of basis elements which exhibit high directional sensitivity 
and are highly anisotropic The curvelet transform has had an important success in a 
wide range of image processing applications including denoising deconvolution etc 
It is expected that directionality capability of the curvelet transform results in better 
edge representation and enhancement in 2D images Therefore in this paper the 
second generation curvelet transform 7 which is conceptually simpler faster and far 
less redundant than the first generation is used for enhance Xray images A new 
nonlinear enhancement operator is proposed and applied to coefficients in the curvelet 
transform domain Experimental results show that the proposed method is superior to 
both histogram equalization and wavelet based contrast enhancement in both visual 
effect and quantitative measurement 
2 
Second Generation Curvelet Transform 
The curvelet transform has gone through two major revisions The first generation 
curvelet transform 6 uses a complex series of steps involving the ridgelet analysis of 
the radon transform of an image The first generation curvelet transform was 
exceedingly slow In order to overcome this the second generation curvelet transform 
based on fast Fourier transform was proposed by 7 
21 
ContinuousTime Curvelet Transforms 
We work in two dimensions ie R2 with spatial variable x with ω  a frequency
domain variable and with r  and θ  polar coordinates in the frequencydomain 
We start with a pair of windows Wr and Vt which we will call the “radial 
window” and “angular window” respectively These are smooth nonnegative and 
realvalued with W taking positive real arguments and supported on 
r ∈
12 2 and 
V taking real arguments and supported on ∈
t
−1 1 
 Xray Image Contrast Enhancement Using the Second Generation Curvelet Transform 
359 
 
For each j≥j0 we introduce the frequency window Uj defined in the Fourier domain 
by 6 



2
 2
2
2
  
 2
3  4
π
θ
θ
j
j
j
j
r V
W
r
U
−
= −
 
 1
where 
j 2
is the integer part of j2 
Define the waveform 
ϕ j x
 by means of its Fourier transform
 
ˆ  
ω
ω
ϕ
j
j
= U
 
ϕ j x
 can be seen as a “mother” curvelet since all curvelets at scale 
2− j
 are 
obtained by rotations and translations of 
ϕ j x
 Introduce the equispaced sequence of 
rotation angles

 l
j
l
⋅
⋅
=
−
 2
2π 2
θ
 with 
⋅⋅⋅
=
1 0
l
such that 
π
θ
2
0

≤
l
 and the 
sequence of translation parameters 
2
2
1



Z
k k
k
∈
=
 
With these notations we define curvelets by 



 
  
 
j l
k
j
j l k
x
x
R
x
l
−
=
θ
ϕ
ϕ
 
2
where 
θ
R  is the rotation by θ  radians 
A curvelet coefficient is then simply the inner product between an element 


f ∈ L2 R2
 and a curvelet 
l k
j 
ϕ 
 
x dx
f x
f
c j l k
R
j l k
j l k
= 
=
2
 
 

    
 
 
ϕ
ϕ
 
3
22 
Digital Curvelet Transforms 
In the definition 1 the window Uj smoothly extracts frequencies near the dyadic 
corona and near the angle Coronae and rotations are not especially adapted to 
Cartesian arrays Instead it is convenient to replace these concepts by Cartesian 
equivalents here “Cartesian coronae” based on concentric squares and shears 
Define the “Cartesian” window 
 
  
   
ω
ω
ω
j
j
j
V
W
U
=
 
4
 
 ω
Wj
 is a window of the form 
0
          j

 
  
2
2
1
≥
− Φ
Φ
=
+
ω
ω
ω
j
j
Wj
 
5
where Φ  is defined as the product of lowpass one dimensional windows 

 2
2



2
1
2
1
ω
ω φ
φ
ω
ω
j
j
j
−
−
=
Φ
 
6
The function φ  obeys 
1
0
≤ φ ≤
 might be equal to 1 on −12 12 and vanishes 
outside of −2 2 
Finally the digital curvelet transform coefficient 7 is obtained by 
360 
H Li and G Huo 
 
ω
ω
ω
ω
θ
θ
d
e
S
U
f
c j l k
b
i S
j
T
l
l


−
−
= 

1 
 
ˆ
   
 
7
3 
Contrast Enhancement Method 
First source Xray images are decomposed in the curvelet transform domain to obtain 
low frequency subband and different scales of highfrequency subbands Then in the 
curvelet transform domain low frequency subband coefficients and highfrequency 
subbands coefficients are processed using nonlinear enhancement operator separately 
Finally the processed coefficients are reconstructed to obtain the enhanced images 
The nonlinear enhancement operator low frequency subband coefficients 
enhancement and highfrequency subbands coefficients enhancement are the keys of 
the proposed method therefore they will be described in detail as follows 
31 
Nonlinear Enhancement Operator 
Inspired by GAG enhancement operator which is used in 5 we first define a 
normalized nonlinear enhancement operator to enhance the second generation 
curvelet coefficients in each subband 




 

 
b
sigm c x
b
sigm c x
a
G x
+
−
−
−
=
 
8
where

1


 1
1
b
c
sigm
b
sigm c
a
+
−
−
−
=


11
 
e x
sigm x
+ −
=
 b is the enhancement 
and attenuation split point and c is the enhancement or attenuation rate 
32 
Low Frequency Subband Coefficients Enhancement 
Low frequency subband contains the basic information and the overall contrast of the 
image Therefore the processing of lowfrequency coefficients is particularly critical 
and can not be ignored In order to overcome the inflexibility of a single operator a 
piecewise nonlinear operator for low frequency subband coefficients enhancement is 
proposed based on the nonlinear enhancement operator defined above The piecewise 
nonlinear operator is defined by 




≥
=
b
Max
i j
C
Max
i j
C
G
i j
C
S
b
Max
i j
C
Max
i j
C
G
i j
C
S
i j
MAGL C
|
 
|

|
 
|
  

|
 
|

|
 
|
  

  

11
11
2
11
2
11
11
1
11
1
11
 
9
where c11i j are low frequency subband coefficients with i and j are row index and 
column index Max is the maximum absolute value of low frequency subband 
coefficients S1 and S2 are gain factors G1 and G2 are nonlinear enhancement operators 
with the same parameter b but different parameter c In order to guarantee the 
continuity of piecewise operator we have S2 = S1G1bG2b b is calculated by b = 
MeanMax where Mean is mean absolute value of low frequency subband 
coefficients In our experiments c1 and c2 are always set to 10 and 20 
 Xray Image Contrast Enhancement Using the Second Generation Curvelet Transform 
361 
 
33 
HighFrequency Subbands Coefficients Enhancement 
Highfrequency subbands contain not only edges and details of the image but also 
noise In order to enhance edges while suppressing noise the noise varianceσ  should 
be estimated and the threshold T for denoising should be determined σ  is estimated 
at the finest subband using MAD method 8 proposed by Donoho and the threshold 
T is calculated by
σ
λ ⋅  where λ  is a constant between 3 and 4 In our experiments λ  
is set to 35 Taking into account noise highfrequency subbands enhancement 
operator is a bit different from that of low frequency subband and is defined by 




≥
+
=
T
i j
C
T
i j
C
T
i j
C
i j
G C
i j
C
S
i j
C
MAG
n
m
m n
m n
m n
m n
m n
H
|
 
|
0
|
 
|

|
 
| |
 
|
  

  







 
10
where cmni j are highfrequency subbands coefficients with m and n are scale index 
and angel index S is gain factor and G is nonlinear enhancement operator Since we 
need to enhance all the edges preserved after denosing and b equals 05 when 
|
 
|

i j
C
m n
 equals T b of operator G is set to 05 here Parameter c of G is set to 10 
in all our experiments 
34 
Contrast Evaluation Criterion for Image 
The contrast of enhanced images is evaluated employing the measure function which 
was proposed in 9 
2
1
1
1
1
2
  
1
  
1
 
 
′
−
′
=
=
=
=
=
M
x
N
y
M
x
N
y
contrast
x y
f
MN
x y
f
MN
C
 
11
where M and N are width and height of the original image 
  
f ′ x y
 is the enhanced 
image Larger the value of equation 11 is better the contrast of the image is 
Information entropy can be used to measure richness of the information in an 
image which is defined as follows 

log
E
i
i
p
p
i
=
 
12
where i is the gray value index and pi is the corresponding probability Larger E is 
richer the image information is 
4 
Experiment Results 
To demonstrate the effectiveness of our method we test it on real Xray images and 
compare it with the existing popular methods of histogram equalization HE and 
wavelet based contrast enhancement The first test image ie Fig1a is a low contrast 
Xray of human chest to resolve the related medical issues S1 is set to 3 and S is set to 
35 for enhancing Fig1a Enhancement results of Fig1a by different methods are  
 
362 
H Li and G Huo 
 
 
Fig 1 Enhancement results o
original image b image enhan
enhanced through proposed me
Table 1 Enh
Method
HE 
Wavelet 
Curvelet
 
shown in Fig1bg The
capture of hysterosalpingo
Fig2a The correspondin
2bg In visual analysis
been enhanced to various le
spread noise and destroys so
 
     
 
     
 
       
        
 
of chest Xray image from left to right and top to bottom
nced through HE c enhanced through wavelet based method
ethod e part of b f part of c g part of d 
hancement evaluation of Ccontrast and E for Fig1 
Ccontrast 
E 
00821 
54346 
00886 
68676 
00995 
73665 
e second test image Fig2a is another low contrast X
ogram S1 is set to 15 and S is set to 35 for enhanc
ng enhancement results of Fig2a are displayed in F
s of both Fig1 and Fig2 it is observed that contrast 
evels by all the three methods But histogram equalizat
ome edge details Wavelet based contrast enhancement 
 
 
 a 
d d 
ray 
cing 
Fig 
has 
tion 
can  
 Xray Image Contrast Enhance
 
 
 
Fig 2 Enhancement results o
bottom a original image 
method d enhanced through p
Table 2 Enh
Method
HE 
Wavelet 
Curvelet
 
suspend noise effectively b
proposed method can enhan
The human visualization
evaluate the performance o
entropy E have been cal
evaluation for Fig1 is give
Table 1 and Table 2 we ca
ement Using the Second Generation Curvelet Transform 
            
 
            
 
        
        
 
f hysterosalpingogram Xray image from left to right and to
b enhanced through HE c enhanced through wavelet ba
proposed method e part of b f part of c g part of d 
hancement evaluation of Ccontrast and E for Fig2 
Ccontrast 
E 
00799 
55892 
01013 
70174 
01180 
71786 
but it causes obvious artifacts and distortion at edges T
nce edges more precisely while removing noise effective
n is not considered as benchmark for image quality so
of above mentioned methods the contrast Ccontrast and 
culated for the output enhanced images Enhancem
en by Table 1 and that for Fig2 is given by Table 2 Fr
an clearly see that the proposed method can better enha
363 
op to 
ased 
The 
ely  
o to 
the 
ment 
rom 
ance 
364 
H Li and G Huo 
 
image contrast and protect image information Both visual effect and quantitative 
measurement have shown that the proposed method is superior to the existing  
ones 
5 
Conclusions 
The proposed method makes use of curvelet transform which is superior in denoising 
and providing a compact representation of linesingularities and also takes advantage 
of effectiveness of nonlinear enhancement operator The method was tested on real X
ray images and compared with the existing popular approaches of histogram 
equalization and wavelet based contrast enhancement Experimental results show that 
the proposed technique is superior to the existing ones in both visual effect and 
quantitative measurement 
Acknowledgments This research was funded by the National Natural Science 
Foundation of China Grant No 60972101 
References 
1 
Kanwal N Girdhar A Gupta S Region Based Adaptive Contrast Enhancement of 
Medical XRay Images In 2011 5th International Conference on Bioinformatics and 
Biomedical Engineering pp 1–5 IEEE Press Wuhan 2011 
2 
Chen SD Ramli AR Minimum Mean Brightness Error BiHistogram Eualization in 
Contrast Enhancement IEEE Trans on Consumer Electronics 49 1310–1319 2003 
3 
Polesel A Ramponi G Mathews VJ Image Enhancement via Adaptive Unsharp 
Masking IEEE Trans on Image Processing 9 505–510 2002 
4 
Zimmerman JB Pizer SM Staab EV Perry JR McCartney W Brenton BC An 
Evaluation of the Effectiveness of Adaptive Histogram Equalization for Contrast 
Enhancement IEEE Trans on Medical Imaging 7 304–312 1988 
5 
Zong X Laine AF Geiser EA Denoising and Contrast Enhancement via Wavelet 
Shrinkage and Nonlinear Adaptive Gain In Proc of SPIE vol 2762 pp 566–574 1996 
6 
Candès EJ Donoho DL Curvelets a Surprisingly Effective Nonadaptive 
Representation for Objects with Edges Vanderbilt University Press Nashville 2000 
7 
Candès EJ Demanet L Donoho DL Ying L Fast Discret Curvelet Transforms 
Applied and Computational Mathematics 1–43 2005 
8 
Donoho DL Denoising by Soft Thresholding IEEE Trans on Information Theory 41 
613–627 1995 
9 
Azriel R Avinash CK Digital Picture Processing Academic Press New York 1982 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 365–372 2012 
© SpringerVerlag Berlin Heidelberg 2012 
MMW Image Blind Restoration Using Sparse ICA  
in Contourlet Transform Domain 
Li Shang12 Pingang Su13 and Wenjun Huai1 
1 Department of Electronic Information Engineering Suzhou Vocational University  
Suzhou 215104 Jiangsu China 
2 Department of Automation University of Science and Technology of China  
Anhui 230026 Hefei China 
3 State Key Lab of Millimeter Waves Southeast University Nanjing 210096 Jiangsu China 
sl0930supghwjjssvceducn 
Abstract Sparse independent component analysis SPICA algorithm is 
effective in blind separation of superimposed images without having any priory 
knowledge about the image’s structure and statistics While a millimeter wave 
MMW image contains the refective information of imaging object and   
much unknown noise of imaging scene so the MMW image is too high blur to 
be discerned To obtain preferable MMW image combined the advantages of 
contourlet sparse transform and SPICA a new blind restoration method 
proposed by us of MMW images operating in the contourlet sparse transform  
domain is discussed in this paper Contourlet transform can retain the better 
contour of an image and make this image sparser in local subspace Here using 
the low frequency band and the high frequency bands of the first layer obtained 
by contourlet transform as the mixed input data of SPICA the task of MMW 
image restoration can be implemented In test the blind restoration of mixed 
natural images is also operated by using our method simultaneity using the 
single noise ratio SNR to measure the restored natural images experimental 
results testify the validity of our method in doing blind separation and it is 
feasible to restore the MMW image using this proposed method Further 
compared with methods of contourlet transform and fast ICA simulations again 
show that this MMW image restoration method proposed is indeed efficient in  
application   
Keywords Millimeter wave MMW Independent component analysis ICA 
Sparse ICA Contourlet sparse transform Image restoration 
1 
Introduction 
Millimeter wave MMW imaging technology has been widely applied in different 
fields because of its advantages 12 However because of the lower sensitivity of 
the hardware imaging system the MMW image obtained is of lower resolution at the 
same time much unknown noise is also added in the MMW image and makes the 
MMW image is difficult to be discerned 23 In fact the MMW image can be regard 
366 
L Shang P–g Su and Wj Huai 
 
as a mixed image obtained by the unknown imaging object and background noise 
Therefore the blind image separation method can be explored in MMW image  
restoration Independent component analysis ICA has been successfully used for 
blind source separation in many fields especially in image processing 4 A number 
of approaches such as natural gradient algorithm NGA 5 and FastICA 6 have 
been proposed However these iterative approaches process the sources directly so 
its separation quality depends heavily on the statistical properties of the sources  To 
avoid the inherent defects of ICA the assumption of separation of sparseness is 
developed which is called sparse ICA SPICA 79 This method can significantly 
improve accuracy and computational efficiency of the existing ICA algorithms 78 
The early SPICA use wavelet transform or wavelet packet transform to sparse the 
sources and behave preferable blind separation effect Although this sparse 
decomposition can represent discontinuities at edge points it will not see the 
smoothness along the contours existing in the natural images 89 In recent years 
the contourlet sparse transform has been used widely in image processing field 7 
10 This method can deal with singularity in higher dimensions especially in natural 
images having smooth contours Therefore here the ICA operated in the contourlet 
sparse transform domain is discussed in separating blind images Using the low 
frequency band and the high frequency bands of the first layer obtained by contourlet 
sparse transform as the mixed input of SPICA the task of MMW image restoration 
can be implemented  
2 
Sparse ICA  
21 
The Blind Source Separation Problem 
In a typical blind source separation BSS task M mixtures are observed which is 
denoted by matrix X  Each of available signals is assumed to be generated by a linear 
mixture of unknown sources denoted by matrix S   where the number of sources is 
usually assumed to be equal to that of the mixed signals Thus a M dimensional 
vector of observed signals is generated by the product of an unknown M
× N
mixing 
matrix A  and N dimensional vector of unknown source signals 7 The task of 
blind separation is to estimate the mixing matrix and then recover the source signals 
successfully  
A typical embodiment of this problem in the context of superimposed reflections is 
depicted in Fig1 The real object a is situated on the optical axis behind a semi
reflecting planar lens d inclined with respect to the optical axis Another object b 
is partially reflected by the lens creating a virtual image c The camera f records a 
superposition of the two images Thus the intensity of the observed mixed image is 
give by the following formula 
1
11
12
1
2
2
21
22
1
2
s
s
x
a
a
s
s
x
a
a
=
+


=
+

 
1
 MMW Image Blind Restoration Using Sparse ICA in Contourlet Transform Domain 
367 
 
where 
1s  and 
2s  are the images of two source objects a and b 
a11
 and 
a12
are 
constants and their specific values depend on the optical geometry and properties of 
the reflective medium It is assumed here that the problem is spatial invariant This 
reasonably good approximation of the physical conditions can be relaxed 7 Consi
dered the generality the Eqn1 is extended to the form in matrix notation  
where 
1
 2


m
x x
 x
 are the M  mixed images and 
1s  and 
2s are the source images 
represented as row vectors The mixing matrix is usually unknown unless side infor
mation regarding the physics of the mixing medium is available 78 Under the as
sumption that the source is statistically independent it is possible to recover sources 
1s  and  
2s  up to a permutation and multiplicative constant by estimating the mix
ing matrix A
 ≈ A
 and estimating the sources by its inversion 
This problem can be well solved using the sparse ICA method to be described in the 
following subsection 
 
Fig 1 The schematic diagram of the context of superimposed reflections 
22 
Sparse ICA 
Supposing that the source images are sparse the majority of pixels have a nearzero 
magnitude and under the assumption of statistical independence of the locations of 
nonzero pixels in the sources there is a high probability that only on signal source 
will contribute to a given pixel in each mixture 8 However in reality the source 
images in a separation problem are mostly nonsparse natural images so a sparse 
linear transform matrix T  is needed By applying the following transform to the 
mixed signals the following formula can be obtained 
Thus the problem at hand is equivalent to separation of linearly mixed sparse sources 
This blind separation in such situation can be handled by the common ICA approach
es At the same time it is noted that the more sparse the source images are the higher  
 
X
= AS
 
2
1
S
X
A
−
 =

 
3
 
 
1
1
12

N
N
i
i
ij
j
ij
j
j
j
T
T
T
i
M
d
x
a s
a
s
=
=


=
=
=
=







 
4
368 
L Shang P–g Su and Wj Huai 
 
separation accuracy is 7 Therefore it is necessary to develop optimal sparse repre
sentation methods to improve the blind image separation quality 
3 
Contourlet Sparse Decomposition 
Contourlet transform is based on an efficient two dimensional nonseparable filter 
banks and provides a flexible multiresolution local and directional approach for 
image processing 7 It is better than wavelet or wavelet packet transform in dealing 
with singularity in two or higher dimensions especially representing images with 
smooth contours There are two stages in the contourlet transform 8 10 multiscale 
analysis stage and directional analysis stage The first stage is used to capture the 
point discontinuities A Laplacian pyramid LP decomposes the input image into a 
detail subimage and bandpass image that is difference between the input image and 
the prediction image In the second stage the bandpass image is decomposed into 
2
k  
12

k
L
=

 wedge shape subimage by the directional filter banks DFB and 
the detail subimage is then decomposed by the LP for the next loop this stage to link 
point discontinuities into linear structures The whole loop can be done 
p
L  
iteratively and the number of direction decomposition at each level can be different 
which is much more flexible than the three directions in wavelet The overall result is 
an image expansion by using basic elements like contour segments Assumed that the 
number of the transform layer is 2 and the number of the orientation of each layer is 
4 Then the low frequency subband and high frequency subbands of each layer 
respectively corresponding to the natural image named Lena and the MMW image 
were shown in Fig 2  It is clear to see that the lowpass image contains the majority 
of energy of the original objection and the contour of the original image is retained 
well  
Using the contourlet transform the mixing set X
= AS
 and separating system 
1
S
X
WX
A
−
=
=
can be rewritten into 
where 
T
C  denotes the contourlet transform and 


T
T W
W
= C
 Considering the 
T
C  
is a linear transform then 
T
W =W
 Thus 
T
S  and 
X T
 are regarded as the new 
source signals and mixed signals The mixing matrix A and separating matrix W  can 
be obtained in the contourlet domain Now a general ICA algorithm such as natural 
gradient NA algorithm and FastICA algorithm can be used to train the separating 
matrix 
T
W  Here the learning rule used is described as 7 
where 
 
η k
 is the learning rate 

ϕ ST 
 is the activation function and is closely re
lated to the distribution of the output signal 
 
 


 
T
T
T
T
T
T
S
WX
X
W
S
C
C
W C
X
=
=
=
=
 
5


 
 





 
1
T
T
T
T
T
T
k
k
k
I
k
W
W
S
W
S


+
+ η
− ϕ
=


 
6
 MMW Image Blind Restoration Using Sparse ICA in Contourlet Transform Domain 
369 
 
 
 
a  
b  
c 
 
 
d 
e 
f 
Fig 2 The original images and the results of contourlet transform corresponding to Lena image 
and MMW image a and d The low frequency subband b and e The 4orientation sub
band of the first layer c and f The 4orientation subband of the second layer  
 
 
a Barbara image 
b  Cameraman image  
c Mixed image 1 
d  Mixed mage 2 
Fig 3 The original images and the corresponding mixed images 
4 
The Experimental Results and Analysis 
41 
Input Data 
In test two natural images with the same size of 512×512 pixels called Barbara and 
Cameraman were first used to test SPICA algorithm used in contourlet transform 
domain Two original images and their mixed images were shown in Fig3 a and d 
and the mixed matrix used was 05444 05080 20102 04214 At the same time 
the original toy gun image and its MMW image with the size of 41 ×41 pixels were 
shown in Fig4 a and Fig4 b In fact in MMW imaging scene the original object 
is usually unknown The MMW image can be regarded as a mixed image being com
posed of the background noise and the original object The MMW image was gener
ated by the State Key Lab of Millimeter Waves of Southeast University which is our 
cooperation group in the research of MMW image processing  
42 
Results of Image Restoration  
The blind separation or restoration of mixed natural images shown in Fig3 c and 
Fig3 d was first operated by SPICA The corresponding separation results were 
shown in Fig5 a  b In order to illustrate the separation validity of SPICA  
370 
L Shang P–g Su and Wj Huai 
 
 
 
 
a Original toy gun image 
b MMW image 
Fig 4 The original toy gun image and the MMW image 
compared separation images obtained by ICA were shown in Fig5 c  d Clearly 
according to the visual effect the separation effect of SPICA is better than that of 
ICA Further the single noise ratio SNR criterion was used to measure the quality of 
restored images and calculated SNR values were listed in Table 1 The experimental 
data prove again that SPICA truly outperforms ICA For the MMW image it  
can be seen as a mixed image by much unknown noise and the imaging object So it 
is feasible to explore the blind separation process of the MMW image by using 
SPICA  
 
 
 
a Source 1
b Source 2
c Source 1 
d Source 2 
A Separated results of SPICA 
B Separated results of common ICA  
Fig 5 The separated sources of natural images corresponding to ICA and SPICA 
a Restoration result of SPICA 
b Restoration result of ICA 
Fig 6 The separated sources of MMW image corresponding to ICA and SPICA 
 
 MMW Image Blind Restoration Using Sparse ICA in Contourlet Transform Domain 
371 
 
Table 1 SNR values of restored nature images using different algorithms 
                 Images 
  Methods 
Barbara 
Cameraman 
Mixed image 1 
Mixed image 2 
ICA 
29281 
18974 
19031 
11957 
SPICA 
34934 
29522 
 
Using SPICA to restore the MMW image the MMW image was first transformed 
by contourlet The number of decomposition layers and directions of each layer was 
supposed to be 2 and 4 respectively After contourlet transform the low frequency 
image and four high frequency subbands of each layer were obtained as shown in 
Fig2 It is well known that the low frequency image retained well the contour of the 
original object and the high frequency subbands contained much unknown noise 
10 We used the low frequency image and the four high frequency subbands of the 
first layer as the mixed images of SPICA the separation results which were shown in 
Fig6 a could be seen as the restoration of the MMW image Meanwhile the separa
tion result of ICA was also shown in Fig 6b Generally in application the original 
object is unknown so it is not suitable to use SNR criterion to measure the quality of 
this restored MMW image But compared the restored MMW image shown in Fig6 
with the MMW image shown in Fig4 b it is distinct to see that the contour of the 
original imaging object can be restored and with naked eyes the original imaging 
object can be discerned without doubting So it can testify that the SPICA algorithm 
is efficient to blindly restore the original object of MMW imaging system 
5 
Conclusions 
In this paper a novel MMW image restoration method using SPICA operated in the 
contourlet sparse transform domain is explored In application the MMW image is a 
mixed image composed of much unknown background noise and the original 
unknown imaging object So it is feasible to use blind separation method to restore 
the imaging object Firstly the mixed natural images are used to testify the validity of 
this SPICA Further the restoration task of the MMW image is discussed Compared 
with general ICA algorithm the experimental result proved that SPICA operated in 
the contourlet sparse transform domain is efficient and applied truly 
 
Acknowledgments This work was supported by the National Nature Science Foun
dation of China Grant No 60970058 the Innovative Team Foundation of Suzhou 
Vocational University Grant No 3100125 and the “Qing Lan Project” of Jiangsu 
Province 
References 
1 Su P Wang Z Xu Z Active MMW Focal Plane Imaging System In Huang DS Jo 
KH Lee HH Kang HJ Bevilacqua V eds ICIC 2009 LNCS LNAI vol 5755 
pp 875–881 Springer Heidelberg 2009 
2 Sundareshan MK Bhattacharjee Supratik Superresolution of Passive Millimeterwave 
Images Using a Combined Maximumlikelihood Optimization and Projectiononto
convexsets Approach In Proc of SPIE Conf on Passive Millimeterwave Imaging Tech
nology Acrosense 2001 Orlando FL UAS vol 4373 pp 105–116 2001 
372 
L Shang P–g Su and Wj Huai 
 
3 Cheng P Zhao JQ Si XC et al LR Imaging Algorithm For Passive Millimeter 
Wave Based on Sparse Representation Journal of Electronics  Information Technolo
gy 32 1707–1711 2010 
4 Hyvärinen A Karhunen JH Oja E et al Independent Component Analysis A Wiley
Interscience Publication New York 1999 
5 Zhang K Chan LW ICA with Sparse Connections Intelligent Data Engineering and 
Automated Learning 9 530–537 2006 
6 Hyvärinen A Fast and Robust Fixedpoint Algorithm for Independent Component Anal
ysis IEEE Transaction on Neural Networks 10 626–634 1999 
7 Liu SS Fang Y A Contourlettransform Based Sparse ICA Algorithm for Blind Image 
Separation Journal of Shanghai University English Edition 11 464–468 2007 
8 Zibulevsky M Pearlmutter BA Blind Source Separation by Sparse Decomposition 
Neural Computation 13 863–882 2001 
9 Bronstein AM Bronstein MM Zibulevsky M et al Sparse ICA for Blind Separation 
of Transmitted and Reflected Images International Journal of Imaging Science and Tech
nology 15 84–91 2005 
10 Do M Vetterl M The Contourlet Transform An Efficient Directional Multiresolution 
Image Representation IEEE Transactions on Image Processing 14 2091–2106 2003 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 373–378 2012 
© SpringerVerlag Berlin Heidelberg 2012 
An Adaptive Non Local Spatial Fuzzy Image 
Segmentation Algorithm 
Hanqiang Liu1 and Feng Zhao2  
1 School of computer science Shaanxi Normal University Xi’an PR China  
maxhqliugmailcom 
2 School of Telecommunications and Information Engineering Xian University of Posts  
and Telecommunications Xi’an PR China  
addzf1119hotmailcom  
Abstract Fuzzy cmeans clustering algorithm FCM is one of the most widely 
used methods for image segmentation In order to overcome the sensitivity of 
FCM to noise in images we introduce a novel non local adaptive spatial con
straint term which is defined by using the non local spatial information of  
pixels into the objective function of FCM and propose an adaptive non local 
spatial fuzzy image segmentation algorithm ANLSFIS In this method the  
nonlocal spatial information of each pixel plays a different role in image seg
mentation ANLSFIS can effectively deal with noise while preserving the 
geometrical edges in the image Experiments on synthetic and real images es
pecially magnetic resonance MR images show that ANLSFIS is more robust 
than the modified FCM algorithms with local spatial constraint 
Keywords  FCM Image segmentation Nonlocal spatial information adap
tive spatial constraint term noisy image Magnetic resonance MR image 
1 
Introduction 
Image segmentation 12 is one of the most crucial research topics in computer vision 
and image understanding Fuzzy cmeans clustering algorithm FCM 3 is one of the 
most widely used clustering methods for image segmentation and many new modified 
versions of this algorithm have been proposed 48  
In order to make the standard FCM more robust to noise the local spatial informa
tion which is obtained from the neighbor window around each pixel in the image is 
introduced into the standard FCM Ahmed et al 8 modified the objective function of 
FCM by incorporating a spatial neighborhood term and applied the proposed algo
rithm named FCMS to the segmentation of MR images Subsequently to reduce the 
computational complexity of FCMS Chen and Zhang 4 proposed two variants of 
                                                           
  This work is supported by the National Natural Science Foundation of China Grant Nos 
60970067 and 61102095 Scientific Research Program Funded by Shaanxi Provincial Educa
tion Department No 11JK1008 Research Fund Program of Key Lab of Intelligent Percep
tion and Image Understanding of Ministry of Education of China No IPIU012011008 
374 
H Liu and F Zhao 
 
FCMS FCMS1 and FCMS2 These two algorithms used the meanfiltered image 
and medianfiltered image respectively to substitute the neighborhood term of the 
objective function of FCMS It is known that the number of gray levels in an image 
is generally much smaller than the number of pixels Based on this fact Szilágyi et al 
9 presented an enhanced fuzzy cmeans clustering algorithm EnFCM to accelerate 
the image segmentation process In this method a linearlyweighted sum image is 
first formed from both the original image and its meanfiltered image and then clus
tering is performed on the linearlyweighted sum image by using the gray level histo
gram instead of the pixels in the summed image Moreover Cai et al 10 proposed a 
fast generalized fuzzy cmeans clustering algorithm FGFCM Similar to EnFCM 
FGFCM utilizes the gray level histogram of a novel nonlinearlyweighted sum image 
to perform the image segmentation The nonlinearlyweighted sum image is formed 
from both the original image and the spatial coordinates and the gray level values 
within the neighbor window around each pixel Recently Krinidis and Chatzis 11 
presented a fuzzy local information cmeans algorithm FLICM This algorithm 
adopts a fuzzy local both spatial and gray level similarity measure to guarantee 
noise robustness and image detail preservation FLICM is relatively independent of 
noise types Furthermore this algorithm is free of any parameter determination 
When the noise level in the image is high the adjacent pixels of a given pixel may 
also have been degraded In this case the modified FCM algorithms with local spatial 
constraint can not obtain satisfactory segmentation In this paper we introduce a non
local spatial constraint term into the objective function of FCM and propose an  
adaptive non local spatial fuzzy image segmentation algorithm ANLSFIS Image 
segmentation experiments show that ANLSFIS is more robust than the modified 
FCM algorithms with local spatial constraint in noise suppression and edge  
preservation 
2 
Adaptive Non Local Spatial Fuzzy Image Segmentation 
Algorithm 
For images heavily contaminated by noise the adjacent pixels of a given pixel may 
also have been degraded so the modified FCM algorithms with local spatial con
straint can not obtain satisfactory segmentation performances In this paper we first 
utilize the nonlocal meanfiltered image 12 to define a novel non local spatial con
straint term Then we introduce the spatial constraint term into the objective function 
of FCM and propose an adaptive non local spatial fuzzy image segmentation algo
rithm ANLSFIS The objective function of ANLSFIS is presented as follows 
2
2
1
1
1
1
c
n
c
n
m
m
m
ki
i
k
i
ki
i
k
k
i
k
i
J
u
x
v
u
x
v
β
=
=
=
=
′
=
−
+
−


 
1
where 
ix′  is the gray value of the ith pixel of the nonlocal meanfiltered image and 
βi is the spatial parameter controlling the penalty effect of the spatial constraint of the 
ith pixel For every pixel in an image there are a set of pixels with a similar neigh
borhood configuration of it Compared with the adjacent pixels of the pixel it is 
 
An Adaptive Non Local Spatial Fuzzy Image Segmentation Algorithm 
375 
 
worthwhile to use these similar pixels to obtain the spatial information of a given 
pixel In detail for the ith pixel its spatial information 
ix′  is computed by the follow
ing formula 
ir
i
ij
j
j W
x
w x
′ =  ∈
 
2
where 
ir
W  denotes a r×r search window centered at the ith pixel in the noisy image 
That means that the pixels in the search window are utilized to compute the spatial 
information of the ith pixel The weights wij 
ir
j
∈W
 depend on the similarity be
tween ith and jth pixels and satisfy 0≤wij≤1 and 
1
ir
ij
j W
w
∈
=

 
The similarity between the ith and jth pixels depends on the similarity of the gray 
level vectors xNi and xNj where Ni denotes a s×s square neighborhood centered at 
the ith pixel This similarity is measured as a decreasing function of the weighted 
Euclidean distance 
2
2




i
j
x N
x N
α
−
 Therefore the pixels with a similar gray level 
neighborhood to xNi have larger weights These weights are defined as 


2
1 exp




ij
i
j
i
w
x N
x N
h
= Z
−
−
 
3
where h is called the filtering degree parameter which controls the decay of the expo
nential function in Eq 3 Zi is the normalizing constant defined as 


2
exp




ir
i
i
j
j W
Z
x N
x N
h
∈
=
−
−

 
4
Intuitively it would be better if we can adjust the spatial constraint separately for each 
pixel so each pixel has its own β value In this paper we utilize the weights wij 

ir
j
∈W
 to determine the spatial parameter 
iβ  for the ith pixel 1≤i≤n It is defined 
as  
min
max
min
1
1
1


min  max 
min 
i
i
l
l
l
l n
l n
l n
β
β
β
β
γ
γ
γ
γ
≤ ≤
≤ ≤
≤ ≤
=
+
−
−
−
 
5
where 
min
max



β
β
 denotes the value range of 
iβ  and 
max

ir
i
ij
j W
w
γ
∈
=
 is the maxi
mum weight among the pixels in the window 
ir
W  The value of 
iγ  1≤i≤n can re
flect the accuracy of 
ix′  estimating for the ith pixel value of the original image non 
noisy image Therefore the larger γi is the bigger the guiding effect of 
ix′  on the 
clustering of xi is Thus 
iβ  obtained in Eq 5 can make the nonlocal spatial con
straint of each pixel able to play a different part in the pixel clustering 
When utilizing the non local spatial information of pixels ANLSFIS can achieve 
better segmentation performances especially for the geometrical edges in the image 
376 
H Liu and F Zhao 
 
By minimizing Eq 1 using the Lagrange multiplier method the update equations of 
membership function uki and cluster center vk are given in Eqs 6 and 7 
1 
1
2
2
2
2
1
m
c
i
k
i
i
k
ki
l
i
l
i
i
l
x
v
x
v
u
x
v
x
v
β
β
−
−
=


′
−
+
−


=




′
−
+
−



 
6
1
1


1

n
n
m
m
k
ki
i
i
i
i
ki
i
i
v
u
x
x
u
β
β
=
=
′
=
+
+


 
7
3 
Experimental Results and Analysis 
In order to demonstrate the performance of ANLSFIS we perform segmentation 
experiments on real images For all the comparation methods the fuzziness index m 
the maximal iteration num T and the threshold ε are set to 2 500 and 105 respective
ly The neighbor window size q×q of FCMS1 FCMS2 EnFCM FGFCM and 
FLICM is set to 3×3 Furthermore we set β=6 for FCMS1 FCMS2 and EnFCM 
According to the results and the parameter analysis presented in Ref 10 the para
meters λs and λg in FGFCM are set to 3 and 6 respectively 
Figure 1a presents the House image with 256×256 pixels We artificially add the 
Gaussian noise of mean M=0 and variance Var=0006 to this image and show the 
noisy image in Fig 1b In this section we use this noisy image to verify the perfor
mance of FCMS1 FCMS2 EnFCM FGFCM FLICM and ANLSFIS The seg
mentation results of these six methods are shown in Fig 1ch In order to quantita
tively assess these six methods we adopt partition coefficient Vpc 13 and partition 
entropy Vpe 14 to evaluate the segmentation results They are defined as follows 
2
pc
1
1
V
c
n
ij
i
j
u
n
=
=
= 
   
8
pe
1
1
V

log

c
n
ij
ij
i
j
u
u
n
=
=
= −
 
9
The idea of these two validity functions is that the partition with less fuzziness means 
better performance So the best clustering is achieved when the value Vpc is maximal 
and Vpe is minimal It is found from Fig 1 that ANLSFIS is the best among these six 
methods for the Gaussian noisy image 
Moreover we perform segmentation experiments on a MR image which is with 
256×170 pixels and shown in Fig 2a We add the Rician noise l=20 to this MR 
image and show the noisy image in Fig 2b We adopt this noisy image to test the 
performance of FCMS1 FCMS2 EnFCM FGFCM FLICM and ANLSFIS on 
MR images The segmentation results on the noisy images are presented in Fig 2c
h The segmentation result reveals that ANLSFIS can effectively remove the noise 
and retain the edges in the MR image and obtains the maximum Vpc and minimum Vpe 
values among these six methods 
 
An Adaptive Non Local Spatial Fuzzy Image Segmentation Algorithm 
377 
 
 
Fig 1 Segmentation results on the “House” image corrupted by the Gaussian noise a origi
nal image b noisy image c FCMS1 result Vpc=08165 Vpe=03754 d FCMS2 re
sultVpc=08071 Vpe=03906 e EnFCM resultVpc=07410 Vpe=05018 f FGFCM 
resultVpc=07398 Vpe=05037 g FLICM result Vpc=08234 Vpe=03609 h 
ANLSFIS result Vpc=08782 Vpe=02609 
 
Fig 2 Segmentation results on the MR image corrupted by the Rician noise a original im
age b noisy image c FCMS1 result Vpc=08552 Vpe=02748 d FCMS2 result 
Vpc=08548 Vpe=02758 e EnFCM result Vpc=07553 Vpe=04422 f FGFCM result 
Vpc=07548 Vpe=04432 g FLICM result Vpc=08500 Vpe=02793 h ANLSFIS result 
Vpc=08731 Vpe=02373 
378 
H Liu and F Zhao 
 
4 
Conclusion 
In this paper we propose an adaptive non local spatial fuzzy image segmentation 
algorithm ANLSFIS In order to overcome the noise sensitivity of standard FCM 
ANLSFIS introduces a non local adaptive spatial constraint term which is defined 
by using the non local spatial information of pixels into the objective function of 
FCM The experimental results on real images show that ANLSFIS outperforms the 
modified FCM algorithms with local spatial constraint In ANLSFIS the algorithm 
parameters are preliminarily discussed and analysized in the experiment section How 
to theoretically choose these parameters deserves to be studied further Moreover the 
number of clusters in ANLSFIS is set manually Thus our further works also include 
adaptive determination for the clustering number in our method 
References 
1 Pal NR Pal SK A Review on Image Segmentation Techniques Pattern Recogni
tion 269 1277–1294 1993 
2 Zhang H Fritts JE Goldman SA Image Segmentation Evaluation A Survey of Un
supervised Methods Comput Vision and Image Under 1102 260–280 2008 
3 Bezdek JC Pattern Recognition with Fuzzy Objective Function Algorithms Plenum 
New York 1981 
4 Chen SC Zhang DQ Robust Image Segmentation Using FCM with Spatial Con
straints Based on New Kernelinduced Distance Measure IEEE Trans Syst Man Cy
bern 344 1907–1916 2004 
5 Fan JL Zhen WZ Xie WX Suppressed Fuzzy Cmeans Clustering Algorithm Pat
tern Recognition Lett 24910 1607–1612 2003 
6 Zhu L Chung FL Wang ST Generalized Fuzzy Cmeans Clustering Algorithm with Im
proved Fuzzy Partitions IEEE Trans Syst Man Cybern B Cybern 393 578–591 2009 
7 Chuang KS Tzeng HL Chen S Wu J Chen TJ Fuzzy Cmeans Clustering with 
Spatial Information for Image Segmentation Comput Med Imaging Graph 301 9–15 
2006 
8 Ahmed MN Yamany SM Mohamed N Farag AA Moriarty T A Modified Fuzzy 
Cmeans Algorithm for Bias Field Estimation and Segmentation of MRI Data IEEE 
Trans Med Imaging 213 193–199 2002 
9 Szilagyi L Benyo Z Szilagyii S Adam HS MR Brain Image Segmentation Using 
An Enhanced Fuzzy Cmeans Algorithm In Proc of 25th Annual International Confe
rence of the IEEE EMBS Cancun Mexico pp 17–21 2003 
10 Cai W Chen S Zhang D Fast and Robust Fuzzy Cmeans Clustering Algorithms In
corporating Local Information for Image Segmentation Pattern Recognition 403 825–
838 2007 
11 Krinidis S Chatzis V A Robust Fuzzy Local Information CMeans Clustering Algo
rithm IEEE Trans Image Process 195 1328–1337 2010 
12 Buades A Coll B Morel JM A Nonlocal Algorithm for Image Denoising In Proc 
IEEE Int Conf on Computer Vision and Pattern Recognition vol 2 pp 60–65 2005 
13 Bezdek JC Cluster Validity with Fuzzy Sets Cybernetics and Systems 33 58–73 
1973 
14 Bezdek JC Mathematical Models for Systematic and Taxonomy In Proc of Eigth In
ternational Conference on Numerical Taxonomy pp 143–166 1975 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 379–385 2012 
© SpringerVerlag Berlin Heidelberg 2012 
MMW Image Enhancement Based on Gray Stretch 
Technique and SSR Theory 
WenJun Huai Li Shang and PinGang Su 
Department of Electronic  Information Engineering Suzhou Vocational University 
Suzhou 215104 Jiangsu China 
hwjsl0930supgjssvceducn 
Abstract In order to improve the intensity and contrast qualities of Millimeter 
Wave MMW images and reduce the image noise for concealed weapons 
Detection a method is proposed through combining the gray stretch technique 
and Retinex theory Gray stretch is first used to preprocess the MMW image 
and then Retinex theory based on SingleScale Retinex SSR is used to 
suppress background clutters As a result both the image contrast and Peak 
Signal to Noise Ratio PSNR are improved efficiently The simulation and 
experimental results have proved that the algorithm is not only effective in 
detecting MMW targets but also has advantages of high speed and is easy for 
engineering applications 
Keywords gray stretch technique MMW image Retinex theory illumination 
image reflectance image image enhancement 
1 
Introduction 
Millimeter Wave MMW imaging technique is an application technology which was 
combined and extended from electromagnetic wave theory and optical imaging 
theory It was proposed firstly in the late 1930s 1 To the 1990s MMW imaging 
technology which is applied to detect concealed weapon in the field of security in
spection is becoming a hot international research spot 2 MMW have good penetra
tion for the clothing buildings and other insulation materials However due to the 
wavelength of millimeter is longer than visible light and infrared light and limited by 
the diffraction of optical system there are some shortcomings such as the imaging 
vague resolution of the image is low and the gray range is narrow That is very ne
cessary to precede the image for identify the target of MMW imaging 1 3 Some 
algorithms of classical edge detection are not very good effects such as Sober Prewitt 
and Robert etc because those are sensitive to noise There are still some problems 
about threshold selection in noisy image edge detection such as Canny algorithm and 
mathematical morphology algorithm Nonlinear transform can remove image noise 
while preserving image detail well Retinex algorithm is a nonlinear method which is 
mainly applied to reduce the nonuniform illumination caused by degraded image and 
focus on the interest target It is used in the field of digital image processing 46 In 
this paper aiming at the characteristics of MMW imaging and the lack of general 
enhancement methods extract the illumination component based on SSR method after 
the gray pretreated  
380 
WJ Huai L Shang and P–G Su 
 
2 
Theory of SSR 
Retinex is a mode of how the human visual system to perceive colors and brightness 
It was proposed by the American physicist Edwin H Land’s 7 As is show in Reti
nex theory visual imaging of an image was understood to the result of both incident 
components and reflected components Incident components determine dynamic 
range of the image can be achieved Reflection property reflected intrinsic nature of 
the image determined by the reflection intensity of different light waves  
SSR algorithm is improved and achieved to a centersurround Retinex 8 It can be 
specific descript by the formula as follow 
log   
log   
  
L x y
S x y
LogR x y
−
=
 
1
Where 
  
S x y
 is visual imaging of an image That is the brightness values which the 
human can see 
  
R x y
 is the reflection component of the image 
  
L x y
 is illumina
tion component Illumination component is usually around the original image with the 
convolution function Show as type 2 
     
  
S x y
G x y
L x y
=
 
2
In which 
  
S x y
 is the image intensity “” is convolution operator 
  
G x y  is sur
rounding function The physical meaning of convolution term can be calculated as the 
illumination component It was eliminated that change on the target visual image for 
variation of the illumination component though the intensity ratio of a pixel value 
and a weighted average of the surrounding area of pointThe form of the surrounding 
function as follow  
2
2
2

exp 
  
c
y
x
K
G x y
+
−
=
 
3
Where K is the normalization factor which value is the total of visual image pixels 
c  is the scale constant of the Gaussianshaped surrounding space  
3 
SSR Algorithm Based on GrayScale Transformation 
Combined with the shortcoming of classic SSR algorithm a MMW image enhance
ment algorithm was proposed based on grayscale transformation in this paper Figure 
1 shows the basic flow of the algorithm 
 
 
Fig 1 The SSR algorithm based on grayscale transformation 
The steps of the algorithm as follow 
 
MMW Image Enhancement Based on Gray Stretch Technique and SSR Theory 
381 
 
1 The original image is contrasted stretching appropriate by piecewise linear 
transformation which is used to pretreated to achieve rough estimates for the image 
brightness The smallest error method was selected to calculate the subpoints 9 
First statistics range is g1 g2 in the dynamic range of gray value in the MMW 
images The actual gray range is min max before the image transformation Then let 
a = min b = max c = T1 d = T2 a = g1 b = g2 the specific transformation formula 
can be get and shows as follows  



+
−
+
−
+
−
=
d
T
f
k
c
T
f
g
f
k
x y
f


min

 

2
2
1
1
0

max
 

 

  
min
2
2
1
1






f x y
T
T
f x y
T
T
f x y
 
4
Where 
min
 
 
1
1
0
−
−
=
T
c g
k
is inhibition coefficient in the background area 





1
2
1
T
T
c
d
k
−
−
=
 is the remaining coefficient in the transition zone and 

max


2
2
2
T
d
g
k
−
−
=
 is drawing coefficient in the target area 10 The gray level of 
the target segment in image is stretched the transition section is keep and the back
ground section was compressed for enhance the contrast The target gray level of 
MMW image used in this paper is in the high value part of the histogram so that the 
image pixels which are above the threshold was regarded as target others pixels be
long to background 
2 The original image and convolution image was respectively transformed by lo
garithm operation and applied convolution function The impact of background was 
eliminated after subtraction both the original image and convolution image in loga
rithm space and the reflection property R of the object was obtained  
3 According to the ranks of the expandable layer to expand the image after obtain 
the length L of the original image date During Gaussian pyramid compared between 
image pixels in McCann99 algorithm the original image size col  2n   row  2n  
was compressed into a col  row rank matrix and placed on top of the pyramid to com
pare firstly Then the image after the comparison operation was extended twice and 
placed in the next level for comparison operations A way of the image edge expand was 
applied which selected the size of 3 x 3 kernel function That can be largely reduced due 
to image distortion arising and maintain the original edge image features 
4 The low frequency components were filtered out in the Logarithmic domain 
The enhanced images after filtering out low frequency components can be obtain 
though the original image subtract the one after Gaussian convolution Finally a loga
rithmic inverse transformation is used to the image data whose dimensions have been 
reduced and output the enhanced image 
4 
Experimental Results 
41 
Data Preprocessing 
Experimental data come from the active millimeter wave imaging system which is the 
State Key Laboratory of MMW in Southeast University 1112 The picture 2a is 
an optical photo of pistol The picture 2b is a MMW imaging of real target in 
clothes  
382 
WJ Huai L Shang and P–G Su 
 
a Optical photo 
b MMW image 
Fig 2 Two images of the imaging target 
42 
MMW Image Enhancement 
421   Experimental of MMW Image Enhancement 
As noted above piecewise linear grayscale stretch for the image when our proposed 
image enhancement method was used firstly Select the appropriate subpoint and 
obtained the result shows in Figure 3 As can be see the target image and the actual 
shape of a pistol are some differences after stretching But most of the background 
noise has been eliminated 
 
 
Fig 3 MMW image after piecewise linear gray stretch 
On the basis of the gray stretch when different surround function of scale constant 
c  eg c = 30 70 and different number of iterations N ie N = 10 30 70 are 
selected  The corresponding results of enhancement image shows in Figure 4 The 
image quality has some improvement obviously as the number of iterations of the 
FrackleMcCann algorithm increasing The overall tone of MMW image is tends to 
smooth sharply with c  increases halo phenomenon is weakly and the edge of target 
image is more clearly 
 
a1 10 
b1 30 
c1 70 
A c=30  
a2 10 
b2 30 
c2 70 
B c=70 
Fig 4 The restore image on different scale constant c  and iterations N 
 
MMW Image Enhancement Based on Gray Stretch Technique and SSR Theory 
383 
 
422   Analysis of Experimental Results 
The PSNR is most commonly used as a measure of quality of reconstruction of loss 
compression codec The signal is the original data and the noise is the error intro
duced by compression When comparing compression codec is used as an approxima
tion to human perception of reconstruction quality one reconstruction may appear to 
be closer to the original than another even though a higher PSNR would indicate that 
the reconstruction is of higher quality 13 But a lot of noise has been included in the 
MMW image and the processed image should contain less noise closer to the real 
target so PSNR can be used to evaluate the two images different Practically the 
target image is known and a higher PSNR would indicate that the reconstruction 
imaging is not similar to the original one and a lower PSNR would show that the 
processed image has fewer noise and higher quality than original MMW imaging 
The PSNR formula as follow 

10log10255255
MSE
PSNR =
 
5

=
=
−
=
M
i
N
j
I i j
I i j
MN
MSE
1
1
ˆ  2
   
1
 
6
Where M and N is the image matrix of rows and columns 
  
I i j  and ˆ  
I i j  are 
gray value in the row i and the column j of original image and enhanced image re
spectively Change relation between the different scales constant c  increases with 
the number of iterations N shows in figure 5  
 
 
Fig 5 The PSNR curve of MMW imaging on different value of C 
It can be see from figure 5 the value of PSNR decreases rapidly with the c  in
creasing according to the number of iterations increases Since the original MMW 
image contains a large number of unknown noise the PSNR of enhanced image is 
greater shows the gray scale is closer to the original MMW image enhanced effect is 
384 
WJ Huai L Shang and P–G Su 
 
poorer Conversely if the PSNR is smaller it shows the gray scale is different the 
original MMW image largely the noise contained in enhance image is reduced and 
the result image is more similar to real target  
From figure 5 when N is between 20 and 30 the change trend of PSNR close to 
smooth and tends to constant 6678db ultimately But when c  is 90 The PSNR is 
decrease sharply when N is less than 20 Then it tends to constant value 6683dB in a 
higher position The reason is the dynamic compression of SSR is stronger when the 
value of c  is smaller The gloom part detailed in image can be better enhancing But 
SSR’s overall tonal fidelity is higher and the detailed in shadow is vague when the 
value of c  is larger By means of experimental comparison we select the value of 
c  equals to 70 That would allow enhanced images have lower noise also can reduce 
the number of iterations and improve the realtime computing 
5 
Conclusion and Future Work  
In view of the insufficiency of conventional method in dealing with MMW image 
enhancement In this paper a nonlinear image enhancement algorithm based on gray
scale transformation and SSR theory is proposed Experimental results show there are 
good enhancement effects to MMW image using this method The overall contrast 
after processing is effectively improved The detect target is more clearly However 
the algorithm can not completely eliminate halo phenomena We will depth study how 
to integrate MMW images in the global and local characteristics in the future re
search and process adaptive smoothing has been rebuilt in the brightness of image 
which can correct the impact due to reflection and scattering of MMW 
Acknowledgements This work was supported by the grants of National Nature 
Science Foundation of China No60970058 and the grants of National Nature 
Science Foundation of Jiangsu Province of China No BK2009131 
References 
1 Roger AD Gleed G Anderton RN Advances in Passive Millimetre Wave Imaging 
In Proceedings of SPIE vol 2211 pp 312–317 1994 
2 Wang NN Qi JH Deng WB Development Status of Millimeter Wave Imaging Sys
tems for Concealed Detection Infrared Technology 313 129–135 2009 
3 Alan L Qi HH Andrew D An Overview of Recent Advances in Passive Millimetre 
Wave Imaging in the UK In Proceedings of SPIE vol 2744 pp 146–153 1996 
4 Wang RG Zhu J Yang WT et al An Improved Local Multiscale Retinex Algo
rithm Based on Illuminance Image Segmentation Tien Tzu Hsueh PaoActa Electronica 
Sinica 385 1181–1186 2010 
5 Bertalmio M Caselles et al Issues about Retinex Theory And Contrast Enhancement 
International Journal of Computer Vision 831 101–119 2009 
6 Park YK Park SL Kim JK Retinex Method Based on Adaptive Smoothing for Illu
mination Invariant Face Recognition Signal Processing 888 1929–1945 2008 
7 Edwin HL An Alternative Technique for The Computation of The Designator in The 
Retinex Theory of Color Vision Proc Natl Acad Sci USA 83 3078–3080 1986 
 
MMW Image Enhancement Based on Gray Stretch Technique and SSR Theory 
385 
 
8 Land EH The Retinex Theory of Color Vision Scientific American 2376 108–128 
1977 
9 Kittler J Minimum Error Thresholding Pattern Recognition 19 41–47 1986 
10 Zhang XJ Sun XL A Research on the Piecewise Linear Transformation in Adaptive 
IR Image Enhancement Electronic Science and Technology 1863 13–16 2005 
11 Sun PG Wang ZX Xu ZY Active MMW Focal Plane Imaging System Journal of 
Suzhou Vocational University 191 70–73 2008 
12 Shang L Su PG Zhou CX Denoising Millimeter Wave Image Using Contourlet and 
Sparse Coding Shrinkage Laser  Infrared 419 1049–1053 2011 
13 HuynhThu Q Ghanbari M Scope of Validity of PSNR in ImageVideo Quality As
sessment Electronics Letters 4413 800–801 2008 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 386–392 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A Study of Images Denoising Based on Two Improved 
Fractional Integral Marks  
Changxiong Zhou1 Tingqin Yan12 Wenlin Tao1 and Shufen Lui1 
1 Department of Electronic and Informational Engineering Suzhou Vocational University 
Suzhou Jiangsu 215104 China  
handzhousinacom 
2 Suzhou Key Lab of Digital Design  Manufacturing Technology Jiangsu 215104 China 
Abstract In this paper applying fractional calculus GrümwaldLetnikov defi
nition a novel image denoising approach based on two improved fractional 
integral masks is proposed Two structures of 3×3 fractional integral masks 
which center is the processed pixel are constructed and discussed The denois
ing performance of the proposed fractional integral marks FIM1 and FIM2 is 
measured using experiments according to subjective and objective standards of 
visual perception and SNR values The simulation results show that SNR of 
FIM1 and FIM2 is prior to the mean filter and the method in Ref 8 
Keywords 
image 
denoising 
fractional 
integral 
GrümwaldLetnikov  
definition 
1 
Introduction 
Image processing is an important filed of information science and engineering and 
image denoising is one of the most fundamental image processing problems in com
puter vision and image processing 1 2 3 The search for efficient image denosing 
methods still is a valid challenge because most algorithms have not yet attained a 
desirable level of applicability The most important of all is that though both edge and 
noise are high frequency information the loss of edge is evident and inevitable in 
denoising process 3 4 On the other hand fractional calculus has long history as 
same as integer calculus in existing fractional calculus definition the fractional diffe
rential operation is performed if the order is positive oppositely the fractional 
integral operation is performed if the order is negative 58 Many denoising algo
rithms are substantially performing an integral calculation for the blocks consisting of 
image pixels and neighborhood pixels of image In general if the image is needed to 
process with filter we should convolute the value of pixels of image with a mask 
Accordingly we can think of naturally that which sizes can be introduced into the 
mask 
The remainder of this paper is structured as follows In Section 2 we introduce the 
fractional integral theory and its amplitudefrequency character In section 3 two 
structures of 3×3 fractional integral masks which center is the processed pixel are 
 
A Study of Images Denoising Based on Two Improved Fractional Integral Marks 
387 
 
described In section 4 we analyze and discuss our algorithm by comparative results 
on noisy images Finally conclusions are drawn in Section 5 
2 
Fractional Integral Theory and Its AmplitudeFrequency 
Character 
Fractional calculus is essentially noninteger order calculus and the order may be real 
or complex Mathematicians have given different definitions from different perspec
tives but these definitions are all not easy to understand and can not be used directly 
in engineering and technology Under the Euclidean measure GrümwaldLetnikov 
fractional calculus definition should be widely applied and researched This definition 
is derived from integer integral definition For
v ∈ 0 1 
 the differential formula is 
directly extended from integer
n ∈ Z +
 to real number v and then fractional diffe
rentiation is obtained 


1
1 

1

 1
lim


0
0
mh
f t
m
v
m
v
h
f t
D
n
m
m
v
a
t
nh
h
v
−
+
−
Γ
+
Γ
+
Γ
−
=

=
−
= −
→
  
1
where t is
  
a b
 and Γ is the function of gamma Similarly to make fractional 
integral operator doing definition of vorder integral of 
f t
 as follows 




+
−
+
+
−
+
=
−
Γ
+
Γ
+
Γ
=

=
= −
→

2 

2
1







1  



lim


0
0
h
f t
v v
h
vf t
f t
h
mh
f t
v
m
m
v
h
f t
I
v
n
m
v
a
t
nh
h
v
  
2
Let 


 
L2 R
f t
∈
be a onedimension signal its Fourier transform is defined as 
dt
f t e
F
t
j
R
ω
ω
−
= 
 


  
3
The fractional integral is 
I v f t
=
t
f
−v
 and its Fourier transform operator can be 
derived 
v
j
I
−
=




ω
ω
 
4
Above
I ω 
 represents the fractional integral operator Its amplitudefrequency 
curve is shown in Fig 1 According to fractional integral operator as the frequen
cy
ω → 0
 the amplitude frequency response of fractional integral opera
tor
ω −v → ∞
 while as
ω → ∞
  the amplitude frequency response
ω −v → 0
 It 
can be illuminated from Fig1 that fractional integral operation has the role of weaken 
highfrequency of signal while keeping that of very high frequency in nonlinear 
manner Meanwhile the component with lower frequency is also strengthened while 
reserving that of very low frequency in nonlinear manner Thereby fractional integral 
operator can remove noise while reserving the textures and details 
388 
C Zhou et al 
 
 
Fig 1 AmplitudeFrequency Curve of Fractional Integral Operator 
3 
Construction of Fractional Integral Mask 
Through formula 2 the fractional integral operation is simplified as multiplying and 
adding Set h =1 the anterior 3 approximate backward fractional partial integral of 
digital image on negative xcoordinate are expressed as follows 

2

2
1


1


 

 
y
f x
v v
y
x
vf
f x y
f x y
x v
v
−
+
+
−
+
=
∂
∂
  
5
The anterior 3 nonezero values of corresponding terms in formula 5 are 1 
v 
2
 + 1
v v
 which are all fractional integral masks’ coefficients according to 
GrümwaldLetnikov fractional integral definition To make the fractional integral 
mask with rotation invariant Ref 8 constructs eight fractional integral masks on 
direction 0 degrees 45 degrees 90 degrees 135 degrees 180 degrees 225 degrees 
270 degrees and 315 degrees respectively then does convolution filter on the above 
eight directions by using eight marks respectively finally takes linear weighting sum 
value as approximate value of fractional integral for pixel fx y  The effect of cen
ter pixel fx y decreases with the increased mark’s size so small mark has good 
denoising performance while reserving image details Because the greatest distance 
between the center pixel fx y and the pixels in eight marks in Ref 8 is two pixels 
so this method’ SNR can not be high On the basis of above formula 5 we propose 
the anterior 3 approximate forward and backward fractional partial integral are ex
pressed as follows 

1


 

1


 
2
y
x
vf
f x y
y
x
vf
f x y
x v
v
−
+
+
+
=
∂
∂
  
6
 
 
A Study of Images Denoising Based on Two Improved Fractional Integral Marks 
389 
 
Corresponding terms in formula 6 we propose two improved marks that is fraction
al integral mask1FIM1 and fractional integral mask2FIM2 shown as following 
Fig2 The sizes of Mark 1 and Mark 2 are 3×3 window which center is the processed 
pixel fx y and the coefficients of center pixel in two marks are 8 and 6 respectively 
while the coefficients of other pixels are fractional integral orderv  Because the dis
tance between the center pixel fx y and every pixel in window is one pixel FIM1 
and FIM2 can raise SNR 
 
v  
v  
v  
 
v  
v  
v  
v  
8 
v  
 
v  
6 
v  
v  
v  
v  
 
v  
v  
v  
a Mask1                                                 b Mask 2 
Fig 2   Proposed Fractional Integral Mask 
The steps of the improved fractional integral image denoising algorithm are 
1 Overlapping the center coordinates x y of fractional integral mask the mask 
center’s coefficient is 8Mask1 or 6 Mask2 with the coordinates x y of the pixel 
fx y to be filtered by fractional integral operation 
2 Multiplying coefficients of the fractional integral masks in 3×3 sizes with the 
gray value of corresponding pixel and adding all product terms respectively to obtain 
weighting sum 
3 Taking the arithmetic mean of the weighting sum value in 3×3 sizes as approx
imate value of fractional integral for pixel fx y 
4 For making every pixel in image can be filtered by fractional integral mask 
translating masks one pixel by one pixel and repeating the steps 1 to 3 thereby the 
approximate value of fractional integral for the whole digital image can be calculated 
4 
Experiments and Comparisons 
This section aims at demonstrating the denoising performance of FIM1 and FIM2 
respectively At first we show the experiment results with four methods of the mean 
filter the method in Ref8 the proposed FIM1 and FIM2 and then obtain the qualit
ative and quantitative analysis for denoising performance in various noise levels All 
of the experiments were performed on images of LENA and CAMERA shown as in 
Fig3 and Fig4 Then we also quantitatively discuss the relationship between SNR 
and noise level The SNR value in this paper is defined as follows 
2
1
1
0

0
2
1
1
0
0
10
ˆ   
  


  

10 log
f i j
f i j
f i j
SNR
N
M
j
i
N
M
j
i
−
=


−
−
=
=
−
−
=
=
  
7
where fxy is clear image 
ˆ  
f i j
 is the image after filtering Firstly we study the 
SNR performance of the mean filter and the method in Ref8 and FIM1 and FIM2 
using images corrupted with additive independent Gaussian noise with mean 0 and 
variance 000300050010 respectively wherein the fractional integral mask has a 
3×3 window the fractional integral order is 07 
390 
C Zhou et al 
 
Experiment results on Lena are shown as Fig3 and Tab1 Fig3a is clear image of 
Lena Fig3 b shows Lena after being corrupted with white Gaussian noise with va
riance  0010 Fig3 c shows Lena after filtering with the mean filter Fig3 d shows 
Lena after filtering with the method in Ref8 Fig3 e shows Lena after filtering with 
FIM1 Fig3 f shows Lena after filtering with FIM2 From visual sense effect of human 
eyes Fig3 shows that two proposed approaches FIM1 and FIM2 have good denoising 
performance for Lena compared with the mean filter Two proposed approaches not only 
remove the noise well but also reserve the edge and texture details information in the 
image in particular for the weak edge In addition compared with the method in Ref8 
FIM1 and FIM2 do not cause the blur effect because of the distance the center pixel fx 
y and every pixel in window being one pixel 
 
  
  
 
    a LENA         bwith noise variance 001      c with mean filter 
  
  
 
    d with method in Ref8            e FIM1                     f FIM2 
Fig 3 Noisy image of Lena and results of denoising by four methods 
Table 1 shows the SNR values of four methods for the image of Lena Data in the 
table 1 shows that the SNR values of the image denoised with FIM1 and FIM2 are 
higher than that with the mean filter and the method of Ref8 in different noise le
vels and shows that FIM1’s performance is prior to the FIM2 when noise level is less 
than 0003 
Table 1 Comparison of SNR for Lena with four methods 
Noise variance 
Noisy image Mean filter 
Ref8 
FIM1 
FIM2 
0001 
2394 
2017 
2127 
2523 
2461 
0003 
1917 
2001 
2091 
2358 
2345 
0005 
1700 
1989 
2059 
2236 
2250 
0010 
1401 
1952 
1985 
2036 
2086 
 
A Study of Images Denoising Based on Two Improved Fractional Integral Marks 
391 
 
Experiment results on Camera are shown as Fig4 and Tab2  Fig4a is clear im
age of Camera Fig4 b shows Camera after being corrupted with white Gaussian 
noise with variance  0010 Fig4 c shows Camera after filtering with the mean 
filter Fig4 d shows Camera after filtering with the method in Ref8 Fig4 e 
shows Camera after filtering with FIM1 Fig4 f shows Camera after filtering with 
FIM2 By eye’s qualitative analysis for Fig4 we observe that that two proposed ap
proaches of FIM1 and FIM2 have good denoising performance for Camera compared 
with the mean filter FIM1 and FIM2  not only remove the noise well but also reserve 
the edge and texture details information in the image in particular for the weak edge 
In addition compared with the method in Ref 8 FIM1 and FIM2 do not cause the 
blur effect because of the distance the center pixel fx y and every pixel in window 
being one pixel 
 
   
   
 
a CAMERA          b with noise variance 001     c with mean filter 
   
   
 
d with method in Ref8               e FIM1                  f FIM2 
Fig 4 Noisy image of CAMERA and results of denoising by four methods 
Table 2 shows the SNR values of four methods for the image of Camera Data in 
the table 2 shows that the SNR values of the image denoised with FIM1 and FIM2 are 
higher than that with the mean filter and the method of Ref8 in different noise le
vels and shows that FIM1’s performance is prior to the FIM2 when noise level is less 
than 0010  
Table 2 Comparison of SNR for Camera with four methods 
Noise variance 
Noisy image Mean filter 
Ref8 
FIM1 
FIM2 
0003 
1976 
1610 
1665 
2067 
2008 
0005 
1762 
1603 
1646 
2010 
1969 
0010 
1464 
1585 
1610 
1881 
1871 
0020 
1171 
1544 
1537 
1699 
1721 
392 
C Zhou et al 
 
5 
Conclusions 
In this paper we devote to introduce factional calculus theory into the research field 
of digital image processing propose a novel fractional integral image denoising algo
rithms which is based on GrümwaldLetnikov fractional calculus definition and con
struct two fractional integral masks in 3×3 window Simulation experiments show the 
availability of the improved method that it is prior to the mean filter and the method 
of Ref 8 especially when the noise deviation is less than 0010 Furthermore the 
subjective evaluations from the figure 3 and table 1 figure 4 and table 2 can be agreed 
upon with the objective evaluation of the above table Certain aspects of improved 
method need to be studied more carefully 
 
Acknowledgements This research was sponsored by the grants of Natural Science 
Foundation of China No 60970058 the grants of Natural Science Foundation of 
Jiangsu Province of China NoBK2009131 Innovative Team Foundation of Suzhou 
Vocational University No3100125 Suzhou Infrastructure Construction Project of 
Science and Technology NoSZS201009 and Qing Lan Project of Jiangsu Province 
of China 
References 
1 
Special issue Fractional Signal Processing and Applications Signal Processing 8311 
2285–2286 2003 
2 
Tatom FB The Relationship between Fractional Calculus and Fractals Fractals 31 
217–229 1995 
3 
Pu Y Zhou J Yuan X Fractional Differential Mask a Fractional Differentialbased 
Approach for Multiscale Texture Enhancement IEEE Transactions on Image 
Processing 192 491–511 2010 
4 
Oldham KB Spanier J The Fractional Calculus Academic Press New York 1974 
5 
Mathieu B Melchior P Oustaloup A Fractional Differentiation for Edge Detection 
Signal Processing 8311 2421–2432 2003 
6 
Pu Y Wang W Fractional Differential Masks of Digital Image and Their Numerical 
Implementation Algorithms Acta Automatica Sinica 3311 1128–1135 2007 
7 
Pu Y Wang W Zhou J et al Fractional Differential Approach to Detecting Textural 
Features of Digital Image and its Fractional Differential Filter Implementation Sci China 
Ser F Inf Sci 519 1319–1339 2008 
8 
Huang G Pu Y Chen Q Zhou J Research on Image Denoising Based on Fractional 
Integral System Engineering and Electronics 334 925–932 2011 in Chinese 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 393–400 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Leaf Image Recognition Using Fourier Transform  
Based on Ordered Sequence 
LiWei Yang1 2 and XiaoFeng Wang2 3 
1 Department of Automation University of Science and Technology of China Hefei Anhui 
230027 China 
2 Intelligent Computing Laboratory Hefei Institute of Intelligent Machines  
Chinese Academy of Sciences PO Box 1130 Hefei Anhui 230031 China 
3 Key Lab of Network and Intelligent Information Processing Hefei University Hefei Anhui 
230601 China 
lwyangwingmailcom xfwangiimaccn 
Abstract There are a number of leaf recognition methods but most of them are 
based on Euclidean space In this paper we will introduce a new description of 
feature for the leaf image recognition which represents the leaf contour with 
the ordered sequence For a leaf image points on the contour represent the most 
important information of the leaf Thus by extracting serial points of the leaf 
contour the unique corresponding ordered sequence can be obtained for a 
contour Then we can compute the amplitudefrequency feature by performing 
the Discrete Fourier transform on the ordered sequence Since the low
frequency part of the Fourier transform represents the global information and 
the highfrequency part the local details we can adopt the amplitudefrequency 
feature for leaf image recognition Experimental results on the famous Swedish 
library and ICL library show that the proposed feature is effective for leaf 
image recognition 
Keywords leaf recognition Fourier transform ordered sequence amplitude
frequency 
1 
Introduction 
On the earth there are a huge number of plant species Plants are the primary food 
producers which sustain all other life forms including people They are the only 
organisms that can convert light energy from the sun into food Some recent work has 
been focused on plant image recognition 15 Wang et al 1 pointed out that plants 
can be basically identified by their leaves Thus the plant recognition methods 
usually depend on physical leaf features such as leaf shape 12 and the pattern of 
veins 3 The morphological and genetic features were also employed to recognize 
different species of plant leaves 45 Most common features are based on shape 
recognition They include global and local information Past techniques used in the 
                                                           
  Corresponding author 
394 
LWYang and XF Wang 
 
shape matching of objects are chain code crosscorrelation Fourier descriptors and 
moments statistical pattern recognition techniques symbolic matching syntactic and 
relaxation methods 6 In order to obtain the scale rotation and shift invariance for 
feature extraction based on the Euclidean distance the time cost will be extremely 
large But if the feature extraction can be performed in the frequency domain these 
issues can be easily solved  
In this paper we propose using the features extracted from frequency domain to 
overcome scale rotation and shift variance problems in Euclidean space First of all 
we should obtain the points on the leaf contour and then compute the ordered 
sequence with the point set We perform the discrete Fourier transform DFT on the 
ordered sequence which is a series ordered value computed by the distance between 
the ordered contour points and their center point Here we will mainly use the linear 
property and cycle shift property of DFT We make use of the linear transformation of 
Fourier transform to normalize the ordered sequence One ordered sequence 
corresponds to a leaf contour with the onetoone relationship If the leaf contour was 
rotated cycle shift of ordered sequence will happen Thus we can use the cycle shift 
nature of Fourier transform to solve the leaf rotation problem Finally the nearest 
neighbor KNN was used for recognition 
The rest of the paper is organized as follows Section 2 introduces the Discrete 
Fourier Transform and its properties In Section 3 we present our feature extraction 
method based on the ordered sequence Section 4 gives the experimental results of 
proposed features on Swedish and ICL leaf library 14 Finally we will make a 
conclusion in Section 5 
2 
Discrete Fourier Transform 
In this section we shall mainly introduce the Discrete Fourier transform and its 
properties  
The discrete Fourier transform DFT is one of the most fundamental operations in 
digital signal processing 7 In cycle sequence as a matter of fact only limited values 
are meaningful so many of its features can be replaced by limited sequence For a 
discrete time series signal X 
 1 2 

X
x
x
x N
=
 
1
N is the length of the effective value of the ordered sequence Its Discrete Fourier 
Transform can be formulated as follows 
2
1
0
1
 
 
        0
1
k
n
N
i
N
k
x n
x k
e
n
N
N
π
× × ×
−
− ×
=
=
×
≤
≤
−

 
2
Discrete Fourier Transform has many good properties and two of them are important 
for plant leaf recognition 
I The linear property Since Fourier transform is a linear operation which means 
that if discrete ordered sequences are multiplied by a constant c ∈ R the 
corresponding frequency domain will have the same scale 
 
Leaf Image Recognition Using Fourier Transform Based on Ordered Sequence 
395 
 
 
 
c g x
c G w
×
↔ ×
 
3
This property can play a very important role in data normalization especially in leaf 
feature normalization process 
II The cycle shift property If the original sequence is processed with cycle shift 
by k bits 
 
 
1 
2   1 2  
X K
x k
x k
x N x
x
x k
=
+
+
 
4
So the corresponding information of every frequency should be multiplied by a 
complex 
e−iwk
 based on w 
 
 
iwk
X k
e
G w
↔ −
•
 
5
Because 
e−iwk
 located on unit cycle the result of Fourier transforms merely changes 
by phase position shifting But the amplitude |G w | is stable 
 
 
iwk
G w
e
G w
−
=
•
 
6
This property is useful for achieving the leaf contour rotation invariance 
3 
Leaf Feature Extraction 
Leaf recognition plays an important role in plant classification and its key issue lies in 
whether selected features are stable and have good ability to recognizing different 
species of leaves 8 
31 
The Leaf Contour Extraction 
There are a couple of methods for leaf contour extraction such as Canny operator 9 
eightdirection chain code method 10 gradient based edge tracing 11 In this 
paper we shall extract the leaf contour based on gradient information 
Before leaf contour extraction the image should be preprocessed in advance 
Firstly the colorful image should be transformed into gray image Secondly the gray 
image should be transformed into binary image with the threshold obtained by OTSU 
12 Then we can extract the leaf contour with eightdirection chain code At last we 
will keep only 100 points of the leaf contour by using linear interpolation on image 
which size is less than 128128 pixels You can change the number of points 
according to practical application Fig1 shows the process of our contour extraction 
 
 
Fig 1 The Process of Contour Extraction 
Fig 2 demonstrates the extracted contours of leaves from Swedish leaf image 
library It is clearly seen that 100 points can not only preserve the global information 
of leaf but also contain local information of leaf 
396 
LWYang and XF Wang 
 
 
Fig 2 Extracted contours of leaves from Swedish library 
32 
The Generation of Ordered Sequence 
In our method the computing of effective sequence is a key step since the sequence is 
the only representation for the leaf contour Generally leaf contours of same plant 
may have the similar sequence while contours of different plants usually have 
different sequences In this way we can distinguish the leaves by their corresponding 
sequence 
Here we use the Euclidean distance between contour point and center point which 
is simple to compute and also accurate to express the contour information 
Firstly the center point Pm x m y m of the leaf contour should be computed with 
Eq 7 and Fig3 shows two leaf contours with their center point 
1
1
1
1

n
n
m
i
m
i
i
i
x
x
y
y
n
n
=
=
=
=


 
7
 
Fig 3 Leaf contours and center points 
Secondly the Euclidean distance is computed by using Eq 8 The sequence keeps 
the order of the points So the sequence and the point set are accordant 
2
2











i
i
i
j
j
j
i
j
i
j
d p x y
p
x
y
x
x
y
y
=
−
+
−
 
8
The illustration of ordered sequences corresponding to 15 leaf contours in Fig2 is 
shown in Fig4 Horizontal axis represent the order of sequence Vertical axis 
represents the value of sequence 
 
 
Fig 4 The Illustration of Ordered sequences corresponding to 15 leaf contours in Fig2 
It should be emphasized that the points on the leaf contour are ordered and the 
computed sequence is also ordered Thus the relationship between sequence and the 
 
Leaf Image Recognition Using Fourier Transform Based on Ordered Sequence 
397 
 
points of leaf contour is onetoone correspondence The normalized formula is 
defined by Eq 9 
min
max
min
x
x
y
x
x
−
=
−
 
9
Through Eq 9 the value of sequence is restricted ranging from 0 to 1 to make sure 
that sequence is unified So the problem of scale variance is solved which also means 
that the ordered sequence ensure the scale invariance 
The reason for why we keep the sequence ordered is that the problem of contour 
rotation can be solved with the cycle shift nature of DFT as shown in Fig5 When 
the leaf contour is rotated in Fig5 b the corresponding sequence will circularly shift 
by n bits Next we transform it into frequency domain the same amplitudefrequency 
feature can be obtained Thus the usage of DFT can achieve the leaf contour rotation 
invariance If the sequence is disordered the problem of contour rotation will be not 
solved by the nature of DFT 
 
a         b 
Fig 5 The leaf contour rotation and ordered sequences a Original leaf contour and ordered 
sequence b Rotated leaf contour and ordered sequence 
33 
Generation of AmplitudeFrequency Features 
After the ordered sequence is obtained and normalized choosing a range of 
frequency we can use Fourier transform in Eq 2 to obtain the Amplitude
Frequency Features It can be seen from Fig6 and Fig7 the Amplitudefrequencies is 
an efficient discriminated feature for recognizing the leaves 
 
 
a                 b 
Fig 6 AmplitudeFrequency features for the same species of leaves 
The both leaf images in Fig6 are scaled and rotated From Fig6 we can see that 
the amplitudefrequency feature is constant That means amplitudefrequency feature 
can overcome the problem of scale and rotation 
398 
LWYang and XF Wang 
 
 
a             b 
Fig 7 AmplitudeFrequency features for the different species of leaves 
The both images in Fig7 are from different classes From Fig7 we can see that the 
amplitudefrequency feature is quite different That means amplitudefrequency 
feature can be regarded as feature for classification 
4 
Experimental Results  
In our experiments nearest neighbor KNN was used for recognition The parameter 
K is set to 1 In this paper experiments were performed on famous Swedish library 
and our ICL library 13 In Swedish library we selected 25 leaf images from each 
leaf species for training and 50 leaf images from each species for test There are 375 
leaf images in training set and 750 leaf images in test set In our ICL library we 
select a subset which contains 50 species of plant leaves We select 10 images from 
each leaf species for training and 20 leaf images from each species for test There are 
500 leaf images in training set and 1000 leaf images in test set 
The experimental result of Fourier descriptors and our amplitudefrequency feature 
by KNN method on Swedish library and ICL library is shown in Table 1 It can be 
seen that the amplitudefrequency feature is more effective  
Table 1 kNN recognition results based on amplitudefrequency features 
Feature 
Accuracy on Swedish Library 
Accuracy on ICL Library 
Amplitude 
frequency 
896 
916 
Fourier 
descriptors 
85333 
806 
 
For leaves from the same plant classes shift scale and rotation variance may 
happen In Fig8 F1 F2 F3 F4 and F5 represent 5 leaf contours from the same class 
It is obviously that shift scale and rotation variance existed 
 
 
Fig 8 Leaves in same class 
F1 
F2 
F3 
F4 
F5 
 
Leaf Image Recognition Using Fourier Transform Based on Ordered Sequence 
399 
 
We select 5 images from each class extract amplitudefrequency feature and then 
calculate the innerclass variances and betweenclasses variances of above 5 leaf 
contours based on Fourier descriptors and amplitudefrequency feature Fig9 shows 
the innerclass variances based on Fourier descriptors and amplitudefrequency 
feature Here red squares represent the innerclass variance based on amplitude
frequency and blue rhombuses represent the betweenclasses variances It is obvious 
that innerclass variances based on amplitudefrequency are better than that based on 
Fourier descriptor That is to say the amplitudefrequency feature is more effective 
than Fourier descriptor for recognizing leaves in the same species 
 
 
Fig 9 Innerclass variances of Fourier descriptors and amplitudefrequency feature for each 
class  
Table 2 Innerclass variances and betweenclass variances for all classes with amplitude
frequency feature and Fourier descriptors 
 
B
S  
w
S  
B 
W
S
S
 
Amplitudefrequency 
26750945 
5080388 
52655 
Fourier Descriptors 
143274857251 
5763918675 
24857 
 
Here we compute the innerclass variances and BetweenClass variances for all 
classes with amplitudefrequency feature and Fourier descriptors as shown in Table 
2 Innerclass variance for all classes based on amplitudefrequency is 5080388 and 
betweenclass variance is 26750945Innerclass variance based on Fourier descriptors 
is 5763918675 and betweenclass variance is 143274857251 If the features can 
make inner variance minimal and make betweenclass variance maximal it will be 
more effective From Table 2 it is obvious that the rate 
B 
W
S
S
= 52655 of 
amplitudefrequency is larger than
B 
W
S
S
= 24857 of Fourier descriptors That 
means amplitudefrequency feature is more effective than Fourier descriptors feature 
in practical classification 
5 
Conclusion 
In this paper we analysis Fourier transform and its property and then we propose a 
new feature for leaf recognition based on it Firstly the leaf image should be pre
processed into binary image and then we extract point set to represent the contour 
400 
LWYang and XF Wang 
 
Through computing the distance between point set and the center point of the set we 
can obtain an ordered sequence Next we will transform the sequence by Fourier 
transform The amplitudefrequency can be regarded as the feature Finally we can 
use this feature to do some experiments on Swedish library and our ICL library with 
KNN classification We also do some more experiments with Fourier descriptors for 
comparison The experiment results show the amplitudefrequency feature is more 
effective than Fourier descriptors so we can use the amplitudefrequency feature to 
recognize plant leaf 
Acknowledgement This work was supported by the grants of the National Science 
Foundation of China Nos 61005010 60975005 60905023 60873012 71001072 
the grant of China Postdoctoral Science Foundation No 20100480708 the grant of 
the Key Scientific Research Foundation of Education Department of Anhui Province 
No KJ2010A289 the grant of Scientific Research Foundation for Talents of Hefei 
University No 11RC05 
References 
1 Wang XF Huang DS Du JX Classification of Plant Leaf Images with Complicated 
Background Applied Mathematics and Computation 205 916–926 2008 
2 Wang XF Du JX Zhang GJ Recognition of Leaf Images Based on Shape 
Features Using a Hypersphere Classifier In Huang DS Zhang XP Huang GB 
eds ICIC 2005 Part I LNCS vol 3644 pp 87–96 Springer Heidelberg 2005 
3 Leslie E Auxin Is Required for Leaf Vein Pattern in Arabidopsis American Society of 
Plant Biologists 1214 1179–1190 1999 
4 Soille P Morphological Image Analysis Applied to Crop Field Mapping Image and 
Vision Computing 18 1025–1032 2000 
5 Pramanik S Bandyopadhyay SK Bhattacharyya D Kim TH Identification of Plant 
Using Leaf Image Analysis In Kim TH Pal SK Grosky WI Pissinou N Shih 
TK Ślęzak D eds SIPMulGraB 2010 CCIS vol 123 pp 291–303 Springer 
Heidelberg 2010 
6 Bhanu B Olivier D Shape Matching of TwoDimensional Objects IEEE Trans on 
Pattern Analysis and Machine Intelligence 62 March 1984 
7 Soontom O Chen YJ Integer Fast Fourier Transform IEEE Transaction on Signal 
Processing 503 March 2002 
8 Krishna S Indra G Sangeeta G SVMBDT PNN and Fourier Moment Technique for 
Classification of Leaf Shape International Journal of Signal Processing Image Processing 
and Pattern Recognition 34 December 2010 
9 Bao P Zhang L Wu XL Canny Edge Detection Enhancement by Scale 
Multiplication IEEE Trans on Pattern Analysis and Machine Intelligence 279 2005 
10 Li X Shen J Group Direction Difference Chain Codes for the Representation of the 
Border In Digital and Optical Shape Representation and Pattern Recognition Orlando 
FL pp 372–376 SPIE Bellingham 1988 
11 Rafael CG Richard EW Digital Image Processing Prentice Hall 
12 Milan S Vaclav H Roger B Image Processing Analysis and Machine Vision 
Thomson Learning 2008 
13 ICL Plant Leaf Images Dataset 
httpwwwintelenginecnEnglishdataset 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 401–406 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Discriminant Graph Based Linear Embedding 
Bo Li124 Jin Liu34 WenYong Dong34 and WenSheng Zhang4 
1 School of Computer Science and Technology Wuhan University of Science and Technology 
430081 Wuhan China 
2 State Key Lab for Novel Software Technology Nanjing China 
3 State Key Lab of Software Engineering Wuhan China 
4 Institute of Automation Chinese Academy of Science Beijing China 
liberol126com 
Abstract LLE is a nonlinear dimensionality reduction method which has been 
successfully applied to data visualization Based on the assumption of local 
linearity LLE can compute the weights between the KNN nodes using the local 
least reconstruction errors which increase the computational cost In this paper 
a method titled Discriminant Graph Based Linear Embedding DGBLE is 
proposed to set the weights between the nodes in the KNN graph directly to 
reduce the computational expense Moreover label information can also be 
taken into account to improve the discriminant power of the original LLE 
Experiments on some benchmark data show that the proposed method is 
feasible and effective    
Keywords LLE graph linear embedding 
1 
Introduction 
Dimensionality reduction from the original data which is often dictated by practical 
feasibility is an important step in pattern recognition tasks Dimensionality reduction 
is also an important process in exploratory data analysis where the goal is to project 
the input data onto a feature space which can reflect the important inherent structure 
of the original data Since there are large volumes of high dimensional data in 
numerous real world applications some of which are perhaps superfluous so 
extracting the most useful features from the real world data not only helps to probe 
into the essential structure of the data but also contributes to accomplish the task of 
classification and visualization at low computational cost   
Over the past several years the study on dimensionality reduction methods has 
being conducted and many useful feature extraction techniques including linear and 
nonlinear methods supervised or nonsupervised ones have been well developed 1 
Among them LLE 2 is a representative nonlinear manifold learning method Based 
on the assumption of the local linearity LLE first constitutes local coordinates with 
the least constructed cost and then maps them to a global one Experiments have 
proven that LLE is an effective method for visualization However some limitations 
are exposed when LLE is applied to pattern recognition  
402 
B Li et al 
One limitation is the outofsample problem 3 Because the weighted matrix of 
LLE is constructed on the training data when a new data point is coming how to 
generalize the results of training samples to the coming data is attracting many 
attentions Another limitation lies in that the classical LLE neglects the class 
information which will impair the recognition accuracy So in this paper we will 
present a discriminant graph based linear embedding method to overcome the above 
problems in the original LLE  
The paper is organized as follows Section 2 describes classical LLE algorithm 
Section 3 presents the proposed Discriminant Graph Based Linear Embedding 
algorithm Experimental results and simulations on YALE and Palm Print data 4 
sets are offered in Section 4 Then the whole paper is finished with some conclusions 
in Section 5 
2 
Locally Linear Embedding 
Let 
1
2




D n
n
X
X
X
X
R
×
=
∈
be n points in a high dimensional space The data 
points are well sampled from a nonlinear manifold of which the intrinsic 
dimensionality is d  d
 D
 The goal of LLE is to map the high dimensional data 
into a low dimensional manifold space Let us denote the corresponding set of 
n points in the embedding space as
1
  2


d n
n
Y
Y Y
Y
R ×
=
∈
 The outline of LLE can 
be summarized as follows 
Step1 For each data point
i
X  identify its k nearest neighbors by kNN criterion or 
ε − ball criterion 
Step2 Compute the optimal reconstruction weights which can minimize the error 
of linearly reconstructing
i
X by its k nearest neighbors 
Step3 Compute the lowdimensional embedding Y for X that best preserves the 
local geometry represented by the reconstruction weights 
Step1 is typically done by using Euclidean distance to define neighborhood 
although more sophisticated criteria may also be used such as Euclidean distance in 
kernel space or cosine distance Step 2 seeks the best reconstruction weights 
Optimality is achieved by minimizing the local reconstruction error of
i
X  


2
1
arg min
k
i
i
ij
j
j
W
X
W X
ε
=
=
−
 
1
Step 3 computes the optimal low dimensional embedding Y  based on the weight 
matrix W obtained from Step 2  




 
T
T
ij
i
j
ij
Y
tr
M Y Y
tr YMY
ε
=
=

 
2
 
 
 
Discriminant Graph Based Linear Embedding 
403 
So based on the weighted matrixW  a sparse symmetric and positive semidefinite 
matrix M can be defined as follows 

 

T
M
I
W
I
W
=
−
−
 
3
3 
Discriminant Graph Based Linear Embedding  
In this section we will propose a discriminant graph based linear embedding method 
to avoid the problems existed in the original LLE Firstly a linear transformation is 
introduced to solve the outofsample problem in traditional manifold learning 
Second the weights between nodes in a KNN graph are set directly by taking the 
label information into account which will be described in the following 
31 
The Weight between Two Nodes 
In the original LE or LPP the weight between two nodes is defined to be a heat kernel 
or simply either 1 or 0 which can not reflect their class information In the proposed 
algorithm the weight between two points is defined based on their local information 
and class information The definition is stated below in details  
Shown in Fig1 is the typical plot of 
ij
W  as a function of
2


i
j
d
X
X
β  In Fig1 
S1 denotes the case that both 
i
X  and 
j
X  are k  nearest neighbors each other 
and
i
X 
j
X  have the same label S2 denotes the case that both 
i
X  and 
j
X  are k  
nearest neighbors each other and
i
X 
j
X  have different labels and S3 denotes the 
other cases The weight 
ij
W  displays the discriminant similarity between 
i
X  
and
j
X The similarity integrates the local neighborhood structure and the class 
information 
2
2
2
If   both 
 and 
 are  



exp

nearest neighbors each other 
with the same label
If   both 
 and 
 are   






exp
 1 exp

nearest neighbors each other 
with di
i
j
i
j
i
j
ij
i
j
i
j
X
X
k
d
X X
X
X
k
W
d
X X
d
X X
β
β
β
−
=


−
−
−






fferent labels
0
otherwise











 
4
32 
Discriminant Graph Based Linear Embedding 
In order to overcome the outofsample problem a linear transformation 
ie
T
Y
= A X
 is plug Thus the objective function of the original LLE can be changed 
into the following form 
404 
B Li et al 
 
Fig 1 Typical plot of 
j
X  as a function of 
2


i
j
d
X
X
β  
 




1
min
min
T
T
T
J
A
tr YMY
tr A XMX A
=
=
 
5
Where M can be computed by Eqn4 and the weights between two nodes can also 
obtained by Eqn5Thus the outline of the proposed LPLDE can be summarized as 
follows 
Step1 Compute the weights between points
i
X and
j
X according to Eqn5 
Step2 Construct matrix M based on Eqn 4   
Step3 Construct a matrix D defined to
T
D
= XMX
 
Step4 Compute the d bottom generalized eigenvalues and the corresponding 
eigenvectors matrixV of D and obtain d dimensional embedding
T
Y
= V X
 
Step5 Adopt a suitable classifier to classify the embedding results 
4 
Experiments 
41 
Experiment on Yale Face Database 
The Yale face database has 165 images for 15 individuals where each person has 11 
images The images demonstrate variations in lighting condition leftright center
light rightlight facial expression normal happy sad sleepy surprised and wink 
and with or without glasses Each image is cropped to be the size of 32 32
×
 Fig2 
shows the cropped images of one person 
 
 
Fig 2 Sample images of one person in Yale database 
We randomly select six images as training set and the rest five images as test set for 
each class The parameter k can be set to 5 when constructing the neighborhood 
 
Discriminant Graph Based Linear Embedding 
405 
graph LDA5 LPP6 and ONPP7 are carried out to extract features At last we 
use the nearest neighbor method for classification Shown in Table1 is the optimal 
accuracy It is found that the proposed method outperforms the other techniques  
Table 1 Performances comparison on Yale face data 
Approaches Recognition rate Dimensions
LDA 
9115 
16 
LPP 
9223 
12 
ONPP 
9342 
14 
DGBLE 
9416 
20 
42 
Experiments on Palmprint Data 
In the PolyU palmprint database there are 100 persons and each with 6 palmprint 
images Fig3 displays the cropped samples The images were cropped with size of 
128 by 128  
We selected the first session images as training samples and the second session 
images as test set When constructing the neighborhood graph k is set to 2 Then we 
use LDA LPP and ONPP to extract the features At last the nearest neighbor 
classifier is adopted to classify these features Shown in Table2 are performances on 
The PolyU palmprint database where the proposed algorithm can gain the best 
results 
 
Fig 3 The cropped sample images from PolyU palmprint 
Table 2 Performances comparision on Palmprint database 
Approaches 
Recognition rate 
Dimensions 
LDA 
9033 
52 
LPP 
9567 
90 
ONPP 
9667 
60 
DGBLE 
9733 
94 
5 
Conclusions 
In this paper a discriminant manifold learning method namely Discriminant Graph 
Based Linear Embedding is proposed for classification The proposed algorithm uses 
the label information to construct the weights between any two nodes in the original 
data So the proposed algorithm becomes more suitable for the tasks of classification 
This result is validated by experiments on realworld data set   
406 
B Li et al 
Acknowledgments  This work was partly supported by the grants of the National 
Natural Science Foundation of China 61070013 U1135005 6117030561070009 
the knowledge innovation program of the Chinese academy of sciences under 
grantY1W1031PB1 the Project for the National Basic Research 12th Five Program 
under Grant0101050302 and the Science and Technology Commission of Wuhan 
Municipality ‘‘Chenguang Jihua’’ 201050231058 Postdoctoral Science Foundation 
of China20080440073 20100470613  201104173，Natural Science Foundation 
of 
Hubei 
Province2010CDB03302 
2011CDC076 
Project 
of 
Hubei 
ProvinceQ20121115 the Open Fund Project of State Key Labof Software 
Engineering SKLSE20100811 State Key Lab of Software Novel Technology 
KFKT2011B21 and Key Lab of Shanghai Information Security Management and 
Technology ResearchAGK2011004 
References 
1 
Jain AK Duin RPW Mao JC Statistical Pattern Recognition A review IEEE 
Transactions on Pattern Analysis and Machine Intelligence 22 4–37 2000 
2 
Roweis ST Saul LK Nonlinear Dimensionality Reduction by Locally Linear 
Embedding Science 290 2323–2326 2000 
3 
Belkin M Niyogi P Laplacian Eigenmaps and Spectral Techniques for Embedding and 
Clustering In Dietterich TG Becker S Ghahramani Z eds Advances in Neural 
Information Processing Systems vol 14 pp 585–591 MIT Press Cambridge 2002 
4 
Biometrics Research Centre httpwww4comppolyueduhkbiometrics 
5 
Martinez AM Kak AC PCA versus LDA IEEE Trans Pattern Analysis and Machine 
Intelligence 232 228–233 2001 
6 
He X Yang S Hu Y Niyogi P Zhang HJ Face Recognition Using Laplacianfaces 
IEEE Trans Pattern Analysis and Machine Intelligence 273 328–340 2005 
7 
Kokiopoulou E Saad Y Orthogonal Neighborhood Preserving Projections In The 
Fifth IEEE International Conference on Data Mining pp 1–7 IEEE Press New York 
2005 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 407–414 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A Performance Analysis of Omnidirectional Vision  
Based Simultaneous Localization and Mapping 
Hayrettin Erturk1 Gurkan Tuna2 Tarik Veli Mumcu3 and Kayhan Gulez3 
1 Yildiz Technical University ElectricalElectronics Faculty 
Electrical Eng Dept Istanbul Turkey 
hayrettinerturkgmailcom 
2 Trakya University Department of Computer Programming Edirne Turkey 
gurkantunatrakyaedutr 
3 Yildiz Technical University ElectricalElectronics Faculty  
Control and Automation Eng Dept Istanbul Turkey 
tmumcugulezyildizedutr 
Abstract This paper presents a performance analysis of omnidirectional vision 
based Simultaneous Localization and Mapping SLAM In omnidirectional 
vision based SLAM robots perform vision based SLAM using only monocular 
omnidirectional cameras In this paper we mainly investigate the use of an 
omnidirectional camera for Extended Kalman Filter EKF based SLAM To 
evaluate the success of omnidirectional vision based SLAM we have also 
conducted the same simulations using a laser range finder LRF Main 
contributions of this paper are the use of an omnidirectional camera to perform 
SLAM in the Unified System for Automation and Robot Simulation 
USARSim environment which is controlled by MATLAB in our study The 
results of USARSim simulations show that depending on the environmental 
conditions omnidirectional cameras can be used as an alternative to other range 
bearing sensors and stereo cameras  
Keywords Omnidirectional camera SLAM USARSim MATLAB 
1 
Introduction 
In robotic mapping a robot explores an unknown environment and attempts to build a 
map of it 1 Robots sense the landmarks in the environment via their proprioceptive 
and exteroceptive sensors after calculating the range values they create a map of the 
environment in a probabilistic way  Laser range finders and sonar sensors are two 
examples of range and bearing sensors that can be used for SLAM 2 Researchers 
prefer laser range finders since they provide very accurate measurements in most 
cases In addition to these sensors vision based sensors are drawing the attention of 
research community since they offer exiting opportunities over other type of sensors 
which provide only range and bearing A major limitation in vision based SLAM is 
the narrowness of the camera’s field of view 3 Another alternative is the use of 
omnidirectional cameras in SLAM due to their surround observation capabilities 
despite of their low resolutions 4 5 Omnidirectional camera systems are used in 
different type of applications such as autonomous robot navigation surveillance and 
3D reconstruction 6   
408 
H Erturk et al 
 
There are different types of omnidirectional camera systems but among these 
catadioptric camera systems are the most preferred ones due to their relatively low 
costs A catadioptric vision system CVS is shown in Fig 1 A CVS consists of a 
conventional camera positioned in front of a convex mirror In a CVS the center of 
the mirror is aligned with the optical axis of the camera 
 
Fig 1 Catadioptric omnidirectional camera 19 
Researches show that sensor accuracy is one of the factors that directly affect the 
performance of SLAM 1 2 Omnidirectional cameras have some advantages and 
disadvantages that may affect SLAM performance 7 This paper presents 
simulations to show the efficiency of an omnidirectional camera in SLAM  
The paper is organized as follows Robotic mapping and Omnidirectional vision 
based SLAM are explained in Section 2 The simulation studies with USARSim and 
MATLAB are given in Section 3 Conclusions of the paper and future work are given 
in Section 4  
2 
Omnidirectional Vision Based Simultaneous Localization and 
Mapping 
A true omnidirectional sensor views the world through an entire sphere of view as 
shown in Fig 2 The image obtained from an omnidirectional sensor can provide 360 
degree information around the sensor and the direction angle information data with 
the principle point of camera obtained relatively accurate from the image 8 The 
single viewpoint allows the construction of pure perspective images or panoramic 
images 9 Panoramic sensors are different from omnidirectional sensors since they 
are omnidirectional only in one of the two angular dimensions 9 
 
 
Fig 2 Omnidirectional vision vs panoramic vision 9 
A catadioptric system uses a reflecting surface to enhance the field of view The 
position orientation and shape of the reflecting surface are related to the viewpoint 
 
A Performance Analysis of Omnidirectional Vision Based SLAM 
409 
 
and field of view 9 For a catadioptric system such as the one used in our 
simulations to satisfy the single viewpoint constraint SVC all irradiance 
measurements must pass through an effective viewpoint 10 11 In this way 
geometrically correct perspective images can be constructed and planar and 
cylindrical perspective images reconstructed 11 In 10 a solution of the SVC 
equation for hyperbolic solutions is given  
Let c denote the distance between the camera pinhole F and the effective viewpoint 
F´ as shown in Fig 3 r is defined by 
2
2
r
x
y
=
+
 and k is a constant 
2
2
2
2



1


2
2
4
c
k
c
k
z
r
k
−
−
−
−
=
    
k ≥ 2
 
1
 
Fig 3 Parabolic mirror and camera geometry of catadioptric cameras 
The solution comprising the parameters c and k has five types of solutions planar 
conical spherical ellipsoidal and hyperboloidal The omnidirectional viewing system 
simulated for the USARSim environment belongs to the hyperboloidal solutions 
defined by 1 with k2 and c0 11 
3 
Simulation Studies 
MATLAB and USARSim have been used in the simulation studies USARSim is a 
simulation platform of robots and environments 12 It is the basis for the RoboCup 
rescue virtual robot competition USARSim consists of environmental models many 
sensor models and robot models of some experimental and commercial robots It also 
includes drivers to interface with external control frameworks We are planning to use 
Corobot mobile robots in our field tests but Corobot is not supported by USARsim 
Hence Pioneer P2AT robots have been used in the USARSim simulations since they 
support many sensor models The USARSim MATLAB Toolbox is used to interface 
with USARSim and develop robot control programs in MATLAB 13 With this 
toolbox it has been possible to get sensor measurements from the USARSim 
environment and develop our SLAM and Foreign Object Detection algorithms in 
MATLAB The toolbox also allows learning true robot position and orientation In 
addition to the toolbox USAR Screenshot Tool and FRAPS 14 have been used to 
get images from the Pioneer P2AT robot during the simulation  USAR Screenshot 
Tool comes with USARSim MATLAB Toolbox  
410 
H Erturk et al 
 
   
 
Fig 4 a A Simulated omnidirectional camera b A simulated P2AT with an omnidirectional 
camera mounted on a pillar driving autonomously in the simulation environment 
A robot named “explorer” with a laser range finder and an omnidirectional camera 
as exteroceptive sensors has been created in USARSim To set up the simulation of 
the catadioptric camera USARBotini file has been edited Though there are many 
map options available in USARSim we have chosen a simple map to be able to 
evaluate the performances of laser range finder based SLAM and omnidirectional 
vision based SLAM effectively Omnidirectional vision based SLAM requires 
omnidirectional cameras An omnidirectional camera in USARSim looks upwards 
towards a parabolic convex mirror and seems to capture the reflection in the mirror 
12 It depicts the complete 360° surroundings with the perspective distortion Unreal 
engine used by USARSim supports only planar reflecting surfaces The security 
camera trick is used to simulate a nonplanar mirroring surface 12 Fig 4 a shows 
a simulated omnidirectional camera and Fig 4 b shows a simulated P2AT robot 
with an omnidirectional camera mounted on a pillar in USARSim during SLAM 
operation  
31 
Performing Omnidirectional Vision Based SLAM 
While the robot wanders around it needs to detect the features in the environment 
The goal of omnidirectional vision based SLAM is to measure the relative distances 
from the robot to the features 15 The main steps of our approach are as follows 
1 Acquiring an image from USARSim by using the USARSim Matlab Toolbox 
2 Calibrating the mirror system by detecting the center of the image and 
specifying external and internal boundaries of the omnidirectional image 
3 Detecting the contours of features by using an image processing technique 
4 By using the knowledge of the optics of the mirror system to calculate the 
distances to the features’ contours  
5 Returning this information to the SLAM algorithm in MATLAB 
Since the acquired image in step 1 is obtained from an omnidirectional camera the 
image has to be flipped  
image=imflipudomnidirectionalimage 
This function automatically flips the acquired image from the camera updown 
In Step 2 the center of the omnidirectional image is found to calibrate the mirror 
system of the camera To automatically calibrate the camera we have developed a 
 
A Performance Analysis of Omnidirectional Vision Based SLAM 
411 
 
script but it is not functioning properly yet But this is not a concern for this study 
since we control a P2AT robot in the simulation environment and we do not have full 
control on the robot Hence we have set the center of the camera manually  
In Step 3 the contours of features by using an image processing technique is 
detected Radial lines departing from the image center are scanned and searched for 
the first intensity step between white and black 16 Before doing this the 
omnidirectional image is unwrapped into a rectangular image as shown in Fig 5 
unwrappedimage theta = imunwrapimage center angstep Radiusmax 
Radiusmin 
 
 
Fig 5 Conversion of a spherical image to an unwrapped image 
Unwrapped image is segmented into black and white  
Blackandwhiteimage = img2bwunwrappedimage BWthreshold 
The distance in pixel corresponds to the number of pixels from left image border 
until first black point 
distanceinpixel = getpixeldistanceblackwhiteimage  Radiusmin 
For conversion of the image distances in pixels to metric distances the optics of 
the cameramirror system shown in Fig 6 is used 
 
 
Fig 6 The optics of the cameramirror system 
The following equation is derived by using the rectangular triangle in Fig 6 
412 
H Erturk et al 
 
 
tan 
d
h
θ
θ
=
 
2
The relation between θ which is the distance in pixels and the angle ρ can be 
approximated by a first order Taylor expansion 
1
θ
α ρ
≈
 
3
Combining the equations 1 and 2 yields the equation 3 However due to the 
hyperbolic shape of the mirror this formula is limited to small distances 
 
tan
d
h
ρ
ρ
α


=




 
tan
d
h
ρ
ρ
α


=




 
4
The basis of our method has been derived from the method in 15 Instead of this 
method an alternative method colorbased freespace detection is proposed in 17 
32 
Performance Comparison of Omnidirectional Vision Based SLAM and 
EKF Based SLAM Using a Laser Range Finder 
To evaluate the performance of omnidirectional vision for SLAM applications we 
have compared the performance of omnidirectional vision based SLAM with EKF 
based SLAM using a LRF The evaluation is based on calculating the differences of 
the estimated positions of a P2AT robot in USARSim environment with the real 
positions obtained from USARSim Table 1 lists the averaged overall error bounds of 
the methods compared  
Table 1 Comparison of overall error bounds 
Overall mean value of the error 
bound 
Omnidirectional vision 
based SLAM 
SLAM using a laser range 
finder 
x 
811 cm 
383 cm 
y 
743 cm 
247 cm 
theta 
521° 
217° 
 
Though the algorithms we use have some major drawbacks related to the 
implementation problems resulting from using different applications together our 
simulation results provide some valuable insights into the performance of 
omnidirectional vision based SLAM 
4 
Conclusions  
This paper focuses on using omnidirectional cameras as range bearing sensors in 
SLAM applications and presents a simulation study based performance analysis of 
omnidirectional 
vision 
based 
SLAM 
The 
advantagesdisadvantages  
of omnidirectional vision based SLAM and the design challenges related to 
omnidirectional vision based SLAM are investigated and shown with the results of 
 
A Performance Analysis of Omnidirectional Vision Based SLAM 
413 
 
the simulation studies in the USARSim platform Though the results of our 
simulations show that omnidirectional vision based SLAM has bigger overall error 
bounds global x global y and theta than EKFbased SLAM using a laser range 
finder omnidirectional cameras are cost effective alternatives to range bearing 
sensors and stereo cameras The results of our simulations also show that the 
omnidirectional SLAM method proposed in this study is suitable for indoor 
environments since the success of this method depends mainly on the environmental 
conditions which can affect the camera and the distances to features 
While our performance analysis on omnidirectional vision based SLAM provides 
valuable insight into design issues for omnidirectional vision based SLAM 
applications they are only a first step As a future work field tests with autonomous 
robot platforms ie Corobot 18 will be conducted to show the efficiency of 
omnidirectional vision based SLAM   
 
Acknowledgments This research has been supported by Yildiz Technical University 
Scientific Research Projects Coordination Department Project Number 20100402
ODAP01 and Project Number 20100402KAP05 
References 
1 Dissanayake G Newman P Clark S DurrantWhyte HF Csorba M A Solution to 
The Simultaneous Localization and Map Building SLAM Problem IEEE Transactions 
on Robotics and Automation 173 229–241 2001 
2 Williams SB Efficient Solutions to Autonomous Mapping and Navigation Problems 
PhD Dissertation University of Sydney 2001 
3 Suttasupa Y Sudsang A Niparnan N 3D SLAM for Omnidirectional Camera In 
Proceedings of the 2008 IEEE International Conference on Robotics and Biomimetics pp 
828–833 2009 
4 Kim S Oh SY SLAM in Indoor Environments Using Omnidirectional Vertical and 
Horizontal Line Features Journal of Intelligent and Robotic Systems 51 31–43 2008 
5 Kim J Yoon KJ Kim JS Kweon I Visual SLAM by SingleCamera Catadioptric 
Stereo In Proc of the SICEICASE International Joint Conference 2006 
6 Rituerto A Puig L Guerrero JJ Visual SLAM with An Omnidirectional Camera In 
Proceedings of the 2010 International Conference on Pattern Recognition pp 348–351 
2010 
7 Burbridge C Spacek L Condell J Nehmzow U Monocular Omnidirectional Vision 
based Robot Localisation and Mapping In Proc of the TAROS 2008 2008 
8 Li M Imou K Wakabayashi K 3D Positioning for Mobile Robot Using 
Omnidirectional Vision In Proceedings of the 2010 International Conference on 
Intelligent Computing Technology and Automation pp 7–11 2010 
9 Nayar SK Omnidirectional Video Camera In Proceedings of the DARPA Image 
Understanding Workshop 1997 
10 Baker S Nayar SK A Theory of SingleViewpoint Catadioptric Image Formation 
International Journal of Computer Vision 35 1–22 1999 
11 Schmits T Visser A An Omnidirectional Camera Simulation for the USARSim World 
In Iocchi L Matsubara H Weitzenfeld A Zhou C eds RoboCup 2008 LNCS 
vol 5399 pp 296–307 Springer Heidelberg 2009 
414 
H Erturk et al 
 
12 USARSim 2010 httpsourceforgenetprojectsusarsim 
13 MATLAB USARSim Toolbox 2010 
httproboticsmemdrexeleduUSAR 
14 FRAPS show fps Record Video Game Movies screen capture software 2011 
httpwwwfrapscom 
15 Scaramuzza D Siegwart R AppearanceGuided Monocular Omnidirectional Visual 
Odometry for Outdoor Ground Vehicles IEEE Transactions on Robotics 245 1015–
1026 2008 
16 Scaramuzza D Martinelli A Siegwart R Appearancebased SLAM with Map Loop 
Closing Using an Omnidirectional Camera In Proceedings of the IEEE International 
Conference on Intelligent Robots and Systems IROS 2006 2006 
17 Nguyen Q Visser A A Color Based Rangefinder for an Omnidirectional Camera In 
Proc of the 2009 IEEERSJ International Conference on Intelligent Robots and 
SystemsIROS 2009 pp 41–48 2009 
18 Corobot 2010 httproboticscorowarecom 
19 OmniAlert 360 Camera 2011 httpwwwremoterealitycom 
productcomponentsproductsmenu116omnialert360scamera
productsmenu92 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 415–422 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Trajectory Estimation of a Tracked Mobile Robot  
Using the SigmaPoint Kalman Filter with an IMU  
and Optical Encoder 
Xuan Vinh Ha1 Cheolkeun Ha1 and Jewon Lee2 
1 School of Mechanical Engineering University of Ulsan Republic of Korea 
xuanvinhhagmailcom hacheol21cyahoocokr 
2 Prigent Ltd 
benzydadhotmailcom 
Abstract Trajectory estimations of tracked mobile robots have been widely 
used to explore unknown environments and in military applications In this 
paper we estimate the precise trajectory of a tracked skidsteered mobile robot 
that contains an inertial measurement unit IMU and an optical encoder For a 
systematic estimation we implement a sigmapoint Kalman filter SPKF 
which produces more accurate trajectory information is easier to calculate and 
requires no analytic derivations or Jacobians The proposed SPKF compensates 
for the limitations of the IMU and encoder in trajectory estimation problems as 
observed from our experimental results  
Keywords Tracked mobile robots Inertial Measurement Unit IMU Sensor 
fusion SigmaPoint Kalman Filter SPKF 
1 
Introduction 
Recently several studies have proposed and developed various approaches to estimate 
the position of mobile robots For example one study used an extended Kalman filter 
with a lowcost GPS unit and inclinometer to develop a localization scheme for 
Ackerman steering vehicles in indoor autonomous navigation situations by estimating 
the positions of the vehicles and their sensor biases 1 In another study a 
navigational system consisting of a MEMSbased digital inplane threeaxis IMU an 
active beacon system and an odometer was utilized to obtain more precise robot 
position data and to monitor the robot environment in realtime 2 In 3 a relative 
localization method was used to determine the navigational route of a convoy of 
robotic units in an indoor environment using a discrete extended Kalman filter based 
on lowcost laser range systems and builtin odometric sensors Yet discrepancies 
remain between the position measurements and the navigational estimations from 
odometric infrared and ultrasonic measurements and Kalman filtering techniques in 
skidsteered vehicles as presented in 4 Consequently 5 introduced the unscented 
Kalman filter for a known kinematic model of robots Then in 6 a family of 
                                                           
 Corresponding author 
416 
XV Ha et al 
improved derivative nonlinear Kalman filters called sigmapoint Kalman filters 
SPKFs was used and can be applied to the problem of looselycoupled GPSINS 
integration A novel method to account for latency in GPS updates has also been 
developed for the SPKF  
Thus this paper focuses on using the estimated system in order to obtain the 
precise trajectory of a fourwheeled tracked skidsteered mobile robot Hazard 
Escape I based on IMU kinematic motion and velocity constraints It describes the 
details of the sensor fusion technique that combines the IMU sensor with the optical 
encoder For a systematic estimation it implements the sigmapoint Kalman filter 
which produces more accurate position information is easier to calculate and 
requires no analytic derivations or Jacobians The estimated results are compared with 
the measurement results from the IMU and encoder sensors installed on a four
wheeled tracked skidsteered mobile robot as shown in Fig 1 
  
 
Fig 1 The NT  Hazard Escape I Mobile Robot 
This paper is organized as follows Section 2 introduces the IMU kinematic motion 
model and the velocity constraints The sigmapoint Kalman filter is discussed in 
Section 3 The experimental setup and results are described in Section 4 Section 5 
presents the conclusion  
2 
IMU Kinematic Model and Velocity Constraints 
21 
IMU Kinematic Motion Model 
We define a navigational reference frame NXYZ and robot body frame Bxyz as 
shown in Fig 2 Let PNt=XNtYNtZNtT 
3
∈   and VNt=VxtVytVztT 
3
∈   denote the position and the velocity vectors respectively of the IMU in the N 
frame Furthermore let 
  
Θ = φ θ ψ T
3
∈   denote the attitude angles We also 
define the IMU acceleration and the angular rate measurements in the B frame as aB = 
aBxaByaBzT
3
∈   and ωB = ωBx ωBy ωBzT
3
∈   respectively Then it is 
straightforward to calculate the transformation from the B frame to the N frame as 
given by the following matrix  
N
B
c c
s c
c s s
s s
c s c
C
c s
c c
s s s
s c
s c s
s
c s
c c
θ ψ
ψ φ
ψ φ θ
φ ψ
ψ θ φ
θ ψ
φ ψ
θ φ ψ
φ ψ
θ φ ψ
θ
θ φ
φ θ


−
+
+


=
+
−
+




 −

 
1
 
Trajectory Estimation of a Tracked Mobile Robot Using the SPKF 
417 
where 
cos s
sin
cθ
θ
θ
θ
=
=
 and the same notiation convention is used for 
anglesφ andψ  
 
Fig 2 An IMU kinematic model of a fourwheeled tracked skidsteered mobile robot 
Note that the Euler angles transform according to the matrix   
1
sin
tan
cos
tan
0
cos
sin
0
sin
 cos
cos
 cos
q
φ
θ
φ
θ
θ
θ
φ
θ
φ
θ
Θ




=
−






 
2
Assuming that there is no moving bias after subtracting the constant offset and local 
gravity vector the accelerometer model and the gyros model can be described as 7 
B
B
accel
a
x
w
=
 +
 
3
B
B
gyros
r
w
ω
=
+
 
4
where the true acceleration vector the true angular rate vector of the vehicle in the B 
frame the acceleration of white noise and the angular rate of white noise are 



T
B
Bx
By
Bz
x
x
x
x
 =



 



T
B
Bx
By
Bz
r
r
r
r
=
 
waccel
 and 
wgyros
 respectively Then the 
kinematic motion equation for the IMU can be simplified such that 
N
N
N
N
B
B
B
P
V
V
C a
q ω
Θ
=
=
Θ =



 
5
22 
Velocity Constraints 
The IMU coordinate is located at xM yM 0 in the B frame as shown in Fig 2 Using 
two wheel encoders in the left and right sides of the vehicle we obtain the IMU 
velocity vector 



T
B
Bx
By
Bz
v
v
v
v
=
 in the B frame Due to the low speed of the 
mobile robot the wheel slips and the longitudinal ICR location S are very small and 
can be neglected from the velocity constraints Based on 8 since the four tracked 
wheels of our robot always contact the ground and the IMU is fixed on the robot 
platform the velocity constraints for the IMU device can be simplified as  
418 
XV Ha et al 
0
vBz
=
 
6
Bx
M
v
v
y ω
=
−
 
7
By
M
v
= x ω
 
8
where v  and ω  are the velocity and angular speed of robot respectively as measured 
by two optical encoders  
3 
SigmaPoint Kalman Filter Design 
Now we define the state variable vector
9
 

 
 
 T
N
N
X k
P
k V
k
k
=
Θ
∈  We 
rewrite the IMU kinematic equation shown in 5 in discretetime form such that  
 


1   
1

1
 

1   
1
X k
f X k
u k
w k
X k
T
g X k
u k
w k
=
−
−
=
−
+ Δ
−
−
9
where the IMU input signals at the kth sampling time are  

 
 T
B
B
u k
x
k r
k
= 
 the 
data sampling period is ∆T and the nonlinear function g of the process model is given 
by  
 


1  

1
 
 

1

 

1
N
N
B
B
accel
B
gyros
V
k
g X k
u k
w k
C
k
x
k
w
k
q
r
k
w
k
Θ




−
−
=
+
−




+
−



 
10
Using the velocity constraints from 68 the IMU velocities in B frame can be 
described as the measurements
3
 

 
 
 T
Bx
By
Bz
y k
v
k v
k v
k
=
∈  Including the 
wheel encoder measurement noises and ground topography denoted by nk the 
measurement model can be rewritten in discretetime form as  


 

 
 
 
T
N
B
N
y k
h X k
n k
C
V
n k
=
+
=
+
 
11
Then the SPKF for the system described by 9 and 11 can be implemented by 
concatenating 
the 
state 
and 
noise 
components 
together 
as
  
 
 
 
a
T
T
T
T
X k
X
k
w k
n k
=
 The implementation for the SPKF is as follows 6 
• Initialize with 
ˆ
ˆ
0
0
0
0
0
T
a
a
T
X
X
X




= Ε
=



  
 12



0
0
0
ˆ
ˆ
0
0
0
0
0
0
0
0
0
T
a
a
a
a
a
P
P
X
X
X
X
Q
R






= Ε
−
−
=










 
13
Iterate for each time step 
k ∈1 
∞ via 
 
 
Trajectory Estimation of a Tracked Mobile Robot Using the SPKF 
419 
• The sigma point calculation 
ˆ
ˆ
ˆ

1

1

1

1

1

1
a
a
a
a
a
a
k
X
k
X
k
c P
k
X
k
c P
k
χ


−
=
−
−
+
−
−
−
−



  
14
• Time update 


 
1

1  

1
x
x
w
k k
f
k
u k
k
χ
χ
χ
−
=
−
−
 
15
2
 
0
ˆ  
1
 
1
L
i
x
m
i
i
X k k
W
k k
χ
=
−
=
−

 
16



2
 
0
ˆ
ˆ
 
1
 
1
 
1
 
1
 
1
L
T
i
x
x
i
i
C
i
P k k
W
k k
X k k
k k
X k k
χ
χ
=
−
=
−
−
−
−
−
−

17
• Measurement update 


 
1
 
1
 
1
x
n
Y k k
h
k k
k k
χ
χ
−
=
−
−
 
18
2
 
0
ˆ 
1
 
1
L
i
m
i
i
y k k
W
Y
k k
=
−
=
−

 
19



2
0
ˆ
ˆ
 
1
 
1
 
1
 
1
L
T
i
U
C
i
i
i
S
W
Y k k
y k k
Y k k
y k k
=
=
−
−
−
−
−
−

 
20



2
 
0
ˆ
ˆ
 
1
 
1
 
1
 
1
L
T
i
x
U
i
C
i
C
W
k k
X k k
Y
k k
y k k
χ
=
=
−
−
−
−
−
−

 
21
U 
U
K
C
S
=
 
22


ˆ
ˆ
ˆ
 
 
1
 
 
1
X k
X k k
K y k
y k k
=
−
+
−
−
 
23
 
 
1
T
U
P k
P k k
KS K
=
−
−
 
24
where c
L
λ
=
+
is a scaling parameter that determines the spread of the sigma
point 
4 
Experiments 
41 
Experiment Setup 
In our experimental setup the MTI IMU and the two optical encoders 61C110108
02 were installed on a mobile robot as shown in Fig 3 A notebook computer was 
used to obtain the data from the two sensors via a DSP board During the experiment 
the measurement sampling frequencies for the IMU and the optical encoders were set 
to 100 Hz and 5 Hz respectively The robot was programmed to travel in a straight 
line in an arbitrary direction for 3145 meters in 15 seconds   
420 
XV Ha et al 
 
Fig 3 The experimental setup for a mobile robot with an IMU and two optical encoders 
42 
Experiment Results 
The experiment was performed in realtime to test the trajectory estimation of the 
mobile robot In general robot positions are inaccurate due to errors from the 
encoders the IMU sensors and the realtime experimental condition among other 
factors Thus the SPKF was used to estimate the position of the mobile robot in this 
localization problem    
 
0
3
6
9
12
15
005
000
005
010
015
020
025
 
 
Velocity X direction ms
T im e  s
 En co de r
 IM U
 SP KF
 
Fig 4 The velocities from the IMU the encoder and the SPKF estimation along the X
direction in the N frame 
0
3
6
9
1 2
1 5
1 2
1 0
0 8
0 6
0 4
0 2
0 0
 
 
Velocity Y direction ms
T im e  s 
 E n c o d e r
 IM U
 S P K F
 
Fig 5 The velocities from the IMU the encoder and the SPKF estimation along the Y
direction in the N frame  
Figures 4 and 5 present graphs of the velocities from the encoder the IMU and the 
SPKF estimation along X and Ydirections in the navigation frame As these figures 
illustrate the estimated velocities from the SPKF are more accurate than the velocities 
from the individual IMU or the encoder Note that the velocity is zero at the robot’s 
starting and ending positions the estimated velocity along the Ydirection 
especially  
 
Trajectory Estimation of a Tracked Mobile Robot Using the SPKF 
421 
0 0
05
10
1 5
2 0
2 5
1 0
8
6
4
2
0
 
 
Yaxis m
X ax is m 
 R e fe re nce
 E n cod e r
 IM U
 S P K F
 
Fig 6 The trajectories of the encoder IMU and SPKF estimation as compared to a reference 
line 
0
3
6
9
1 2
1 5
0
2
4
6
8
1 0
 
 
Position errors m
T im e  s 
 E n c o d e r
 IM U
 S P K F
 
Fig 7 The position errors of encoder IMU and SPKF estimation 
0
3
6
9
1 2
1 5
0 8
0 6
0 4
0 2
0 0
0 2
 
 
Heading angle error rad
T im e  s 
 E n c o d e r
 IM U
 S P K F
 
Fig 8 The heading angle errors of the encoder IMU and SPKF estimation 
In Fig 6 the SPKF trajectory remains near the reference line while the IMU 
trajectory diverges from the reference line Thus the position error graphs for the 
SPKF and the encoder remain near zero while the position error graph of the IMU 
diverges rapidly as shown in Fig 7 Additionally the heading angle error of the 
SPKF is also close to zero as shown in Fig 8 Consequently the root mean square 
error RMSE of the SPKF is the smallest 01711m compared to that of the 
encoder and IMU 03121m 38075m respectively  
5 
Conclusion 
This paper presented an experiment to estimate the given trajectory of a tracked 
mobile robot using measurements of the IMU and encoder sensors Error sources such 
as noise constant offset and bias limit the ability of these devices to obtain accurate 
trajectory estimations However based on the IMU kinematic motion and velocity 
422 
XV Ha et al 
constraints we implemented the sigmapoint Kalman filter in order to systematically 
estimate the precise trajectory of a fourwheeled tracked skidsteered mobile robot 
The SPKF was implemented because it produces more accurate position information 
is easier to calculate and requires no analytic derivations or Jacobians This novel 
systematic estimation was performed on the commercialized Hazard Escape I mobile 
robot and the estimated results from the SPKF were more accurate than the sensor 
measurements producing the smallest RMSE and the smallest heading angle error 
among other factors 
 
Acknowledgements This work was supported by the 2011 Research Fund of 
University of Ulsan in Korea 
References 
1 
Weinstein AJ Moore KL Pose Estimation of Ackerman Steering Vehicles for 
Outdoors Autonomous Navigation In 2010 IEEE International Conference on Industrial 
Technology pp 579–584 2010 
2 
Lee T Shin J Cho D Position Estimation for Mobile Robot Using Inplane 3axis 
IMU and Active Beacon In IEEE International Symposium on Industrial Electronics pp 
1956–1961 2009 
3 
Espinosa F Santos C MarrónRomera M Pizarro D Valdés F Dongil FJ 
Odometry and Laser Scanner Fusion Based on a Discrete Extended Kalman Filter for 
Robotic Platooning Guidance Sensor 9 8339–8357 2011 
4 
Tamas L Lazea G Robotin R Marcu C Herle S Szekely Z State Estimation 
Based on Kalman Filtering Techniques in Navigation In IEEE International Conference 
on Automation Quality and Testing Robotics pp 147–152 2008 
5 
Suliman C Moldoveanu F Unscented Kalman Filter Position Estimation for An 
Autonomous Mobile Robot Bulletin of the Transilvania University of Brasov 3 2010 
6 
van de Merwe R Wan EA SigmaPoint Kalman Filters for Integrated Navigation 
Institute of Navigation Annual Technical Meeting 2004 
7 
Flenniken IV WS Wall JH Bevly DM Characterization of Various IMU Error 
Sources and the Effect on Navigation Performance In Proceedings of the 18th 
International Technical Meeting of the Satellite Division of The Institute of Navigation 
pp 967–978 2005 
8 
Yi J Wang H Zhang J Song D Jayasuriya S Liu J Kinematic Modeling and 
Analysis of SkidSteered Mobile Robots With Applications to LowCost Inertial
MeasurementUnitBased Motion estimation IEEE Transactions on Robotics 25 1087–
1097 2009 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 423–432 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Development of a Mobile Museum Guide Robot  
That Can Configure Spatial Formation with Visitors 
Mohammad Abu Yousuf1 Yoshinori Kobayashi12 Yoshinori Kuno1  
Akiko Yamazaki3 and Keiichi Yamazaki1 
1 Saitama University Saitama Japan 
2 Japan Science and Technology Agency JST PRESTO Kawaguchi Japan 
3 Tokyo University of Technology Hachioji Japan 
yousufyosinorikunocvicssaitamauacjp 
ayamazakimediateuacjp BYI06561niftycom 
Abstract Museum guide robot is expected to establish a proper spatial forma
tion known as “Fformation” with the visitors before starting its explanation of 
any exhibit This paper presents a model for a mobile museum guide robot that 
can establish an Fformation appropriately and can employ “pause and restart” 
depending on the situation We began by observing and videotaping scenes of 
actual museum galleries where human guides explain exhibits to visitors Based 
on the analysis of the video we developed a mobile robot system that can guide 
multiple visitors inside the gallery from one exhibit to another The robot has 
the capability to establish the Fformation at the beginning of explanation after 
arriving near to any exhibit The robot can also implement “pause and restart” 
depending on the situation at certain moment in its talk to first elicit the visi
tor’s attention towards the robot Experimental results suggest the efficacy of 
our proposed model  
Keywords Fformation humanrobot interaction mobile guide robot pause 
and restart 
1 
Introduction 
Museum guide robots are one of the important application areas considered in re
search in human robot interaction 12 In the case of a museum guide robot both 
robot and visitor should have equal direct and exclusive access to the space where 
the target exhibit exists In our previous research we investigated some nonverbal 
actions of museum guide robots during explanation of exhibits 3 In our previous 
research however we considered only the situations that occur after the robot starts 
its explanation We assumed that the visitors were already in the proper position to 
enjoy the explanation However guide robots need to bring about such situations in 
order to really work as guides effectively This paper therefore is concerned with the 
issue of initiating explanation in a natural humanrobot interaction 
424 
MA Yousuf et al 
 
Kendon’s analysis on spatial formation known as Fformation explains that “An F
formation arises whenever two or more people sustain a spatial and orientational rela
tionship in which the space between them is one to which they have equal direct and 
exclusive access 4” In humanhuman interaction when people group themselves 
they automatically form an Fformation In humanrobot interaction if a proper F
formation is not formed during the initiation to talk the robot should do some actions 
to establish an Fformation Goodwin 5 also showed some systematic ways in which 
speakers obtain the attention of a recipient When a current speaker begins to utter a 
sentence and if she finds recipients are not gazing towards him or her the speaker 
can use “restart” andor “pause” in the delivery of the utterance  
There have been several studies about robot’s controlling the position of people 
during interaction with them Hüttenrauch et al found that people follow the F
formation in their interacting with robots 6 Although their observations revealed 
that humans formulate Ospace toward a robot they did not study how a robot should 
formulate its spatial relationship Tasaki et al utilized Hall’s proximity theory 7 to 
determine a combination of sensors and robot behavior along with the current dis
tance from the interacting person 8 Kuzuoka et al studied the effect of body orien
tation and gaze in controlling the Fformation 9 Yamaoka et al focused on the posi
tions and body orientations to implement their information presenting robot 10 
While this research is impressive so far very few studies have revealed how a robot 
should behave to initiate interaction with multiple visitors before starting its explana
tion  In this paper we focus on how a guide robot establishes an appropriate F
formation to initiate interaction with multiple visitors We also examine how “pause 
and restart” plays an important role to attract the attention of visitors 
2 
Interaction between Human Guide and Visitors at Museum 
In order to develop our museum guide robot we analyzed some video footage record
ed at the National Japanese American Museum in Los Angeles The videos record 
human guides explaining exhibits to a small group of visitors Transcript 1 and Figs 
1a to 1e convey a typical fragment of such human guide behavior at the museum 
In transcript 1 MG moves to another exhibit Fig 1a MG clears his throat in the 
3rd line “Clearing throat” is one kind of indication that he is waiting for the visitors to 
come to the exhibit In line 4 MG employs a pause of 50 seconds By this time the 
visitors are following MG Figs 1b 1c MG deploys “restart” and a “pause” of 
02 seconds 6th line while asking an involvement question to the visitors “Have you 
all heard picture brides” At line 8 some visitors move their head vertically in re
sponse to the MG’s question  From lines 1013 V1 V2 and V3 offer verbal res
ponses to MG’s question In lines 14 and 15 MG asks the visitors to come closer 
Fig 1d an indication that they should form a proper Fformation Fig 1e  
From this analysis two interaction patterns are derived to design our robot system 
1 Robots should have the capability to establish a proper Fformation 
2 Robots should deploy “pause and restart” depending on the situation 
 
 
Development of a Mobile Museum Guide Robot That Can Configure Spatial Formation 
425 
 
Transcript 1 Picture bride DataCollected at National Japanese American Museum 
MG Guide V1V2 V3 Visitors 
1   MG Okay let’s come over here  
2            Guide moves to another exhibit 
3   MG clears his throat 
4            50 people follow MG 
5   MG Okay so you ah heard of uh picture brides   
6            Have you all heard picture brides 02 See the  
7            picture up there those are picture brides 
8            20 some people moves their head vertically 
9   MG You all heard of you never heard of picture brides 
10 V1   No= 
11 V2   =Not me 
12          08 
13 V3   Uhhuh 
14 MG  Or if you wanna come closer this way so that other   
15           people could leave uh on the other sides 
 
 
 
Fig 1 Human guide and visitors’ interaction at an actual museum 
3 
Mobile Museum Guide Robot System 
Based on our findings we developed a mobile museum guide robot system utilizing a 
humanoid robot RobovieR Ver3 Overview is shown in Fig 2 The robot can move 
via wheels installed on the bottom and can move its head and arms by controlling its 
joints Its head which incorporates eye cameras and an ear microphone moves along 
three axes Yaw Roll and Pitch like a human head Our system utilizes two general
purpose PCs connected by a wired network In our vision system we incorporate 
three Logicool USB cameras and two laser range sensors The three USB cameras are 
attached to a pole installed on the back of the robot and can detect and track visitors’ 
faces and their face directions The two laser range sensors are attached to another 
long pole which is kept at a fixed position just in front of the experimental area One 
of the two laser range sensor detects the ellipsoidal marker which is attached to the 
robot’s body to obtain the position and orientation of the robot while the other laser 
range sensor is used to track the position and body orientation of the visitors  
Our system consists of four software units the face detection  tracking unit the 
body tracking unit the robot’s position tracking unit and the robot control unit Dur
ing its explanation of exhibits the robot performs predetermined bodily nonverbal 
actions such as facing towards the visitors gesturing with its hands and pointing to 
the exhibits 
Symbols Used in the Transcript 
  
Vocalizations 
which 
are 
difficult to convey in text 
50 20 
08 02 
Pauses are timed in seconds 
and inserted within paren
theses 
 
Slight rising tone 
 
Stretched sound 
 
Speeding up the pace of 
delivery 
 
Final 
rising 
tone 
which 
mayor may not indicate a 
question 
 
Short untimed pause within 
an utterance 
= 
Overlap 
did 
Guess at unclear word 
 
Simultaneous utterances 
 
Stopping fall in tone with 
some sense of completion 
426 
MA Yousuf et al 
 
 
Fig 2 System overview of our mobile guide robot 
4 
Proposed Modeling of Interaction 
41 
Model of FFormation 
For establishing a proper Fformation we consider the following parameters 
• Distance between the robot and the visitors Ranges from 90cm to 120cm Fig 
3a  
• Distance between the robot and the exhibits About 110 cm fixed in all cases for 
all exhibits Fig 3a 
• Visitor’s body orientation Should be in the direction between the robot and the 
exhibit Fig 3b 
• Visitor’s face direction Should be towards the robot or the exhibit 
• Robot’s body orientation The robot turns its body 300 towards the exhibit to ex
plain the exhibit Fig 3c 
• Robot’s field of view FOV  Set at a limit of 1500 Fig 3c 
After arriving at an appropriate position for explanation of the exhibit the robot 
should examine whether or not a proper Fformation has been established  
 
Fig 3 Standing position of the robot and the visitors to establish an Fformation 
Development of a Mobile Museum Guide Robot That Can Configure Spatial Formation 
427 
 
42 
Model of “Pause and Restart” to Achieve Mutual Gaze 
The robot observes the visitors’ face direction and body orientation before beginning 
its explanation of any exhibit If at the beginning of the speaker’s robot turn the 
face direction is detected as not directed toward the robot or exhibit or body orienta
tion is detected as not in the direction between the robot and the exhibit the robot 
employs the “pause and restart” strategy The format of “pause and restart” which is 
implemented in our system is as follows 
                    Beginning + pause + new Beginning 
 
 
 
              ………X 
In this format the solid line below the sentence structure indicates that the recipient is 
gazing toward the speaker no line indicates that the recipient is gazing elsewhere and 
the ‘X’ marks the point at which the recipient’s gaze reaches the speaker The dotted 
line represents the time required for the recipient to move hisher gaze from some 
other position to the speaker In order to implement “pause and restart” we consider 
the first sentence of each script explaining the exhibits as in the following 
Script 1 First sentence for explaining the first exhibit “Te Nave Nave Fenua” 
Robot This is a 20 This is a famous work of Gauguin 
In our proposed model the guide robot deploys “pause and restart” depending on the 
situation In all scripts a restart with a preceding pause of 2 seconds is used 
5 
Experiment with Humanoid Robot 
To test the robot’s effectiveness an experiment was performed in the laboratory Four 
paintings were placed in the area as shown in Fig 4 
 
Fig 4 a Overview of the experimental area b Mobile guide robot and four paintings 
A total of 16 graduate students 8 groups with 2 members in each group partici
pated in the experiment Among the 8 groups Group A groups no 1 3 5 and 7 
participated in the sessions where the robot explained the first two paintings as the 
proposed guide robot system outlined above and the remaining two paintings as a 
robot not equipped with the capacity to form an Fformation On the other hand 
Group B groups no 2 4 6 and 8 participated in the sessions where the robot did 
this in reverse order Participants were not informed of which robot was which  
428 
MA Yousuf et al 
 
Initially the robot with the proposed system waits in the middle of the experimen
tal area A schematic diagram of the main tasks performed by the robot is given in 
Fig 5 The order of these main tasks is as follows 
1 When the robot finds visitors coming into its immediate vicinity it says “May 
I explain these paintings to you” If the visitors’ gaze turns toward the robot’s 
direction for three seconds the robot system considers the visitors to be highly 
interested in the exhibits Fig 5a 
 
Fig 5 Schematic diagram of main tasks 
2 The robot then guides the visitors to the first exhibit Fig 5b 
3 After arriving at the predefined position near the first exhibit the robot follows 
the following steps to establish a proper Fformation 
i 
First the robot verifies the distance between itself and the visitors 
ii If the visitors are not within range the robot turns its head towards the 
visitor and says to them “Please come closer” or “Please move back a 
little” depending on the situation 
iii Next the robot turns 300 clockwise to orient towards the first exhibit 
iv Then the robot verifies the body orientation of the visitors 
v If the visitor’s body orientation is not in the direction between the ro
bot and the exhibit the robot turns its head towards the visitor and 
starts its explanation using “pause and restart” 
vi Next the robot verifies the direction of the visitors’ faces  
vii If the face direction is towards the robot or the exhibit the robot be
gins to explain the first exhibit If not the robot starts its explanation 
with a “pause and restart” 
4 After completing its explanation the robot moves to the next exhibit and at the 
same time invites the visitors to follow along Fig 5c 5d 5e 
5 The robot repeats task 3 to explain the next exhibit 
6 Finally after explaining all four exhibits the robot returns to its initial position 
and waits for more visitors to arrive Fig 5f 
In the experiment the robot based on our proposed model was compared with a robot 
that did not employ the proposed model 
a Proposed Robot Robot behaves based on the model outlined in this paper 
b Conventional Robot Robot begins its explanation after finding the faces of 
visitors It does not care whether or not a proper Fformation is formed nor 
does it utilize the ‘pause and restart’ strategy It explains the exhibits with 
the same preprogrammed nonverbal behaviors as the proposed robot 
Development of a Mobile Museum Guide Robot That Can Configure Spatial Formation 
429 
 
We videotaped all sessions In addition we recorded all laser range finder and camera 
data so that we could obtain the exact motions of the robot and the participants for 
later analysis After the experiments we asked participants to subjectively rate the 
robot’s effectiveness on a sevenpoint Likert scale with the range 1very ineffective 
2ineffective 3somewhat ineffective 4undecided 5somewhat effective 6
effective 7very effective The questionnaire items were as follows 
1 Did you think that the robot attended to you adequately during explanation 
2 Did you think that the robot was able to attract your attention to listen to its 
explanation 
3 Overall evaluation about the robot 
6 
Experimental Results 
We have examined the experimental results from the following three viewpoints 
1 
Autonomous capability of the robot Can the robot correctly judge the situation 
and behave properly according to our proposed model Sections 61 62  63 
2 
Effectiveness of the robot actions Can the robot’s actions make the participants 
form a proper Fformation and attract their attention Sections 61 62  63 
3 
Subjective evaluation Do the participants prefer proposed robot Section 64 
61 
Control of Visitors’ Standing Position 
We recorded the sensor data from all sessions in the experiment for analysis As 
covered in section 5 the robot explained the first two paintings as the proposed robot 
and the remaining two as the conventional robot to Group A groups no 1 3 5 and7 
and in the reverse order to Group B groups no 2 4 6 and 8 We found from the 
sensor data that there were 11cases 5 cases for Group A 6 cases for Group B where 
some visitors were not in proper position The robot took the initiative to control 
visitors’ standing position in all 11 cases Thus the success rate of robot’s decisions 
was 100   
In the 5 cases for Group A out of 10 participants 6 were out of proper Fformation 
range and after the robot’s action 5 of these 6 moved inside the range In the 6 cases 
for Group B out of 12 participants 9 were out of proper range and after the robot’s 
action 6 of these 9 moved inside the range The total success rate was thus 73 the 
robot corrected 11 out of 15  
On the other hand when the robot explained the paintings as a conventional robot 
we found from the sensor data that 17 participants in both groups 7 in Group A 10 in 
Group B were out of range Of the total of 17 participants only 6 moved inside the 
range to form a proper Fformation when the robot began its explanation The “suc
cess rate” for the conventional robot was thus 35 6 out of 17 although here we 
only use the term “success rate” for convenience since the robot did not try to correct 
the visitors 
62 
Control of Visitors’ Body Orientation 
From the recorded sensor data we counted the number of visitors who successfully 
changed their body orientation to a direction towards meaning oriented to the space 
430 
MA Yousuf et al 
 
between the robot and the painting after the robot’s employed a “pause and restart” at 
the beginning of its explanation In 2 out of 8 cases in Group A  and 3 out of 8 cases 
in Group B the robot noticed that some visitors’ body orientations were not in the 
direction between the robot and the painting  The robot employed “pause and res
tart” in all 5 cases meaning its success rate at deploying the strategy was 100   
In 2 cases among those of Group A the robot noticed 2 out of 4 participants’ orien
tations were not in the direction between the robot and the painting while in 3 cases 
among those of Group B it noticed 3 out of the 6 participants were incorrectly orien
tated After the robot’s actions both of the incorrectly oriented participants in Group 
A and 2 of the 3 in Group B changed their body orientations appropriately The total 
success rate was therefore 80 the robot corrected 4 out of 5 
On the other hand when the robot explained the paintings as a conventional robot 
would we found from the recorded sensor data that 3 out of 16 participants in Group 
A and 1 out of the 16 participants in Group B were not oriented towards the robot and 
the painting Out of a total 4 incorrectly orientated participants 2 changed their body 
orientations towards the robot and the painting just after the robot started its 
explanation The conventional robot thus scored a “success rate” of 50 2 out of 4    
Fig 6 shows a typical example of a visitor changing his body orientation in the di
rection between the robot and the painting after the robot’s use of “pause and restart”  
In Fig 6a the robot noticed that the visitor’s V1 body orientation was not in the 
direction between the robot and the painting It then turned its head towards V1 and 
employed “pause and restart” to attract the visitors’ attention Fig 6b V1 turned 
his body towards the robot and mutual gaze was established Fig 6c V1 fully 
turned his body towards the robot Fig 6d 
 
Fig 6 A participant changing his body orientation after the robot’s use of “pause and restart” 
63 
Control of Visitors’ Face Direction 
Here we examined those visitors whose body orientations were in the direction be
tween the robot and the painting but whose faces were directed elsewhere We 
counted the number of such visitors from the recorded sensor data When performing 
as the proposed model the robot noticed 2 cases out of 8 in Group A and 4 cases out 
of 8 in Group B where some visitors’ face directions were not towards the robot or the 
painting  The robot employed “pause and restart” in all 6 cases so its deployment 
success rate was 100   
In the 2 cases in Group A the robot noticed 3 out of 4 participants’ face directions 
were not towards itself or the painting while in the 4 cases in Group B it noticed 4 out 
of the 8 participants’ face were directed elsewhere After the robot’s actions 2 of the 
Development of a Mobile Museum Guide Robot That Can Configure Spatial Formation 
431 
 
3 participants with faces directed elsewhere in Group A and 3 out of the 4 in Group B 
changed their face directions towards the robot or the painting The total success rate 
was therefore 71 the robot corrected 5 of the 7 participants 
On the other hand when the robot explained the paintings as a conventional robot 
2 out of 16 participants in Group A and 3 out of 16 participants in Group B had faces 
not directed towards the robot or the painting Out of the total of 5 whose face direc
tions were not towards the robot or the paintings only 1 shifted his face appropriately 
just after the robot started its explanation for a “success rate” of 20 1 out of 5  
Although the number of cases is small and we cannot yet draw any definite conclu
sion the results suggest that the “pause and restart” strategy can have a logical effect 
64 
Subjective Evaluation 
We conducted a subjective evaluation of the experiment among the participants the 
results of which are as follows For the question “Did you think that the robot at
tended to you adequately during explanation” Fig 7a repeated measures of 
ANOVA 
revealed 
a 
significant 
difference 
between 
the 
two 
conditions 
F115=1508 p=00014 The same was true of the question “Did you think that 
the robot was able to attract your attention to listen to its explanation” Fig 7b 
where repeated measures of ANOVA also showed a significant difference between 
the two conditions F115=859 p=00103 Finally the participants’ overall evalua
tion Fig 7c here too repeated measures of ANOVA showed a significant differ
ence between the two conditions F115=603 p=00266 
 
Fig 7 Results of subjective evaluation 
7 
Discussion and Conclusion 
In this paper we proposed a museum guide robot that can establish a proper F
formation and employ the “pause and restart” depending on the situation  Based on 
our analysis of videos collected at real museums we have found that a human guide 
and visitors always create an Fformation and moreover that if the visitors are not 
ready to listen to the presentation then the human guide may repeat sentences andor 
deploy pauses during hisher explanation Based on this we developed a museum 
guide robot system that is able to move from one exhibit to another in a museum gal
lery to appropriately formulate an Fformation system and also to employ the “pause 
and restart” if necessary We verified the effectiveness of our system in experiments 
432 
MA Yousuf et al 
 
using human participants In future work we plan to conduct experiments in an actual 
museum to further confirm the effectiveness of our robot system 
 
Acknowledgement This work was supported in part by the Ministry of Education 
Culture Sports Science and Technology under the GrantinAid for Scientific Re
search KAKENHI 2130031623252001 and JST CREST 
References 
1 Nieuwenhuisen M Gaspers J Tischler O Behnke S Intuitive Multimodal Interaction 
and Predictable Behavior for The Museum Tour Guide Robot Robotinho In Proceedings 
of the 10th IEEERAS International Conference on Humanoid Robots Humanoids pp 
653–658 2010 
2 Faber F Bennewitz M Eppner C Görög A Gonsior C Joho D Schreiber M 
Behnke S The Humanoid Museum Tour Guide Robotinho In Proceedings of the IEEE 
International Symposium on Robot and Human Interactive Communication ROMAN 
pp 891–896 2009 
3 Yamazaki A Yamazaki K Kuno Y Burdelski M Kawashima M Kuzuoka H Pre
cision Timing in Humanrobot Interaction Coordination of Head Movement and Utter
ance In Proceedings of the CHI 2008 pp 131–140 ACM Press 2008 
4 Kendon A Conducting Interaction–Patterns of Behavior in Focused Encounters Cam
bridge University Press 1990 
5 Goodwin C Restarts Pauses and The Achievement of a State of Mutual Gaze at Turn 
Beginning Sociological Inquiry 5034 272–302 1980 
6 Hüttenrauch H Eklundh KS Green A Topp EA Investigating spatial relationships 
in humanrobot interactions In IEEERSJ International Conference on Intelligent Robots 
and Systems IROS 2006 pp 5052–5059 2006 
7 Hall ET Proxemics Current Anthopology 9 83–108 1968 
8 Tasaki T Komatani K Ogata T Okuno H Spatially Mapping of Friendliness for 
HumanRobot Interaction In Proceedings of the IEEERSJ International Conference on 
Intelligent Robots and Systems Edmonton pp 521–526 August 2005 
9 Kuzuoka H Suzuki Y Yamashita J Yamazaki K Reconfiguring Spatial Formation 
Arrangement by Robot Body Orientation In ACMIEEE International Conference on 
HumanRobot Interaction HRI 2010 pp 285–292 2010 
10 Yamaoka F Kanda T Ishiguro H Hagita N How Close A Model of Proximity Con
trol for Informationpresenting Robots In HumanRobot Interaction HRI 2008 pp 137–
144 2008 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 433–441 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A Novel Image Matting Approach Based  
on Naive Bayes Classifier 
Zhanpeng Zhang12 Qingsong Zhu1 and Yaoqin Xie1 
1 Shenzhen Institutes of Advanced Technology Chinese Academy of Sciences  
Shenzhen 518055 China  
2 Sun YatSen University Guangzhou 510006 China 
zpzhangqszhuyqxiesiataccn 
Abstract Image matting is a fundamental technique used in many image and 
video applications It aims to softly extract foreground from the image 
accurately In this paper we propose a new matting approach based on naive 
Bayes classifier to 
produce 
matting 
results 
with 
higher 
accuracy 
Spatiallyvarying probabilistic models for the classifier are established 
Confidence values are defined to make better use of the classification results The 
results are then refined and combined with closedform matting to obtain the 
final alpha matte We conduct qualitative and quantitative evaluations Results 
show that our method outperforms many recent algorithms 
Keywords image matting naive bayes classifier foreground extraction image 
segmentation 
1 
Introduction 
Image matting refers to the problem of extracting the opacity mask typically called 
alpha matte of the foreground as well as the foreground and background color from 
the target image It is one of the fundamental techniques used in many image and video 
editing tasks Specifically for a pixel i with color Ii in the image it can be model as a 
liner combination of the foreground color Fi and background color Bi as Eq1 
1

i
i
i
i
i
I
F
B
α
α
=
+
−
 
1
Eq1 is under constrained since αi Fi and Bi on the righthand side are all unknown 
making it a significant challenge for computer vision 
Many recent approaches require additional information from user input to build 
more constrains to solve the illposed problem Trimaps 1 and scribbles 2 are the 
two most common methods labeling some pixels which are definite foreground or 
background like Fig1b with corresponding alpha value to be 1 or 0 respectively 
Existing matting algorithms can be classified into samplingbased or 
propagationbased Samplingbased algorithms explicitly estimated the triplet α F B 
for every unlabeled pixel by analyzing nearby labeled pixels These algorithms usually 
fit a parametric mode to the color distributions of the samples like Bayesian Matting 
1 which models the colors of the samples by oriented Gaussian distributions 
However the model assumptions may fail in some scenes Recently many 
434 
Z Zhang Q Zhu and Y Xie 
 
nonparametric algorithms are employed estimating the triplet directly with the sample 
colors under the liner model defined by Eq1 For example the sampling algorithms in 
robust matting 3 and many later approaches 45 assign a confidence value to a pair 
of foreground and background sample and choose the samples with high confidence 
The confidence values are generally measured by how well the sample explains the 
unlabeled pixel in the linear model Eq1 These algorithms perform well when the 
true foreground and background colors are in the sample set 
For propagationbased algorithms some affinities assumptions are made in order to 
derivate a constrained objective function Poisson matting 6 deduces that the alpha 
matte gradient is proportional to the image gradient under the smoothness assumption 
A closed form solution proposed by Levin obtains a quadratic cost function in α 
analytically under the color line model 2 This approach is called as closedform 
matting and has been applied in many other approaches drawing extensive studies 7 
However due to the color line assumption in closedform matting it may fail in 
regions with long and thin structures or holes Many other propagationbased 
approaches may also fail in these situations This is because only neighboring pixels are 
used in the modeling process lacking information from further regions Some 
approaches 34 use color information from nearby samples to fill the gap However 
gathering appropriate samples is still a challenging problem and additional 
assumptions may be introduced causing other types of artifacts 
c
a
b
c
d
 
Fig 1 a Original image b Trimap c Closedform matting 2 result d Our result Notice 
the two red ellipses The detail is missed in closedform matting while not in the result of our 
method 
In this paper we employ naive Bayes classifier to identify the foreground and 
background pixels The classification results are then refined and combined with 
closedform matting As our approach can enhance the discrimination between the 
foreground and the background results show that our approach outperforms many 
 
A Novel Image Matting Approach Based on Naive Bayes Classifier 
435 
 
recent algorithms and produces better alpha mattes for the images in which the 
closedform matting usually fails like Fig1 
2 
ClosedForm Matting 
Levin et al proposed a closedform solution for image matting in 2 The key 
assumption in closedform matting is color line model It assumes that in a small 
window the foreground color and background color of each pixel i can be formulated 
as linear mixtures of two constant colors respectively In other words the foreground 
or background colors for all pixels in a small window lie on a single line in the RGB 
color space 
Based on the color line model the alpha value for pixel i in a small window w can be 
expressed as a liner transform of the pixel color      
w
i
b
a I
c
i
c
c
i
∀ ∈
+
= 

α
 
2
where c denotes the color channel for RGB color space a and b are constant in w 
To derivate an alpha matte obeying the color line model or Eq2 the algorithm 
aims to find the optimal a b and α which minimize the cost function 
2
2
   


j
c
c
c
i
j
i
j
j
i w
c
c
j I
J
a b
a I
b
a
α
α
ε
∈
∈
=
−
−
+



（
） 
3
Here wj is a small window centered at pixel j ε is a regularization parameter while 
smoothing the alpha matte 
A quadratic function of α can be obtained by minimizing the cost function Also 
parameters a b can be eliminated in the deducing process The quadratic function is as 
follows 
α
α
α
L
a b
J
T
    =
 
4
pecifically α is an N×1 vector where N is the number of unlabeled pixels L is 
typically called matting Laplacian matrix as one of the most significant contributions 
of closedform matting drawing many further studies and applications Formally L is 
an N×N matrix with i jth element as 
1
3
k|  
1

1 

 

|
|
|
|
k
ij
i
k
k
j
k
i j
w
k
k
I
I
I
w
w
ε
δ
μ
μ
−
∈
−
+
−
 +
−

 
5
where δij is the Kronecker delta |wk| is the number of pixels in this window ∑k is a 3×3 
covariance matrix μk is a 3×1 mean vector of the colors in a window wk and I3 is the 
3×3 identity matrix 
Combined with constrains provided by the user trimaps or scribbles the objective 
function can be defined as 




 
β
α
β
λ α
α
α
α
−
−
+
=
D
L
J
T
T
 
6
Here D is an N×N diagonal matrix whose elements are 1 for labeled pixels and 0 
otherwise β denotes the alpha value for the labeled pixels in the trimap 1 for 
436 
Z Zhang Q Zhu and Y Xie 
 
foreground and 0 for background λ is weighted parameter with a relatively large 
number like 100 In this paper we mainly talk about the alpha value since the 
foreground F and background B can be obtained easier with the estimated α like 2 
Closedform matting works well when the local region fits the color line model To 
ensure this assumption holds and reduce computation cost the windows size in Eq5 is 
usually small 3×3 in Levin’s implementation That means the “propagation step” is 
relatively small implicitly Oversmoothing may happen in regions with thin structures 
or small gaps as Fig1c Details will be lost in these situations He et al 7 improves it 
by introducing adaptively window sizes in different regions of the image It is shown 
that larger window size can improve the matting result since large window may cover 
disconnected regions of the foregroundbackground However with larger window 
size it is more like to break the color line model Therefore it is still hard to decide the 
appropriate windows size  
3 
Matting with Naive Bayes Classifier 
Different from the previous approaches we employ naive Bayes classifier to decide 
whether a pixel belong to the foreground or background The result is then “softened” 
by a sigmoid function Confidence values are also computed for every alpha value The 
results and the confidence values are then combined with closedform matting 
providing more accurate alpha matte 
31 
Naive Bayes Classifier 
Naive Bayes classifier 8 is a simple probabilistic classifier based on Bayes theorem 
assuming that features in a class are independent with each other For class variable C 
with n features in the model using Bayes theorem the probability that an instance is in 
class c is 




| 


  



|

2
1
2
1
2
1
n
n
n
k
p k k
c
k
p c p k k
k
p c k k
=
 
7
where ki is the instance’s value for feature i With the assumption that every feature is 
independent classification can be done by selecting the highest posterior of the 
classification variable with the following function 
∏
=
=
=
=
n
i
i
i
c
c
C
k
p K
c
C
p
1

|



argmax
 
8
32 
Classification Process 
Features Selection Color attribute is the most straight forward feature reflecting a 
pixels characteristics To provide more information of the region texture colors of the 
4neighbor pixels are also selected That means for a pixel i its feature vector k = g1 
g2 g3 g4 g5 where gi is the color vector of a pixel In our implementation we use 
CIELAB color space with every channel ranges from 0 to 255 so a 15dimensional 
vector is selected for a pixel  
 
A Novel Image Matting Approach Based on Naive Bayes Classifier 
437 
 
Classifier Parameters Estimation We do not apply a uniform classifier probabilistic 
model for all the unlabeled pixels in the image Instead the probabilistic model is 
spatiallyvarying Specifically for an unlabeled pixel i we collect other unlabeled 
pixels with spatial distance less than r 30 in our implementation Pixels from this 
unlabeled region share a same probabilistic model Pixels labeled by the users are 
selected as samples to estimate the parameters We expect to obtain both local 
foreground and background characteristics so samples are selected according to their 
spatial distance to the current unlabeled region We expand form the border of the 
unlabeled region and collect foreground and background samples until the numbers of 
the foreground and background samples are larger than that of unlabeled region 
respectively To make the features independent to some extent so as to satisfy the naive 
Bayes’ assumption better before parameters estimation we apply PCA 9 to reduce 
the dimension of the features vector for the samples and unlabeled pixels 
Gaussian distribution is employed to model the probability of each feature of the two 
classes foreground and background The mean and variance can be computed with the 
collected samples The class prior pc in Eq7 is calculated by assuming that the 
foreground and background are equiprobable That means pc = 05 for both the two 
classes With the computed parameters unlabeled pixels can be classified as Eq8 
 
a
b
c
d
 
Fig 2 a Original image b Refined result c Result after combined with closedform matting 
d Closedform matting only Parts of the images are zoomed in for clearer distinction 
Results Refinement With the Gaussian distribution and strong independence 
assumptions in the classifier as well as the binary classification result errors are 
unavoidable To evaluate the results from the classifier pixelwise confidence values 
are computed Also the binary results are “softened” with a sigmoid function so as 
obtain values ranging from 0 to 1 indicating the opacity mask 
Instinctively if the unlabeled pixels are more similar to the samples used to estimate 
the classifier more accurate results can be obtained To measure the similarity we 
calculate Ds for every unlabeled pixel Ds is defined as follows 
 
438 
Z Zhang Q Zhu and Y Xie 
 
min
  
Ds
E i j
j
S
=
∈
 
9
where E· denotes the Euclidean distance in the original feature space S is the 
collected sample set The confidence value is 
 
exp
s  
G i
ρD i
=
−
 
10
And the binary results are “softened” with a sigmoid function as 
1
 
1 exp
  
s  
R i
C i
D i
ϑ
= +
−
 
11
Ci is the result of the classifier for pixel i 1 and 1 for background and foreground 
respectively ρ and θ in the above equations are normalize parameters In our 
implementation we set ρ = 05 and θ = 20 The result R is like Fig2b In practice the 
target foreground in matting is usually connected not several separated components 
We select connected components with area less than 1 of the area of the largest one 
and set the confidence values of these components to be zero 
33 
Combined with ClosedForm Matting 
We use the refined result R in Eq11 and its corresponding confidence to define the data 
term as a quadratic function with the minimum at R The data term is then combined 
with Eq6 The final objective function is 



R
E
R
D
L
J
T
T
T
−
−
+
−
−
+
=
α
γ α
β
α
β
λ α
α
α
α






 
12
The first and second term is as defined in Eq6 For the third term R is treated as a 
vector E is an N×N diagonal matrix whose elements are the confidence values for 
corresponding unlabeled pixels and zeros otherwise γ is a weighted parameter 01 in 
our implementation The combined result is like Fig 2c 
4 
Experiments 
We conduct qualitative and quantitative comparisons of our method with other recent 
related matting algorithms including closedform matting 2 learning based matting 
10 large kernel matting 7 and robust matting 3 Like closedform matting 
learning based matting also used a small window but it employs learning techniques 
instead of the color line assumption Large kernel matting improves the efficiency of 
closedform matting by using larger window size Robust matting combines the 
samplingbased and propagationbased algorithms similar to our approach to some 
extent 
Qualitative Comparisons We compare the result of our method and closedform 
matting visually in Fig 3a The images and ground truth are provided by 11 We can 
see that closedform matting may fail in the regions with gaps These regions are often  
 
 
A Novel Image Matting Approach Based on Naive Bayes Classifier 
439 
 
0
10
20
30
40
T1
T2
T3
T4
SAD error
test  images
Our method
Closedfrom
T1
T2
T3
T4
a
b
Image cuts            Closedform             Our method          Ground truth
 
Fig 3 a Qualitative comparison between closedform matting and our method with 4 test 
images form 11 Only parts of the images are shown for clearer distinction The red arrows 
show the regions where closedform matting fails while our method provides better results b 
Quantitative comparison between our method and closedform matting according to the SAD 
sum of absolute difference error 
recognized as definitely foreground and the details are missed With the classification 
process and samples from further regions our method can get background samples 
with which to indentify the background pixels and avoid this situation to some extent 
However compared to the ground truth our method still has some artifacts This is 
mainly because of the error in the classification and results refinement process The 
sigmoid function simply based on the Euclidean distance in the feature space is not 
always effective 
Quantitative Comparisons Fig3b shows the quantitative comparison between our 
method and closedform matting We can see that our method can provide results with 
less SAD sum of absolute difference error To conduct more comprehensive 
evaluation we also use the matting benchmark of 11 with 8 test images and 3 
different trimaps with different sparsity level for each of them The average SAD 
errors of each method for the 3 different types of trimaps are presented in Fig4 The 
comparison shows that our method is performing the best Our method combines 
nearby samples and the smooth assumption for local region providing better results 
Compared to robust matting our method does not need to find the true foreground and 
background color So the accuracy can be higher in regions with color ambiguity 
Memory and Computation Cost The memory cost in our classification process is 
relative small since we just establish spatiallyvarying probabilistic model That means 
the sample size used for parameters estimation is not large less than 3000 pixels for 
foreground and background respectively We implement our algorithm in Matlab and 
run it on a 30 GHz CPU The classification process typically takes 20 seconds for an 
800 × 600 image varying with the size N of the unlabeled region in the trimap The 
running time can be further reduced with parallel implementation like GPU since  
 
440 
Z Zhang Q Zhu and Y Xie 
 
 
Fig 4 Quantitative comparisons on average SAD error of alpha value in trimaps with different 
sparsity level small large user For the details of the sparsity level categories refer to 11 
classification processes for different regions of pixels are independent However the 
matting Laplacian matrix needs large size of memories with the size of N×N as 
described in Section 2 And the time for computing the matting Laplacian matrix and 
solving Eq12 in Section 32 is about 20 seconds for an 800 × 600 image depending  
on N 
 
Limitation Because the classification process is based on the color information our 
method may fail in complex scenes or regions with foreground and background colors 
overlapping Also the naive Bayes classifier is based on a strong assumption and we 
used Gaussian distribution to model the probability In some situations these 
assumptions may not hold 
5 
Conclusion 
In this paper a new matting approach based on naive Bayes classifier is proposed and 
evaluated The binary classification results are “softened” with a sigmoid function and 
confidence values are computed to make better use of the results The results are then 
combined with closeform matting to obtain the final alpha matte Quantitative and 
qualitative comparisons between our method and other recent algorithms show that our 
method produce better results However color ambiguity or complex scenes are still 
challenging for our method Future work may concentrate on providing better 
classification results and weaker model assumptions 
Acknowledgments This study has been financed partially by the Projects of National 
Natural Science Foundation of China Grant No 50635030 60932001 61072031 
61002040 the National Basic Research 973 Program of China Subgrant 6 of Grant 
No 2010CB732606 and the Knowledge Innovation Program of the Chinese Academy 
of Sciences and was also supported by the China Scholarship Council CSC and 
China Postdoctoral Project 
 
A Novel Image Matting Approach Based on Naive Bayes Classifier 
441 
 
Reference 
1 Chuang YY Curless B Salesin DH et al A Bayesian Approach to Digital Matting In 
Proceedings of IEEE Conference on Computer Vision and Pattern Recognition CVPR pp 
264–271 2001 
2 Anat L Dani L Yair W A Closed Form Solution to Natural Image Matting In 
Proceedings of IEEE Conference on Computer Vision and Pattern Recognition CVPR pp 
61–68 2006 
3 Wang J Cohen MF Optimized Color Sampling for Robust Matting In Proceedings of 
IEEE Conference on Computer Vision and Pattern Recognition CVPR pp 17–22 2007 
4 Rhemann C Rother C Gelautz M Improving Color Modeling for Alpha Matting In 
Proceedings of British Machine Vision Conference pp 1155–1164 2008 
5 Gastal ESL Oliveira MM Shared Sampling for Realtime Alpha Matting Computer 
Graphics Forum 29 575–584 2010 
6 Sun J Jia J Tang CK Shum HY Poisson matting ACM Transactions on 
Graphics 233 315–321 2004 
7 He KM Sun J Tang XO Fast Matting Using Large Kernel Matting Laplacian 
Matrices In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition 
CVPR pp 2165–2172 2010 
8 Pedro D Michael P On the Optimality of the Simple Bayesian Classifier under Zeroone 
Loss Machine Learning 29 103–130 1997 
9 Jolliffe IT Principal Component Analysis Springer 1986 
10 Zheng YJ Kambhamettu C Learning Based Digital Matting In Proceedings of IEEE 
International Conference on Computer Vision ICCV pp 889–896 2009 
11 Rhemann C Rother C Wang J et al A Perceptually Motivated Online Benchmark for 
Image Matting In Proceedings of IEEE Conference on Computer Vision and Pattern 
Recognition CVPR pp 1826–1833 2009 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 442–450 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Detecting Insulators in the Image of Overhead 
Transmission Lines  
Jingjing Zhao Xingtong Liu Jixiang Sun and Lin Lei 
School of Electronic Science and Engineering National University of Defense Technology 
Changsha Hunan PR China 410073 
zhaojingjing63gmailcom 
Abstract Detecting and localizing the insulators automatically are very impor
tant to intelligent inspection which are the prerequisites for fault diagnose A 
novel method for insulators detection in the image of overhead transmission 
lines based on lattice detection is presented in this paper Firstly lowlevel vis
ual features of images are analyzed feature points are generated and grouped by 
their appearance similarities through mean shift clustering then a insulator lat
tice model consistent with the geometric relationship between candidate point 
clusters is proposed by voting mechanism subsequently performing lattice 
finding using an MRF model combined with the spatial context information to 
localize multiple insulators jointly Finally extracting the minimum bounding 
rectangle of the target image Since the location of each insulator is constrained 
by its neighbors each of them provides knowledge about the others the MRF 
model is a natural choice for inferring insulators locations while enforcing spa
tial lattice constraints and image likelihood constraints The experimental re
sults indicate that the method can effectively detect the deformed insulators of 
different kinds under complex background 
Keywords insulator detection overhead transmission lines deformed lattice 
detection texture features MRF intelligent inspection 
1 
Introduction 
As the key component of Smart Grid it is imperative to have an effective plan to main
tenance the transmission system With the rapid spread of the transmission network the 
traditional method of manual inspection is difficult to meet the growing requirements of 
the power grid which is rapid developed By contrast the technology of helicopterUAV 
patrol has already become a trend for its efficiency reliability and lowcast Even though 
we can profit from the advantages of helicopterUAV patrol some key problems such as 
recognition of potential encroachments and identifying transmission lines that need re
pair were also overly dependent on manual identification This situation led to the Low 
efficiency of inspection In order to overcome various defects in the transmission line 
inspection it is essential that an intelligent inspection system should be developed 
Therefore the concept of “inspection robot” is born 
The intelligent inspection system takes images of overhead transmission lines then 
identifies and locates the transmission lines which are needed repair rapidly by  
 
Detecting Insulators in the Image of Overhead Transmission Lines 
443 
 
analyzing these images automatically Insulator is an important component of over
head transmission lines which is used to prevent the live parts of transmission lines to 
form ground channel The intelligent diagnosis of the insulator can be achieved 
through the full use of computer vision technology which refer to insulator detection 
image segmentation and fault identification from the aerial images combined with the 
expert system to generate strategy report As the first step in intelligent diagnosis 
detecting insulators in image accurately and automatically is of great significance it is 
the basis for postprocessing Most of insulators detection methods have been pro
posed based on the infrared close range images photographed in laboratory 1 2 
However the backgrounds of infrared images are relatively simple which are differ
ent from the optical images photographed in real world For optical images the insu
lators are often “deformed” due to variations in viewing angle lighting or partial 
occlusion which make the detection of insulators to be difficult therefore there is 
rare research involved about that In recent years some scholars have suggested that 
the color information can be used to extract and recognize insulators 3 4 but such 
method can only be applied to the detection of glass insulators lack of versatility for 
detecting other types of insulators 
This paper studies on the problem of insulators detection in optical images Essen
tially it belongs to the field of “target detection” in computer vision Target detection 
which is widely used for the analysis of complex scenes in intelligent system is a 
class of image analysis method that can automatically detect the target appears or not 
in the image and video based on the geometry and statistical properties of the target 
When we take a photograph of the same target with different angle or distance we 
may get various understanding with different appearance but the target itself does not 
change changing only happened in the appearance The human perceptual system has 
the ability of extracting the invariant features of various appearances so we can rec
ognize almost everything easily Extraction of invariant information is one of the 
characteristics of human perception but for computers they should select and extract 
the invariant features to recognize the patterns and then detect and identify targets 
Therefore how to find the invariant features of insulators image is the focus of the 
study in this paper 
As we discussed above the images taken by the intelligent inspection system al
ways have complex background we can’t use the background modeling method to 
extract the insulators target Since the targets are the insulators string the appearances 
of the photometrical and geometrical elements remain highly correlated It can be 
seen as a kind of near regular texture pattern 5 composed of deformed versions of 
one or more basic texture elements thus we consider using characteristics of texture 
structure to detect the insulators together Recently some scholars have found that 
nearregular textures are not merely random collections of isolated texture elements 
but exhibit specific geometric topological and statistical regularities and relations 
They proposed “deformed lattice detection” theory for automatic detection of de
formed texture patterns in realworld images 611 
By studying on the mechanism of “deformed lattice detection” theory this paper 
presents a novel method to detect the insulators in the image of overhead transmission 
lines from the perspective of the characteristics of texture structure First lowlevel 
444 
J Zhao et al 
 
visual invariant features of images are analyzed in order to generate highlevel insula
tor lattice model Second lattice finding is performed by using an MRF model and 
multiple insulators are searched for jointly Lastly the insulator targets are located by 
extracting the minimum bounding rectangle The method is robust for it combines 
spatial context information with image content 
2 
Theory of “Deformed Lattice Detection” 
“Deformed lattice detection” is a kind of structurebased texture detection method 
which inform us that near regular texture can be described by a pattern element and 
two smallest linearly independent generating vectors t1 and t2 5 and the translation 
subgroup of all near regular texture can be characterized by a degree4 graphical 
model where each element is a node that has four neighbors representing its own 
copies offset by plus or minus t1 and t2 Although the “copies” may be not faithful 
the appearances of the deformed elements remain highly correlated which can be 
seen as deformed lattice pattern For deformed lattice pattern the geometric offsets of 
neighbors in the lattice is replaced by original terms allowing local variations of the 
t1 t2 lattice basis vectors The soft constraints on the geometry and appearance of 
deformed lattice pattern are represented as pairwise compatibility function and joint 
compatibility function in a degree4 Markov Random Field MRF model 11 For an 
image I that contains a deformed version of a true periodic pattern the “deformed 
lattice detection” theory first estimates the ideal pattern element which can be de
scribed by an appearance template T0 and the lattice generating vectors t1 t2 then 
infers accurate image locations 
0
1
n
x x 
x
（
…
）of all elements forming the repeated pat
tern in image I 
The underlying topological lattice structure of near regular texture under a set of 
photometric and geometric deformation fields was first proposed and used by Liu et 
al for texture analysis 6 7 Subsequently Hays et al developed the first deformed 
lattice detection algorithm for real images without presegmentation 8 Then Lin and 
Liu developed the first deformed lattice tracking algorithm for dynamic near regular 
texture 9 10 Recently Park et al have proposed to formulate the detection of the 
underlying deformed lattice in an unsegmented image as a spatial multitarget track
ing problem 11 using Meanshift Belief Propagation MSBP method 12 Our 
work is partially inspired by Park et al and we detect insulators based on the frame
work of their theory  
3 
MRF Model of Insulators 
The MRF model can be represented as an undirected graph G = NE where each 
node in N represents a random variable in set X and each edge in E represents a statis
tical dependency between random variables in X In the insulators detection the ran
dom variables are the image locations of insulators and edges in the MRF model 
represent two kinds of dependencies spatial constraints between neighboring insula
tors and appearance consistency constraints between each candidate image patch and 
the reference insulator pattern element 
 
Detecting Insulators in the Image of Overhead Transmission Lines 
445 
 
The meaning of introducing MRF into the work of insulators detection lies in as a 
kind of manmade texture the insulator is not independent of each other the location 
of each insulator is constrained by its neighbors so finding some of them provides 
knowledge about where the others may be The result would be more effective if mul
tiple insulators are searched for jointly rather than one at a time More specifically as 
we know the distance between neighboring insulators is more or less constant which 
can be seen as spatial constraints in the MRF thus we can define a pairwise compati
bility function
i j
i
j
ϕ  x x 
 where the offset vector 
i
j
x
− x
should be “similar” to vector 
t1 or t2 Furthermore another piece of information that can help localize each insulator 
is that the image patch centered at xi should look like the same as the ideal insulator 
template T0 the difference in appearance should be small This constraint is added to 
the MRF as and a joint compatibility function
i
i
i
φ  x y 
 where the 
iy  represents as an 
image likelihood measurement The figure 1 shows the MRF model of insulators 
 
Fig 1 This is the MRF model of insulators The latent variables x represent locations of insula
tor to be inferred while the image measurements y quantifying appearance similarity between 
each candidate image patch and the ideal pattern element and the spatial neighborhood con
straints provided by the lattice basis vectors t1 t2 
4 
Detecting Insulators in the Image of Overhead Transmission 
Lines 
Through the above analysis we can conclude that if we want to detect the insulators 
automatically in the image of overhead transmission lines there are some problems 
must be focused on 
1 How to analyze the lowlevel visual feature to find the reference pattern element T0  
of insulator and the lattice basis vector pair  t1 t2 of highlevel insulators lattice 
model 
2 How to represent the insulators lattice model by MRF and establish the joint com
patibility function and the pairwise compatibility function to detect the multiple in
sulators jointly 
3 How to localize the target area  
 
446 
J Zhao et al 
 
In this section we present a method of automatically detecting insulators in optical 
images based on the “deformed lattice detection” theory 
41 
Insulators Lattice Model Proposal 
The effectiveness of associative detecting result depends on the insulator reference 
pattern element T0 and the t1 t2 basis vectors between neighbor insulators which 
form a lattice that represent insulator Therefore how to construct the lattice model of 
insulators is our primary task We view this step as a discovery process starting with 
lowlevel visual feature and ending with a highlevel insulator lattice model proposal 
We achieve this lattice model proposal phase using the algorithm was used in 11 
We first study the content of insulators image extracting the feature points which 
reflected the repeated structure of insulator texture Corners as lowlevel visual fea
ture for image understanding and analysis of graphics can effectively retain the im
portant information while saving computation KLT corner detection algorithm consi
dering the change of image brightness can find the features with high contrast of 
image To reveal more repeating points locally we apply KLT in a 50×50 pixel block 
and detect feature points more than 30 in every block 
To find the repeating features that can represent insulator we cluster corners by the 
content of image patch through mean shift clustering 13 We can adopt a voting 
mechanism to examine a cluster of feature points with similar appearance and propose 
the insulators lattice mode Introduction of voting mechanism in the insulators lattice 
model is equivalent to adding highlevel information from a global perspective to 
determine a t1 t2 vector pair and reference pattern element T0 Specifically we ran
domly sample three points abc and compute the affine transformation that maps 
them from image space into the integer lattice basis 000110 then transform 
all the other points from image space into their equivalent lattice positions via the 
same affine transform and count those points whose lattice space coordinates are 
around an integer position which can be seen as supporting votes Random selection 
of sample three points abc multiple times the transform with the largest number 
of votes  is chosen to generate insulators lattice model The corresponding t1 t2 
vector pair is regard as insulators lattice generating vectors and the reference pattern 
element T0 is extracted centered at the origin of the proposed t1 t2 vector pair with 
size min
1
2
t  t by min
1
2
t  t  
42 
Associated Detection 
Since the location of each insulator is constrained by its neighbors each of them pro
vides knowledge about the others the MRF model is a natural choice for inferring 
insulator locations while enforcing spatial lattice constraints and image likelihood 
constraints As the spatial relationship between adjacent insulators is related by the 
offset vector t1 or t2 which can be represent as a pairwise compatibility func
tion
i j
i
j
ϕ  x x 
 For each insulator its appearance is similar as the reference pattern 
element which is the foundation of joint compatibility function
i
i
i
φ  x y 
  When the 
texture structure of insulators is mapped to the MRF we can use MSBP to solve the 
problem 11 12 
 
Detecting Insulators in the Image of Overhead Transmission Lines 
447 
 
The meaning of joint compatibility function
i
i
i
φ  x y 
 is to measure the similarity of 
the content between candidate image patch and the reference pattern element As 
similarity measurement the image likelihood map should not only be effective to 
distinguish the difference between the different targets but also be tolerant of the 
noise in a class The image likelihood map is taken as a prior density function on the 
image location of texture elements and the joint compatibility function in the lattice 
MRF is given by 
1
 i j 
 i j 
 i j 
 x
y

exp

y

φ
α
=
−
−
 
1
 i j 
 i j 
y
NCCT I x

=
 
2
Where 
x i j 
is the location of node i j at the ith row and jth column in the insulators 
lattice space 
I x i j 
 is an image patch centered at the location of node i j and T0 is 
the appearance template of reference pattern element  
The pairwise compatibility function 
i j
i
j
ϕ  x x 
 specifies the spatial constraint be
tween neighbor insulators which can be defined by the geometric characteristics of 
t1 t2 vector pairs in the insulators lattice The “deformed lattice detection” theory 
use the normalized error below to measure the spatial consistency of two vector pairs 
1
2
i
i
t
t
（ ， ）and
1
2
j
j
t
t
（ ， ） 
1
1
2
2
2
2
1
2
1
2
1
2
2
2
=



i
j
i
j
i
i
j
j
i
i
t
t
t
t
E t
t
t
t
max
t
t
−
−
（ ， ， ， ）
 
3
where 
2
is 
L2 
vector 
norm 
The 
pairwise 
compatibility 
function 
1
 i j 
 i j

 x
x

ϕ
±
and
1
 i j 
 i
 j 
 x
x

ϕ
±
can be defined based on the equation 3  
2
1
1
 i j 
 i j

 i j 
 i j

 x
x

exp
h x
x
 
ϕ
β
±
±
=
−
×
 
4
0
0
0
0
0
1
1
1
1
1
 
m
m
m
 i j 
 i j

 i j 
 i j

 i j 
 i
 j 
 i j 
 i j

 i j 
 i
 j 
h x
x

 x
x
x
x
 x
x
x
x

±
±
+
±
+
=
   
 
5
2
1
1
 i j 
 i
 j 
 i j 
 i
 j 
 x
x

exp
v x
x
 
ϕ
β
±
±
=
−
×
 
6
0
0
0
0
0
1
1
1
1
1
  
m
m
m
 i j 
 i
 j 
 i j 
 i
 j 
 i j 
 i j

 i j 
 i
 j 
 i j 
 i j

v x
x

 x
x
x
x
x
x
x
x

±
±
+
±
+
=
   
，
 
7
Where 
m
x  i j 
 is the location of node i，j in the mth iteration The equation 5 and 
the equation 7 are used to measure similarity between the assumed insulator element 
vector pair 
1
2
m
m
t
t
（ ， ） and the reference vector pair t1 t2 The equation 4 measures 
the spatial similarity between the left insulator and the right insulator while the  
equation 6 measures the spatial similarity between the up insulator and the down 
insulator With de definition above we can locate the insulators with MSBP The 
parameters are set to 
5
α
= β
=
by experience 
448 
J Zhao et al 
 
 
a The input image 
b The result of associated detection 
 
c The location of the insulators 
Fig 2 The result of glass insulators detection 
 
a The input image 
b The result of associated detection 
 
c The location of the insulators 
Fig 3 The result of porcelain insulators detection 
 
Detecting Insulators in the Image of Overhead Transmission Lines 
449 
 
43 
Location 
The purpose of insulators detection is to detect the insulator appears or not in the im
age automatically and localize the targets for the future work of image segmentation 
Due to the influence of the image quality there are various uncertain disturbance 
factors in the associated detection stage which leads to the result of missing detec
tion In order to enhance the robustness of the algorithm we analyze the geometric 
characteristics of the insulators region and extract the minimum bounding rectangle as 
the final location result 
5 
Experimental Results 
We have tested our proposed algorithm on real images of different kinds of insulators 
there are parts of experimental results below The detecting targets in figure 2 are 
suspension glass insulator strings The background of the insulators image is waters 
whose color is similar with that of glass insulators what is more the luminance of 
each insulator image patch is not exactly the same Therefore only from the aspect of 
color information using the method of template matching is hard to overcome the 
influence of the interference We combined the texture structure information with the 
color information to detect multiple insulators the experimental result shows the me
thod can overcome the impact of background and locate the insulators accurately  
The detecting targets in figure 3 are suspension porcelain insulator strings There 
are not only green plants but also tower in the background On one hand the color of 
tower is similar with porcelain insulators on the other hand the sizes of insulators are 
not completely the same due to the variations in viewing angle there is a certain 
degree of deformation in the image The method conquers the disadvantages and 
detects the insulator targets 
6 
Discussion 
Variations in viewing angle lighting and partial occlusion are the common problem 
in the imaging of overhead transmission lines which make the traditional feature 
matching methods based on the appearance similarity to be inefficient to detect insu
lators Therefore additional constraints need to be introduced to improve the detect
ing results Since the insulators are not merely random collections of isolated  
elements but exhibit specific geometric topological and statistical regularities and 
relations these spatial context information can be used to detect insulators jointly 
when the local observation information are insufficient to detect the targets We ana
lyze the texture structure of insulator strings and detect the targets based on the 
theory of “deformed lattice detection” The experimental results show that the method 
can overcome the disadvantage of imaging conditions and detect different kinds of 
insulators under a complex background of overhead transmission lines automatically 
How to diagnose the fault area of the insulators is our feature work 
Acknowledgements This work was supported by the National Natural Science 
Foundation of China NO61105031 The authors thank the Electric Power Authority 
of Shaoxing for providing the data and Minwoo Park for the material of “deformed 
lattice detection” Thanks are due to Jian Zhao and Shujin Sun for critical discussions 
450 
J Zhao et al 
 
Reference  
1 He H Yao J Jiang Z Wang X Li W Infrared Thermal Image Detecting of High 
Voltage Insulator Contamination Grades Based on Support Vector Machine Automation 
of Electric Power Systems 2924 70–74 2005 
2 Li Z Yao J Yang Y Stationary Waveletdomain Local Adaptive Denoising Method 
for Insulator Infrared Thermal Image High Voltage Engineering 354 833–837 2009 
3 Lin J Han J Chen F Defects Detection of Glass Insulator Based on Color Image 
Power System Technology 351 127–133 2011 
4 Huang X Zhang Z A Method to Extract Insulator Image from Aerial Image of Helicop
ter Patrol Power System Technology 341 194–197 2010 
5 Liu Y Lin WC Hays J Nearregular Texture Analysis and Manipulation ACM 
Transactions on Graphics 233 368–376 2004 
6 Liu Y Collins RT Tsin Y A Computational Model for Periodic Pattern Perception 
Based on Frieze and Wallpaper Groups IEEE Transactions on Pattern Analysis and Ma
chine Intelligence 263 354–371 2004 
7 Liu Y Tsin Y Lin WC The Promise and Perils of Nearregular Texture International 
Journal of Computer Vision 6212 145–159 2005 
8 Hays J Leordeanu M Efros AA Liu Y Discovering Texture Regularity as a Higher
Order Correspondence Problem In Leonardis A Bischof H Pinz A eds ECCV 
2006 Part II LNCS vol 3952 pp 522–535 Springer Heidelberg 2006 
9 Lin WC Liu Y Tracking Dynamic NearRegular Texture Under Occlusion and Rapid 
Movements In Leonardis A Bischof H Pinz A eds ECCV 2006 Part II LNCS 
vol 3952 pp 44–55 Springer Heidelberg 2006 
10 Lin WC Liu Y A Latticebased MRF Model for Dynamic Nearregular Texture Track
ing IEEE Transactions on Pattern Analysis and Machine Intelligence 295 777–792 
2007 
11 Park M Brocklehurst K Collins RT Liu Y Deformed Lattice Detection in Real
World Images Using MeanShift Belief Propagation IEEE Transactions on Pattern Analy
sis and Machine Intelligence 3110 1804–1816 2009 
12 Park M Liu Y Collins RT Efficient Mean Shift Belief Propagation for Vision Track
ing In Computer Vision and Pattern Recognition Anchorage Alaska 2008 
13 Comaniciu D Meer P Mean shift A Robust Approach Toward Feature Space Analysis 
IEEE Transactions on Pattern Analysis and Machine Intelligence 245 603–619 2002 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 451–457 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Realizing Geometry Surface Modeling of Complicated 
Geological Object Based on Delaunay Triangulation 
Xiangbin Meng12 Panpan Lv1 Xin Wang2 and Hua Chen1  
1 College of Science China University of Petroleumhuangdao China 
herholiness163com  
2 Geophysical Research Institute of Shengli Oil Field Branch CoDongying China 
mengxb68sinacom 
Abstract The subsurface geological structures are considerably complicated 
which often appear in the form of normal fault reverse fault fold pinchout and 
irregular body etc In order to model geometrically the face structure of the geo
logic horizon and fault some key algorithms including the Delaunay subdivi
sion and limited Delaunay subdivision are applied to examine techniques such 
as curved surface intersection division suture united output and so on while 
the compatibility of the complicated geological structure such as geologic hori
zon and fault were maintained on geometry and topology The analysis propos
es the geometric distribution factors of geological object model for the further 
3D modeling of the complicated geological object 
Keywords Delaunay subdivision Curved Surfaces Intersection Curved Sur
faces division Curved Surfaces suture Curved Surfaces united output 
1 
Preface 
It is an important project to build a complicated seismic model in the field of geologi
cal prospecting The basic work is to build a geometric model of geological object 
This is also one of the frontier subjects in geoscience field 14 In this paper the 
main issue is how to build the geometric model of the block surface by using the 
techniques of curved surfaces intersection5 curved surfaces division curved surfac
es suture and curved surfaces united output2 on the basis of the results of the 3D 
reconstruction of geologic horizon fault etc 
2 
Block Modeling Process Workflow  
In this paper we have put forward an effective block modeling process workflow by 
studying the common methods of block modeling on the basis of practical working 
experience Fig1 
According to the range of data points coordinates we first set up the outline of a 
triangle Then we add the data points to the triangle one by one using BowyerWatson 
algorithm Finally we delete all triangles which are connected to the vertexes of the 
outline of triangle and then we will obtain the result of 2D Delaunay division about  
 
                                                           
  Supported by ”the Fundamental Research Funds for the Central Universities”11CX04059A 
452 
X Meng et al 
 
begin
interactive explanation module
surface reconstruction module
solid modeling module
geometry modeling module
interpolation module
Data
management
module
interactive
3D
visualization
module
begin
begin
interactive explanation module
surface reconstruction module
solid modeling module
geometry modeling module
interpolation module
Data
management
module
Data
management
module
interactive
3D
visualization
module
interactive
3D
visualization
module
 
Fig 1 Block modeling process workflow 
the data points We obtain the intersection between horizons by making curved sur
face intersection in the nets of 2D Delaunay division which is the restrictive condi
tion The limited segments are added into the result of 2D Delaunay division after 
they are standardized After all the limited segments are added into the result we 
should check the consistency of the qualification and the subdivision results in  
geometry and topology If the result is affirmative we will achieve the result of sub
division and qualification After proceeding curved surface division curved surface 
suture curved surface united output etc on the basis of the result of 2D limited divi
sion we will complete the geometric modeling of the complicated geological object 
surface 69 
3 
Design and Implementation of The Modeling Algorithm 
about Complicated Geological Object Geometric Surface   
The methods of modeling about complicated geological object geometric surface 
include curved surfaces intersection curved surfaces division curved surfaces cut 
curved surfaces united output etc At the same time we build the spatial topological 
relations of the complicated geological object in the exploration areas That can pro
vide reference for the next entity modeling 
4 
Design and Implementation of Curved Surfaces Intersection 
Algorithm  
Curved surfaces intersection is a basic operation for determining the topological rela
tions between intersection and curved surface It is the basis of curved surfaces divi
sion curved surfaces trimming and curved surfaces transition Fig2  
As shown in Fig2 First by using the preliminary treatment of bounding box  We 
obtain the conditions that determine if two triangles have common point 
x1minx2maxx2minx1maxy1miny2maxy2miny1maxz1minz2maxz2min
z1max  
 
Realizing Geometry Surface Modeling of Complicated Geological Object 
453 
 
begin
reading information of triangulation
bounding box criterion
overlapping test of triangles
intersection line sort
extend the borders of surfaces
extend the borders of intersection line
end
N
begin
reading information of triangulation
bounding box criterion
overlapping test of triangles
intersection line sort
extend the borders of surfaces
extend the borders of intersection line
end
begin
begin
reading information of triangulation
bounding box criterion
bounding box criterion
overlapping test of triangles
intersection line sort
extend the borders of surfaces
extend the borders of intersection line
end
end
N
 
Fig 2 Curved surfaces intersection process 
Because the sampling data points are not sufficient the geometry intersection line 
is different from the surface analysis results In order to make the analysis of surfaces 
in line with the intersecting lines we need to extend the borders of surfaces In this 
paper we extend the surfaces by using the “boundary extension method with weighted 
factors That is by calculating the direction and the distance of trend with the me
thod of weighted average we can determine the terminal point of extension 5 
With the technology of surface extension with weighted factors we can efficiently 
solve the two kinds of problems in the practical application Fig3 
 
Fig 3 Intersection lines of surfaces 
For the case that the endpoints are not on the boundary of two curved surfaces we 
can use the  boundary extension method with weighted factors” to extend the lines 
with weighted factors with the endpoints reaching the surface boundary at the same 
time 
41 
Design and Implementation of Curved Surfaces Cut Algorithm  
Because the earths crust is very uneven indeed the intersection we get is often tor
tuous and irregular geometrically The triangle spreading division technology makes 
454 
X Meng et al 
 
full use of the adjacent characteristics in the triangulation ensuring that the surfaces 
after division are consistent with original surfaces in geometry and topology Thus it 
can solve the problem of curved surface division better 
The surface triangulation and the corresponding intersection after pretreatment by 
limited Delaunay subdivision should meet either one of the following 1 The inter
section extends to the surface boundary2 The intersections connect end to end 
Then triangle spreading method can be used Fig4 
 
Fig 4 The recursive method of triangle spreading 
For a curved surface as long as we divide the triangulation into two parts along the 
existing segments or rings in the triangulation we will finally divide all the surfaces 
along the intersection 
42 
Design and Implementation of the Curved Surface Suture Sechnique 
When the contour shapes of different levels are different the suture results are prone 
to warping The method of solving this problem is traversing to suture the triangle on 
the border adding equipartition point such as midpoint and reducing the differences 
among the sides of all the triangles on boundary therefore homogenizing the suturing 
boundaries 10 
Curved surface suture depends on the number and the geological structure of the 
suturing surfaces After the pretreatment of the data we divide the surfaces to be su
tured into two types two intersecting closed curved lines and one closed curved lines 
A
B
C
D
C
B
E
A
 
Fig 5 Two connection points and four connection points 
For type 1 the suturing method is to translate two wire frames to make their center 
points coincide with each other Then we calculate the difference of the number of 
boundary points on the two wire frames and determine the connection type Finally 
we can construct a triangle The connection type shows in Fig5 
For type 2 we discuss two cases One case is the wire frame with two intersection 
points and the other is the wire frame with four intersection points 
 
Realizing Geometry Surface Modeling of Complicated Geological Object 
455 
 
When there are wire frames with two intersection points the processing approach 
is to structure the triangular network for the nodes except A and B with the method of 
the first suturing type and to select DC or EF as the pair of initial points Then we can 
get the final result by adding the triangle ACD and BEF into the triangular network 
Fig6  
A
C
E
B
F
D
C
E
F
D
 
Fig 6 Wireframe with 2 vertexes 
When there are wire frames with four intersection points Fig7a the processing 
approach is to select the wire frame MNPQFig7b and then construct the triangu
lar network with MP or NQ as the initial points based on the first suturing type Next 
we construct triangular network of ADMP Fig7c and BCNQ Fig7 d in the 
same way  
 
Fig 7 Wireframe with 4 vertexes 
After completing the above steps we intensify the suturing of triangles Finally 
surface united output technique is to number geological surfaces that belong to a geo
logical object into a triangular mesh file and to form triangular mesh of block geome
tric surface  
5 
Real Data Process 
Above is the elaboration of the theoretical methods Next we apply this algorithm in 
the real seismic data processing The following is the specific modeling process of a 
work area more than 100 square kilometers First model curved surfaces intersection 
Fig3 based on the result of 2D Delaunay triangulation Second model limited 2D 
Delaunay triangulation on the limited condition of first step Third do surface seg
mentation and surface cutting for the result of limited subdivision Fig8 Fourth 
stitch the surface after segmenting the surface Then model surface of a single block 
by uniform output Finally output the entire block model in the whole area and gen
erate geometric surface model based on complicated geologic object of triangulation 
subdivision Fig9 
456 
X Meng et al 
 
 
Fig 8 Surface Segmentation And Surface Cutting 
 
Fig 9 Surface Uniform Output  
6 
Conclusion  
By applying in practice and the results of 3D reconstruction of surface structure which 
include geological horizon etc we realize the modeling of block geometric surface 
The technique of curved surface intersection determines the exact spatial relation 
between fault plane and geological horizon The technique of curved surfaces divides 
geological surface into effective geological horizon and fault plane based on limited 
geological boundary and the condition of intersection Surface suturing technique 
determines the spatial relation among different geological horizons so that the effec
tive surface stitching algorithm can be put forward Finally by using the surface unit
ed output technique form the triangular mesh of block geometric surface as a basis for 
limited 3D Delaunay subdivision 
Of course the geometric surface modeling of complicated geological object is a 
very complicated problem Due to the limitation of our knowledge there may be 
some flaws in the conclusion and methods However we hope this research can pro
vide some references and discussions for improving further the geometric surface 
modeling of complicated geological object 
References 
1 Houlding SW 3D Geoscience Modeling computer Techniques for Geological Characte
rization Springer Heidelberg 1994 
2 Meng XB Wei ZQ Study on Seismic Imaging Block modeling Method In IASP 
2010 03 2010 
 
Realizing Geometry Surface Modeling of Complicated Geological Object 
457 
 
3 Meng XB Wei ZQ Achieving Complex Geological Object Solid Modeling Based on 
TIN and TEN In ICIC 2010 06 2010 
4 Wang XF 3D Geological Object Modeling Quo School of Civil Engineering Hehai 
University 
5 Chu J Wei ZQ SurfaceSurface Intersection Based on Delaunay Triangulation Journal 
of System Simulation 1021 155–158 2009 
6 Meng XH Wang WM The Geological Model and Application of ComputerAided 
Design Principles Geological Publishing House 11 2000 
7 Yang Q Restricted Delaunay Triangulation University of Aeronautics and Astronautics 
Doctoral Dissertation Beijing August 2001 
8 Mallet JL Discrete Smooth Interpolation ACM Transactions on Graphics 82 121–
144 1989 
9 Mallet JL Discrete Smooth Interpolation in Geometric Modeling Computeraided De
signer 178–191 1992 
10 Meng XH Cai Q et al Tetrahedral Mesh Generation for Surface Delaunay Triangula
tion Algorithm Journal of Engineering Graphics 1 2006 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 458–465 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Researching of the Evolution of Discontent  
in Mass Violence Event 
FanLiang Bu1 and YuNing Zhao2 
1 Chinese People’s Public Security University Beijing China 
2 Chinese People’s Public Security University Beijing China 
bufanliangsinacom dycppsu163com 
Abstract This paper uses baseAgent method to build a new model of re
searching the evolution of discontent in mass violence event in which model 
we introduce emotion factor relation factor risk factor and relevance factor as 
the attribute value of the individual Agent then using psychophysics’ correla
tion theory to quantize the change of individual discontent In the paper by set
ting different initial values we make relevant experiments to study the process 
of evolution of discontent of a typical mass violence event that has happened 
and three other different circumstances All of what we achieved from this pa
per can offer help to the police to understand the inherent mechanism and law 
of mass violence event also can provide guide for taking pointed measures to 
prevent and manage mass violence event 
Keywords mass violence event discontent evolution model 
1 
Introduction 
Nowadays more and more mass violence events break out Domestic and foreign 
researchers have paid much more attention on these events So far there has been two 
different ways of studying mass violence events one is to study the economic reasons 
or the psychology of the participants12 The other is using computer science and 
information technology extracting various factors to build models gains the regular 
patterns and features by simulating 35 In this paper we extract some parameters 
related to the emotion evolution of participants before the events to build models on 
the basis of the latter view68 After running considerable simulations in different 
settings we get the regular pattern of the emotion evolution among participants 
which provides theoretical basis to understand mass violence events 
2 
AgentBased Modeling 
Agentbased modeling ABM is a modeling method developed from artificial  
intelligence in 1970s As the basic abstract unit of the system Agent has some simple 
attributes and status so it has some kind of intelligence We define some simple rules 
for these Agents so that they could interact between each other and obtain correspond
ing system model finally It’s a bottomup modeling method By using Agentbased 
models and simulations not only could we give answers to the problems but also 
 
Researching of the Evolution of Discontent in Mass Violence Event 
459 
 
display all dynamic features of system evolution We could also set up different as
sumptions conditions or limited factors for the models to understand system features 
more adequately More importantly it could resolve various problems of complex 
system highly formally by using the interactions between Agents Based on the cha
racters mentioned above the method has been widely applied in the field of computer 
science technology Figure 1 shows the basic steps of ABM 
 
Fig 1 Basic step of ABMS 
3 
The Model of Discontent Emotion Evolution  
31 
Basic Parameters of Individual Agent 
In this paper we extract four parameters from participants’ individual attributes and so
cial environment emotion factor relationship factor risk factor and relevance factor 
Emotion factor is used to reflect the amount of dissatisfaction emotion of individu
al Agent which has a bearing on the behaviors about to happen It is represented 
byQ and limited in 0 1 The more the dissatisfaction is the larger the likelihood of 
violence is 
Relationship factor represents the individual Agent’s relationship with other 
Agents in the crowd The closer the relationship between two Agents is the more 
proximity the relationship factors of their own are and more impact on each other’s 
behavior they have It’s represented by R  and the value is between 0 1 
Risk factor represented by M  refers to the rationality level of individual Agent 
whose valve is set between 0 1 The bigger the value is the more likelihood the 
Agent gets excited and act violently 
Relevance factor represents the relevance level between individual Agent and the 
event causing the gathering It’s represented byG and its value is between 01 The 
bigger the value is the more relevance with the event Agent has 
So each individual Agent in the crowd could be expressed as 

i 
i
i
i
i
I Q R M G



 
32 
Adjustment Function of Mood 
According to Fechner’s theory the change of human’s perceiving strength is in pro
portion to the change of logarithm of irritant intensity9 as shown in equation 1 
460 
FL Bu and YN Zhao 
 
φ
ψ
= K • lg
 
1
ψ  represents the amount of human’s perception andφ represents the intensity of 
physical irritant while K is a constant 
We assume that there were N Agents in the crowd and choose two individual 
Agents randomly
iI and
jI in a time step to analyze Here we consider the irritant 
amount received by the Agent relevant to the relevance level with the event the rela
tionship with other Agent other Agents’ emotion and rationality status of other 
Agent so we get the assemble stimulate amount of Agent
iI shown as equation 2 
 







+
•
−
+
⋅
+
⋅
•
=
Q t
R
R
M
G
K
j
j
i
j
i
100
1
3
5
1
φ
 
2
1
K is a correction factor which makes the value of φ in a reasonable region So the 
emotion factor of Agent
iI after evolution is expressed by equation 3 


 
 
















+
•
−
+
+ ⋅
⋅
•
•
+
=
+
Q t
R
R
M
G
K
K
Q t
t
Q
j
j
i
j
i
i
i
100
1
3
5
lg
1
1
2
 
3
2
K is a correction factor which makes the value of

Qi t + 1
in a reasonable region 
4 
Simulation Experiment and Results Analysis 
41 
Verification Experiment 
Firstly we use a typical mass violence event that has happened as an example to veri
fy the reasonableness of the model According to actual situation of the event we set 
the number of gathered crowd 30 containing 12 victim’s relatives 6 lawless persons 
12 bystanders We set the Agent 112 with higher value of Q and G  the Agent 
1317 with higher value of M  the Agent 1830 with lower value of Q  The spe
cific setting of the four factors is shown in the figure 2  
 
 
 
Fig 2 The specific setting of the four factors in the group 
 
Researching of the Evolution of Discontent in Mass Violence Event 
461 
 
In order to make the thirty Agents’ value of Q  reasonable we set the value of the 
correction factor
0 02
80
2
1
=
=
K
K
 Using the equation 3 we can acquire the 
evolution curve of discontent for the thirty Agents as showing in the figure 3 
 
 
Fig 3 The evolution curve of discontent for the thirty Agents 
From the figure 3 we can know that the emotion factor Q  of the victim’s relatives 
is higher than the lawless person and bystander also the increasing range is larger 
The bystanders’ discontent is increasing slowly if the number of bystander is large 
this change will lead to group violence Because the evolution of discontent is accord 
with the mechanism of mass violence events so the model presented in this paper is 
rational We can use this model to research the evolution law of discontent in mass 
violence event 
42 
Simulation Experiment 
In this section we design three experiments In the first experiment we suppose the 
crowd contain 18 Agents all have low discontent and relevancy between the Agents 
and the trigger event is same We input relevant value to the model the figure 4 
shows the settings 
Under this circumstance we get the evolution curve of every Agent’s discontent 
emotion as the figure 5 
 
 
Fig 4 The settings of the first experiment 
462 
FL Bu and YN Zhao 
 
 
Fig 5 The evolution curve under the first experiment 
From the figure 5 we can see that the crowd’s mean initial value of discontent 
emotion is 014 After experiment most Agents’ discontent emotion changes slightly 
while several Agents’ discontent emotion increased considerably At the end of expe
riment crowd’s mean value of discontent emotion is 024 
On the basis of first experiment we add four Agents with higher initial value of 
Q  the figure 6 shown the settings of the second experiment 
 
 
 
Fig 6 The settings of the second experiment 
Under this circumstance we get the evolution curve of every Agent’s discontent 
emotion as the figure 7 
 
 
Fig 7 The evolution curve under the second experiment 
We can see from figure 7 that the crowd’s mean initial value of discontent emotion 
is 014 After the second experiment crowd’s discontent emotion increases At the 
50th seconds most Agents’ value concentrate at 036 and 020 means that if the 
crowd includes a small number of Agents with higher value of discontent emotion 
 
Researching of the Evolution of Discontent in Mass Violence Event 
463 
 
crowd’s discontent emotion will be polarization At the end the crowd’s mean value 
of discontent emotion is 028 higher than the first experiment 
On the basis of the first experiment we add four Agents that have higher initial 
value of M to the crowd the figure 8 showing the settings of the second experiment 
Under this circumstance we get the evolution curve of every Agent’s discontent 
emotion as the figure 9 
 
 
 
Fig 8 The settings of the third experiment 
 
Fig 9 The evolution curve under the third experiment 
We can see from figure 9 that the crowd’s mean initial value of discontent emotion 
is 014 After adding four Agents that have higher initial value of risk factor the 
crowd’s increasing range of discontent emotion is larger than the condition under the 
previous experiment At the 50th seconds most Agents’ value concentrate at 036 and 
024 and the crowd’s mean value of discontent emotion is 031 higher than the con
dition under the previous experiment 
43 
Results Analysis 
In the first experiment because the crowd does not receive any stimulation and insti
gating from outside the evolution of discontent emotion shows spontaneity and ran
domness the average value of discontent emotion increases slightly from 014 to 
024 In the second experiment we can see that the average value of discontent emo
tion increases from 014 to 028 suggesting that the evolution of discontent is influ
enced by the four Agents especially discontent emotion mainly distributes at 036 
and 020 at last time step At last experiment we can see that the average value of 
discontent emotion increases from 014 to 031 and discontent emotion mainly  
464 
FL Bu and YN Zhao 
 
distributes at 036 and 024 at last time step which suggests that the Agents with high 
value of risking have lager influence to the crowd also larger than influence from the 
Agents with high value of discontent emotion So if a crowd has more persons with 
risking the crowd will have higher probability of conducting violence 
On the basis of curve of discontent emotion in the figure 5 7 and 9 we draw a re
lation schema between crowd’s average value of discontent and time under the above 
three experiments shown in the figure 10  
 
 
Fig 10 Relation curve between crowd’s average value of discontent emotion and time under 
the above three experiments 
In the figure 10 A B C indicates relationship between crowd’s value of discontent 
emotion and time under the above three experiments respectively From the figure we 
can see that the crowd containing high inflammatory person have larger increasing 
range which suggest the high risky persons’ influence to probability of conducting 
violence is larger than others’ 
5 
Conclusion 
In this paper we present a new model to research individual evolution of discontent 
emotion in the crowd and use a typical mass violence event that have occurred to test 
and verify the model’s rationality Then we design three experiments under different 
conditions what we obtain from the results of the experiments is 1 the crowd’s 
evolution of discontent showing spontaneity and randomness when the crowd’s initial 
discontent emotion is low 2 when adding some persons with high value of discon
tent emotion the crowd’s discontent be influenced slightly and the average value of 
discontent is increased 3 when adding some persons with high value of risk the 
crowd’s discontent is influenced greatly larger than the condition of adding some 
persons with high value of discontent and the average value of discontent increases 
considerably What we can draw from the results is for a crowd conducting violence 
instigating from the lawless persons are more dangerous than the influence from the 
person with high value of discontent emotion We should pay more attention to the 
persons with high value of risk  
 
Researching of the Evolution of Discontent in Mass Violence Event 
465 
 
References 
1 
Craig AA Brad JB Human Aggression Annual Review of Psychology 531 27–51 
2002 
2 
Berkowitz L Cochran ST Embree MC Physical Pain and the Goal of Aversively 
Stimulated Aggression Journal of Personality and Social Psychology 404 687–700 
1981 
3 
Bu FL Feng PY Analysis of AgentBased “NonOrganization and NonDirect Inter
est” Collective Events In 2th IEEE International Conference on Emergency Management 
and Management Sciences pp 417–421 IEEE Press Beijing 2011 
4 
Bu FL Sun JZ An Analysis for AgentBased Mass Violence Event In 2th IEEE In
ternational Conference on Emergency Management and Management Sciences pp 422–
425 IEEE Press Beijing 2011 
5 
Bu FL Sun JZ AgentBased Modelling and Simulation System for Mass Violence 
Event In 4th IEEE International Symposium on Computational Intelligence and Design 
pp 211–215 IEEE Press Zhejiang 2011 
6 
Bu FL Zhao YN Analysis of Mass Violence Event Based on Analytic Hierarchy 
Process In 4th IEEE International Symposium On Computational Intelligence and De
sign pp 174–177 IEEE Press Zhejiang 2011 
7 
Bu FL Zhao YN Modeling and Simulation of Mass Violence Event Based on Agent 
Technology In 2th IEEE International Conference on Emergency Management and Man
agement Sciences pp 426–428 IEEE Press Beijing 2011 
8 
Bu FL Zhao YN Modeling and Warning Analysis of Mass Violence Events In IEEE 
International Conference on Automatic Control and Artificial Intelligence pp 2048–2052 
IET Press XiaMen 2012 
9 
Narens L Mausfeld R On the Relationship of the Psychological and the Physical in 
Psychophysica Psychological Review 993 467–479 1992 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 466–474 2012 
© SpringerVerlag Berlin Heidelberg 2012 
An Efficient TwoStage Level Set Segmentation 
Framework for Overlapping Plant Leaf Image 
XiaoFeng Wang12 and Hai Min23 
1 Key Lab of Network and Intelligent Information Processing Department of Computer 
Science and Technology Hefei University Hefei Anhui 230022 China 
2 Intelligent Computing Lab Hefei Institute of Intelligent Machines  
Chinese Academy of Sciences POBox 1130 Hefei Anhui 230031 China  
3 Department of Automation University of Science and Technology of China  
Hefei Anhui 230027 China 
xfwangiimaccn minhai361gmailcom 
Abstract In this paper an efficient twostage segmentation framework was 
proposed to address the plant leaf image with overlapping phenomenon which 
is built based on the leaf approximate symmetry and level set evolution theory 
In the presegmentation stage a straight line was manually set on the target leaf 
to approximate the principal leaf vein and the Local ChanVese LCV model 
was used on the global image region to help searching the socalled un
overlapping contour in target leaf In the formal segmentation stage the sym
metry detection was performed based on the predefined approximated principal 
vein to obtain the narrowband evolution region and the second initial contour 
Next the LCV model was once again used to find the complete target leaf con
tour in the narrowband evolution region Finally experiments on some real leaf 
images with overlapping phenomenon have demonstrated the efficiency and ro
bustness of the proposed segmentation framework 
Keywords approximate symmetry level set overlapping leaf principal vein 
twostage segmentation 
1 
Introduction 
Recently segmenting the leaf from plant images and performing classification based 
on leaf features has become an efficient mean in the field of digital plant research 
Plant leaf is very suitable for image processing since its shape structure is relatively 
stable and leaf can be easily found and collected in most of the time of four seasons 
1 It should be noted that the leaf segmentation result will directly influence the 
performance of leaf feature extraction and final classification Thus some efficient 
approaches have been proposed for leaf image segmentation up to now Manh et al 
2 used deformable templates and combined the color features to segment the weed 
leaf Persson et al 3 proposed segmenting the leaf image by using the active shape 
models Zheng et al 4 proposed the meanshiftbased color segmentation method 
for green leaves Wang et al 1 proposed an automatic markercontrolled watershed 
 
An Efficient TwoStage Level Set Segmentation Framework 
467 
 
method combined with presegmentation procedure and morphological operation to 
segment leaf images  
However most of the proposed methods can not address the plant leaf image with 
overlapping phenomenon ie leaves are overlapped with each other due to the origi
nal growth condition The uncovered complete leaf in upper layer is the segmentation 
target Without the prior knowledge the segmentation result will not be satisfied since 
the interferential leaf in underlying layer will produce the false contour To resolve 
this problem we shall introduce a twostage segmentation framework based on the 
leaf approximate symmetry The segmentation process can be divided into two stages 
ie presegmentation stage and formal segmentation stage Here we used our pre
viously proposed Local ChanVese LCV model 5 as the segmentation model for 
both two stages LCV model contains the controlled global term and local term It is 
very suitable for twostage image segmentation task since it can not only capture the 
regional image information but also segment the detailed objects  
Before segmentation a straight line should be manually set on the target leaf in up
per layer to approximate the principal vein Then in the presegmentation stage the 
level set evolution driven by LCV model is performed on the global image region to 
help searching the socalled unoverlapping contour in target leaf Next the symmetry 
detection is implemented based on the predefined approximated principal vein to 
obtain the narrowband evolution region and the second initial contour Finally in the 
formal segmentation stage the LCV model is once again used to extract the complete 
leaf contour in the narrowband evolution region  
The rest of this paper can be organized as follows In Section 2 we briefly intro
duce the leaf approximate symmetry and LCV model Our twostage level set seg
mentation framework is presented in Section 3 In Section 4 the proposed framework 
is validated by some experiments on leaf images with overlapping phenomenon Fi
nally some conclusive remarks are included in Section 5 
2 
Background Knowledge 
21 
Leaf Approximate Symmetry 
Leaf vein is the vascular bundle growing in the leaf to transfer water and nutrition 
The principal vein is the longest and obvious vein located in the center of the leaf 
Since the physiological function of leaf is to receive sunlight and transpire water the 
shapes of both sides of the principal vein should be almost same under the same ideal 
conditions of sunlight and water Fig1 shows some basic shapes of common simple 
leaf It can be seen that the shapes of leaves preserve certain symmetry although the 
shapes vary widely It should be noted that practical leaf may not preserve complete 
symmetry due to the growth position orientation and inclination angle of leaf Some 
trivial shape difference may exist in two sides of the principal vein So the symmetry 
of leaf based on principal vein is often called approximate symmetry 
In this paper the leaf approximate symmetry is adopted to find the complete target 
leaf in upper layer of the overlapping leaf image It should be noted that we do not 
directly extract the principal vein but manually set a straight line on the target leaf to 
468 
XF Wang and H Min 
 
approximate its principal vein The reasons are as follows First the principal veins 
are not obvious in some leaves and often connected with the lateral veins and minor 
veins So how to extract the principal veins is in itself a difficult problem in the field 
of leaf image processing 6 Second the shift and deformation may exist in the leaf 
contour of both sides of the principal vein due to the different conditions of sunlight 
and water The approach of manual setting straight line to approximate principal vein 
can reduce the sideeffect of shift and deformation on the final segmentation result 
 
 
   
  
 
Fig 1 The basic shapes of common simple leaf 
22 
Local ChanVese Model 
The local ChanVese LCV model was built based on the techniques of curve evolu
tion local statistical function and level set method By incorporating local information 
into regional level set model LCV model can not only capture the regional image 
information but also segment the detailed objects with the controlling balance para
meters The overall energy functional in LCV model 
ELCV
 consists of three parts 
global term 
G
E  local term 
L
E  and regularization term 
R
E  Thus the overall ener
gy functional can be described as 
LCV
G
L
R
E
E
E
E
α
β
=
⋅
+
⋅
+
 
 1
where α  and β  are two positive parameters which govern the tradeoff between the 
global term and the local term 
The global term 
G
E  is directly derived from the ChanVese model 7 Using the 
level set formulation the boundary C  is represented by the zero level set of a Lip
schitz function 

φ Ω → R
 The global term is stated as follows 
2
1
2
0
1
2
0
2
 
 
  
   
  
1
   
EG
c c
u
x y
c
H
x y dxdy
u
x y
c
H
x y
dxdy
φ
φ
φ
Ω
Ω
=
−
+
−
−


 
2
where 
 
H z  is the Heaviside function 
The introduction of local term is to statistically analyze each pixel with respect to 
its local neighborhood In the same manner as global term the local term 
L
E can be 
reformulated in terms of the level set function 
  
φ x y
 as follows 
 
An Efficient TwoStage Level Set Segmentation Framework 
469 
 
2
1
2
0
0
1
2
0
0
2


 
  
  
   
  
  
1
   
L
k
k
E
d d
g
u
x y
u
x y
d
H
x y dxdy
g
u
x y
u
x y
d
H
x y
dxdy
φ
φ
φ
Ω
Ω
=
∗
−
−
+
∗
−
−
−


 
3
where 
k
g  is a averaging convolution operator with k
×k
 size window 
1d  and 
2
d  
are the intensity averages of difference image 
0
0

  
  
gk
u
x y
u
x y
∗
−
 inside C  and 
outside C  respectively  
The regularization term 
R
E  is composed of two items The first item is the length 
penalty term which is used to control the smoothness of the zero level set and further 
avoid the occurrence of small isolated regions The second item is used to force the 
level set function to be close to a signed distance function 8 It plays a key role in 
the elimination of timeconsuming reinitialization procedure 
2
 

0
 
1
   
  

   1
2
ER
L
P
x y
x y dxdy
x y
φ
μ
φ
φ
μ
δ φ
φ
φ
Ω
Ω
=
⋅
=
+
=
⋅
∇
+
∇
−


 
4
where μ  is the parameter which can control the penalization effect of length term if 
μ  is small then smaller objects will be detected if μ  is larger then larger objects 
are detected 
The minimization of 
ELCV
 can be done by introducing an artificial time variable 
t ≥ 0
 and moving φ  in the steepest descent direction to a steady state as follows 
2
2
0
1
0
0
1
2
2
0
2
0
0
2
2
  



  
  
 




  
  
 

 





k
k
u
c
g
u
x y
u
x y
d
t
u
c
g
u
x y
u
x y
d
div
div
ε
ε
φ
δ φ
α
β
α
β
φ
φ
μ δ φ
φ
φ
φ
∂
=
−
⋅
−
+
⋅
∗
−
−
∂
+
⋅
−
+
⋅
∗
−
−
∇
∇
+
⋅
+ ∇
−
∇
∇
 
5
For more details about the LCV model readers can refer to the literature 5 
3 
TwoStage Segmentation Framework 
In this section we shall present and discuss the details of the twostage level set seg
mentation framework It should be noted that the final objective is to find the unco
vered and complete leaf in upper layer ie target leaf The covered and incomplete 
leaf in underlying layer ie background leaf is regarded as the interferential object 
To obtain satisfying segmentation result we usually require that the test image should 
contain only one complete target leaf and one half side of the target leaf contour is un
overlapped  
31 
The Presegmentation Stage 
Before presegmentation we should first set a straight line to approximate the prin
cipal vein for target leaf Here we adopt the simple twopoint form to define the 
470 
XF Wang and H Min 
 
straight line Two points  1 1
x
y
 and  2 2
x
y
are manually selected at the two ends 
of the target leaf Thus the straight line equation of approximate principal vein is 
built as follows 
0
Ax
By
C
+
+
=
 
6
where
1
2
A
y
y
=
−

2
1
B
x
x
=
−
 and 
1 2
2 1
C
x
y
x
y
=
−
 
Next we should set the first initial contour and initialize the level set function φ  
to a sign distance function Benefit from the less sensitivity to the location of initial 
contour of LCV model the initial contour can be placed anywhere The parameters of 
LCV model should be set which include the timestep
t
Δ  the grid spacing h  the 
regularization parameter ε  the window size of averaging convolution operator k  
the controlling parameter of global term α  the controlling parameter of local term β  
and the length controlling parameter μ   
After the level set evolution stops the zero level sets contours should be extracted 
from the level set function φ  Due to the complicated background of overlapping leaf 
image the extracted contours usually contain several contours with different lengths 
ie



1
2




n 
C
C
 C
 which are generated from the target leaf and the interferential 
leaves or branches To provide precise results for the next formal segmentation stage 
the interferential contours should be removed According to the prior knowledge that 
only one target leaf exists in the test image the longest contour 

C  will be kept as 
the presegmentation result 


1

i 
i n
C
argmaxlength C
≤ ≤
=
 
7
Actually the contour 

C  belongs to both the target leaf in upper layer and back
ground leaf in underlying layer Now it is necessary to find the contour belonging to 
unoverlapped half side of target leaf which is called unoverlapping contour 
T
C  
First contour 

C should be further divided into two parts ie 
1
C  and 
2
C  according 
to the approximate principal vein For any point   
x y  it can be classified into 
1
C  or 
2
C  based on the following rule  
1
2
  
0
  
0
x y
C
A x
B
y
C
x y
C
∈


∗
+
∗
+

∈


 
8
where A  B  and C are the coefficients in Eq6  
Then it needs to judge which one between 
1
C  and 
2
C  is the unoverlapping con
tour According to the growth characteristic of plant leaf most of the leaf contours are 
smooth and bending towards the principal vein Correspondingly the normal direc
tions of all leaf contour points are generally towards the principal vein It should be 
noted that the overlapping contour is actually composed of two parts from the target 
leaf contour and background leaf contour respectively As a result a sudden change 
of normal direction will occur at the junction of the target leaf contour and  
 
An Efficient TwoStage Level Set Segmentation Framework 
471 
 
background leaf contour Thus we can theoretically judge which contour is the un
overlapping contour according to the continuity of normal directions of contour point 
Considering that there are some tiny sawtooth on the leaf contour the statistical anal
ysis to the continuity of normal directions should be performed based on the points 
sampled from contour with the same interval in practical numerical implementation 
In this way the computations can be greatly decreased and precise statistical analysis 
result will be obtained 
32 
The Formal Segmentation Stage 
After obtaining the unoverlapping contour 
T
C  the symmetric curve 
S
C  with re
spect to the approximate principal vein can be obtained For each point   
T
x y
∈C
 
the symmetric point 


S
S
x
y
on 
S
C  can be computed as follows 
2
2
2
2
2 
  

2 
  

S
S
x
x
A Ax
By
C
A
B
y
y
B Ax
By
C
A
B

=
−
+
+
+

=
−
+
+
+

 
9
T
C  and 
S
C  are then connected to generate a complete and closed curve Here we 
adopt the narrowband idea from level set theory and build a narrowband evolution 
region which regards 
c
C  as its center line and has a bandwidth of 2w  Different 
from the presegmentation stage the second initial contour in formal segmentation 
stage can be automatically constructed which is a square with size of r
×r
 centered 
on the center of 
T
C  Since the second initial contour should located in the narrow
band evolution region the value of r  should be smaller than that of 2w   
Finally the second levels set evolution based on LCV model can be performed in 
the narrowband evolution region with the second initial contour Here some evolu
tion parameters controlling the segmentation detail should be updated since the over
lapping phenomenon still exists in the narrowband evolution region The first up
dated parameter is the length controlling parameter μ  A smaller value should be set 
for μ  according to the setting rule of μ  In the same way the controlling parameter 
of global term α  should also be smaller than the controlling parameter of local 
term β  After the formal segmentation the final obtained zero level set will be the 
contour of target leaf 
4 
Experimental Results 
In this Section we shall present the experimental results of twostage level set seg
mentation framework on some leaf images with overlapping phenomenon The pro
posed segmentation framework was implemented by Matlab 7 on a computer with 
Intel Core 2 Duo 22GHz CPU 2G RAM and Windows XP operating system We 
used the same parameters of the timestep
01
t
Δ =
 the grid spacing
h =1
 
1
ε = for 
H  
ε z
 and 
εδ  z
 the window size of averaging convolution operator 
k = 15
 the 
controlling parameter of local term
1
β =  for all the experiments in this section In our 
472 
XF Wang and H Min 
 
experiments α  has two values 1 and 01 for presegmentation stage and formal 
segmentation stage The length controlling parameter μ  also has a scaling role like 
α  Two corresponding values ie 
001 2552
∗
 and 
0001 2552
∗
 are adopted In 
conclusion there are totally two parameters whose values need to be dynamically 
adjusted in our experiments 
 
 
 
a 
b
c
d 
 
 
e 
f
g
h 
Fig 2 The whole segmentation process of proposed twostage level set segmentation frame
work on the leaf image with overlapping phenomenon a Original image b The approximate 
principal vein and initial contour c The zero level sets after the level set evolution in pre
segmentation stage d The longest contour 

C  e 
1
C yellow line  and 
2
C  blue line f 
Center line 
c
C  g The narrowband evolution region and the second initial contour h The 
final segmentation result of formal segmentation stage Image size=183 160
×
 
Fig2 demonstrates the whole process of proposed twostage level set segmentation 
framework on the leaf image with overlapping phenomenon It can be seen from Fig2 
a that an interferential leaf is overlapped with the target leaf at the topleft corner of 
image The approximate principal vein red straight line and initial contour green 
triangle are shown in Fig2 b The green lines in Fig2 c correspond to the zero 
level sets after the level set evolution in presegmentation stage Fig2 d shows the 
longest contour 

C and Fig2 e demonstrates the obtained 
1
C  yellow line and 
2
C  
blue line By performing the statistical analysis to the continuity of normal direc
tions it can be judged that 
1
C  is the required unoverlapping contour 
T
C  After 
computing the symmetric curve 
S
C  with respect to the approximate principal vein 
T
C  and 
S
C  are then connected to generate the center line 
c
C as shown in Fig2 f 
The blue lines in Fig2 g are the boundary of narrowband evolution region with a 
bandwidth of 16 The green square in Fig2 g shows the second initial contour in 
formal segmentation stage with size of12 12
×
 Fig2 h demonstrates the final seg
mentation result of formal segmentation stage It can be seen the target leaf is seg
mented perfectly 
 
An Efficient TwoStage Level Set Segmentation Framework 
473 
 
 
 
a
b
c
 
 
d 
e 
f 
Fig 3 The segmentation experiment of proposed framework on larger leaf image with over
lapping phenomenon a The approximate principal vein and initial contour b The longest 
contour 

C  c 
1
C and 
2
C  d Center line 
c
C  e The narrowband evolution region and the 
second initial contour f The final segmentation result Image size= 304
× 297
 
 
 
a
b
c
 
 
d
e
f
Fig 4 The segmentation experiment of proposed framework on larger leaf image with over
lapping phenomenon a The approximate principal vein and initial contour b The longest 
contour 

C  c 
1
C and 
2
C  d Center line 
c
C  e The narrowband evolution region and the 
second initial contour f The final segmentation result Image size= 573
× 297
 
Fig3 and Fig4 further show the segmentation results for two larger leaf images 
respectively The bandwidth of narrowband evolution region in Fig3 is 20 and the 
second initial contour is a square with size of 16 16
×
 Since the size of the test leaf 
image in Fig4 is larger the corresponding bandwidth is increased to 26 and the size 
of the second initial contour is also increased to 20 Fig3 f and Fig4 f show that 
two target leaves are efficiently segmented from the background leaves 
474 
XF Wang and H Min 
 
5 
Conclusions 
In this paper we proposed a novel twostage level set segmentation framework for 
plant leaf image with overlapping phenomenon The segmentation process can be 
divided into two stages ie presegmentation stage and formal segmentation stage 
Here we used the LCV model as segmentation model for both two stages since it can 
not only capture the regional image information but also segment the detailed objects 
Besides the prior knowledge of leaf approximate symmetry is introduced to reduce 
the sideeffect of shift and deformation on the segmentation result The experiments 
on some real leaf images with overlapping phenomenon have demonstrated the effi
ciency and robustness of the proposed segmentation framework 
Acknowledgement This work was supported by the grant of the National Natural 
Science Foundation of China No 61005010 the grant of China Postdoctoral Science 
Foundation No 20100480708 the grant of the Key Scientific Research Foundation 
of Education Department of Anhui Province No KJ2010A289 the grant of Scientific 
Research Foundation for Talents of Hefei University No 11RC05 
References 
1 
Wang XF Huang DS Du JX Xu H Heutte L Classification of Plant Leaf Images 
with Complicated Background Applied Mathematics and Computation 2052 916–926 
2008 
2 
Manh AG Rabatel G Assemat L Aldon MJ Weed Leaf Image Segmentation by 
Deformable Templates Journal of Agricultural Engineering Research 802 139–146 
2001 
3 
Persson M Astrand B Classification of Crops and Weeds Extracted by Active Shape 
Models Biosystem Engineering 1004 484–497 2008 
4 
Zheng LY Zhang JT Wang QY MeanShiftBased Color Segmentation of Images 
Containing Green Vegetation Computers and Electronics in Agriculture 651 93–98 
2009 
5 
Wang XF Huang DS Xu H An Efficient Local ChanVese Model for Image Seg
mentation Pattern Recognition 433 603–618 2010 
6 
Fu H Chi ZR Combined Thresholding and Neural Network Approach for Vein Pattern 
Extraction from Leaf Images Vision Image and Signal Processing 1536 881–892 
2006 
7 
Chan TF Vese LA Active Contours without Edges IEEE Transactions on Image 
Processing 102 266–277 2001 
8 
Li CM Xu CY Gui CF Fox MD Level Set Formulation without ReInitialization 
A New Variational Formulation In Proc IEEE International Conference on Computer Vi
sion and Pattern Recognition CVPR 2005 pp430–436 2005 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 475–482 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Behavior Analysis of Software Systems  
Based on Petri Net Slicing 
Jiaying Ma1 Wei Han1 and Zuohua Ding12 
1 Lab of Intelligent Computing and Software Engineering Zhejiang SciTech University 
Hangzhou Zhejiang 310018 PR China 
2 Institute of Logic and Cognition Sun Yatsen University  
Guangzhou Guangdong 510275 PR China 
sdmajiayingyahoocn hzhanwei163com zouhuadinghotmailcom 
Abstract This paper presents a new method to analyze system behavior A 
software system is modeled by a Petri net and then the Petri net is sliced into 
several parts based on Tinvariant It has been shown that the behavior of the 
slices is equivalent to the behavior of the original Petri net Thus we can analyze 
the system behaviors by checking each slice With this method state space ex
plosion problem in the static analysis can be solved to some extent The Dining 
Philosophy problem has been used to demonstrate the benefit of our method 
Keywords behavior analysis petri net model slicing tinvariant 
1 
Introduction 
Traditionally system behavior analysis can be conducted by applying static analysis 
techniques to the systems These techniques include reachability based analysis 
techniques symbolic model checking flow equations and dataflow analysis etc 
However they may hit state space explosion problem since they have to exhaustively 
explore all the reachable states to detect system errors Many techniques have been 
proposed to combat this explosion including state space reductions 12 abstraction 
6 and integer programming techniques 2 With respect to these efforts 
unfortunately the explosion issue is still there which is the main technical obstacle to 
transition from research to practice 
In this paper we propose a new method to analyze the system behavior A system is 
modeled by a Petri net and the Petri net is then sliced to several slices based on 
Tinvariants of the net finally we check the behavior of the whole system by checking 
each slice In this way the state space explosion problem can be solved to some extent 
Program slicing technique was first proposed by M Weiser 17 The main ideas of the 
slicing technique is to abstract the statements that may affect some points of interest 
and then divide the large and complex system model into small manageable parts It has 
                                                           
  Corresponding author 
476 
J Ma W Han and Z Ding 
 
been widely used in the analysis of formal models such as hierarchical state machine 
8 attribute grammars 16 ObjectZ specifications 4 CSPOZ specifications 3 
This paper is structured as follows In Section 2 we present the slicing technique 
based on Tinvariant In section 3 we prove the behavior equivalence of the Petri net 
and its slices In Section 4 we use the dining philosophy problem as the example to 
illustrate our method to check system behavior Section 5 is the discussion on the 
related work Section 6 is the conclusion of the paper 
2 
Petri Net Slicing Based on Tinvariants 
21 
Petri Net 
In this paper we use Petri net to model software systems 
 
Definition 1 A Petri net is a directed bipartite graph that can be written as a tuple 


 

P T F M0
 where P is the set of places T is the set of transitions 




P
T
T
P
F
×
∪
×
⊂
is the set of arcs and 
0
M  is the initial marking 
In our Petri net model a place is used to denote a software state and a transition is used 
to denote an event The initial marking indicates the start states 
A transition is enabled if all the input places have nonzero markings Only enabled 
transitions can be fired We assume that as soon as a transition gets enabled it starts to 
fire Firing is based on two indivisible primitives 1 Removal of the markings from the 
input places and insertion of the markings in the output places two transitions which do 
not share any places can be fired independently 2 Determination of the new marking 
distribution function in both input and output places after firing 
p
t1
t2
a
t1
t2
p
b
 
Fig 1 The Petri net representation of conflict 
Conflict is common in a Petri net and it often leads to deadlock status Basically 
conflict is a reflection of the competitive relationship which makes the token distri
bution uncertain and thus makes the resources distribution results diverse In a Petri 
nets if a place has more than one outtransitions but the tokens some corresponding 
share resources in the place are not enough to fire all of the enabled transitions at the 
same time then only one or some of them can be enabled while the rest are dis
enabled Conflicts are classified as two kinds forward conflict which is usually called 
conflict and backward conflict which is usually called contact For the first kind the 
 
Behavior Analysis of Software Systems Based on Petri Net Slicing 
477 
 
sharing resource is the token in place while for the second kind conflict the token is the 
capacity of postplace Two kinds of conflicts can be described as in Fig 1 
In the next we will show how to divide the conflict structure into two different slices 
based on Tinvariant which will help us to reduce the complexity in the behavior 
analysis 
22 
The Algorithm of Petri Net Slicing 
Definition 2 Tinvariant Assume T
is a Tvector of Petri net N
 
Z
S
C T
→
×

is the incidence matrix

 

 



j
i
i
j
j
i
W s t
s
W t
C s t
−
=
 and 
θ0
is a zero vector If there is a Tvector satisfying 
0

T = θ
C
 then T is a 
Tinvariant of Petri net N  Especially if 
T  θ0
 then T is a nonnegative invariant 
If there is no other Tinvariant T′ satisfying 
T  T
′
θ0 
 then T is a minimal 
invariant 
Tinvariant of a Petri net describes the locality of a system ie a dynamic that be
gins from arbitrary initial marking 
0
M can come back to the marking 
0
M after a 
sequence of transitions 
The following algorithm in Fig 2 shows how a Petri net model is sliced into a set 
of small Petri net pieces based on Tinvariant 
sliceset=φ
do
if transitionsliceset ̸= transitionN
do

small invariant=ﬁnd smallest invariant
set of minimal invariant
transition connected=φ
invariant connected=φ
for ∀ transition ∈ small invariant
transition connected=ﬁnd place connected
end
invariant connected= all transition connected
slice= small invariant+invariant connected
sliceset=sliceset+slice
sliceset of invariant= set of invariantsmall invariant
 untilset of minimal invariant=φ 
uncovered transition set=transitionNtransitionsliceset
for ∀ transition ∈ uncovered transition set
add according to preplacepostplace
 
Fig 2 Petri net Slicing algorithm 
First the Tinvariants are computed and the minimal invariants are selected Then the 
smallest invariant is chosen to construct the slice ie after getting a smallest invariant 
478 
J Ma W Han and Z Ding 
 
the algorithm checks the preplaces of each transition and composes a cycle as a Petri 
net slice 
If the invariant set becomes empty without covering all the transitions in the Petri 
net N  then the algorithm checks the preplaces and postplaces of the transition one by 
one If the preplaces or postplaces of a transition are in some slice then the transi
tion is added into the slice Accordingly the corresponding postplaces or preplaces 
are also added into the slice For a transition if its preplaces and postplaces belong to 
no slices it will be skipped and will be checked later after all uncovered transitions are 
checked In this way the algorithm guarantees that every transition in N belongs to 
some slice 
The modular Petri net or Petri net pieces obtained from the slicing algorithm is 
called as Petri net slice 
23 
Complexity Analysis 
Now we compute the complexity of the algorithm The complexity of finding 
Tinvariant is the same as that of solving the algebra equation 
0

T = θ
C
 Let the 
number of places and transitions in the Petri net be m and n respectively The com
plexity of solving the equation is 


O n3
 Finding a smallest invariant is to compute 
the number of the invariants and then sort bubble sort them thus the complexity is 


O n2
 Checking the preplaces and postplaces of every transition is similar to 
locating the positive negative zero number of an array and the complexity is 
Omn
for m lines and n volumes In summary the complexity of the slicing algo
rithm is 


O n3
 
3 
Justification of the Behavior Equivalence 
In this section we will prove that the original Petri net model and the Petri net slices 
have the same behavior 
Definition 3 Behavior Equivalence Two Petri net P and Q are behaviorally equiv
alent iff there exists an onetoone mapping between the reachability graphs of P and 
Q  
We have the following observation from the algorithm 
─ The change of states in Petri net is determined by the initial marking and fire 
sequence of transitions 
─ Slicing algorithm reserves all the input and output information of every transition 
─ The initial marking remains the same before or after slicing 
Hence we can prove the following result 
 
 
Behavior Analysis of Software Systems Based on Petri Net Slicing 
479 
 
Theorem 4 A Petri net and its slices obtained from Tinvariant are behaviorally 
equivalent 
Proof In order to prove the behavior equivalence we only need to prove that the firing 
sequences remain the same after slicing 
Assume that 
n
n
k
k
t
t
t
t
t t t
s
1
1
1  1 2 3
−
−


 is a firing sequence Next we show that 
every 
kt  can fire in Petri net slices 
kt  may appear in more slices but the structures 
of 
kt  are the same ie its preplaces and postplaces are the same so for the con
venience we may assume that it appears in only one slice There are two cases 
Case 1 The preplace of 
kt  is not shared 
Assume that transition 
kt  appears in a slice say S  then all of its preplaces and 
postplaces are in S  so 
kt  can fire 
Case 2 There is at least one shared place in the preplaces 
Let place
a
p be a shared place for two transitions 
kt  and 
mt
 
mt
belongs to 
another firing sequence say 
2s  Assume that the transition 
kt  is fired first and 
then
a
p is released after the firing of sequence 
1s  or 
kt waits until sequence
2s is 
finished and 
a
p  is released In both situations the transition
kt  can fire and the firing 
sequence 
1s  can execute completely 
4 
An Example Dining Philosophy Problem 
We use the dining philosophy problem as the example to show our method for behavior 
analysis Dijkstra’s 7 dining philosopher problem is a very wellknown example for 
the static analysis A group of N philosophers is sitting around a table The 
philosophers alternate between thinking and eating Initially n forks are placed on the 
table between each pair of philosophers In order to eat a philosopher must first pick up 
both forks next to him The forks are put down when the philosopher finishes eating and 
starts thinking This problem is interesting because of the possibility of a circular 
deadlock Deadlock may occur if all philosophers pick up their forks in the same order 
say the right fork followed by the left fork In this case there is one deadlock state 
corresponding to the situation in which all philosophers have picked up their right fork 
and are waiting for their left fork 
The Petri net for 5 philosophers is shown in Fig3 
For every philosopher and every fork the corresponding behavior is an independent 
process If we take the first philosopher as the example then the behavior can be de
scribed as 
1
4
3
2
1
4
3
2
1
p
p
p
p
p
t
t
t
t
⎯→
⎯
⎯→
⎯
⎯→
⎯
⎯→
⎯
 and the behavior of a fork 
can 
be 
described 
as 
21
34
2
21
3
2
1
p
p
p
p
t
t
t
⎯→
⎯
⎯→
⎯
⎯→
⎯
or 

21
34
2
21
12
11
10
p
p
p
p
t
t
t
⎯→
⎯
⎯→
⎯
⎯→
⎯
 
480 
J Ma W Han and Z Ding 
 
t1
t2
t3
p2
p3
p1
p21
p22
t4
p4
t6
t7
p8
p6
p7
p5
p23
t8
t9
t10
t11
p10
p11
p9
p12
p24
t12
t13
t14
t15
p14
p15
p13
p25
t16
p16
t17
t18
t19
p18
p19
p17
t20
p20
t5
 
Fig 3 Petri net model for five philosophers 
t1
t2
t3
p2
p3
p1
p21
p22
b
t4
p4
t6
t7
p8
p6
p7
p5
p23
t8
t9
t10
t11
p10
p11
p9
p12
p24
t12
t13
t14
t15
p14
p15
p13
p25
t16
p16
t17
t18
t19
p18
p19
p17
t20
p20
t5
p4i+1
t4i+1
p4i+2
t4i+2
p4i+3
t4i+3
t4i+4
p4i+4
p20+i+1mod6
p20+imod6
a
 
Fig 4 Slices with Tinvariant for five philosophers 
After applying the Tinvariant based algorithm to the Petri net model we get 5 
Tinvariants 
11110000000000000000


00001111000000000000


00000000111100000000


00000000000011110000


00000000000000001111
5
4
3
2
1
=
=
=
=
=
T
T
T
T
T
 
1
 
Behavior Analysis of Software Systems Based on Petri Net Slicing 
481 
 
The corresponding two slices are shown in the Fig 4 Fig4a describes one behavior 
of the dinning philosopher problem stopthinking→ get LeftFork and RightFork 
consequently → eating → thinking Easy to see that the transitions in slice a can 
fire ie slice a can execute Fig4b describes another behavior In Fig4b if tran
sitions 
17
13
9
1 5




t
t t t
t
 get tokens from
25
24
23
22
21




p
p
p
p
p
 at the same time 
then transitions 
18
14
10
6
2




t
t
t
t
t
 will be waiting for these tokens forever ie the 
system comes to the deadlock Hence this slice describes the deadlock behavior of the 
dinning philosopher problem 
Remark 1 The slices obtained with Sinvariant based method 10 are related to each 
others thus the execution of one slice is dependent on the execution of other slices In 
other words we can not analyze the dinning philosopher problem based on their slices 
5 
Related Work 
The research of Petri net slicing focuses on two aspects one is to extract the statements 
that may affect the property under study and the other is to divide a complex Petri net 
model into small manageable parts and then study the smaller parts 
Chang and Wang 5 presented a static slicing algorithm that slices out all sets of 
paths known as concurrent setsso that all paths within the same set should be executed 
concurrently M Llorens 11 defined a group of slicing criterion Paths can be gener
ated based on the slicing criterion This method can be used for the debugging and 
model checking Rakow 1415 presented another static slicing technique to reduce 
the Petri net size In his method for a given place set the input places incoming tran
sitions and outgoing transitions are added to the set iteratively to construct a subnet 
The Petri nets size is reduced thus the state space explosion problem can be solved to 
some extent W J Lee and H N Kim 10 proposed a static slicing technique in which 
minimal Sinvariants are used to partition a Petri net into concurrent units Petri net 
slices and the partitioned models can be analyzed by a compositional reachability 
analysis technique They also proved that the boundedness and liveness will be retained 
in the partitioned models  
6 
Conclusion 
In this paper a Tinvariant based Petri net slicing technique is presented The slices 
obtained by this technique have the equivalent behavior of the original Petri net This 
technique can be used to study the behavior of a class of concurrent systems that use 
resource sharing as their synchronization mechanism Instead of checking the whole 
system we can check each slice to get the behavior of whole system In the future we 
will consider applying this technique to the system test 
 
Acknowledgments This work is supported by the NSF under Grant No61170015 
Zhejiang NSF under Grant NoZ1090357 and Key Project of Ministry of Education of 
China under Grant No 10JZD0006 
482 
J Ma W Han and Z Ding 
 
References 
1 Ascher UM Petzold LR Computer Methods for Ordinary Differential Equations and 
DifferentialAlgebraic Equations Society for Industrial  Applied Mathematis Philadel
phia 1998 
2 Avrunin GS Buy UA Corbett JC Dillon LK Wileden JC Automated Analysis of 
Concurrent Systems With The Constrained Expression Toolset IEEE Transactions on 
Software Engineering 1711 1204–1222 1991 
3 Brückner I Slicing CSPOZ Specifications In Proc of the 16th Nordic Workshop on 
Programming Theory pp 71–73 Uppsala University Sweden 2004 
4 Brückner I Wehrheim H Slicing ObjectZ Specifications for Verification In Treharne 
H King S C Henson M Schneider S eds ZB 2005 LNCS vol 3455 pp 414–433 
Springer Heidelberg 2005 
5 Chang CK Wang H A Slicing Algorithm of Concurrency Modeling Based on Petri Nets 
In Proc of the Intermational Conf on Parallel Processing 1986 pp 789–792 IEEE 
Computer Society Press 1986 
6 Clarke EM Grumberg O Long DE Model Checking and Abstraction ACM Trans
actions on Programming Language Systems 165 1512–1542 1994 
7 Dijkstra EW Hierarchical Ordering of Sequential Processes Acta Informat 2 115–138 
1971 
8 Heimdahl MPE Whalen MW Reduction and Slicing of Hierarchical State Machines 
In Jazayeri M ed ESECFSE 1997 LNCS vol 1301 pp 450–467 Springer Heidelberg 
1997 
9 Lee J Scheduling Analysis with Resources Share Using The Transitive Matrix Based on 
Pinvariant In Proceedings of the 41st SICE Annual Conference 2002 pp 5–7 2002 
10 Lee WJ Kim HN A SlicingBased Approach to Enhance Petri Net Reachability Anal
ysis Journal of Research and Practice in Information Technology 32 131–143 2000 
11 Llorens M Oliver J et al Dynamic Slicing Techniques for Petri Nets Electronic Notes 
in the Theoretical Computer Science 223 1–12 2008 
12 Long DL Clarke LA Task Interaction Graphs for Concurrency Analysis In Proceed
ings of 11th ICSE Pittsburgh Penn USA pp 44–52 1989 
13 Murata T Petri Nets Properties Analysis and Applications Proceedings of the IEEE 77 
541–580 1989 
14 Rakow A Slicing Petri Nets with an Application to Workflow Verification In Geffert V 
Karhumäki J Bertoni A Preneel B Návrat P Bieliková M eds SOFSEM 2008 
LNCS vol 4910 pp 436–447 Springer Heidelberg 2008 
15 Rakow A Slicing Petri Nets In Proceeding of the Workshop on FABPWS 2007 Satellite 
Event Siedlce pp 55–76 2007 
16 Sloane AM Holdsworth J Beyond Traditional Program Slicing In Proc of the Inter
national Symp on Software Testing and Analysis San Diego CA pp 180–186 1996 
17 Weiser M Program Slicing IEEE Transactions on Software Engineering 10 352–357 
1984 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 483–489 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Aircraft Landing Scheduling Based on Semantic Agent 
Negotiation Mechanism 
ZhaoTong Huang XueYan Song JiZhou Sun and ZhiZeng Li 
School of Computer Science and Technology Tianjin University Tianjin China 
zhaotongtjueducn 
Abstract The aircraft landing scheduling problem ALS is a typical NPhard 
optimization problem and exists in the Air Traffic Control for a long time 
Many algorithms have been proposed to solve the problem and most of them 
are centralized With the development of the aerotechnics the concept of free 
flight has been proposed Airplanes could change their flight paths during the 
flight without approval from a centralized en route control In order to support 
free flight a distributed system based on MultiAgent System is proposed in 
this paper The kernel of the system is semantic agent negotiation mechanism 
With the method aircrafts in the system could make landing sequence 
considering their own states The Experimental results show that the proposed 
algorithm is able to obtain an optimal landing sequence and landing time 
rapidly and effectively 
Keywords Aircraft landing scheduling problem semantic agent negotiation 
free flight 
1 
Introduction 
The Aircraft Landing Scheduling ALS problem is one of the important problems in 
Air Traffic Control ATC The problem is to determine the landing sequences and 
landing time for a given set of aircrafts 1 A lot of research has been done on the 
optimization of ALS problem in recent years The linear programming LP method 
has been applied in 1 2 Heuristic algorithms are used to solve the ALS problem 
owing to the powerful ability of parallel and global search 46 
In this paper we propose a distributed agent approach Semantic Agent Negotiation 
Mechanism SANM which is based on MultiAgent System MAS to solve the ALS 
problem The main idea of the SANM is that aircrafts can make the landing sequence 
through cooperation SANM solves the problem consider the whole condition and 
single aircraft while other algorithms only consider the whole condition Semantic 
knowledge is used to describe the negotiation rules in SANM SANM has two steps 
1 every aircraft uses its own algorithm here we will discuss the genetic algorithm 
to get a landing scheduling 2 then all aircrafts use the negotiation mechanism to 
optimize the scheduling and get a better result   
The remainder of this paper is organized as follows The ALS problem in multi
runway systems focus on minimizing the cost of the aircraft sequence hereafter is 
described in Section 2 Section 3 introduces the model of SANM Genetic algorithm 
484 
Z–T Huang et al 
that used to solve the ALS problem is introduced in Section 4 The negotiation rules 
are proposed in Section 5 Extensive simulation study is reported in Section 6 and the 
paper ends with some conclusions in Section 7 
2 
ALS Problem Formulation 
Assume that a set of n aircrafts will land at a multirunway airport The objective of 
ALS problem is to find an optimal arrival sequence for aircrafts In arrival sequence 
time interval of adjacent aircrafts will land at the same runway should be no less 
than
ij
S 
ij
S is the required separation time between aircraft i and aircraft j  Each 
aircraft must land within the bounds of the time window 
iE 
iL  
Delay time of aircraft can be defined as 1 For each aircraft extra cost caused by 
delay should be defined as 2 

 

0

1

i
i
i
i
i
t
T
L
t
T
i
else
d
i
N
−
 
=
∀ ∈
 
 1


 

 
i
i
i
i
i
i
i
i
i
i
i
i
g T
t
E
t
T
i
h t
T
T
t
L
c
−
 
−
 
=
  
1

i
N
∀ ∈
 
2
i
C is the extra cost for aircraft i 
it stands for the time allotted that aircraft will land 
on the runway 
ig is the penalty cost positive per unit of time for landing before 
iT for aircraft i  
ih  is the penalty cost positive per unit of time for landing 
after
iT for aircraft i  
Objective function of ALS problem can be described as equation 3 4 
 
1
min
N
i
i
f
= d
=

 
 3
1
min
N
i
i
f
= C
=

 
 4
3 
Semantic Agent Negotiation Mechanism SANM 
The nextgeneration air transportation system should allow airplanes to change their 
flight paths during the flight without approval from a centralized en route control 
With the development of aerotechnics and requirement of free flight the solution of 
ALS problem should consider not only the macro aspect but also the status of every 
aircraft So the centralized system cannot satisfy the development tendency and a 
distributed system is necessary In this paper a solution based on MultiAgent System 
is proposed semantic agent negotiation mechanism The method has two steps 1 
every aircraft uses algorithm here we select genetic algorithm discussed in section 
V to get a good scheduling 2 aircrafts use the negotiation mechanism to optimize 
the scheduling to satisfy their own benefits  
SANM is based on MultiAgent System and described in Figure 1 

1

i
i
N
A
∈
represents the aircraft agent Controller agent and aircrafts agent use Agent 
Communication language ACL to exchange message Genetic Algorithm 
Negotiation strategy and Proposal Generation Mechanism are invoked by aircraft 
 
Aircraft Landing Scheduling Based on Semantic Agent Negotiation Mechanism 
485 
agent Genetic Algorithm is used to get an initial solution Proposal Generation 
Mechanism is invoked to create proposal Negotiation strategy supplies the rule 
abides by aircraft agent when consulting proposal  
In order to simplify the model of SANM we describe the model with a tuple 5 

   
M
= Ag GA S I V R
  
5
 1

n 
Ag
A
A C
=
is the set of aircraft agents and controller agent GA is Genetic 
Algorithm 
S
is 
negotiation 
strategy 
I
is 
proposal 
generation 
mechanism
1 

p
p
p
n
c
V
v
v
v
=
  
ivp
is the valuation of aircraft agent i for 
proposal p  R reserves the best scheduling at present 
The detail of the algorithm is described as follow 
 
1
iA  uses GA to get a scheduling  
2
iA  sends its scheduling to other aircraft agents and controller agent 
3 Aircraft agents and controller agent grade the scheduling according to their 
own benefits The less delay and external cost the higher grade will be given 
4 Calculate the score for each landing scheduling The scheduling which gets the 
highest score will be selected as initial solution 
5 Initial solution is not suitable for all aircrafts Aircraft agent uses the proposal 
generation mechanism I to create the proposal for initial solution Proposal generation 
mechanism will be discussed in section 5 
6 All the aircrafts use the negotiation strategy S to decide which proposal will  
be accepted V will be used in this step Negotiation strategy will be discussed in 
section 5 
7 If the terminal conditions time constraint or negotiation suspension are not 
satisfied go to 5 otherwise reserve the scheduling in R  and then return R  
 
 
 
Fig 1 SANM 
4 
Genetic Algorithm for ALS Problem 
Genetic Algorithm GA is adaptive heuristic search algorithm premised on the 
evolutionary ideas of natural selection and genetic In this section we give genetic 
486 
Z–T Huang et al 
algorithm with novel constraints for ALS Problem Chromosome coding which 
describes the landing scheduling is defined as Table 1 Consider that N aircrafts will 
land on three runways In the first row of table 1 the sequence number of aircrafts is 
assigned according to the order of predicted landing time The corresponding runways 
are given in the second row 
Table 1 Chromosome coding 
Aircraft 
1 
2 
3 
4 
5 
… 
N1 
N 
Runway 
2 
1 
3 
1 
2 
… 
3 
2 
 
The chromosome coding has to satisfy two constraints  
1 The load of the runways should be balanced  
In formula 6 
ir represents the number of aircrafts assigned in the ith runway 
N and R  stand for the total number of aircrafts and runways respectively The usage 
rate of the runway should be in a certain range 1
1

R
a
R
a
−
+
  
1
01
ir
N
− R
≤
 
0
1
i
R
∈
−
 
 6
2 Security constraint Three or more aircrafts in continuous time can not be 
assigned in the same runway Formula 7 describes the security constraint 
and
iP represents the runway assigned to the aircraft i 
1
2
i
i
P
P
+
+
≠
 If
1
i
i
P
= P +

1

i
N
∈
 
 7
5 
Proposal Generation Mechanism and Negotiation Strategy 
51 
Proposal Generation Mechanism 
When a landing scheduling is send to an aircraft agent the agent will evaluate the 
scheduling and put forward a set of aircrafts that can swap the runway with it For 
example in table 2 aircraft 17 is put on runway 2 A set of aircrafts it can swap with 
is 14 15 16 18 19 The aircraft that can swap with it must between 13 and 20 
because of time constraint Then aircraft 17 calculates the delay and cost if it  
swaps the runway with 14 15 16 18 and 19 respectively Suppose it swaps the 
runway with aircraft 16 the value of object function 4 is less than previous 
scheduling Finally aircraft 17 will put forward a proposal swap runway with  
aircraft 16  
52 
Negotiation Strategy  
In our model Strategies are adopted to direct the adaptive negotiation process The 
negotiation strategies are expressed in OWL since it has a textbased representation 
which imposes few restrictions on protocols and through the use of OWL schema it  
 
 
Aircraft Landing Scheduling Based on Semantic Agent Negotiation Mechanism 
487 
has a sufficiently strict syntax to permit automated validation and processing of 
information in an unambiguous way Negotiation strategies allow selections from a 
range of proposal provided by aircraft agents Figure 2 shows sequence decides the 
proposal according to semantic descriptions  
 
Table 2 Example of Landing Scheduling 
ID 
Times 
Runway 
ID 
Times 
Runway 
Runway1 
Runway2 
Runway3 
1 
56 
3 
11 
346 
3 
3 
2 
1 
2 
113 
2 
12 
378 
2 
4 
5 
6 
3 
113 
1 
13 
400 
2 
8 
12 
7 
4 
122 
1 
14 
448 
1 
9 
13 
10 
5 
155 
2 
15 
467 
1 
14 
17 
11 
6 
156 
3 
16 
491 
3 
15 
20 
16 
7 
218 
3 
17 
500 
2 
19 
 
18 
8 
237 
1 
18 
653 
3 
 
 
 
9 
327 
1 
19 
679 
1 
 
 
 
10 
332 
3 
20 
705 
2 
 
 
 
 
 
 
 
 
 
 
 
Fig 2 Execution Step 
6 
Experiment 
We write the program of SANM with Visual Studio 2010 To fully investigate the 
effectiveness of our algorithm we compare it with the method first come first service 
FCFS on the given dataset In table 3 we give 20 aircrafts which will land on three 
runways S L and H stand for small middle and heavy plane respectively Previous 
earliest and latest landing time of aircrafts are given in table 3 
First of all every aircraft works out a landing scheduling by using genetic 
algorithm Total delay of each scheduling is given in Figure 3 We can see that the 
landing scheduling provided by aircraft 6 has the least delay time 550s Then we will 
use proposal generation mechanism and negotiation strategy to optimize the initial 
solution The final result is given in the third part of the Table 4 
Aircraft 1 
Aircraft 2 
1 Create 
2 Send proposals to Aircraft 2 
3Evaluate the proposal 
4 Send back the value of evaluation 
  5 Select the best scheduling 
488 
Z–T Huang et al 
 
Fig 3 Dealy time of every aircraft 
Table 3 Real time data 
Serial 
Number 
Type 
Times 
Earliests 
Latests 
Serial 
Number 
Type 
Times 
Earliests 
Latests 
1 
S 
18 
0 
50 
11 
L 
343 
300 
380 
2 
H 
37 
0 
60 
12 
H 
398 
350 
440 
3 
H 
53 
10 
80 
13 
H 
487 
460 
520 
4 
H 
68 
20 
100 
14 
S 
489 
460 
520 
5 
L 
69 
20 
100 
15 
L 
500 
470 
540 
6 
S 
134 
100 
170 
16 
H 
596 
550 
640 
7 
S 
138 
100 
170 
17 
L 
619 
580 
670 
8 
S 
182 
150 
210 
18 
H 
635 
590 
680 
9 
S 
240 
200 
280 
19 
S 
642 
600 
680 
10 
H 
255 
210 
290 
20 
H 
719 
680 
760 
Table 4 FCFS GA and Negotiation 
FCFS 
ID 
Delays 
GA 
ID 
Delays 
Negotiation 
ID 
Delays 
Schedulings 
Runway1 
 
 
 
 
1 
0 
Runway1 
 
 
 
 
2 
0 
Runway1 
 
 
 
 
2 
0 
0 
4 
24 
4 
63 
5 
82 
114 
7 
121 
8 
116 
8 
107 
252 
10 
78 
11 
29 
11 
20 
343 
13 
0 
15 
0 
15 
0 
500 
16 
0 
17 
0 
17 
0 
619 
19 
121 
20 
0 
20 
0 
719 
Runway2 
 
 
 
 
2 
0 
Runway2 
 
 
 
 
3 
0 
Runway2 
 
 
 
 
3 
0 
10 
5 
82 
5 
98 
4 
79 
104 
8 
107 
10 
0 
10 
0 
255 
11 
20 
12 
0 
12 
0 
398 
14 
12 
13 
5 
13 
5 
492 
17 
0 
16 
0 
16 
0 
596 
20 
0 
18 
55 
18 
55 
690 
Runway3 
 
 
 
3 
0 
Runway3 
 
 
 
1 
0 
Runway3 
 
 
 
1 
0 
18 
6 
86 
6 
0 
6 
0 
116 
9 
78 
7 
94 
7 
94 
214 
12 
0 
9 
90 
9 
90 
312 
15 
12 
14 
0 
14 
0 
489 
18 
0 
19 
0 
19 
0 
642 
 
Aircraft Landing Scheduling Based on Semantic Agent Negotiation Mechanism 
489 
In Table 5 we list the delay time of different methods which solve the ALS 
problem It is obvious that our method has the better optimization performance 
compared with FCFS and GA  
Table 5 Data Compare 
 
FCFS 
GAMax 
GAMin 
SANM 
Delays 
741 
628 
550 
532 
Cost 
13770 
12960 
11950 
10320 
7 
Conclusion 
In this paper an algorithm of SANM is proposed to solve aircraft landing scheduling 
problem Two techniques Proposal Generation Mechanism and Negotiation Strategy 
are designed in our algorithm The simulation results show that this method is 
reasonable and it can reduce the total time delay optimize the approach sequencing 
The modeling and decision process of this methodology is easy to realize and suitable 
to be used in terminal area aircraft approach sequencing it can help the controllers 
make a quick decision and reduce their workload if being used  
 
Acknowledgements This study is supported by grants from National Natural Science 
Foundation of China and Civil Aviation Administration of China61039001 and 
Tianjin Municipal Science and Technology Commission11ZCKFGX04200 
References 
1 
Ciesielski V Scerri P An Anytime Algorithm for Scheduling Aircraft Landing Times 
Using Genetic Algorithms Australian Journal of Intelligent Information Processing 
System 4 206–213 1998 
2 
Beasley JE Krishnamoorthy M Sharaiha YM Abramson D Scheduling Aircraft 
Landings  the Static Case Transportation Science 34 180–197 2000 
3 
Xu XH Yao Y Application of Genetic Algorithm to Aircraft Sequencing in Terminal 
Area Journal of Traffic and Transportation Engineering 4 121–126 2004 
4 
Yang Q Scheduling Arrival Aircrafts on Multiple Runways Based on an Improved 
Genetic Algorithm Journal of Sichuan University 38 65–70 2006 
5 
Zhang H Hu M Multirunway Collaborative Scheduling Optimization of Aircraft 
Landing Journal of Traffic and Transportation Engineering 9 115–120 2009 
6 
Rong J Geng S Valasek J Ioerger TR Air Traffic Control Negotiation and 
Resolution Using an Onboard Multiagent System In Proc Digital Avionics Syst Conf 
vol 2 pp 7B21–7B212 2002 
7 
Pěchouček M Šišlák D Agentbased Approach to Freeflight Planning Control and 
Simulation IEEE Intell Syst 241 14–17 2009 
8 
Lomuscio R Wooldridge M Jennings NR A Classification Scheme for Negotiation 
in Electronic Commerce Int Journal of Group Decision and Negotiation 121 31–56 
2003 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 490–496 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A RealTime Posture Simulation Method  
for HighSpeed Train 
Huijuan Zhou12 Bo Chen2 Yong Qin2 and Yiran Liu1 
1 College of Mechanical and Electronical Engineering North China University of Technology 
Beijing China 
2 State Key Laboratory of Rail Traffic Control and Safety Beijing Jiaotong University  
Beijing China 
zhhjuansinacom 2828900163com 276356856qqcom 
qinyong21246126com 
Abstract A method of realtime posture simulation aimed to high speed mov
ing train is presented This method decomposes the full train length in smaller 
train elements so that dynamic posture simulation of train is transformed into 
posture simulation of each part of the train Then according to the realtime  
locomotive mileage track geometry and the center mileage yaw angle and roll 
angle can be calculated for simulate the posture of each part The whole train 
posture is obtained through connected each part’s posture together in space
time continuum The method is simple accurate efficient and practical and is 
used in actual system of comprehensive inspection train visualization 
Keywords highspeed train real time posture simulation 
1 
Introduction 
With the successful operation of JingJin Intercity Railway BeijingShanghai High
speed Railway and other highspeed railways Chinese highspeed railway is now 
developing rapidly Realtime posture of the highspeed moving train has an impor
tant effect on realtime train monitoring and operation safety For the limits of moni
toring equipment cost and data transmission rate the realtime train’s posture can’t be 
obtained However the computer simulation technology makes it possible for real
time posture simulation of the highspeed moving train with the development in com
puter 
There are many methods for realtime posture simulation such as active distur
bance rejection control technique1 and neuralpredictive controller2 in satellite 
attitude simulation system the timedomain response method based on stochastic 
process theory of posture simulation of warship3 the method of bodywork posture 
simulation based on course simulation4 the sixdegreeoffreedom movement post
ure simulation5 In train posture simulation it’s usually used to build solid model by 
Simpack and AdamsRail or establish dynamic equations by MatlabSimulink 
But Simpack and AdamsRail is very difficult to solve the problem for multi
contacts collision and largescale systems6 Each MatlabSimulink module is a 
black box for the user and its graphical user interface can’t show the realtime 3D 
 
A RealTime Posture Simulation Method for HighSpeed Train 
491 
posture of the train78 Hence the present simulation software does not display real
time posture of the highspeed moving train the railway line and the surrounding 
buildings with the highprecision Especially it’s impossible to realize above func
tions for these simulation software in BrowseServer model This paper puts forward a 
method combined vehicle dynamics parameters with 3D GIS and BS architecture to 
simulate realtime posture aimed to the highspeed moving train 
2 
Algorithm for RealTime Posture Simulation  
For rail vehicle system there are six kinds of basic carbody movement lateral oscilla
tion bounce response stretching vibration yaw motion pitch response and roll  
motion 
Lateral oscillation roll and shake motion of carbody are called the lateral vibration 
Because of the elastic constraints of the vehicle structure and the suspension system 
the lateral oscillation and roll vibration always couple together Bounce and pitch 
response happen in longitudinal vertical plane belonged to vertical vibration Bounce 
response is the vibration of the pre and post bogies with the same phase and amplitude 
in the vertical plane Otherwise if the vertical vibration direction of the pre and post 
bogies is opposite the vibration is pitch response The longitudinal stretching vibra
tion of carbody generally appears when rail vehicle start brake driving and shunting 
which is closely related to train dynamics911 Therefore this paper proposed the 
realtime posture simulation method considered kinetic parameters such as pitch angle 
β  yaw angle ϕ  and roll angle φ  which would influence the realtime posture of 
the train and didn’t consider the influence of longitudinal vibration 
The method can simulate the realtime posture of highspeed moving train in 3D 
with vivid and continuum spacetime The flow chart of posture simulation is shown 
in Figure 1 
Firstly the train is divided into multiple parts so that posture simulation of train is 
transformed into posture simulation of each part of the train And then in the case of 
known mileage of the locomotive each part’s center mileage is calculated Thirdly 
according to the track geometry at the center point of each part the realtime posture 
simulation method of each part is determined Finally realtime posture of each part 
of the train connected together in spacetime continuum will fulldimensionally simu
late the realtime posture of highspeed moving train Specific steps are as follows 
1 The train model is divided into n  parts from head to tail shown in Fig 2 and 
posture simulation of the train is transformed into posture simulation of each part of 
the train  
2 Train length is d  then each part of the train length is 
d n
 Train head mileage 
is 
0s  Then the central mileage of 
 
210

n
i i

=
 part is 
1

2
0
−
−
−
=
n i
d
n
d
s
s
 
1
 
492 
H Zhou et al 
 
Fig 1 Flow chart of highspeed train posture simulation 
 
Fig 2 Train divided into n  parts 
3 In the process of highspeed train movement realtime posture simulation me
thod of each part of the train in realtime mileage S is as follows 
 
h  is the distance from the part center to the center point of left and right track 
θ  is the horizontal angle of left and right track center and  
θ = arcsin v2 gr
 
shown in Fig 3Where v  is design speed g  is gravity acceleration r  is 
track curve radius and α  is track curve tangent direction angleshown in  
Fig 4 
 
A RealTime Posture Simulation Method for HighSpeed Train 
493 
•
•
•
L
R
O
θ
h
 
Fig 3 Horizontal angle θ  and the distance h  
x
y
α
 
Fig 4 Track curve tangent angle α  
 




L
L
L
z
y
L x
 is the coordinate of the left track center point 




R
R
R
z
y
R x
 
is the coordinate of the right track center point The coordinate of the center O  
of the part is  
cos 
2

2
sin 
2

   
θ
θ
•
+
+
+
•
−
+
=
h
z
z
y
y
h
x
x
O x y z
R
L
R
L
R
L
 
2
 
Assuming that the train wheels and tracks contact intimately and there isn’t or 
minor pitch response then the pitch angle 
β = 0
 and set yaw angle 
ϕ = α
 
roll angle 
φ = θ
 
3 
Application 
In a ‘863’ project about visualization system of highspeed comprehensive inspection 
train based on 3D WebGIS the simulation method above is used to simulate the  
494 
H Zhou et al 
realtime posture when the inspection train is running in Jinghu highspeed railway 
The system has been applied in comprehensive inspection center and the following is 
actual application  
1 The inspection train model is divided into n=100 parts from head to tail so that 
posture simulation of the train is transformed into posture simulation of each part of 
the train 
2 Train length is 
m
d
= 80
 each part of the train length is 
m
d n
80
80 100
=
=
 At a given time the mileage of train head is 
km
s
0 = 120
 Then the central point mileage of 
 
210

n
i i

=
 part is 

1
0 0004
9996
119
1

2
0
km
i
n i
d
n
d
s
s
−
−
=
−
−
−
=
 
3
 
3At this moment track curve tangent angle is 
α = π 4
at realtime mileage 
10

119996
=
=
km i
s
 Realtime posture simulation method of the part 
i = 10
 
is as follows 
 
The distance from the part center to the center point of left and right track is 
m
h
52
=
 
m s
km h
v

100

360
=
=
 
 2
10
m s
g =
 
m
r
= 3000
 then 
the horizontal angle is  
0 3398
3000
10
100
arcsin
arcsin
2
2
=
×
=
=
gr
v
θ
 
4
 
The coordinate of the left track center L  is

56
12




=
L
L
L
z
y
x
 the 
coordinate 
of 
the 
right 
track 
center 
R
is 

5565
13




=
R
R
R
z
y
x




135655
R
R
R
x
y
z
=
 and the coordi
nate of the center O  of the part is  
6071
76
119168

cos 
2

2
sin 
2

  

=
•
+
+
+
•
−
+
=
θ
θ
h
z
z
y
y
h
x
x
O x y z
R
L
R
L
R
L
 
5
 
Yaw angle 
π 4
α
ϕ
=
=
 and roll angle 
φ = θ = 0 3398
  
Realtime dynamic posture simulation of each part of the train is obtained based 
above method and the simulation effect is showed in Fig4 In this project realtime 
train information including GPS mileage and velocity is transmitted every 5 seconds 
from train to the ground center The train’s position and posture are drived by the 
accepted realtime data and updated in accordance with 24 frames every one second 
In some certain the train’s position is delayed for the limit of transmission time and  
 
 
A RealTime Posture Simulation Method for HighSpeed Train 
495 
 
Fig 5 Actual posture simulation for highspeed comprehensive inspection train 
transmission rate So there are some other algorithms to calculate and estimate the 
next position and velocity according to the fore data accepted12 The system  
requires Inter Core2 multicore CPU 4G RAM 1T hard disk and NVIDIA Series 
GeForece8800 Graphics card 
4 
Conclusions 
The method advanced in this paper decomposing the full train length in smaller train 
element and using the same realtime posture simulation method aimed at each train 
element reduces the simulation difficulty of realtime highspeed train posture and 
improves the simulation accuracy At the same time the method has the advantages of 
simply calculation practical and efficient for its simulation according to curve charac
teristics of track and geometric features of train Combined 3D GIS and 
BrowseServer the highspeed train posture and its position velocity can be showed 
in a vivid 3D platform The actual application has proved the method accuracy and 
efficiency 
Acknowledgement The authors wish to acknowledge the support and motivation 
provided by the 863 project NO0912JJ0203 and Beijing college students scientific 
research and entrepreneurial action project  
References 
1 Lian M Han ZY Fu HY Application of Active Disturbances Rejection Control 
Technique to Satellite Attitude Simulation System Optics and Precision Engineer
ing 183 616–622 2010 
2 Li YG Han JG Li K et al NeuralPredictive Controller and Its Application to Satel
lite Attitude Simulation System Computer Measurement  Control 115 359–364 2003 
3 Qin XC Mao P Lu HM The Simulation of One Warship’s Main Motion Gestures 
Computer Simulation 154 59–51 1998 
4 Dong ZH Cheng YZ Fu Q et al Producing Method of Bodywork Attitude Informa
tion Based on Road Simulation Electronic Measurement Technology 273 57–67 2002 
496 
H Zhou et al 
5 Hao YN Wang JZ Wang SK A Study on a SixDegreeofFreedom Platform Jour
nal of Beijing Institute of Technology 223 331–334 2002 
6 Zhu HR Chen G et al Probe into Method of Safety Analysis for Recreation Facilities 
Based on Virtual Prototype Technology China Safety Science Journal 143 12–15 2004 
7 Seeking technology MATLAB 70 from the Entry to the Master Posts  Telecom Press 
2006 
8 Zhou LN Tang GJ Luo YZ Simulation Framework for Spacecraft Attitude Dynam
ics and Control Based on MatlabSimulink Journal of System Simulation 1710 2517–
2524 2005 
9 Han BL Chen ZY Lu ZJ Online Detection of Offset Generated by Vibration of 
Rolling Stock Based on Machine Vision 394 787–792 2008 
10 Jiao S Railway Vehicles Elastic Vibration Analysis Method Dalian Jiaotong University 
Master’s Degree Thesis 2008 
11 Zhong CM Truck Center of Gravity Position of the Roll Angle Detection Rolling 
Stock 11 13–15 1993 
12 Zhou HJ Qin Y Xing ZY et al Design and Implementation of Comprehensive In
spection Data Visualization System Based on 3D GIS In The 1st International Confe
rence on Railway Engineering Highspeed Railway Heavy Haul Railway and Urban Rail 
Transit pp 618–623 2010 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 497–504 2012 
© SpringerVerlag Berlin Heidelberg 2012 
HighOrder Terminal SlidingMode Observers  
for Anomaly Detection 
Yong Feng12 Fengling Han3 Xinghuo Yu1 Zahir Tari3 Lilin Li1 and Jiankun Hu4 
1 School of Electrical and Computer Engineering RMIT University  
Melbourne VIC 3000 Australia 
yongfengxyue88727rmiteduau  
2 Department of Electrical Engineering Harbin Institute of Technology Harbin 150001 China 
yfenghiteducn 
3 School of Computer Science and Information Technology RMIT University 
Melbourne VIC 3000 Australia 
fenglinghanzahirtarirmiteduau 
4 School of Engineering and Information Technology University of New South Wales at the 
Australian Defence Force Academy Canberra ACT 2600 Australia 
jhuadfaeduau 
Abstract This paper proposes a highorder terminal slidingmode observer 
used for the anomaly detection in TCPIP networks It can track the fluidflow 
model representing the TCPIP behaviors in a router level A smooth control 
signal of the observer can be generated based on the highorder slidingmode 
technique for estimation of the queue length dynamics in the router The distri
buted anomaly in the TCPIP network incurred by an abnormal behavior can be 
detected using the smooth control signal The proposed scheme requires only 
the average queue length in a router for anomaly detection The simulations are 
presented to verify the effectiveness of the proposed method 
Keywords TCPIP network model congestion control observers sliding 
mode control 
1 
Introduction 
Anomaly detection aims at identifying cases when activities deviate from the normal 
Anomalous events occur less frequently however once it occurs the consequences 
could be quite dramatic Therefore abnormal detection has significant meaning in the 
security of Internet community thus attracts outstanding researches in areas of artifi
cial intelligence machine learning state machine modeling and statistical approaches 
etc 1 Main anomaly detection methods include statistical methods expert systems 
clustering neural networks support vector machines outlier detection schemes etc 
2 Control theory methods among others have demonstrated great effectiveness in 
monitoring network traffic Since a network anomaly can be considered as a perturba
tion in the traffic flow of the router level an observer can be designed to estimate the 
network anomaly  Therefore the observer can collaborate with Active Queue Man
agement AQM in the router 3 to detect and estimate anomalies 47 
498 
Y Feng et al 
 
For avoiding congestion collapse on the Internet or restoring a congested computer 
network to a normal state congestion control and network congestion avoidance 
techniques have been utilized widely in router level A router in TCPIP networks has 
two different functions AQM and anomaly detection The latter can be implemented 
using an observer The anomaly detection is essential in enterprise and provider net
works for diagnosing events like attacks or failures which severely impact perfor
mance security and Service Level Agreements SLAs 8  
Thanks to the dynamic model of the TCPIP traffic flow 9 10 and the contribu
tion of control theoretic analysis of random early detection RED 11 in which li
nearized the interconnection of TCP and a bottlenecked queue The original work of 
designing the AQM control system using the RED scheme opens a new area of tradi
tional control theory Since then traditional control methods Proportional P and 
ProportionalIntegration PI controllers were first used in the queue utilization and 
delay in network traffic 3 4 Soon after the feedback control was introduced to 
address the stability issue and to cope with the timevarying nature of the multiple 
delays in 5 The feedback control mechanism ensures the regulation of the queue 
size of the congested router as well as flow rates to a prescribed level  
The construction of an observer allows the reconstruction of the system states so 
that the disturbance deemed as an intrusion can be identified in the router level Re
cently following the work in observer based network anomaly estimation some new 
anomaly detection methods have been proposed Among them slidingmode observ
ers 1216 demonstrated significant potentials In 12 an observer was proposed to 
detect constant anomalies for TCPAQM networks In 13 a new technique based on 
control theory and the construction of observers was developed using a simplified 
model describing the average dynamics of the TCP congestion window size and the 
queue length at the router In 14 an unknown slidingmode observer for anomaly 
detection in TCPIP networks was proposed to distinguish falsetrue positives and 
falsetrue negatives in a prescribed finite time In 15 a secondorder sliding mode 
observer for detecting anomalies in TCP networks was investigated based on a fluid 
model of TCP network Slidingmode control SMC is a nonlinear control method 
which alters the dynamics of a nonlinear system using a discontinuous control signal 
and forces the system to slide along a predesigned slidingmode manifold 1719 
Slidingmode observers apply SMC strategies so that they have some significant ad
vantages compared to other observers such as strong robustness fast response and 
high precision  
Following our previous work 16 this paper proposes a highorder terminal slid
ingmode observer used for the anomaly detection in TCPIP networks The input of 
our observer is based on the information measurement and parameters of a router 
such as the TCPIP congestion window size and the queue length With the help of the 
highorder slidingmode technique a smooth control signal of the observer can be 
generated automatically and used for estimation of the queue length dynamics which 
represents an anomaly in the TCPIP network incurred in the router Simulations are 
carried out and the estimation results are analyzed to validate the proposed method 
 
HighOrder Terminal SlidingMode Observers for Anomaly Detection 
499 
 
2 
Simplified Dynamic Model of a TCPIP Network 
It is well known that the TCPIP model is a descriptive framework for the Internet 
Protocol Suite of computer network protocols The TCPIP model has 5 layers physi
cal data link network transport and application layers  Computer network topolo
gy is the physical communication scheme used by connected devices It is the layout 
pattern of interconnections of the various elements nodes links peripherals etc of 
a computer network There are five basic types of network topologies and hybrid 
topologies the former include bus ring star mesh and tree the latter are a combina
tion of two or more of the four basic topologies  
This paper considers a TCPIP network topology as shown in Fig1 where N ho
mogeneous sources connect to a destination through a router which links computers 
to the internet Routers in TCPIP networks play an important role during the time of 
the network congestion In Fig1 the router can include two different mechanisms an 
AQM and an observer for anomaly detection Packets are the basic unit of transmis
sion on the Internet AQM controls the length of the packet queues for managing the 
congestion by dropping the packets when necessary The observer can detect the con
gestion window of the router in the TCPIP network and further detect the abnormal 
behavior in the network 
 
 
Internet
TCP senders
Anomaly 
traffics
Router
TCP receivers
Router
Observer
AQM
detection
regulation
 
Fig 1 TCP network topology 
The TCPIP network topology shown in Fig1 can be described using a simplified 
TCPIP model It can be expressed by the following coupled nonlinear timedelay 
differential equations 4 
 
500 
Y Feng et al 
 








+
=
+
−
=
−
−
−
−
=
Tp
C
q t
t
R
d t
C
R t N
W t
t
q
R t
p t
R t
R t
R t
W t W t
R t
t
W
 


 
 
 


 

 

 

2
 
 
1




    
1
where Wt is the average TCPIP window size in packets qt the average queue 
length in packets Rt the roundtrip time in seconds C the link capacity in pack
etssec Tp the propagation time delay in seconds N the load factor number of 
TCPIP sessions pt the probability of the packet mark and dt the queue length 
dynamics  
In 1 dt represents an additional traffic which perturbs the normal TCPIP net
work behaviors in the router level In normal condition of the TCPIP networks dt is 
around zero but when an anomaly intrusion happens it will suddenly increase There
fore the anomaly intrusion in the TCPIP networks can be detected by monitoring dt 
based on the measurement and the parameters of the router 
The equilibrium point of system 1 W0 q0 p0 can be defined by 
W = 0
 and 
q = 0
as 
2
0
02
W p =
 W0=R0CN R0=q0C+Tp After determining the equilibrium 
point system 1 can be linearized around its equilibrium point W0 q0 p0 as follows 
2 
2
0
0
0
0
2
2
2
0
0
0
0
1
 

 



 




2
1
 
 
 
RC
N
W t
W t
W t R
q t
q t R
p t R
R C
R C
N
N
q t
W t
q t
R
R
δ
δ
δ
δ
δ
δ
δ
δ
δ

=−
+
−
−
−
−
−
−


=
−



2 
where δW= W−W0 δq= q−q0 δp= p−p0  
Define a new state variable xt= δWt output yt= δqt and input ut= δpt 
Then the TCPIP network 2 can be linearized to a timedelay system 12 



+
+
=
−
+
−
+
+
−
+
=
 
 
 
 




 


 
 
d t
Hy t
Gx t
y t
h
E u t
h
D y t
Dy t
h
M x t
Mx t
x t
d
d
d


   
3
where
2
  0

d
M
M
N
R C
=
= −

2
1  0

D
R C
= −

2
1  0

Dd
R C
=
 
2
2
0
 2

Ed
R C
N
= −
 
 0
G
= N R
 
1 0
H
R
= −
 The simplified model 3 can be used for the feedback con
trol of the router management and the TCPIP network stability 9 15 In addition 
this model can be utilized for the anomaly detection in the TCPIP networks 1216  
 
HighOrder Terminal SlidingMode Observers for Anomaly Detection 
501 
 
The objective of the paper is to design a smooth control signal of an observer of 
system 3 for estimating the queue length dynamics dt which represents an ano
maly in the TCPIP network  
3 
Design of HighOrder SlidingMode Observer 
For estimating dt in the simplified dynamic model of the TCPIP network 3 a 
slidingmode observer of the system 3 is proposed as follows 



+
+
=
−
+
−
+
+
−
+
=
                     
 
ˆ 
ˆ 




 

ˆ
ˆ 

ˆ
v t
Hy t
Gx t
y t
h
E u t
h
D y t
Dy t
h
M x t
Mx t
x t
d
d
d


 
4
where 
xˆ t
 and 
yˆ t
 represent the state estimation for the system states xt and yt 
respectively and vt is the control signal of the observer  
Define the errors between the estimations and the true states as 
ˆ 
 
 
x t
x t
ex t
−
=
 
ˆ 
 
 
y t
y t
ey t
−
=
 The error observer equation can be obtained from 3 and 4 as 
 
 


 
 
 
 
x
x
d
x
y
x
e t
Me t
M e t
h
e t
Ge t
v t
d t
=
+
−


=
+
+



 
5
It is assumed that system 3 and its observer 4 satisfy the following assumptions 
Assumption 1 The derivative of the additional traffic signal dt is bounded 
dm
d t
  ≤
 
6
where dm is a positive constant 
Assumption 2 The control signal vt in 4 satisfies the following condition 
vm
Tv t
  ≤
 
7
where both T and vm are also two positive constants respectively 
Theorem 1 The observer error ey and its derivative in system 5 will converge to 
zero in finite time if a TSM manifold is chosen as 8 and the control signal vt is 
designed as 912 
 
 
 

t
e
e t
s t
p q
y
y
+ β
= 
 
8
 
 
 
v t
t
v
v t
n
eq
+
=
    
9
 
 
 

t
e
Ge t
t
v
p q
y
x
eq
+ β
=
 
10
 
 
 
w t
Tv t
t
v
n
n
=
+

       
11
502 
Y Feng et al 
 

 
sgn
 
s t
K
w t
= −
 
12
where K=vm+dm+η η0 both vm and dm are defined in 6 and 7 β  0 is a constant 
After reaching the slidingmode surface st=0 it can be seen from 8 that both 
ey t
 and 
ey t
 will converge to zeros within finite time Then the estimation of the 
additional traffic dt can be obtained from 5 
 
ˆ 
v t
d t
= − n
 
13
Therefore dt can be estimated using the smooth control signal of the observer 
4 
Simulations 
In order to evaluate the effectiveness of the proposed method for the anomaly detec
tion in the paper some simulations are carried out The parameters of the system 3 
for simulations are assumed as follows 
N=60 TCP sources 
C=3750 packetss Tp=02s R0=02045 q0 = 1708 M = Md 
= −03824 D = −00064 Dd=00064 Ed= −3995208 G=2933201 H= −48887 
The queue length dynamics dt in the simulation is assumed to be a square wave
form signal described by the function 





1
50
10
5 sgn sin
 
+
−
=
π
πt
d t
 
14
A TSM observer is designed based on Theorem 1 and the design parameters are β = 
1 p = 5 q = 3 η =10 The simulation results are shown in Figs2 and 3 The estima
tion result of the queue length dynamics dt in 3 is shown in Fig 2 and the 
zoomedin view of the estimation is shown in Fig3 It can be seen that dt can be 
estimated using the proposed method  
 
0
10
20
30
40
50
60
70
80
5
0
5
10
15
time sec
additional traffic and its estimation
dt

dt
 
Fig 2 Estimation of the queue length dynamics 
 
HighOrder Terminal SlidingMode Observers for Anomaly Detection 
503 
 
4
6
8
10
12
14
16
5
0
5
10
15
time sec
additional traffic and its estimation
dt

dt
 
Fig 3 Zoomedin view of Fig2 
As aforementioned the queue length dynamics dt represents the additional traffic 
perturbing the normal TCP network behaviors in the router level During the normal 
condition there is no anomaly appears in the TCPIP network dt is around zero 
When an anomaly happens dt will rise beyond the normal range From the simula
tion results it can be seen that dt can be estimated quickly and accurately which 
means that a distributed anomaly in the TCP network can be detected using the pro
posed method 
5 
Conclusion 
This paper has proposed a highorder slidingmode observer used for the anomaly 
detection in the TCPIP networks The observer can track the fluidflow model 
representing the TCPIP behaviors in a router The highorder slidingmode controller 
is designed for the observer and the smooth control signal of the observer can be ob
tained for estimation of the queue length dynamics in the router which represents a 
distributed anomaly in the TCPIP network The proposed anomaly detection scheme 
requires only one signal which is the average queue length in a router The simulation 
results have shown the effectiveness of the proposed anomaly detection method  
Acknowledgement This work was supported in part by the National Natural Science 
Foundation of China 61074015 and also in part by ARC Linkage Project 
LP100200538 of the Australian Research Council 
References 
1 Labit Y Gouaisbaut F Ariba Y Network Anomaly Estimation for TCPAQM net
Works Using an Observer In The Third International Workshop on Feedback Control 
Implementation and Design in Computing Systems and Network Annapolis USA pp 63–
68 2008 
504 
Y Feng et al 
 
2 Steinwart I Hush D Scovel C A Classification Framework for Anomaly Detection 
Journal of Machine Learning Research 6 211–232 2005 
3 Ryu R Rump C Qiao C Advances in Active Queue Management AQM based TCP 
Congestion Control Telecommuniaction Systems 4 317–351 2004 
4 Ariba Y Gouaisbaut F Rahme S Labit Y Robust Control Tools for Traffic Monitor
ing in TCP networks In 18th IEEE International Conference on Control Applications 
Saint Petersburg Russia pp 525–530 2009 
5 Hollot C Misra V Towsley D Gong W On Designing Improved Controllers for 
AQM Routers Supporting TCP Flows In IEEE INFOCOM Anchorage AK USA vol 3 
pp 1726–1734 2001 
6 Hollot C Misra V Towsley D Gong W Analysis and Design of Controllers for 
AQM Routers Supporting TCP Flows IEEE Trans Automatic Control 476 945–959 
2002 
7 Ariba Y Gouaisbaut F Labit Y Feedback Control for Router Management and 
TCPIP Network Stability IEEE Trans Network and Service Management 64 255–266 
2009 
8 Kind A Stoecklin MP Dimitropoulos X Histogrambased Traffic Anomaly Detec
tion IEEE Trans Network and Service Management 62 1–12 2009 
9 Misra V Gong W Towsley D Fluidbased Analysis of a Network of AQM Routers 
Supporting TCP Flows with an Application to RED In ACMSIGCOMM Stockholm 
Sweden vol 304 pp 151–160 2000 
10 Firoiu V Borden M A Study of Active Queue Management for Congestion Control In 
IEEE INFOCOM Tel Aviv Israel vol 3 pp 1435–1444 2000 
11 Hollot C Misra V Towsley D Gong W A Control Theoretic Analysis of RED In 
IEEE INFOCOM Anchorage AK USA vol 3 pp 1510–1519 2001 
12 Ariba Y Labit Y Gouaisbaut F Network Anomaly Estimation for TCPAQM Net
works Using an Observer In 3rd ACM International Workshop on Feedback Control Im
plementation and Design in Computing Systems and Networks Annapolis USA pp 45–
50 2008 
13 Rahme S Labit Y Gouaisbaut F Sliding Mode Observer for Anomaly Detection in 
TCPAQM Networks In The Second Inter Conf on Communication Theory Reliability 
and Quality of Service Colmar France pp 113–118 2009 
14 Rahme S Labit Y Gouaisbaut F An Unknown Input Sliding Observer for Anomaly 
Detection in TCPIP Networks In 2009 International Conference on Ultra Modern Tele
communications and Workshops St Petersburg Russia pp 1–7 2009 
15 Rahme S Labit Y Gouaisbaut F Floquet T Second Order Sliding Mode Observer 
for Anomaly Detection in TCP Networks From Theory to Practice In Proc of 49th IEEE 
Conf on Decision and Control Atlanta GA pp 5120–5125 2010 
16 Feng Y Han F Yu X Tari Z Li L Hu J Terminal Sliding Mode Observer for 
Anomaly Detection in TCPIP Networks In 2011 International Conference on Computer 
Science and Network Technology ICCSNT Harbin China vol 1 pp 617–620 2011 
17 Feng Y Yu X Man Z Nonsingular Adaptive Terminal Sliding Mode Control of Ri
gid Manipulators Automatica 3812 2159–2167 2002 
18 Feng Y Han X Wang Y Yu X Secondorder Terminal Sliding Mode Control of Un
certain Multivariable Systems International Journal of Control 806 856–862 2007 
19 Feng Y Zheng J Yu X Truong NV Hybrid Terminal Slidingmode Observer De
sign Method for a Permanentmagnet Synchronous Motor Control System IEEE Trans 
Industrial Electronics 569 3424–3431 2009 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 505–512 2012 
© SpringerVerlag Berlin Heidelberg 2012 
An Automated Bug Triage Approach A Concept Profile 
and Social Network Based Developer Recommendation 
Tao Zhang and Byungjeong Lee∗ 
School of Computer Science University of Seoul Seoul Korea 
kerrykingieeeorg bjleeuosackr 
Abstract Generally speaking the largerscale open source development 
projects support both developers and users to report bugs in an open bug 
repository Each report that appears in this repository must be triaged for fixing 
it However with huge amount of bugs are reported every day the workload of 
developers is so high In addition most of bug reports were not assigned to 
correct developers for fixing so that these bugs need to be reassigned to 
another developer If the number of reassignments to developers is large the 
bug fixing time is increased So who are appropriate developers for fixing 
bug is an important question for bug triage In this paper we propose an 
automated developer recommendation approach for bug triage The major 
contribution of our paper is to build the concept profileCP for extracting the 
bug concepts with topic terms from the documents produced by related bug 
reports and we find the important developers with the high probability of fixing 
the given bug by using social networkSN As a result we get a ranked list of 
appropriate developers for bug fixing according to their expertise and fixing 
cost The evaluation results show that our approach outperforms other 
developer recommendation methods 
Keywords bug triage concept profile social network fixing cost re
assignment developer recommendation 
1 
Introduction 
With a great amount of largescale software projects have been developed software 
maintenance becomes a challenging task due to many bugs appearing in the source 
code files 1 In order to help developers track and fix these bugs for performing 
software maintenance well bug tracking systems 2 have been introduced for 
allowing developers to serve as testers and report related bugs Most wellknown 
open source systems eg Eclipse JBoss include an open bug repository for keeping 
the bug reports 
Currently for fixing each bug in the open bug repository the related bug report 
need to be arranged to a developer for fixing it This process is called bug triage 3 
However with a great number of bugs are reported the workload of fixers is so high 
                                                           
∗ Corresponding author 
506 
T Zhang and B Lee 
Moreover reassignment 4 is a serious problem as well A lot of bug reports were 
not assigned to appropriate developers and need to be reassigned to other developers 
The more the number of reassignments is the lower the probability of bug fixing is 
Meanwhile the fixing time is increased  
In order to reduce the fixing time for submitted bugs and improve the probability 
of bug fixing it is necessary to recommend appropriate developers for bug fixing In 
this paper we present an effective way of automatically recommending developers 
for bug fixing This approach comprises three parts The goal of the first part is to 
build the concept profile CP For achieving this purpose we extract the bug 
concepts with topic terms from the documents deriving the corresponding bug reports 
After the concept profile is built when a new bug comes we decide which bug 
concept the new bug belongs to and then extract the corresponding developers from 
the concept profile For the second part we utilize social network SN 5 to find the 
important developers with the high probability of fixing this bug The final part is to 
rank the candidates according to the developers experience and fixing cost We 
expect the proposed method can avoid the excessive number of reassignments 
improve the probability of bug fixing and reduce the bug fixing time 
The remainder of this paper is organized as follows Section 2 presents some 
related work We describe the details of our developer recommendation approach in 
Section 3 Section 4 shows the evaluation results on the collected bug reports from the 
famous open source project JBoss and we analyze the experimental results We 
summarize our work and introduce future work in Section 5 
2 
Related Works 
As previous work J Anvik et al propose a semiautomated approach for 
recommending expert developers to fix the given bug 6 Based on their work W 
Wu et al present a bug triage approach called DREXDeveloper Recommendation 
with KNearestNeighbor Search and Expertise Ranking 7 which collects the bug 
reports and comments from the open bug repository of Mozilla Firefox for 
recommending developers to fix given bugs For the evaluation experiment authors 
compare different ranking metrics according to the performance of bug triage The 
results demonstrate DREX produces higher accuracy than traditional text 
categorization method when recommending ten developers for each testing bug 
Specially in the results two metrics as Outdegree and Frequency show the best 
performance than others   
J Park et al propose a costaware triage algorithm CosTRIAGE 8 The 
algorithm models developer profiles to indicate developers estimated costs for 
fixing different type of bugs Authors apply Latent Dirichlet Allocation LDA to 
verify the bug types and quantify each value of the developers profile as the average 
time to fix the bugs in corresponding type For a given bug report authors can find 
out all developers estimated costs by determining the most relevant topic using the 
LDA model In this method they consider the probability and cost to fix the given 
bug to determine which one is the most appropriate developer to resolve the bug 
An Automated Bug Triage Approach A CP and SN Based Developer Recommendation 
507 
Even though the purpose of our study is same as the above proposed approaches 
there are some differences first the key idea of LDA is similar to concept profile 
even so both of the forming process and purpose are different second W Wu et al 
use social network metrics to rank the importance of the developers but we find the 
important developers from social network according to the possibility of fixing the 
given bug without using the social network metrics finally towards candidate 
developers we combine their expertise and cost of fixing historical bugs for ranking 
the candidates The ranking algorithm is different with other methods 
3 
A Concept Profile and Social Network Based Developer 
Recommendation 
Our approach to recommending expert developers to fix the given bug consists of the 
following steps First we build the concept profiles which include the bug concepts 
with the topic terms and the documents produced by corresponding bug reports 
Next when a new bug comes we verify which bug concept the new bug belongs to 
and we extract the developers from the documents related to this bug concept By 
constructing social network we find the important developers according to their 
probability of fixing the new bug Finally we use the ranking algorithm to 
recommend topk developers for fixing the given bug 
31 
Building the Concept Profiles  
In our work we define the concept profile CP as follows 
 
Definition 1 Concept Profile CP 
A concept profile constitutes a pairC D where C denotes a bug concept with the 
topic terms and D denotes a set of documents produced by the bug reports related to 
the bug concept A document includes a set of features eg id description fixing 
time comments of corresponding bug report 
 
To build concept profiles we need to address two key problems as follows  
1 Categorizing Bugs One effective way is to cluster the bugs in the training set 
We adopt kmeans clustering algorithm 9 to cluster the bug reports existing in the 
training set for categorizing bugs In the clustering process we define the similarity 
measure between bug reports as the distance between data points For the similarity 
measure between bug reports we use wellknown cosine measure 10 to compute the 
textural similarity between two freeform text combining title and description of bug 
reports The clustering process is an iterative process utile the error measure is a 
minimum value In the other words once the iterative process ends each data point is 
close to the reference point which is selected randomly in its cluster than to any other 
reference point and each reference point is the centroid of its cluster  
2 Extracting Topic Terms After clustered the existing bug reports in the training 
set the bug concepts are determined Now we need to extract the topic terms which 
508 
T Zhang and B Lee 
have higher frequency of appearing in the bug reports which belong to the same bug 
concept We introduce normalization to transform the frequency to the weight value 
we set a threshold θ1 to determine the topic terms while the weight values of the terms 
are more than the threshold Fig 1a shows a bug concept with the topic terms 
We note that there are four topic terms related to the bug concept C1 control 
CVS repository and type The values on the links represent the weight values 
which are more than the threshold θ1 Fig 1b shows an example of concept profile 
which includes the bug concept C1 and a set of documents D1 D1 includes the 
documents deriving from the clustered bug reports related to C1  
    
 
Fig 1 An example of bug concept and concept profile The threshold θ is set to 005  
32 
Retrieving Candidate Developers Using Social Network 
When a new bug comes at first we need to determine the most relevant bug concept 
with the new bug In detail we calculate the frequency of the topic terms of each bug 
concept appearing in the title and description of the new bug report Once the highest 
frequency is found we identify that the new bug belongs to the corresponding bug 
concept due to the highest frequency of related topic terms  
 
 
Fig 2 An example of social network which include five nodes 
Fig 2 shows an example of a social network There are five nodes in the social 
network These nodes represent a set of developers extracted from the concept profile 
and the links stand for the cooperative relationship of these developers In detail 
C1 
CVS
repository
control
type 
006469 
008356 
007278 
008895 
  a bug concept 
C1
CVS 
repository
control
type 
D1 d11 d12 d13d1n 
  bconcept profile 
dev121 
dev323
dev418 
dev29 
dev511
An Automated Bug Triage Approach A CP and SN Based Developer Recommendation 
509 
some nodes which launch the links represent the assignees who are arranged to fix the 
bugs existing the concept profile some nodes receiving the links represent the 
developers who participate the comments of the bug reports fixed by the 
corresponding developers who lunch these links The numeral in each node represents 
the number of bug reports fixed by the related developer 
Based on the number of fixed bug reports and launched links in the social network 
we compute the probability of fixing the given bug for each developer in the social 
network as follows 
 
Definition 2 The probability of fixing the given bug  
              


=
=
×
=
N
k
devk
devi
N
k
devk
devi
i
new
nl
nl
nb
nb
dev
b
P
1
1

|

 
where 
 
nbdevi stands for the number of bug reports fixed by developer devi 
 
N is the total number of developersor nodes in the social network 
 
nldevi represents the number of links which are lunched by developer devi 
 
When getting the probability of fixing the given bug for each developer we set a 
threshold θ2 to determine the important developers with the high probability of fixing 
the new bug If the probability is more than θ2 we add the corresponding developer to 
the candidate list  
33 
Ranking Developers for Recommending to Fix the Given Bug 
For a list of candidate developers we need to rank these developers for finding the 
most appropriate developers to fix the given bug We define a ranking algorithm to 
get the ranking score as follows 
 
Definition 3 Developer Ranking Algorithm  
            


=
=
×
−
+
×
=
M
k
k
i
M
k
k
i
i
dev
dev
dev
dev
dev
C
C
E
E
RS
1
1





1





core
α
α
 
where 
 
Edevi represents the developers experience which is defined by the ratio of 
the number of fixed bug reports by devi to the number of bug reports assigned 
to devi 
 
Cdevi stands for the fixing cost for the candidate developer devi which is 
defined by the inverse of the average fixing time of all bug reports which are 
fixed by devi 
 
α is a weight vector where 0 ≤ α ≤ 1 
 
M represents the total number of candidate developers when a new bug 
comes  
 
By utilizing the ranking algorithm which combines the candidates experience and the 
fixing cost of historical bug reports we get a ranked list of candidate developers 
510 
T Zhang and B Lee 
4 
Experimental Results 
41 
Setup 
For the evaluating experiment we collect 836 bug reports labeled with resolved 
from Jan 2010 to Dec 2011 in the JBoss open bug repository We find that 2325 
developers participated in the fixing and commenting activities We divide the data to 
training set and testing set The training set includes 736 bug reports and the testing 
set includes 100 bug reports We execute the preprocesseg stemming stop words 
removal and tokenization11 to all bug reports and cluster the bug reports in the 
training set As a result we get 91 bug concepts and 915 topic terms when the 
threshold θ1 is set to 005 In addition we find 651 active developers after removing 
the inactive developers who did not work more than 30 days   
When a new bug from the testing set comes we verify the bug type of this new 
bug and utilize the social network and developer ranking algorithm for 
recommending the appropriate developers to fix the given bug In this experiment we 
set the threshold θ2 to 005 and the weight vector α to 06  
We adopt PrecisionRecall measure12 to evaluate the performance of different 
developer recommendation approaches It is defined as follows  
 
Definition 4 PrecisionRecall Measure   
             











Pr
dev b
dev b
dev b
b
new
new
new
new
DEV
RDEV
DEV
ecsion

=
 
             











Re
dev b
dev b
dev b
b
new
new
new
new
RDEV
RDEV
DEV
call

=
 
where 
 
DEVdev bnew denotes all recommended developers for the new bug report 
bnew  
 
RDEVdev bnew represents the real developers who participate in the fixing 
and commenting activities to the new bug report  
42 
Results and Discussion 
In this section we apply SVM based recommendation DERX with Frequency 
DERX with Degree and our approachRecommendation 1 to same data sets In 
addition we also compare our method without using social network and ranking 
algorithmRecommendation 2 Fig 3 shows the comparison results From left to 
right side of the curves six data points represents the number of recommended 
developers from 1 to 6  
We can see that the uppermost curve shows the highest precision and recall than 
others while employing our recommendation approach On the other hand the 
performance of DREX with Frequency and DREX with Degree is better than SVM
based recommendation method In our opinion the major reason is lack of ranking 
process for SVMbased recommendation method For Recommendation 2 that is the 
first phase of our method without using social network and ranking algorithm we 
note this method shows the worst performance  
An Automated Bug Triage Approach A CP and SN Based Developer Recommendation 
511 
 
Fig 3 Performance of our approach with different developer recommendation methods 
In order to analyze the major reason of this result we compare the different 
processes as each developer recommendation methods Table 1 gives the comparison 
results We note that both of COSTRIAGE and our recommendation approach use 
concept profileor LDA to extract the bug conceptsor bug types and corresponding 
topic terms Comparing with BRSimilar bug reports retrieval concept profileor 
LDA resolves overspecialization due to introducing bug types There are only 
DERX and our methodRecommendation1 utilize the social network but for our 
method we did not use the social network metrics to rank developers Moreover 
except for SVMbased recommendation and Recommendation2 other studies also 
utilize the ranking algorithm for recommending topk appropriate developers to fix 
the new bug However our study combines developers experience and fixing cost as 
the factors of ranking algorithm it is different from other methods In summary our 
approach includes concept profile social network and developer ranking algorithm to 
recommend experienced developers for bug triage So we think this is the major 
reason that the performance of our method is better than others and it is expected to 
improve the probability of bug fixing 
Table 1 The comparison results of different processes as each developer recommendation 
methodBR Similar bug reports retrieval Concept Concept profile or LDA SN Social 
network Ranking Developer ranking algorithm 
Criticism 
Recommendation1 
Recommendaiton2 
SVM 
based 
DER
X 
COSTRIAGE 
BR 
NA 
NA 
Y 
Y 
NA 
Concept 
Y 
Y 
NA 
NA 
Y 
SN 
Y 
N 
N 
Y 
N 
Ranking 
Y 
N 
N 
Y 
Y 
5 
Conclusion and Future Work 
In this paper we propose a new idea for recommending appropriate developers for 
bug triage We utilize concept profile to extract the bug concepts and corresponding 
topic terms construct social network to find the important developers with the high 
probability of fixing the given bug and rank the candidate developers according to 
their experience and fixing cost The experimental results on JBoss open bug 
512 
T Zhang and B Lee 
repository demonstrated that our recommendation approach outperforms other recent 
studies  
In the future we will examine our method to more open bug repositorieseg 
Eclipse Mozilla for demonstrating the efficiency and availability of our 
recommendation approach Moreover we plan to investigate other factors affecting 
the developer recommendation for enhancing the ranking algorithm so that improving 
the accuracy of developer recommendation   
 
Acknowledgments This research was supported by Basic Science Research Program 
through the National Research Foundation of KoreaNRF funded by the Ministry of 
Education Science and TechnologyNo 20110026461 
References 
1 Kagdi H Gethers M Poshyvanyk D Hammad M Assigning Change Request to 
Software Developers Journal of Software Evolution and Process 24 3–33 2012 
2 Jalbert N Weimer W Automated Duplicate Detection for Bug Tracking System In 
International Conference on Dependable System  Networks pp 52–61 2008 
3 Matter D Kuhn A Nierstrasz O Assigning Bug Reports Using a Vocabularybased 
Expertise Model of Developers In 6th IEEE International Working Conference on Mining 
Software Repositories pp 131–140 2009 
4 Jeong G Kim S Zimmermann T Improving Bug Triage with Bug Tossing Graphs In 
7th Joint Meeting of the European Software Engineering Conference and the ACM 
Symposium on the Foundations of Software Engineering pp 111–120 2009 
5 Chen IX Yang CZ Lu TK Jaygarl H Implicit Social Network Model for 
Predicting and Tracking the Location of Faults In Annual IEEE International Computer 
Software and Applications Conference pp 136–143 2008 
6 Anvik J Hiew L Murphy GC Who Should Fix This bug In 28th International 
Conference on Software Engineering pp 361–370 2006 
7 Wu W Zhang W Yang Y Wang Q DREX Developer Recommendation with K
NearestNeighbor Search and Expertise Ranking In 18th AsiaPacific Software 
Engineering Conference pp 389–396 2011  
8 Park J Lee M Kim J Hwang S Kim S CosTRIAGE A CostAware Triage 
Algorithm for Bug Reporting Systems In 25th AAAI Conference on Artificial 
Intelligence pp 139–144 2011 
9 Kanungo T Mount DM Netanyahu NS Piatko CD Silverman R Wu AY An 
Efficient Kmeans Clustering Algorithm Analysis and Implementation IEEE Transaction 
on Pattern Analysis and Implementation 24 881–892 2002 
10 Tata S Patel JM Estimating The Selectivity of TFIDF based Cosine Similarity 
Predicates ACM SIGMOD Record 36 75–80 2007 
11 Wang X Zhang L Xie T Anvik J Sun J An Approach to Detecting Duplicate Bug 
Reports Using Natural Language and Execution Information In 30th International 
Conference on Software Engineering pp 461–470 2008 
12 Wu R Zhang H Kim S Cheung SC Relink Recovering Links between Bugs and 
Changes In Joint Meeting of the European Software Engineering Conference and the 
ACM Symposium on the Foundations of Software Engineering pp 15–25 2011 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 513–519 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A New Method for Filtering IDS False Positives  
with Semisupervised Classification 
Minghua Zhang and Haibin Mei 
Information College Shanghai Ocean University Shanghai China 
mhzhangshoueducn 
Abstract Constructing alert classifiers is an efficient way to filter IDS false 
positives Classifiers built with supervised classification technique require large 
amounts of labeled training alerts which are difficult and expensive to prepare 
This paper proposes to use semisupervised learning technique to build alert 
classification model to reduce the number of needed labeled training alerts Ex
periments conducted on the DARPA 1999 dataset have demonstrated that the 
semisupervised alert classification model can improve the classification per
formance dramatically especially when the labeled alert training dataset is small 
As a result the feasibility of deploying alert classifier for filtering false positives 
is enhanced 
Keywords intrusion detection system false positive semisupervised learning 
EM algorithm 
1 
Introduction 
Filtering false positives is an important and basic work in intrusion detection system 
IDS Recently some researchers have observed that building alert classifier or 
classification model based on machine learning can do the job trick and reduce false 
positives dramatically 13 However current alert classifiers are built on the super
vised learning techniques which require large amounts of labeled training data In 
reality such alert training data are very difficult and expensive to obtain As a conse
quence practicability of these methods has been greatly limited 
This paper has made a study on the applicability of semisupervised learning to build 
intrusion alert classification model The goal of semisupervised learning is making use 
of large number of unlabeled data that are easy to achieve to improve the quality of 
machine learning For classification semisupervised learning is a special classification 
technique 4 which learns from both labeled data and unlabeled data This makes it 
very suitable for building an accurate alert classification model when labeled alert 
training data are scarce 
2 
Related Work 
In order to filtering false positives generated by IDS existing solutions include alert 
correlation 5 and modeling false positives by frequent or periodic alert sequences 6 
Recently methods based on classification have also been presented The most famous 
514 
M Zhang and H Mei 
 
work is done by Pietrazek 1 in which he constructed an adaptive learner called 
ALAC to classify alerts based on RIPPER algorithm A Naïve Bayesian NB classifier 
is proposed in 3 and used in a multitier intrusion detection system Moon et al 2 
applied data mining technique to the classification and developed an alert classifier 
using decision tree This method can classify alerts automatically But the above me
thods are all based on the supervised learning and the main problem is that obtaining a 
large representative training dataset that is fully labeled is difficult time consuming 
and expensive The power of semisupervised learning for building classification 
model has been demonstrated in many applications In the field of network security 
semisupervised learning has also been used 7 However to the best of our know
ledge this is the first practice to apply semisupervised learning to intrusion alert 
classification 
3 
Construction of Alert Classification Model 
The alert contains many inherent properties some are redundant and others are very 
specific or general To improve classification efficiency we firstly adopt the informa
tion gain method 8 to select a subset of inherent properties as classification features 
31 
Alert Generative Model 
In this paper we choose generative modelsbased approach 9 one of oftenused 
semisupervised learning methods Alert generative model is a probability model which 
explicitly states how the alerts are generated Alert data can be generated by a mixture 
model which is parameterized by θ The mixture model consists of mixture compo
nents and each component which is parameterized by a disjoint subset of θ corres
ponds to a class 
jc
∈C
 where 
1
2
| |
 

C 
C
c c
c
=
 represents the classes of alert data 
this paper only considers two class C=“true positive” “false positive” Formally 
every alert object xi is created in two steps First a mixture component is selected 
according to the class probabilities Pcj|θ Then this component is used to generate an 
alert object according to its own distribution model Pxi|cjθ The likelihood of creating 
alert object xi is equation 1 
1

| 

|  
|
 
C
i
j
i
j
j
P x
P c
P x
c
θ
θ
θ
=
= 
 
1
where Pcj|θ is the jth mixture component Every alert object has a class label yi If alert 
object xi was generated by mixture component cj then let yi=cj 

|
 
i
j
P x
c θ  represents 
the probability distribution to generate an alert object Assume that X = a1 a2…an is 
a feature vector extracted from a raw alert A the second term 

|
 
i
j
P x
c θ  in equation 
1 becomes 
1


|
 


|
 
i
i
i
j
x
x n
j
P x
c
P
a
a
c
θ
θ
=


 
2
 A New Method for Filtering IDS False Positives with Semisupervised Classification 
515 
 
where 

a ix k
 is the value of the kth feature ak of alert object xi Since the features are 
conditionally independent of other features in the same alert when the class label is 
given the equation can be further expressed as equation 3 
1


1

|
 


|
 

|
 
i
i
i
n
i
j
x
x n
j
x k
j
k
P x
c
P
a
a
c
P a
c
θ
θ
θ
=
=


= ∏
 
3
Let 


|
 
ix k
j
P a
c θ  in equation 3 be parameter 
k |
θa cj
 and the mixture component 
distribution Pcj|θ be parameter 

jc
jc
C
θ
∈
 the complete model parameter set can be 
described as 


|
1
2

 


 

k
j
j
a c
k
n
j
c
j
a
a a
a
c
C
c
C
θ
θ
θ
=
∈
∈
∈
 With equation 1 
and 3 the alert generative model 

i | 
P x θ  is shown in equation 4 

1
1

| 

| 

|
 
i
C
n
i
j
x k
j
j
k
P x
P c
P a
c
θ
θ
θ
=
=
= 
∏
 
 4
32 
Alert Classification Based on Generative Model 
Suppose the estimated of parameters θ is θ

 for a given alert object xi  the probability 
that xi belongs to category cj can be calculated by equation 5 
ˆ
ˆ

|
  
| 
ˆ

|
 
ˆ

| 
i
j
j
j
i
i
P x
c
P c
P y
c
x
P x
θ
θ
θ
θ
=
=
 
5
Using equation 1 and 3 equation 5 finally becomes 
|
|

1
|
|

1
1
ˆ
ˆ

| 

|
 
ˆ

|
 
ˆ
ˆ

| 

|
 
k
i
k
i
a
j
x k
j
k
j
i
C
a
g
x k
g
g
k
P c
P a
c
P y
c
x
P c
P a
c
θ
θ
θ
θ
θ
=
=
=
=
=
∏

∏
 
6
For an unlabeled alert object xi its estimated class yi is the one which obtains the 
maximum value of the posterior probability that is
1| |
ˆ
arg max 
|
 
i
j
i
j
C
y
P y
c
x θ
=
=
=
 
Suppose the labeled alert training data is 
1
1
2
2
|
|
|
|







l
l
l
D
D
D
x y
x
y
x
y
= 
 


  
and use the MAP estimate we can find that 
arg max
 |
l 
P
D
θ
θ
θ
=
 All estimated 
parameters in θ

 that result from maximization are the familiar ratios of empirical 
counts The estimated probability of 
k |
θ a cj
 is the number of the alerts whose attribute 
value ak is akv and class label is cj divided by the total number of alerts whose category 
are class cj The calculation equation of 
k |
θ a cj
is in 7 
516 
M Zhang and H Mei 
 



1
|


1
1




|
 



l
k
j
k
l
D
ij
k v
i
i
a c
k v
j
a
D
ij
k m
i
m
i
S N a
x
P a
c
S N a
x
θ
θ
=
=
=
≡
=



 
7
where Sij is determined by the label of the ith alert object satisfying 
1  
0  
i
j
ij
i
j
y
c
S
y
c
=

= 
≠

 
the function Nakvxi is defined as 





1 



0 
i
i
x k
k v
k v
i
x k
k v
a
a
N a
x
a
a
=

= 
≠

 After applying the 
Laplace smoothing operation equation 7 becomes 

1
|


1
1
1



ˆ
ˆ

|
 



l
k
j
k
l
D
ij
k v
i
i
a c
k v
j
a
D
k
ij
k m
i
m
i
S N a
x
P a
c
a
S N a
x
θ
θ
=
=
=
+
≡
=
+



 
8
In the same way the class prior probabilities 
θ jc
 can be estimated as follows 
|
|
1
1
ˆ
ˆ

| 
l
j
D
ij
i
c
j
l
S
P c
C
D
θ
θ
=
+
≡
=
+

 
9
This paper also proposes an EM algorithm which utilizes both labeled alert training 
data and unlabeled alert data to improve the accuracy of parameter estimates We firstly 
give some relevant definitions 
Definition 1 Weak labeled dataset Let unlabeled alert dataset Du =
1
2
|
|




u
u
u
D u
x
x
x
 
then its weak labeled dataset is Dp= 
1
1

u 
u
x
y

 … 
|
|
|
|


u
u
u
u
D
D
x
y

 where 
u
iy  is the 
predicted label for 
u
ix  where i=12…|Du| 
Definition 2 Weighted labeled dataset It consists of triples xiyipi where yi is the 
label of xi pi represents the probability that xi is assigned the label yi 0
1
ip

≤  For 
the labeled dataset 
1
1
2
2
|
|
|
|







l
l
l
D
D
D
x y
x
y
x
y
= 
 


  each element is as
signed a maximum weight 1 For the weak labeled dataset Dp its weighted labeled 
dataset is
Dpw
=  
1
1
1




u
u
u
x
y
p

 …
|
|
|
|
|
|



u
u
u
u
u
u
D
D
D
x
y
p


 where pi satisfies 
05
1
ip
≤
≤  i=12…|Du| 
Definition 3 Expanded labeled dataset It is a subset of weighted labeled dataset in 
which the weight is equal or bigger than a given threshold value ρ  
The EM algorithm mainly consists of three steps First based on the limited amount of 
labeled alert training dataset Dl the parameters of generative model are estimated and 
a NB classification model is built Second the weight labeled dataset of the weak 
labeled dataset 
Dpw
 is created by using the NB classification model and the expanded 
labeled dataset 
Dsw
 constructed based on 
Dpw
 Finally the optimal classification  
 
 A New Method for Filtering IDS False Positives with Semisupervised Classification 
517 
 
model ˆθ = arg max

P Dl |   
P
θ
θ
θ  is rebuilt using the expanded training dataset 
t w
D  
The algorithm iterates the above three steps until the class members in 
t w
D  do not 
change much 
4 
Experiments 
41 
Experimental Dataset and Preprocessing 
The DARPA 1999 10 dataset is a collection of data files including tcpdump files 
BSM and NT audit data files and directory listings files We adopt Snort to detect 
attacks and generate alerts by reading the inside tcpdump data files in five weeks First 
we use the type temporal and spatial characteristic of alert to identify and eliminate the 
redundant alerts more details about the redundant elimination method refer to 11 
Then the alerts meeting the following criteria are labeled as true alerts a matching 
the source IP address b matching the destination IP address and c alert time stamp 
in the time window in which the attack has occurred All remaining alerts are labeled as 
false alerts 
42 
Results and Analysis 
In order to evaluate the performance of the proposed semisupervised classification 
method we conduct experiments to compare it with two typical supervised methods 
the traditional NB and the rule based RIPPER Fig 1 illustrates the classification ac
curacy of the three methods with only 20 training labeled alerts Our approach shows a 
better classification accuracy 867 which is respectively more than 10 percent and 
7 percent higher than that with the NB76 and the RIPPER79 method 
 
RIPPER
NB
Proposed
65
70
75
80
85
90
Classification Accracy
The Three Methods
 
Fig 1 Classification accuracy with only 20 labeled alerts 
518 
M Zhang and H Mei 
 
Fig 2 compares the number of labeled alerts required by different methods when 
classification accuracy achieves more than 88 The proposed method only need 69 
labeled alerts while NB and RIPPER methods need more than 530 and 409 alerts 
respectively With varying number of labeled alerts we compare the classification 
accuracy with the NB and RIPPER methods Results in Fig 3 show that the proposed 
method performs significantly better than the other methods even with a very small 
fraction of labeled alerts 
 
RIPPER
NB
Proposed
0
100
200
300
400
500
Number of Labeled Alerts
The Three Methods
 
Fig 2 Number of labeled alerts needed for achieving 88 classification accuracy 
 
0
100
200
300
400
500
600
70
75
80
85
90
Classification Accuracy
Number of Labeled Alerts 
 Our method
 RIPPER method
 NB method
 
Fig 3 The comparison of three methods on classification accuracy 
 A New Method for Filtering IDS False Positives with Semisupervised Classification 
519 
 
5 
Conclusion 
This paper proposes a novel approach to alert classification for filtering IDS false 
positives An alert generative model and an EM algorithm based on the generative 
model is designed Using the intrusion detection test datasets experiments are per
formed to compare the approach with two supervised methods based on Naïve Bayes 
and RIPPER Experimental results show that the proposed method can significantly 
improve alert classification accuracy to limited sized labeled alert data With this 
approach manual effort is substantially reduced and the feasibility of deploying alert 
classifier is enhanced So it is more suitable to the real network environment 
References 
1 Pietraszek T Using Adaptive Alert Classification to Reduce False Positives in Intrusion 
Detection In Jonsson E Valdes A Almgren M eds RAID 2004 LNCS vol 3224 
pp 102–124 Springer Heidelberg 2004 
2 Shin MS Kim EH Ryu KH False Alarm Classification Model for NetworkBased 
Intrusion Detection System In Yang ZR Yin H Everson RM eds IDEAL 2004 
LNCS vol 3177 pp 259–265 Springer Heidelberg 2004 
3 Bildoy J Clausen S Klausen TE Classifying Alerts in Multitier Intrusion Detection 
Systems Master Agder University College Arendal 2004 
4 Chapelle O Scholkopf B Zien A Semisupervised Learning MIT Press Cambridge 
2006 
5 Fatima LS Mezrioui A Improving the Quality of Alerts with Correlation in Intrusion 
Detection International Journal of Computer Science and Network Security 712 2007 
6 Nehinbe JO Automated Method for Reducing False Positives In Proc of 2010 Int Conf 
on Intelligent Systems Modelling and Simulation pp 54–59 2010 
7 Li H Hu Z Wu Y Wu F Behavior Modeling and Abnormality Detection Based on 
Demisupervised Learning Method Journal of Software 183 527–537 2007 
8 Liu H Yu L Toward Integrating Feature Selection Algorithms for Classification and 
Cluster IEEE Transactions on Knowledge and Data Engineering 173 491–502 2005 
9 Nigam K Using Unlabeled Data to Improve Text Classification PhD Thesis Carnegie 
Mellon University Pittsburgh PA USA 2001 
10 Lippmann R Haines JW Fried DJ Korba J Das K The 1999 DARPA Offline In
trusion Detection Evaluation Computer Networks 44 579–595 2000 
11 Gong J Mei HB Ding Y Wei DH A multifeature Correlation Redundance Elimi
nation of Intrusion Event Journal of Southeast University 353 366–371 2005 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 520–527 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Dualform Elliptic Curves Simple Hardware 
Implementation 
Jianxin Wang and Xingjun Wang 
Department of Electronic and Engineering 
Beijing Tsinghua University Beijing 100084 China 
wangjianxin09mailstsinghuaeducn  
Abstract Performing standard Weierstrassform curves’ operations based on 
Edwardsform curves’ addition law the overall security of elliptic curves can 
be strengthened while remain compatible with existing ECC system We 
present a simplified algorithm for finding such dualform elliptic curves over 
prime field 
p
F with
p ≡ 3mod 4
 Using the generated curves algorithms for 
implementing dualform operations on affine projective and twisted coordi
nates are further discussed and optimized for the case of Weierstrassform oper
ations The algorithms are implemented on FPGA and show competitive time 
and area performance both in Edwards form and Weierstrass form 
Keywords Elliptic curve Edwards curve birational equivalence FPGA 
1 
Introduction 
Elliptic curve cryptosystems ECC was proposed by Koblitz 1 and Miller 2 in the 
mid1980s showing comparable level of security with shorter key sizes and computa
tionally more efficient algorithms3 However as discussed in 45 sidechannel 
attacks which recover the private key using side channel information such as compu
ting time and power consumption are potential threats to existing Weierstrassform 
ECC system 
Edwards curves was recently introduced in 6 and Bernstein and Lange further 
proposed twisted Edwards curves 7 Edwards curves have unified addition law 
which have the native ability to countermeasure some kinds of sidechannel attacks 
Furthermore Bernstein et al in 8 suggest that Edwardsform group formula need 
fewer operations than fastest known Weierstrassform formula Because of super 
timing and security performance of Edwards curves work has been done to try to 
perform Weierstrass curves’ operations based on Edwardsform addition law 12 
As discussed in 79 only certain simplified Weierstrassform can be transformed 
to Edwardsform However performing standard Weierstrassform curves’ operations 
based on Edwardsform curves’ addition law the overall security of elliptic curves 
can be strengthened while remain compatible with existing ECC system This paper 
will focus on 
p
F with 
p ≡ 3mod 4
and discuss finding such dualform curves and 
optimizing algorithm for hardware implementation For the sake of simplicity and 
 
DualForm Elliptic Curves Simple Hardware Implementation 
521 
without confusion we just take Edwardsform curves for twisted Edwards curves and 
take Weierstrassform curves for simplified Weierstrass curves 
The organization of this paper is as follows Section 2 reviews Weierstrassform 
and Edwardsform curves defining the main parameters and discussing the group 
law In Section 3 birational equivalence between Weierstrass and Edwards form is 
investigated and simplified algorithm to generate dualform curves is further pro
posed The algorithms for hardware implementation are discussed and optimized in 
Section 4 Finally Section 5 concludes the paper  
2 
Elliptic Curves Arithmetic  
21 
WeierstrassForm Elliptic Curves 
Let 
p ≥ 5
 be a prime and 
p
F  be the finite field of order p  For 

p
a b
∈ F
 an ellip
tic curve E  can be written in the simplified Weierstrass form as see 10 
b
ax
x
y
E F
p
+
+
=
3
2


 
1
22 
MontgomeryForm Elliptic Curves 
Montgomery originally introduced this form of curves for speeding up the Pollard and 
Elliptic curve methods of integer factorization 11 The Montgomeryform elliptic 
curves over 
p
F  is defined as follow 


0
2 


3
2

p
p
A B
F
B
F
A
X
AX
X
BY
M
∈
±
∈
+
+
=
 
2
The transformability from Weierstrassform to Montgomeryform is thoroughly dis
cussed in 12 which is very important for our dualform implementation We just 
show the result as 3 

9 27
2
 
3
3
3
3
2
2
3
2
B
A
A
t
B
A
t
v
−
+
−
+
=
 
3
23 
EdwardsForm Elliptic Curves 
Edwards in 6 presented a unified addition law for the curves 

1
2
2
2
2
2
x y
c
y
x
+
=
+
 
over a nonbinary field k  including
p
F  Bernstein and Lange in 7 further genera
lized the Edwards curves and introduced the twisted Edwards curves over the prime 
field
p
F  which is described as follow with coefficients a  and d  over 
p
F  and non
zero 
2
2
2
2
 
1

dx y
y
ax
E
E a d
= +
+
 
4
Points on 
EE a d
 
 can be added by the unified addition law as 5 even if the two 
points are the equal The unified addition law is very important for the antiside
channel implementation as we discuss in the following section 
522 
J Wang and X Wang 






−
−
+
+
=
+
2 1 2
1
1 2
2
1
2 1 2
1
2 1
1 2
2
2
1
1
1

1






dx x y y
ax x
y y
dx x y y
x y
x y
y
x
x y
 
5
Theorem 32 in 7 gives that every twisted Edwards curve over 
p
F is birationally 
equivalent over 
p
F  to a Montgomery curve and 6 showed that a significant num
ber of elliptic curves over GFp roughly 14 of isomorphism classes of elliptic 
curves are birationally equivalent to a twisted Edwards curve And the twisted Ed
wards curve 
EE a d
 
is birationally equivalent to the Montgomery curve 
M A B
   espe
cially over 
p
F  with 
p ≡ 3mod 4
 where 

4


2
d
a
B
d
a
d
a
A
−
=
−
+
=
 
6
3 
Birational Equivalence  
31 
Birational Transformation 
Based on the work of 7 and 12 we can now deduce the birationally mapping be
tween twisted Edwards form and simplified Weierstrass form Substitute 6 into 3 
giveswith 
 48
14

2
2
d
ad
a
aw
+
+
= −
 and 
 864
33
33

3
2
2
3
d
ad
a d
a
bw
+
−
−
= −
 
w
w
p
b
a t
t
v
W F
+
+
=
3
2


 
7
The transformation and reverse transformation from 
EE a d
 
 to 


W Fp
 is shown as 
8 and 9 
5


5   121


1
 4 1

t
a
d
a
d y
y
v
a
d
y
x
y
=
−
+
−
−
=
−
+
−
 
8
Here just omitting the special cases for simplicity Although these would bring in two 
more inversion which is the most expensive operation when incorporated with pro
jective or inverted twisted Edwards coordinates the additional inversion can be elim
inated 
5 
5  12
12
6 

6
d
a
t
a
d
t
y
v
d
a
t
x
−
+
−
+
=
+
−
=
 
9
As we can deduce from projective twisted Edwards coordinates and inverted twisted 
Edwards coordinates in 67 we just need about 8 extra field multiplications to trans
form the point in simplified Weierstrass form to twisted Edwards form performing 
point operations in twisted Edwards form and then transform back 
32 
Birational Equivalence 
Reference 12 gives the transformability between Weierstrassform and Montgom
eryform curves and theorem 34 in 7 shows that for a prime 
Fp
 with 
p ≡ 3mod4
 every Montgomeryform curve over
p
F is birationally equivalent over 
 
DualForm Elliptic Curves Simple Hardware Implementation 
523 
p
F   to an Edwardsform curve In this section we discuss the birational equivalence 
between Edwardsform and Weierstrassform curves over
p
F  with 
p ≡ 3mod4
  
As mentioned in section 6 of 7 if a  is a square in 
p
F  and d is nonsquare 
twisted Edwards curves have unified and complete addition law Without loss of ge
nerality we focus our discussing on these kinds of twisted Edwards curves Taking 
these into consideration we can generalize theorem 1 in 12 as follows 
Corollary If a is a square in 
p
F  and d is nonsquare  
Table 1 Criterion 
1

3mod4 1
= −
−
≡
p
p
d  
a − d
 
EE a d
 
|
8
QNR 
QNR 
ND 
QNR 
QNR 
D 
QR 
QNR 
D 
QR 
QR 
D 
 
with QR quadratic residue QNR quadratic nonresidue D divisible by 8 ND non
divisible by 8 
Proof With
p ≡ 3mod4
 Montgomeryform curve is birationally equivalent over 
p
F  
to an Edwardsform curve As the theorem 1 of 12 indicated if and only if 
A + 2
is 
quadratic nonresidue and
A− 2
is quadratic residue 
EE a d
 

 is nondivisible by 8 
With 8 we get 

4 
d
a a
−
 is quadratic nonresidue and 

4 
d
a
d
−
is quadratic 
residue Because a  is a square in 
p
F  then a  should be quadratic residue For the 
product of a nonresidue and a nonzero residue is a nonresidue and 

4 
d
a a
−
 is 
quadratic nonresidue then 

1
a − d
 is quadratic nonresidue Also the inversion of 
quadratic nonresidue is quadratic nonresidue So 


a − d
 is quadratic nonresidue 
As the same again for 

4 
d
a
d
−
 we get d is quadratic nonresidue 
Table 2 Algorithm 1  
Algorithm 1 Generate dualform elliptic curves whose Edwardsform has complete addition law 
INPUT A prime
p ≡ 3mod4
 
OUTPUT A Weierstrassform elliptic curve with the Edwardsform which has complete addition law and 
with cofactor 4 
1 Generate a  and d  and put 
EE a d
 
 
2 Check a  is a square d  and 


a − d
 is quadratic nonresidue If not go to step 1 
3Set 
 48
14

2
2
d
ad
a
aw
+
+
= −
 
 864
33
33

3
2
2
3
d
ad
a d
a
bw
+
−
−
= −
 get 


W Fp
 
4Compute 



EW Fp
and check 
l
E
W Fp
4



=
 for some prime l  Go to Step 1 if not 
5 Check other security tests and output the parameters if it passes all tests
524 
J Wang and X Wang 
With this conclusion we can simplified the algorithms to generate Elliptic curves 
with cofactor 4 discussed in 12 for 
p ≡ 3mod4
 as shown in table 2  
As discussed in 2 we can apply all security tests used for finding Weierstrass
form curves to check the generated dualform curves So the security of Edwards
form curves is guaranteed and equal to that of Weierstrassform curves One generat
ed group for field 
p
F with
1
2
2
2
2
64
96
224
256
−
+
−
−
p =
 is shown in table 3 
Table 3 Group parameter Example 
Generated curve from algorithm 1 
p =0xFFFFFFFEFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF FF00000000FFFFFFFFFFFFFFFF  
E
a =0xFFFFFFFEFFFFFFFFFFFFFFFFFFFFFFFE14000001040000004100000020000001 
E
d =0x168D9 
n =0x3FFFFFFFBFFFFFFFFFFFFFFFFFFFFFFFDF36BCC0EA4B456F51C94F1D2F7FF8254  
W
a =0x3C4CFFFFD951FFFFDB098000129B4A45C6D726869E29A3DBEDDFF2D8096672AC  
bW
=0x8BD877D54CD8B264E09D5141B88BE222352167ABEDBCC61F861B33B600699623 
4 
Hardware Implementation Issue and Result 
In this section we discuss and optimize the algorithms for hardware  
implementation based on the curves on table 3 Firstly we optimize the montgomery 
multiplication 
41 
Montgomery Multiplication and Group Law 
Montgomery multiplication 13 is key operation for optimizing the field multiplica
tion on 
p
F  But as we all know the Montgomery transformation 
p
xR
x
x
mod
→  =
 
and the reverse transformation 
p
xR
x
x
mod


−1
=
→
11 is required for montgomery 
multiplication Integrating the dualform transform into projective and inverted 
twisted Edwards coordinates’ operations we can further eliminate the montgomery 
transformation overhead which further simplify the hardware control logic 
As transform from  


W Fp
 to 
EE a d
 
 in projective or inverted twisted Edwards 
coordinates in 7 indicate the transform just need one field multiplication If we 
omit
p
xR
x
x
mod
→  =
 all of three coordinates just introduce exactly one extra coef
ficient 
c = R−1
  And the same for the reverse transform both inversions will just 
eliminate the introduced extra coefficient Also notice that if we transform parameter 
of  
EE a d
 
 as 
p
aR
a
a
mod
′ ← ′ =
 and
p
bR
b
b
mod
′ ← ′ =
 the Edwardsform group 
law in projective coordinates will just get the same extra coefficient for all three coor
dinates Combined the transform and reverse transform from  


W Fp
 to 
EE a d
 
 we 
can totally eliminate the montgomery transform and reverse transform 
 
DualForm Elliptic Curves Simple Hardware Implementation 
525 
42 
Implementation Results and Performance Comparison 
Based on the curves generated by Algorithm 1 we implement the dualform elliptic 
curves processor on FPGA for verification purposes We design and compare the 
performance on affine projective and inverted coordinates respectively All of fol
lowing results are based on a Xilinx FPGA Spartan 3 xc3s5000  
 
Affine Coordinates Weierstrassform Edwardsform and dualform processor were 
implemented separately and timearea performance was compared in table 4 From 
table 4 we can see that the point multiplication time in dual forms for 


W Fp
 is 27 
slower then Edwards form and the area is about 35 larger 
Table 4 Affine Coordinates Result 
Form 
Weierstrass 
Edwards 
DualForm 
AREANumber of Slices 
11339 
12357 
16928 
TIMEClock cyclesWFp 
533378 
1140554 
1452226 
 
Also it is apparent that for affine coordinate point multiplication in Weierstrass 
form is twice as fast as Edwardsform and Dualform with area about 50 smaller 
The overall performance shows that dualform is not quite useful in affine coordinate 
Projective and Inverted Coordinates Using Montgomery multiplication algorithm 
discussed previously we compared the performance of Edwards form and dual form 
in table 5 for projective and inverted coordinates 
Table 5 ProjectiveInverted Coordinates Result  
Form 
Projective coordinates 
Inverted coordinates 
Edwards 
DualForm 
Edwards 
DualForm 
AREANumber of Slices 
14773 
18755 
16609 
20207 
TIMEClock cyclesWFp 
104538 
104731 
97443 
97552 
 
Table 5 show that when integrate the dualform operation into projective or in
verted twisted coordinates we get almost 30 boost in area while keeping the oper
ating time about the same Also we can see that inverted coordinates is trading 7 
larger in area for 7 faster in time 
Compared with Existing Result We also compare the implementation with reported 
result in 256bit length prime field 1417 in table 6 From table 6 we can see that our 
dualform implementations have better timearea performance trading almost 30 
boost in size for 35 decrease in clock cycles while at the same time enhancing the 
overall security 
Table 6 Result Comparision 
ECC system 
Platform 
Max freq MHz 
Clock cycles 
Area 
McIvor et al 17 
XC2VP1257 
3446 
151360 
15755 slices 256 mults 
Sakiyama et al 14 
XC3S50005 
40 
 
27597 slices 
dualform projetive 
XC3S50005 
3475 
104731 
18755 slices 64 mults  
dualform inverted 
XC3S50005 
3541 
97552 
20207 slices 64 mults 
526 
J Wang and X Wang 
5 
Conclusions 
In this paper we discuss the issues relating to dualfrom ie the twisted Edwards 
form and simplified Weierstrass form elliptic curves over 
p
F with 
p ≡ 3mod 4
 
from curves generating to hardware implementation We generalize the theorem of 
birational equivalence and optimize the algorithm for generating such curves With 
generated curves algorithm for hardware implementation is optimized and compared 
with reported result 
Affine coordinate in Edwards form is not quite suitable for dual form for both tim
ing and area performance are not quite satisfied Projective and inverted twisted coor
dinates are designed and optimized for Weierstrassform operations and both show 
comparable timing and area performance Compared with reported result both have 
better timing and area performance trading almost 30 boost in size for 35 faster in 
clock cycles And dualform inverted implementation also outperforms the projective 
one in timing which is 7 faster 
Future work could be incorporating more countermeasures of sidechannel attacks 
into the processor to enhance the overall security levels  
Acknowledgments The authors would like to express their thanks to Dr Wang and 
the members of Department of Electronic and Engineering for their support assis
tance and inspiration Moreover special thanks to anonymous reviewers for their 
contributions to improve the manuscript  
References 
1 Koblitz N Elliptic Curve Cryptosystems Mathematics of Computation 48 203–209 
1987 
2 Miller VS Use of Elliptic Curves in Cryptography In Williams HC ed CRYPTO 
1985 LNCS vol 218 pp 417–426 Springer Heidelberg 1986 
3 Lenstra AK Verhul ER Selecting Cryptographic Key Sizes J Cryptol 14 255–293 
2001 
4 Kocher PC Timing Attacks on Implementations of DiffieHellman RSA DSS and 
Other Systems In Koblitz N ed CRYPTO 1996 LNCS vol 1109 pp 104–113 
Springer Heidelberg 1996 
5 Biehl I Meyer B Müller V Differential Fault Attacks on Elliptic Curve Cryptosys
tems In Bellare M ed CRYPTO 2000 LNCS vol 1880 pp 131–146 Springer Hei
delberg 2000 
6 Edwards HM A Normal Form for Elliptic Curves Bulletin of the American Mathemati
cal Society 443 393–422 2007 
7 Bernstein DJ Birkner P Joye M Lange T Peters C Twisted Edwards Curves In 
Vaudenay S ed AFRICACRYPT 2008 LNCS vol 5023 pp 389–405 Springer Hei
delberg 2008 
8 Bernstein DJ Lange T Faster Addition and Doubling on Elliptic Curves In Kurosawa 
K ed ASIACRYPT 2007 LNCS vol 4833 pp 29–50 Springer Heidelberg 2007 
9 Verneuil V Elliptic Curve Cryptography on Standard Curves Using the Edwards Addi
tion Law 2011 not published yet 
 
DualForm Elliptic Curves Simple Hardware Implementation 
527 
10 Hankerson DR Vanstone SA Menezes AJ Guide to Elliptic Curve Cryptography 
Springer 2004 
11 Montgomery PL Speeding the Pollard and Elliptic Curve Methods of Factorizations 
Math Comp 48 243–264 1987 
12 Okeya K Kurumatani H Sakurai K Elliptic Curves with the MontgomeryForm and 
Their Cryptographic Applications In Imai H Zheng Y eds PKC 2000 LNCS 
vol 1751 pp 238–257 Springer Heidelberg 2000 
13 Koc CK Acar T Kaliski BS Analyzing and Comparing Montgomery Multiplication 
Algorithms IEEE Micro 163 26–33 1996 
14 Sakiyama K Mentens N Batina L Preneel B Verbauwhede I Reconfigurable 
Modular Arithmetic Logic Unit Supporting Highperformance RSA and ECC over GFp 
International Journal of Electronics 945 501–514 2007 
15 Kocabas U Fan J Verbauwhede I Implementation of Binary Edwards Curves for 
veryConstrained Devices In 21st IEEE International Conference on Applicationspecific 
Systems Architectures and Processors ASAP 2010 pp 185–191 2010 
16 McIvor C McLoone M McCanny J Hardware Elliptic Curve Cryptographic Processor 
over GFp IEEE Trans Circuits and Systems I 539 1946–1957 2006 
17 Chatterjee A Gupta IS FPGA Implementation of Extended Reconfigurable Binary 
Edwards Curve Based Processor In 2012 International Conference on Computing Net
working and Communications ICNC pp 211–215 2012 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 528–537 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Genetic Based Autodesign of Fuzzy Controllers  
for Vector Controlled Induction Motor Drives 
Moulay Rachid Douiri and Mohamed Cherkaoui 
Mohammadia Engineering School Department of Electrical Engineering Avenue Ibn Sina 
765 AgdalRabat Morocco 
douirirachidhotmailcom 
Abstract This work presents the Genetic algorithm based autodesign of fuzzy 
logic controller for speed controller of the indirect field oriented controlled in
duction motor drives to automate and at the same time to optimize the fuzzy 
controller design process To do this the normalization parameters member
ship functions and decision table are converted into binary bit string This  
optimization requires a predefined performance index The task of such a de
sign algorithm is the modification of the existing knowledge and at the same 
time the investigation of new feasible structures 
Keywords fuzzy logic controller genetic algorithms indirect field oriented 
induction motor 
1 
Introduction 
Field oriented control FOC or vector control VC proposed by Blaschke 1 and 
Hasse 2 has become an industry standard for control of induction machines in high 
performance drive applications Control of induction motor using the principle of 
field orientation gives control characteristics similar to that of a separately excited dc 
machine Orientation is possible along mutual flux or stator flux or the rotor flux 
however orientation of the stator current space vector with respect to the rotor flux 
alone gives natural decoupling between the torque and flux producing components of 
the stator current space vector VC induction motor drive outperforms the dc drive 
because of higher transient current capability increased speed range and lower rotor 
inertia It is due to this reason that modern high performance drive application is mov
ing towards using induction machine as the drive element 
Today the new trends in this field now involve the application of modern non
linear control techniques to further enhance the performance of such controllers as 
well as optimizing drive operation based on a specific requirement 3 4 5 The 
research underlying this paper involves the development of a novel synthesis metho
dology to automate and at the same time to optimize the performance of fuzzy con
trollers based on a predefined objective function for any particular application It also 
aims in particular to design an optimal fuzzy controller for induction motor drives 
with indirect field oriented control Two intelligent techniques were used in this paper 
namely fuzzy logic and genetic algorithms 
 
Genetic Based Autodesign of Fuzzy Controllers 
529 
Fuzzy logic first developed by Zadeh 1965 6 is an approach for handling com
plex problems using reasoning that is approximate as opposed to precise formally 
deduced logic The key difference between fuzzy logic and probability theory is that 
the former is interested in capturing partial truths that is how to reason about things 
that are not wholly true or false while the latter is concerned with making predictions 
about events based on a partial state of knowledge 6 7  Fuzzy logic is derived 
from fuzzy set theory whereby subjects in a set have degrees of membership de
scribed by membership functions and where each subject can belong to one or more 
fuzzy sets Membership in a fuzzy set is denoted by a membership value between 0 
and 1 it can be thought of as the possibility of association of a particular subject with 
a particular set as opposed to the probabilistic likelihood of an event occurring Fuzzy 
logic is useful in situations where vagueness exists there are no clear cut definitions 
and results cannot be categorized as true or false outcomes 6 7 The fuzzy 
logic controller FLC to be investigated is the Mamdanis type 8 although there 
exist other types for example the Sugenos 9 and the Yamakawas 10  
The underlying principles of GAs were first published by Holland in 1962 12 
The mathematical framework was developed in the late 1960s and presented in Hol
lands pioneering book in 1975 Genetic Algorithms are search algorithms which are 
based on the genetic processes of biological evolution They work with a population 
of individuals each representing a possible solution to a given problem Each indi
vidual is assigned a fitness score according to how well it solves the given problem 
For instance the fitness score might be a performance index for a dosed loop control 
system In nature this is equivalent to assessing how effective an organism is at com
peting for resources The highly adapted individuals will have relatively large num
bers of offsprings Poorly performing ones will produce few or even no offspring at 
all The combination of selected individuals produces super fit offsprings whose 
fitnesss are greater than that of the parents In this way the individuals evolve to 
become more and better suited to their environment 12 13 
This paper is organized as follows The principle of indirect field oriented control 
is presented in the second part the fuzzy logic speed controller in section three the 
genetic algorithm optimization based autodesign of fuzzy speed controllers in the 
fourth section the five part is devoted to illustrate the simulation performance of this 
control approach a conclusion and reference list at the end 
2 
Indirect Field Oriented Control 
The field orientation concept implies that the currents supplied to the machine should 
be oriented in phase and in quadrature to the rotor flux vector 
ψqdr
 This can be 
achieved by selecting 
e
ω  to be the instantaneous speed of 
ψqdr
and locking the 
phase of the reference system such that the rotor flux is aligned with the daxis result
ing in the mathematical constraint 1 At any instant d electrical axis is in angular 
position 
eθ  relative to α  axis The angle 
eθ  is the result of the sum of both rotor 
angular and slip angular positions as follows 
530 
MR Douiri and M Cherkaoui 


e
r
sl
e
r
sl
r
sl
t
t
t
t
θ
θ
θ
ω
ω
ω
ω
ω
=
+


=
+
=
+

 
1
where 
r
ω  and 
rθ  are the position and rotor angular velocity 
θsl
and 
sl
ω are the 
position and sliding angular velocity 

0
r
rd
qr
ψ
ψ
ψ
=
=
  
2
The main equations of indirect vector control 


m
r
sl
qs
r
r
L
R
i
L
ω
= Ψ
 
3
r 
r
r
m ds
r
L
d
L i
R
dt
Ψ + Ψ =


 
4
3  


4
m
em
qs
r
r
p L
i
L
Γ
=
Ψ
 
5
3 
Fuzzy Speed Controller 
A conventional PI controller can be described by 

0
 
t
em
p
i
k e
k
e t dt
Γ
=
+ 
 
6
where kp and ki are the proportional and the integral gain coefficients and e=ωr
 ωr is 
the speed error between the command speed ωr
 and the actual motor speed ωr If the 
above integral equation is converted into a differential equation by taking the deriva
tive with respect to time the equivalent equation will be 



em
p
i
k e
k e
Γ
=
+


 
7
The PI controller 7 can be written in a fuzzy rule form as follows 
If ek is LVe and Δek is L
e
V   then ΔΓ
emk is 
LV Γem
    
8
with LV linguistic variable 
The most significant variables entering the fuzzy logic speed controller have been 
selected as the speed error e and its change e  the output this controller is 

Γ em
 The 
equation inputoutput controller FLC written at time k 
 
Genetic Based Autodesign of Fuzzy Controllers 
531 

 
 
 
r
r
e k
k
k
ω
ω
=
−
 
9
 
 

1
e k
e k
e k
=
−
−

 
10



 

1
 
em
em
em
k
k
k
Γ
= Γ
−
+ Γ
 
11
The principle of this strategy is shown in Fig 1 
e
e
Γ

r
ω
dt
d
e
G 
e
G
G Γ
r
ω


Γem
r
ω
Fuzzification
Inference 
Mechanism
Defuzzification
G
Γ
 
Fig 1 Basic structure of the fuzzy logic controller for indirect field oriented control 
The fuzzy sets are characterized by standard designations NB negative big NM 
negative medium NS negative small AZ approximate zero PS positive small PM 
positive medium and PB positive big Fuzzy distribution is symmetric and non
equidistant in our choice We have chosen also in our application the triangularshaped 
membership function In order to design a universal FLC we can transform the values 
range in standard ranges Therefore the input and output gains are introduced 
 
 
 


 
 
 
G
G
e
e
G
e
k
e
k
k
G
G
G
e k
e k
k
Γ
Γ
=
=
= Γ






 
12
From behavior study of the system closedloop speed based on experience we can 
establish the command rules which connect output with inputs 14 7 As we have 
seen there are seven fuzzy sets which imply fortynine possible combinations of 
these inputs in which fortynine rules They can be presented in a matrix called ma
trix inference shown in the Table 1 
We choose minmax inference method for each rule we obtain the partial mem
bership function by relation 13 11 




min



12
i
C
i
i
R
G
O
G
i
m
μ
μ
μ
Γ
=
Γ
=


 
13
where μCi is a membership factor assigned to each rule Ri μOi ΓG

 is the membership 
function related in operation imposed by rule Ri 
The resulting membership function is then given by 11 6 


1
2


max






m
G
R
G
R
G
R
G
μ
μ
μ
μ
Γ
=
Γ
Γ
Γ




 
14
The defuzzification process employs the center of gravity method As a result the 
control increment is obtained by the following formula 11 6 
532 
MR Douiri and M Cherkaoui 
 
 


 

G
G
G
G
G
G
d
d
μ
μ
Γ
Γ
Γ
Γ
=
Γ
Γ








 
15
Table 1  The fuzzy linguistic rule table 
Γ  
e
NB 
NM 
NS 
ZE 
PS 
PM 
PB 
 
 
 
e  
NB 
NB 
NB 
NB 
NB 
NM 
NS 
AZ 
NM 
NB 
NB 
NB 
NM 
NS 
AZ 
PS 
NS 
NB 
NB 
NM 
NS 
AZ 
PS 
PM 
AZ 
NB 
NM 
NS 
AZ 
PS 
PM 
PB 
PS 
NM 
NS 
AZ 
PS 
PM 
PB 
PB 
PM 
NS 
AZ 
PS 
PM 
PB 
PB 
PB 
PB 
AZ 
PS 
PM 
PB 
PB 
PB 
PB 
4 
Genetic Algorithms Based Fuzzy Logic Controller 
The genetic algorithm is applied to automate and optimize the fuzzy controller design 
process This optimization requires a predefined objective function Moreover the 
normalization parameters membership functions and decision table are converted into 
binary bit string constructed by cascading  
An individual bit length is 597 bit and composed of three gene block 
Gene block 1 Normalization factors Determine the proper domain of the control 
surface which represents 10 bit normalization factors for speed error 10 bit 
normalization for speed error derivative 10 bit denormalization factor for control output  
Gene block 2 Membership functions To have complete freedom in partitioning 
the state space asymmetrical membership functions should be chosen that 
consequently suggest three different design parameters ie M1 M2 and M3 for each 
membership functions Fig 2 Let us consider seven membership functions for each  
 
1
M
2
M
3
M
μ
 
Fig 2 Membership function parameters 
 
Genetic Based Autodesign of Fuzzy Controllers 
533 
variable for a controller with two inputs 42 parameters are required to define the 
entire set of membership functions where every parameter is encoded with 10bit 
resolution 
Gene block 3 Decision table Every consequent part of a fuzzy rule should be en
coded in a binary form Since every consequent can take on only one of seven differ
ent values based on Table 1 every consequent can be represented by only three bits 
ie 3×49 parameters = 147 bits will represent the entire decision table Fig 3 
e
G 
Ge
GΓ
M1
M2
M3
M42
R1
R2
R3
49
R
 
Fig 3 Bitstring representation of entire controller 
Each individual represents a possible solution to the problem a particular fitness 
function is required for the evaluation of the individuals 12 13 15 In this way 
for every particular chromosome ie each solution the fitness function returns a 
single numerical value which indicates the quality of that solution In the context of 
optimization it is the performance index of the closed loop system that becomes the 
fitness function Our goal is to have a response speed with a short rise time small 
overshoot and nearzero steady state error In this respect a multiple objective func
tion is required 


0
0
0
1
2
3
4


 
05
t
t
t
dz
J
e dt
z
z t dt
e tdt
δ dt
=
+
−
+






 
16
1  Measure of a fast dynamic response 
2 The penalty on the multiple overshoot of the response where δdzdt detects 
the instances that overshoots or undershoots occur 
0
0
1
0


0
0
dz
if
dz
dt
dz
dt
if
dt
δ
+
−

=

=

≠


 
17
and |zzt| determines the response deviation from the desired value 
3 Measure the steady state error 
The genetic algorithm with the free parameters shown in Table 2 was able to find 
the nearoptimum solution with a population of 44 individuals in almost 358 genera
tions Fig 5 This is due to the large number of design parameters involved in concur
rent optimization The principle of genetic algorithms based fuzzy logic controller is 
shown in Fig 4 
534 
MR Douiri and M Cherkaoui 
IFOC
IM
+

e
e
Γ

r
ω
dt
d
e
G 
e
G
G
Γ
r
ω


Γem
r
ω
Fuzzification
Inference 
Mechanism
Defuzzification
Genetic 
Algorithms
G
Γ
 
Fig 4 Basic structure of the genetic algorithms based fuzzy logic controller for indirect field 
oriented control 
 
Fig 5 Speed of convergence 
The optimization algorithm and the motor drive response are then verified under 
loading and unloading conditions A speed command of 50 rads at 002 s is given to 
the drive system the full load is applied at 02 s then load is completely removed at 
04 s and then accelerated further to 100 rads full load is applied at 08 s then load 
is completely removed at 1 s Later after speed reversal of 50 rads at 12 s full load 
is applied at 14 s and the load is fully removed at 16 s Fig 5 shows the speed opti
mization result and response of the drive system 
The FLCGA speed response Fig 6 shows that the drive can follow the low 
command speed very quickly and smoothly without overshoot no steadystate error 
and rapid rejection of disturbances with a low dropout speed Fig 7 and Table 3 
The current responses are sinusoidal and balanced well as the decoupling between the 
flux and torque is verified Figs 8 and 9 
Table 2  Genetic algorithm parameters 
GA property 
Value 
GA property 
ValueMethod 
Number of generations 
358 
Selection method 
Roulette wheel 
No of chromosomes in each generation 
44 
Crossover method 
Doublepoint 
No of genes in each chromosome 
3 
Crossover probability 
08 
Chromosome length 
597 
Mutation rate 
005 
 
Genetic Based Autodesign of Fuzzy Controllers 
535 
a 
a’ 
Fig 6 Rotor speed acceleration and reversal a FLC and FLCGA a speed error 
 
b 
b’ 
Fig 7 Zoom speed b starting transient performance and overshoot b response due to load 
and unload change 
 
c 
c’ 
Fig 8 Three phase stator current c FLC c FLCGA 
 
d 
d’ 
Fig 9 Electromagnetic torque response d FLC d FLCGA 
536 
MR Douiri and M Cherkaoui 
Table 2 Summary of results 
 
Rise time s 
Overshoot  
Settling time  
Steady state error  
FLC 
003 
29 
008 
07 
FLCGA 
002 
005 
0001 
04 
5 
Conclusions 
This work uses the genetic algorithm based autodesign of fuzzy logic controller as 
the speed controller of the indirect field oriented controlled induction motor drives 
By comparison with FLC controller it testifies that this method is not only robust but 
also can improve dynamic performance of the system The GAFLC proposed ap
proach achieves Good pursuit of reference speed Starting without overshoot Rapid 
rejection of disturbances with a low dropout speed Good support for changes in 
engine parameters 
References 
1 Blaschke F The Principle of Field Orientation as Applied to the New Transvector 
ClosedLoop Control System for Rotating Field Machines Siemens Review 345 217–
219 1972 
2 Hasse K On the Dynamics of Speed Control of a Static AC Drive with a Squirrel Cage 
Induction Machine Dissertation Tech Hochsch Darmstadt 1969 
3 Silva WG Acarnley PP Finch JW Application of Genetic Algorithm to the Online 
Tuning of Electric Drive Speed Controllers IEEE Transactions on Industrial Electron
ics 471 217–219 2000 
4 Hazzab A Bousserhane IK Kamli M Design of Fuzzy Sliding Mode Controller by 
Genetic Algorithms for Induction Machine Speed Control International Journal of Emerg
ing Electric Power Systems 12 1016–1027 2004 
5 Rubaai A CastroSitiriche MJ Ofoli AR DSPBased laboratory Implementation of 
Hybrid FuzzyPID Controller using Genetic Optimization for HighPerformance Motor 
Drives IEEE Transactions on Industry Applications 446 1977–1986 2008 
6 Zadeh LA Fuzzy Sets Information and Control 83 338–353 1965 
7 Zhao ZY Tomizuka M Isaka S Fuzzy Gain Scheduling of PID Controllers IEEE 
Transactions on Systems Man and Cybernetics 235 1392–1398 1993 
8 Mamdani EH Application of Fuzzy Logic Algorithms for Control of Simple Dynamic 
Plant Proceedings of IEEE 12112 1585–1588 1974 
9 Takagi T Sugeno M Fuzzy Identification of Systems and its Applications to Modeling 
and Control IEEE Transactions on Systems Man and Cybernetics 151 116–132 1985 
10 Yamakawa T Fuzzy Controller Hardware System In Proceedings of 2nd IFSA Con
gress Tokyo Japan pp 827–830 1987 
11 Chen SM A Fuzzy Approach for RuleBased Systems Based on Fuzzy Logics IEEE 
Transactions on Systems Man and Cybernetics 265 769–778 1996 
12 Holland J Adaptation in Natural and Artificial Systems An Introductory Analysis with 
Applications to Biology Control and Artificial Intelligence University of Michigan Press 
Ann Arbor 1975 
 
Genetic Based Autodesign of Fuzzy Controllers 
537 
13 Goldberg DE Genetic Algorithms in Search Optimization and Machine Learning Addi
sonWesley Reading 1989 
14 Uddin MN Radwan TS Rahman MA Performance of FuzzyLogicBased Indirect 
Vector Control for Induction Motor Drive Transactions on Industry Applications 385 
1219–1225 2002 
15 Cardoso FDS Martins JF Pires VF A Comparative Study of a PI Neural Network 
and Fuzzy Genetic Approach Controllers for an ACDrive In 5th IEEE International 
Workshop on Advanced Motion Control AMC 1998 Coimbra pp 375–380 1998 
Appendix Induction Motor Parameters 
Rated power = 75Kw Rated voltage = 220V Rated frequency = 60Hz Rr = 017Ω Rs 
= 015Ω Lr = 0035H Ls = 0035H Lm = 00338H J = 014Kgm2 
 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 538–546 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Very Short Term Load Forecasting  
for Macau Power System  
Chong Yin Fok and Mang I Vai  
University of Macau Department of Electrical and Computer Engineering Macau China 
kelvinfokcemmacaucom 
fstmivumacmo 
Abstract This paper presents the implementation of very short time load fore
casting VSLF for Macau power system with the forecasting period ranging 
from several minutes to 8 hours The methodology adopted is the hybrid model 
with ANNbased and similar days methods included weather information va
riables which are seldom considered as input variables of VSLF in other litera
tures It is shown that weather information is one of influence factors of the 
VSLF for a small city like Macau and the MAPE of VSLF for 15minutes to 3
hours ahead load is 096 and 085 for Jan 2011 and Jul 2011 respectively  
In this work the author also utilizes the result of VSLF to adjust a day ahead 
short term load forecasting STLF result by this approach it is demonstrated 
that MAPE of STLF can be reduced by 20 for the data of Jul 2011  
Keywords very short term load forecasting hybrid model 
1 
Introduction 
Short term load forecasting STLF is essential to daily dispatch planning and opera
tion unit commitment economic dispatch load flow analysis and even resource man
agement Economic dispatch is getting more and more concern due to the cost of fuel 
oil and purchased electricity from other electric utilities dramatically increases Accu
rate STLF can help dispatcher to deploy the suitable and cheaper generation source 
and can save the cost in the degree of hundred thousand Patacas Macau Currency 
1USD ≈  8 Patacas per day for Macau case On the other hand network security is 
also crucial in Macau due to less endurance on electricity outage in Macau flourishing 
tour and gambling industry Accurate STLF can also provide the good balance of the 
tradeoff between economic dispatch and network security  
In order to improve the accuracy of STLF very short time load forecasting VSLF 
is introduced in this paper to adjust the result of STLF Furthermore VSLF can also 
help dispatcher to forecast ranging from 15minutes up to fewhours ahead peak load 
for dispatching the most suitable unit for coming peak load to fulfill the network secu
rity as well as saving the generation cost 
In the past several decades lot of efforts was put in developing the short and very 
short term load forecasting model with different approaches Time series methods and 
statistical methods such as moving average 15 exponential smoothing methods 
 
Very Short Term Load Forecasting for Macau Power System 
539 
 
25 linear regression models 35 moving average ARMA models 57 Kalman 
filtering method 5 are difficult to model the nonlinear relationship between input 
variables and forecasted load On the other hand lot of literatures focus on machine 
learning technique support vector machines SVM 913 and intelligent algorithm 
ANN 814 which are good candidates of load forecasting model due to they can 
model complex and nonlinear relationship without knowledge of detail analytical 
model between input variables and forecasted load Hybrid models with different 
stages of forecasting models and forecasted load adjustment 2 1516 or combina
tion of different models 17 are adopted for improving the accuracy of load forecast
ing Among VSLF models only few works consider weather information involved in 
the input variables selection due to lagging weather effect on very short term load 
1719 Refer to one of STLF ANN model 20 hourly weather information is se
lected to be the inputs of STLF to forecast onehour ahead load In this work by ex
amining high correlation between hourly weather information and coming 15 minutes 
to few hours load for a small city like Macau the author utilizes the hourly weather 
information to forecast very short time load ranging from 15 minutes to few hours or 
even several minutes by interpolating methods as the load within 15 minutes can be 
approximated to be monotonic linear relationship 
In this work the author aims at developing the hybrid forecasting model to forecast 
15minutes can be several minutes by interpolation up to 8hours ahead load for 
Macau system and the result of VSLF can also help to improve a day ahead STLF 
accuracy The content of the paper is organized as follows First of all introduction is 
given in section 1 In section 2 load characteristic of Macau power system is dis
cussed for the choice of forecasting model Section 3 presents the selection of fore
casting model and the applied strategy of hybrid model of VSLF Section 4 shows the 
result of VSLF Section 5 discusses how to use the VSLF result to modify a day ahead 
STLF for Macau power system In the last section conclusion is drawn 
2 
Load Characteristic of Macau 
First of all load characteristic of Macau is analyzed for choosing suitable input va
riables and models They come from two main sources statistical analysis of histori
cal load and the information from dispatcher experience 
 
i 
The load incremental rate of Macau power system in summer and winter is   
shown in fig 1 and fig 2 
ii    Dispatcher experience  
1  Sudden raining and sunshine can affect load change for coming few couple mi
nutes and few hours 
2 The load characteristic is similar for the similar days and the load trend won’t 
change abruptly in recent days 
3 For the coming day ahead forecasting even though the forecasted maximum tem
perature is same as that of historical day the load level can vary a lot due to accu
mulative effect of weather and different residents’ custom in previous recent days 
during different period 
 
To deal with the above observations of the characteristic of load profile and expe
rience from dispatchers for 1 the author tries to examine the relationship between  
 
540 
CY Fok and MI Vai 
 
 
Fig 1 Similar load incremental rate for the days in Jun 2011 
 
Fig 2 Similar load incremental rate of winter 2010 and summer 2011 
Table 1 Correlation analysis of current load and weather information at current and previous 
hours 
Correlation 
Temperature 
3h 
Temperature  
2h 
Temperature  
1h 
Temperature 
current 
Load 
0839 
0855 
0860 
0846 
Correlation 
Dew point  
3h 
Dew point   
2h 
Dew point 
1h 
Dew point  
current 
Load 
0833 
0838 
0841 
0835 
 
 
 
Very Short Term Load Forecasting for Macau Power System 
541 
 
hourly weather information and coming hourly load Table 1 for 2 the VSLF model 
includes the similar days and average incremental rate of recent 3 days methods for 
3 it is necessary to utilize the trend of load from VSLF to adjust the day ahead STLF 
which will be discussed in section 5 
• Among different hourly weather information obtained from Macau observatory 
temperature and dew point temperature are chosen to be the variables which have 
highest correlation with the load The next step is to examine how long the weather 
information is used to forecast the load It can be shown in table 1 that current 
weather information is suitable to forecast the load within three hours 
• Only historical hourly weather information is used as input variables of VSLF 
without future hourly forecasted weather information involved since weather fore
cast is another source contributed to the error of load forecasting and there is also 
lack of completed forecasted hourly weather information from observatory 
3 
Methodology 
Based on the previous discussion ANN similar day method and average incremental 
rate of recent 3 days method are chosen to be the individual forecasters of VSLF 
which are categorized into two groups with and without weather information input 
variables and is summarized in table 2  
ANN The incremental rate instead of load level is used for the ANN inputs due to 
incremental rate is more stable than load level when the weather change rapidly 18    
Similar days approach Euclidean norm 2122 examines the similarity of two 
highest correlation inputs from historical data In this work twosteps Euclidean norm 
is implemented for the reason that temperature is the dominant input than others the 
10 most similar days in term of temperature are filtered out first and then the 3 most 
similar days are obtained by evaluating Euclidean norm of other inputs  
Recent 3 days approach recent days have strong relationship in term of load trend 
due to residents behavior and enterprises consumption is similar The number of days 
used for the average of the load incremental rate is determined by MAPE of data of 
previous months It is selected to be 3 days in this work 
Table 2 Individual forecasters of VSLF model 
 
without  weather information 
with weather information  
ANN 
Similar 
days 
Recent 3 
days 
ANN 
Similar  
days 
Input 
The last 5 
load incre
mental rate  
The last 2 
hour load 
level + 
current 
load level 
The average 
of load in
cremental 
rate of last 3 
recent days 
The last 3 
load incre
mental rate + 
current hour 
temperature 
and dew 
point tem
perature 
The current 
hour temper
ature + dew 
point tem
perature + 
current load 
level  
for ANN the current hourly weather information is only suitable to forecast load up to 3 hour 
based on correlation analysis 
542 
CY Fok and MI Vai 
 
Furthermore many literatures described that using a combined forecasting method 
17 23 can enhance the accuracy of the load forecast In this work hybrid model is 
also adopted by combining different individual forecasters running in parallel which 
can also be extended by including more suitable individual forecasters in parallel later 
on By examining the previous load forecasting MAPE by different individual fore
casters two possible approaches are as follows 
Method 1 Weighting is assigned between different forecasters to give the final re
sult 17  
Or 
Method 2 Individual forecaster with the smallest MAPE of last hour is used for 
the forecasting model for next few hours 
Ambiguous combined effect on load forecasting of different forecasters method 1 
is avoided in this work Instead the reason of the choice of method 2 for the hybrid 
model is that within several hours in one day the characteristic of the trend of pre
vious load is similar to that of forecasting load same method has lowest MAPE result 
for a specific continuous hours in specific day which is examined by the large sample 
size of historical data shown in table 4 Furthermore the probability of same individ
ual forecaster applied for previous 1 hour and next few hours with lowest MAPE 
should be very close to 50 and 3333 for two and three forecasters respectively if 
two MAPE have no relationship after examining the recent historical data in large 
sample size in table 3 it is more than 50 and 3333 so we use the previous hour 
MAPE by each individual forecaster to evaluate which method is used for next 15 
minutes and few hours forecasting Based on table 3 the author prefers to use 3 fore
casters with previous 1 hour MAPE evaluation to forecast next 3 hour load since 
395 is most far away from its reference value 3333 for 3 forecasters compared 
with other cases     
Table 3 The probability of lowest MAPE for previous and forecasted load when same method 
applied 
 
Same method applied Lowest 
MAPE for previous 1 hr  and 
next forecasted 1 hr  
Same method applied Lowest 
MAPE for previous 1 hr  and 
next forecasted 3 hr 
Hybrid model with 2 
forecasters 
523 
526 
Hybrid model with 3 
forecasters 
377 
395 
Table 4 Methods with lowest MAPE in forecasted day 
 
0 
1 
2 
3
4
5
6
7
… 21 
22 
23 
4 Jul 
A 
B 
B 
B
B
B
C
C
 
B
C 
B 
5 Jul 
C 
C 
B 
B
B
B
A
A
 
C
B 
C 
6 Jul 
A 
B 
A 
C
C
B
A
C
 
B
A 
A 
7 Jul 
B 
C 
B 
B
B
B
A
A
 
A
A 
A 
… 
 
 
 
 
 
 
 
 
 
 
 
 
25Jul 
C 
B 
A 
A
A
B
C
C
 
C
C 
C 
Individual forecaster with the lowest MAPE applied for every hour per day A ANN B 
similar days C recent  
 
Very Short Term Load Forecasting for Macau Power System 
543 
 
By method 2 there is also a mechanism for the model to store the times of differ
ent forecasters utilized within a day for further analyzing the characteristic of the load 
during different period of a day As a result further improving the forecasted load by 
using different models during a day 1718 24 can be accomplished which can be 
further analyzed and done in future works 
The flowchart of the hybrid model is shown in fig 3 There are many developed 
individual forecasters to fit different load characteristic of Macau for a specific day or 
hour Two or three most suitable forecasters among them are chosen to be the candi
dates and run in parallel in hybrid model the system also evaluates the MAPE of the 
rest of the forecasters in offline mode Once the system discovers that the individual 
forecaster in hybrid model with large MAPE for recent period this forecaster need to 
be improved or replaced by other offline forecasters that had better performance in 
the mentioned period   
 
  
Fig 3 The flowchart of the hybrid model 
4 
VSLF Result  
The ANN model was trained by Nov and Dec 2010 May and Jun 2011 data and fore
cast the load for Jan and Jul for verification The result of MAPE of hybrid model for 
3 hours table 5 and 8 hours table 7 is as follows and the result of 8hours ahead 
load forecasting for Jul is utilized to adjust the STLF result in the next section Fur
thermore MAPE can be improved with hourly weather information involved in the 
individual forecaster as shown in table 6  
544 
CY Fok and MI Vai 
 
Table 5 MAPE of individual forecasters and hybrid model 
Hybrid model 3 forecasters 
ANN 
Similar days 
Recent 3 days 
Hybrid 
MAPE 3 hours Jan 
201 
102 
058 
0955 
MAPE 3 hours Jul 
099 
081 
082 
0853 
Table 6 MAPE of individual forecasters with and without weather information inputs 
Different individual 
forecasters  
ANN 
Similar days 
Recent 
3
days 
Input withwithout 
weather info 
weather info wo  
weather  
info 
weather  
info 
wo  
weather  
info 
wo  
weather  
info 
MAPE3 hours Jul 
099 
110 
0810 
0813 
0823 
Table 7 MAPE for 8th hour load of individual forecasters and hybrid model 
Hybrid model 3 forecasters 
ANN 
Similar days 
Recent 3 days 
Hybrid 
MAPE 8th hour Jul 
231 
173 
186 
187 
 
If the MAPE of three forecasters is low and similar the MAPE of the hybrid model 
can be improved otherwise the hybrid model averages the MAPE of the individual 
forecasters The high MAPE forecaster will worsen the result of hybrid model how
ever it is necessary to include several individual forecasters in the hybrid model as 
each forecaster can provide better MAPE for a specific day or specific period of the 
day especially for Macau flourishing gaming development and rapid change of load 
characteristic year by year   
5 
The Result of VSLF to Adjust STLF  
Currently ANN 25 and similarday methods are developed for STLF which is ap
plied in dispatch center of Macau and dispatcher is required to perform the next day 
load forecasting before 530pm of current day according to the requirement of the 
regulator  
For ANN methods the actual load of current day should be used for the inputs of 
load forecasting for tomorrow However due to the incomplete load data of current 
day after 530pm the load data of yesterday is used for the inputs By availability of 
VSLF in this work the actual load of current day and forecasted load after 530pm 
can be utilized as inputs to enhance the accuracy of next day forecasted load as shown 
in table 8 
For the similar day methods it is recognized by other literatures 18 and by dis
patcher experience that the load level can vary a lot among different similar days in 
term of weather information due to recent residents and enterprises’ custom as well as 
the accumulative effect of previous consecutive days’ weather information That in
formation is inherited in the result of VSLF in the current day As a result we can use 
the VSLF forecast load at 0000 of next day to adjust the next day STLF by 1 and 
2 The adjusted result is shown in the table 8 and the MAPE is reduced by 12 and 
20 for the current two approaches by this work 
 
Very Short Term Load Forecasting for Macau Power System 
545 
 
Result from a dayahead load STLF forecasting from 0002345 L1 L2 
L3……L96 Result from 8th hour ahead load VSLF forecasting at 000 S1。Adjusted 
STLF by VSLF result from 0002345 Ladj1  Ladj 2 Ladj3……Ladj 96 。Adjusted 
coefficient  α1 α2 α3…… α96 
Ladj1 = L1+α1S1L1 
1
Ladji = Li+αiS1L1 
2
where 0α1≤1 0αi≤1 if |L1S1|L1003    α1=0 αi=0 if |L1S1|L1≤003 
Table 8 The MAPE of adjusted result of STLF by VSLF of this work 
 
Reference ANN 
actual load from 
024 of current day 
as inputs 
This work ANN actual 
load from 017 of current 
day plus VSLF result of 
1724 of current day as 
inputs  
Original approach 
ANN actual load from 
024 of yesterday as 
inputs  
MAPE ANN 
Jul 
225 
237 
12 
272 
 
 
This work Similar days 
with VSLF adjustment 
Original approach 
Similar days 
MAPESimilar 
days Jul 
 
295      20 
372 
6 
Conclusion  
In this work the author developed the VSLF hybrid model to forecast the load rang
ing from 15 minutes or even several minutes up to few hours by utilizing hourly 
weather information as input variables which is seldom to be considered in other 
VSLF literatures For small power system with weather information involvement the 
MAPE of forecasters can be improved The strategy of hybrid model is also discussed 
and the MAPE of VSLF for 15minutes to 3hours ahead load is 096 and 085 for 
Jan 2011 and Jul 2011 respectively Furthermore due to limited input variables and 
time constraints of performing current STLF the result of VSLF is utilized for facili
tating to perform STLF to reduce the MAPE by 12 to 20 for July data 
 
Acknowledgement I would like to express my special gratitude to the great support 
of my bosses Benjamin Yue Calvin Ho dispatch center my company Companhia de 
Electricidade de Macau the feedback of dispatchers and the opinion of previous 
STLF project author Cheuk Fung Wong 
References  
1 Box GE Jenkins PGM Time Series Analysis  Forecasting and Control HoldenDay 
1970 
2 Song KB Ha SK Park JW Kweon DJ Kim KH Hybrid Load Forecasting Me
thod with Analysis of Temperature Sensitivities IEEE Trans Power Syst 212 869–876 
2006 
546 
CY Fok and MI Vai 
 
3 Papalexopoulos AD Hesterberg TC A Regressionbased Approach to Shortterm 
Load forecasting IEEE Trans Power Syst 54 1535–1550 1990 
4 Haida T Muto S Regression Based Peak Load Forecasting Using a Transformation 
Technique IEEE Trans Power Syst 94 1788–1794 1994 
5 Moghram I Rahman S Analysis and Evaluation of Five Shortterm Load Forecasting 
Techniques IEEE Trans Power Syst 44 1484–1491 1989 
6 Amjady N Shortterm Hourly Load Forecasting Using Timeseries Modeling with Peak 
Load Estimation Capability IEEE Trans Power Syst 153 498–505 2001 
7 Galiana FD Shortterm Load Forecasting Proceedings of IEEE 7512 1558–1572 
1987 
8 Swarup KS Satish B Integrated ANN Approach to Forecast Load IEEE Computer 
Applications in Power 15 46–51 2002 
9 Cortes C Vapnik V Supportvector Network Mach Learn 20 273–297 1995 
10 Cristianini N Tylor JS An Introduction to Support Vector Machines and Other Kernel
Based Learning Methods Cambridge Univ Press Cambridge 2000 
11 Chen BJ Chang MW Lin CJ Load Forecasting Using Support Vector Machines A 
study on EUNITE competition 2001 IEEE Trans Power Syst 194 1821–1830 2004 
12 Pang SL Mu G Wang XQ Jin P Ma JG Shortterm Load Forecasting Method 
Based on Load Regularity Analysis for Supporting Vector Machines Proc Northeast Di
anli Univ 264 2006 
13 Hong WC Cheng YL Hung WM Dong YC Electric Load Forecasting by SVR 
with Chaotic Ant Swarm Optimization In IEEE CIS 2010 
14 Hippert HS Pedreira CE Souza RC Neural Networks for ShortTerm Load Fore
casting A Review and Evaluation IEEE Trans Power Syst 161 2001 
15 Senjyu T Mandal P Uezato K Funabashi T Next Day Load Curve Forecasting Us
ing Hybrid Correction Method IEEE Trans Power Syst 201 102–109 2005 
16 Wang Y Xia Q Kang CQ Secondary Forecasting Based on Deviation Analysis for 
Short Term Load Forecasting IEEE Trans Power Syst 262 2011 
17 Daneshi H Daneshi A Real Time Load Forecast in Power System In DRPT 2008 
Nanjing 2008 
18 Charytoniuk W Chen MS Very ShortTerm Load Forecasting Using Artificial Neural 
Network IEEE Trans Power Syst 151 2000 
19 Ding Q Lu JG Liao HQ A Practical Super Short Term Load Forecast Method and 
its Implementations In IEEE PES Power Systems Conference and Exposition 2004 
20 Zainab HO Awad ML Mahmoud TK Neural Network Based Approach for Short
Term Load Forecasting In Power Systems Conference and Exposition 2009 
21 Jain A Srinivas E Rauta R Short Term Load Forecasting using Fuzzy Adaptive Infe
rence and Similarity In World Congress on Nature  Biologically Inspired Computing 
2009 
22 Senjyu T Uezato T Higa P Future Load Curve Shaping Based on Similarity Using 
Fuzzy Logic Approach IEE Proceedings of Generation Transmission Distribu
tion 1454 375–380 1998 
23 Chen Q Milligan J Germain EH Raub R Shamsollahi P Cheung KW Imple
mentation and Performance Analysis of Very Short Term Load Forecaster – based on the 
Electronic Dispatch Project in ISO New England In 2001 Large Engineering Systems 
Conference on Power Engineering LESCOPE 2001 2001 
24 Chen DG York M Neural Network Based Very Short Term Load Prediction In 
Transmission and Distribution Conference and Exposition TD IEEEPES 2008 
25 Wong CF ANN STLF for a Rapidly Expanding Power System In CEPSI 2006 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 547–554 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A Method for the Enhancement of the Detection Power 
and Energy Savings against False Data Injection Attacks 
in Wireless Sensor Networks 
Su Man Nam and Tae Ho Cho 
 College of Information and Communication Engineering Sungkyunkwan University Suwon 
440746 Republic of Korea 
smnamtaechoeceskkuackr 
Abstract Malicious attackers spread various attacks to destroy the system of 
the sensor network False report injection attacks occur on the application layer 
and drain the energy resources of each node Statistical enroute filtering SEF 
is proposed to detect and drop false reports in intermediate nodes during the 
forwarding process In this work we propose a security method to improve the 
detection power and energy savings using four types of keys The performance 
of the proposed method was evaluated and compared to that of SEF against the 
attack Our experimental results reveal that our method improves detection 
power and energy savings by up to 25 and 9 respectively 
1 
Introduction 
Wireless sensor networks WSNs supply economically feasible technologies for 
applications that use wireless communication 12 WSNs are comprised of a number 
of sensor nodes and a base station in a sensor field The sensor nodes operate sensing 
computing and wireless communication modules The base station provides 
information for many uses across the network infrastructure The nodes are vulnerable 
to being captured and compromised due to limited resources 3 Malicious attackers 
use various methods to destroy the sensor network 
The source nodes usually transmit legitimate reports as events occur in the sensor 
network When a legitimate report is generated by a source node intermediate nodes 
forward the report toward the base station On the other hand when a malicious 
attacker captures a node the node becomes a compromised node When a false report 
is generated by the compromised node intermediate nodes transmit the false report to 
the base station During the attack the intermediate nodes drain their energy resources 
due to transmitting and receiving the false report In addition false alarms are caused 
as the false report arrives at the base station That is false report injection attacks 
harm the entire network 
Ye et al 4 proposed statistical en route filtering SEF to detect a false report 
injection attack in the sensor network In SEF when a real event occurs each of the 
detecting nodes generates message authentication codes MACs A centerof 
stimulus CoS node collects the MAC and produces a report When the report is 
548 
SM Nam and TH Cho 
 
forwarded each node statistically verifies the MACs If a forged MAC is detected the 
node drops the report Thus SEF decreases the energy consumption of each node by 
dropping the false report while forwarding processes 
In this work we propose a method to improve detection power and energy 
consumption The proposed method uses four types of keys a new individual key a 
pairwise key a new cluster key and a group key Each node has four keys to encrypt 
and exchange MACs and reports between two nodes The new individual key and the 
cluster key filter out false reports within a cluster region early The proposed method 
improves the detection power and energy savings of each node against false report 
injection attacks  
The remainder of this paper is organized as follows SEF and the motivation for the 
study are described in Section 2 The proposed method is introduced in Section 3 and 
the optimization results are presented in Section 4 Finally conclusions and future 
work are discussed in Section 5 
2 
Background and Motivation 
21 
Statistical Enroute Filtering SEF  
SEF is proposed to prevent false report injection attacks in the sensor network SEF 
aims at early detection and elimination of false reports with low computation and 
communication overhead SEF has four phases 1 key assignment 2 report 
generation 3 enroute filtering and 4 base station verification Phase 1 is 
initially performed and phases 2 3 and 4 are repetitively performed as events 
occur In the key assignment phase each node stores a small number of keys of a 
partition PID from the base station which maintains a global key pool divided into 
many partitions before sensor nodes are deployed In the report generation phase one 
of the detecting nodes is elected as the centerofstimulus CoS node when an event 
occurs Its neighboring nodes select a key to submit message authentication code 
MAC including event information to the CoS node After collecting MACs from the 
neighboring nodes the CoS node produces a report and forwards it to the base station 
In the enroute phase intermediate nodes verify MACs of the report using keys of the 
intermediate nodes while forwarding processes between nodes Finally during a base 
station verification the base station again identifies MACs of the report through keys 
of the global key pool when the report arrives at the base station 
 
 
Fig 1 Filtering of false reports 
 
A Method for the Enhancement of the Detection Power and Energy Savings 
549 
 
Fig 1 shows phase report generation enroute filtering and base station 
verification and false reports via intermediate nodes forwarded from the CoS node to 
the base station When an event occurs within a region including a number of nodes 
the CoS node Fig 1a is elected It then forwards a broadcast to its neighbors  
and the neighboring nodes generate MACs 1 and 4 Fig 1b using PIDs 1 and 4 
submit to submit it if the neighbors detect the same event A compromised node Fig 
1c also submits a fake MAC to the CoS node After collecting MACs the CoS node 
forwards a false report Fig 1d to the base station because a fake MAC is included 
When node A receives the false report the false report is transmitted to node B of the 
next hop because the MACs of the false report and the PID Fig 1e of node B are 
different When node B receives the false report the node verifies the MAC1 of the 
report through the PID 1 of node B If the PID is legitimate node B transmits the false 
report to node C On the other hand when the false report arrives at node C it is 
dropped Fig 1f because its MAC3 is forged by the compromised node If a 
legitimate report is generated the report safely arrives at the base station and the base 
station verifies the report using the global key pool Therefore SEF statistically filters 
out false reports through keys of PIDs of intermediate nodes while a forged MAC that 
occurs from a compromised node is forwarded via PID keys 
22 
Motivation for the Study 
In SEF the detection power of false reports is affected by key authentication of 
intermediate nodes If a false report arrives at the base station without sanctions the 
base station intermediate nodes transmits the false report and are drained even when 
the base station drops the false report 5 Thus the lifetime of the sensor network is 
decreased due to low detection power 
In this paper we propose a method to achieve both improved detection power and 
energy consumption for reducing damage from the false report Our proposed method 
effectively detects false reports using four types of keys 6 When a compromised 
node generates a fake MAC a CoS node verifies NC of the compromised node 
through its NC After dropping the fake MACs the CoS node encrypts a report using 
NG and the report is transmitted while maintaining secure paths using a pairwise key 
PK until the base station Therefore we enhance the detection power of the false 
report including a forged MAC through NC in a CoS node and conserve the energy 
resources of intermediate nodes by preventing the inflow of false reports The 
proposed method detects and drops the false report for both the improved detection 
power and the energy consumption in a CoS node   
3 
Proposed Method 
31 
Assumptions 
We assume a static sensor network Sensor nodes are fixed after they are deployed 
The sensor network is comprised of a base station and a large number of sensor 
nodes eg the Berkeley MICA2 motes 7 The initial paths of the topology are 
established through directed diffusion 8 and minimum cost forwarding algorithms 
9 after distribution of the sensor nodes It is further assumed that every node 
forwards data packets toward the base station along their path An attacker generates a 
550 
SM Nam and TH Cho 
 
false report injection using compromised nodes The resulting false report is 
forwarded from a compromised node to the base station before it is filtered out 
32 
Overview 
In this paper we effectively detect false report injection attacks that occur on the 
application layer using four types of keys – an individual key shared between each 
node and the base station a pairwise key shared between a node and another node a 
cluster key shared between a node and a CoS node and a group key shared by all 
nodes When an event occurs in a range nodes that detect the event transmit event 
data including each MAC and NC to a CoS node After collecting event data from its 
neighbors the CoS node verifies MACs through its NC selects legitimate MACs and 
encrypts a report All intermediate nodes maintain secure paths through the PK while 
forwarding processes into the base station Therefore we improve both detection 
power and energy savings in the sensor network by preventing false report injection 
attacks 
33 
Keys Explanation 
• 
New Individual Key NI Each sensor node has a unique key associated with the 
base station for oneonone communications This key is used to generate a MAC 
when an event occurs For example when a node detects a real event the node 
produces a MAC after encrypting event information through its NI  and it 
transmits event data including the MAC to a CoS node When MACs arrive at the 
base station the base station verifies the MACs through its NIs to detect forged 
MACs That is the NI is used to encrypt and decrypt event information in the 
node and the base station 
• 
Pairwise Key PK Sender and receiver nodes have the same keys to check 
conditions The PKs of two nodes are verified before transmitting a report For 
example a report generated by a CoS node is forwarded via multiple intermediate 
nodes toward the base station The intermediate nodes maintain secure paths 
through PK against an adversary node that captures the report information That 
is PK retains secure paths between all pairs of nodes  
• 
New Cluster Key NC Nodes that are located within a cluster range have a 
common key This key detects forged MACs generated by compromised nodes in 
a CoS node For example a node that detects an event transmits event data 
including its NK to the CoS node After collecting event data the CoS node 
verifies the NK of the event data through its NK The CoS node then forwards a 
report including the MACs to the base station On the other hand if a 
compromised node is located in the same cluster range the compromised node 
also transmits event data including the forged MAC and NC to the CoS node The 
CoS node then verifies the NC of the event data through its NC and drops the 
forged MAC That is NC is used to detect forged MACs that are generated from 
the compromised node  
• 
New Group Key NG Every node and the base station have a key This key is 
used to encrypt or decrypt reports in the CoS node or the base station For 
example the CoS node encrypts a report through NG for preventing information 
extraction by an adversary node If the adversary node captures the report while 
 
A Method for the Enhancement of the Detection Power and Energy Savings 
551 
 
forwarding processes it is difficult to decrypt the report without NG That is NG 
prevents information extraction against the adversary node if the report is 
captured 
34 
The Detection of False Report Injection Attack 
Fig 2 False MAC Detection using NC shows the detection processes of a forged 
MAC in a CoS node when a real event occurs within a cluster region In the region 
shown there are four nodes including a compromised node There is also a node Fig 
2a that is outside of the region When a real event Fig 2b occurs within the 
region a CoS node Fig 2c is elected to generate an event report The nodes of its 
neighbors generate MACs after they encrypt event information through NI The 
neighboring nodes transmits each event data including NC and MAC to the CoS node 
A compromised node Fig 2d transmits forged event data Fig 2e including a 
forged NC and MAC to the CoS node during this time The outside node also sends 
event data After collecting event data the CoS node filters out both the forged MAC 
and the MAC of another region by verifying the NC The CoS node forwards a report 
with legitimate MACs into the base station Our proposed method reduces the flow of 
false reports using early detection within a cluster region The proposed method 
decreases communications traffic between intermediate nodes 
 
 
Fig 2 False MAC Detection using NC 
 
 
Fig 3 Transmission of MAC or Report 
552 
SM Nam and TH Cho 
 
Fig 3 shows keys used for each node between two nodes according to a MAC and 
report In Fig 3a node A detects an event and encrypts event information using its 
NI It then transmits event data including its NCA and MACA to the CoS node B 
After the CoS node B verifies NCA through the NC of the CoS node it drops the 
forged MAC The CoS node B then collects verified MACs and forwards a report to 
the base station When the report arrives at the base station the base station again 
verifies the MACs of the report through its global key pool If a forged MAC is 
detected the base station drops the forged MAC Thus NI and NC are used to detect 
false reports in the CoS node or the base station In Fig 3b the CoS node A encrypts 
a report using NK and the encrypted report is forwarded to intermediate node B at the 
base station The CoS node verifies the PK of intermediate node B and checks the 
condition of the intermediate node during this time If intermediate node B is inserted 
by an adversary and has no key the CoS node forwards the report to another node 
after verifying the PK of the intermediate node When the report arrives at the base 
station it encrypts the report through its NG Thus because PK checks the conditions 
of the intermediate nodes the secure paths of each node are maintained between two 
intermediate nodes Therefore we are able to improve both the detection power and 
energy consumption using four types of keys against false report injection attack 
4 
Performance Results 
The performance of the proposed method was evaluated through simulations as 
compared to SEF A field size of 500 × 500m2 is used and the base station is located 
in the middle of the field It takes 1625 and 125μJ to transmitreceive a byte and 
each MAC generation consumes 15μJ 4 Moreover the size of an original report is 
24 bytes and size of a MAC is 1 byte We have decided to transmit false reports from 
20 compromised nodes in the field A false report per 10 legitimate reports is 
generated by the compromised nodes 
Table 1 Performance against false report injection attacks 
 
a Average energy consumptions of each node 
for specific hop counts 
b Ratio of 
filtered reports 
2 
4 
6 
8 
10 
SEF 
77692 
25037 
52883 
35783 
64986 
7660 
PM 
23260 
44976 
38411 
31950 
32416 
10000  
 
Table 1a shows the average energy consumption in each node for specific hop counts 
when a false report injection attack occurs in the sensor network The energy 
consumption of SEF is usually higher than that of the proposed method PM because 
false reports are statically filtered out in the intermediate node In addition the energy 
consumption of each node is irregular in terms of specific hop count On the other 
hand the proposed method better regulates energy consumption due to hop counts 
than SEF Thus the proposed method reduces energy consumption and prolongs the 
lifetime of the sensor network Table 1b shows the filtered report ratios of SEF and 
the proposed method In SEF the number of false reports that arrive at the base 
station drops in 25 of cases and the filtered report rate is about 75 for intermediate  
 
 
A Method for the Enhancement of the Detection Power and Energy Savings 
553 
 
 
Fig 4 Energy Consumption of the Sensor Network 
nodes On the other hand the proposed method initially detects all forged MACs 
through CK in a CoS node Therefore the proposed method enhances the detection 
power of the false report more than SEF because the CoS node drops all the forged 
MACs 
Fig 4 illustrates the total energy consumption of SEF and the proposed method 
after 1000 events When 200 events have occurred the proposed method conserves 
energy resources by up to 9 more than SEF The proposed method enhances 
detection power by 25 and reduces energy consumption in the sensor network by 
9 because it detects all forged MACs in the CoS node and prevents false report 
inflows 
5 
Conclusion and Future Work  
We use four types of keys to detect attacks and to plan countermeasures against false 
reports Each node has four keys a NK for encrypting event information a PK for 
maintaining secure paths an NC for detecting the false report and an NG for 
encrypting a report The effectiveness of the proposed method is evaluated and 
compared to that of SEF while generating a false report The proposed method 
improves the detection level by about 25 and reduces energy consumption by about 
9 The proposed method effectively detects false report injection attacks using four 
types of keys In future work we will run additional simulation scenarios and perform 
experiments to test the robustness of our method against other types of attacks 
Acknowledgment This work was supported by National Research Foundation of 
Korea Grant funded by the Korean Government No 20120002475 
References 
1 
Akyildiz IF Su W Sankarasubramaniam Y Cayirci E A Survey on Sensor 
Networks IEEE Communications Magazine 40 102–114 2002 
2 
Chan H Perrig A Security and Privacy in Sensor Networks Computer 36 103–105 
2003 
554 
SM Nam and TH Cho 
 
3 
Przydatek BD Song AP SIA Secure Information Aggregation in Sensor Networks 
In Proc of CCNC vol 23 pp 63–98 2004 
4 
Ye F Luo H Lu S Zhang L Statistical Enroute Filtering of Injected False Data in 
Sensor Networks IEEE Journal on Selected Areas in Communications 23 839–850 2005 
5 
Sun CI Lee HY Cho TH A Path Selection Method for Improving the Detection 
Power of Statistical Filtering in Sensor Networks J Inf Sci Eng 25 1163–1175 2009 
6 
Zhu S Setia S Jajodia S LEAP Efficient Security Mechanisms for Largescale 
Distributed Sensor Networks In Proceedings of the 10th ACM Conference on Computer 
and Communications Security pp 62–72 ACM 2003 
7 
Crossbow technology Inc httpwwwxbowcom 
8 
Intanagonwiwat C Govindan R Estrin D Directed Diffusion A Scalable and Robust 
Communication Paradigm for Sensor Networks In MOBICOM pp 56–67 ACM 2000 
9 
Ye F Chen A Lu S Zhang L A Scalable Solution to Minimum Cost Forwarding in 
Large Sensor Networks In Proceedings of the Tenth International Conference on 
Computer Communications and Networks pp 304–309 2001 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 555–562 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Virtual Cluster Tree Based Distributed Data 
Classification Strategy Using Locally Linear Embedding 
in Wireless Sensor Network 
Xin Song12 Cuirong Wang1 Cong Wang1 and Xi Hu1 
1 School of Information Science and Engineering Northeastern University 110819  
Shenyang China 
2 The Key Laboratory of Complex System and Intelligence Science Institute of Automation 
Chinese Academy of Sciences 100190 Beijing China 
neuqsongsohucom 
Abstract With recent advances in wireless communication and low cost low 
power sensors are enabling the deployment of largescale and collaborative 
wireless sensor network WSN which performs different tasks using the sub
sets formation of sensor on specific requirement of event monitoring For  
implementing the differentiated monitoring task of the heterogeneous sensor 
network in the same geographical region efficient classification of the various 
sensed data becomes a critical task due to stringent constraint on network re
sources frequent link and indeterminate variations in sensor readings In this 
paper we present a virtual cluster tree based distributed data classification 
strategy using locally linear embedding LLE The strategy can realize the 
structure representation of original sensed data by LLE to meet the classifica
tion for forming the virtual cluster tree of the various monitoring task The  
theoretical analysis and experimental results show that the proposed strategy 
can effectively reduce the energy consumption using LLE based classifier 
Keywords wireless sensor network distributed data classification virtual  
cluster tree locally linear embedding energy efficient 
1 
Introduction 
One important class of share resources technologies is to build a virtual cluster tree 
which use the connecting sensor nodes subset to observe the same phenomenon The 
existence of such virtual cluster tree capability implies that the sensors are the multip
lexing device if they have sensed different occurrence Then another key issue is how 
to dynamically determine which node should join which virtual cluster tree of imple
menting same task A simple approach is to design a classifier in sensor network for 
determining the classed of newly arrived sensed data from monitoring region How
ever conventional centralized solution is not directly applicable in the new type of 
heterogeneous WSN environment To address the above challenges in this paper we 
present a virtual cluster tree based distributed data classification strategy using locally 
linear embedding in energyconstrained sensor network Our approach performs event 
556 
X Song et al 
 
monitoring in the built virtual cluster tree and implements classification of sensed 
data utilizing a locally linear embedding algorithm for distinguishing task in particu
lar an adaptive and bottomup manner 
The rest of this paper is organized as follows in section 2 we briefly review some 
closely related works The proposed virtual cluster tree based distributed data classifi
cation strategy is derived and discussed in section 3 The validity analysis and per
formance evaluation of the approach is presented in section 4 Finally the conclusions 
and future work directions are described in section 5 
2 
Related Work 
The sensor data collection and object count using data mining technology have been 
extensively studied in the literature Ref 1 proposed a distributed data mining model 
for WSN The model poses a threelayer MLP multilayer perceptron for data aggre
gation in the clustered sensor network Input layer and the first hidden layer are  
between cluster members and then output layer and the second hidden layer are be
tween cluster head nodes In WSN the construction of a classifier might be necessary 
if a WSN is intended to classify phenomena in its sensing environment For avoiding 
the costly data transfer of sensor node the possibility of innetwork classification tree 
construction by the SPRINT Scalable PaRallelizable Induction of decision Tree 
algorithms family was evaluated in Ref 2 Concept resourceaware framework on 
data mining online is a very important for monitoring continuously data aggregation 
and innetwork processing The system implementation was introduced by applying 
data monitoring technologies with light weight classification LWClass light weight 
frequent item LWF and light weight clustering LWCluster 3 In order to decrease 
the traffic load in a tunable manner a new lightweight classification algorithm in 
WSN was proposed in Ref 4 By this algorithm node classification is adaptive to 
topology changes and has no constraint on routing protocols and hardware Because 
WSN provides service to the users in Ref 5 a new classification method based on 
SOM SelfOrganizing feature Map for information fusion of WSN to satisfy user’s 
requirements was proposed SOM is powerful in processing magnanimous clustering 
information The advantage is that such classification and disposal method according 
with particular characters of WSN not only can obtain more exact and more general 
information in application but also can assist in intelligence decisionmaking later 
Decision tree is one of the most important models for data classification application 
For a classification of application the data collection behavior of sensor nodes and 
indicating the metrics can suit to the evaluation of particular application classes 6  
Several model for classifications exist for example neural network fuzzy decision 
statistical models virtual machine genetic models and decision trees 711 For ex
ample in Ref 12 a novel decisiontreebased hierarchical distributed classification 
approach in WSN was proposed In the algorithm the classifiers are iteratively  
enhanced by combining strategically generated pseudo data and new local data even
tually converging to a global classifier for the whole network Rajkamal R and  
Vanaja Ranjan P use artificial neural network which is conventional feedforward 
back propagation network to classify the packets based on nature of the data to over
come the traffic in WSN 13 Yet efficient classification and energy saving for WSN 
have not been sufficiently addressed and researched Through the distributed signal 
 
Virtual Cluster Tree Based Distributed Data Classification Strategy 
557 
 
sensing and classification performed by collaborated sensor is proven to be beneficial 
to increasing the modulation classification reliability 14 Better resource efficiency 
was not achieved by collaborating and sharing of sensor nodes in the same geographi
cal region A novel distributed data classification of WSN applications is now pre
sented to facilitate the multifunction sensor collaboration and to form logically sepa
rating multipurpose sensor network using nonlinear data classifier 
3 
Virtual Cluster Tree Based Distributed Data Classification 
Using LLE in WSN 
Imposing some structure within the network to effectively achieve the application 
objectives is an attractive option for the selforganization of largescale WSNs Clus
ter based organization and arranging clusters in form of a tree simplifies many high
erlevel functions and distributed application deployments 15 We use EADEEG an 
energyaware data gathering protocol for wireless sensor networks protocol to form a 
cluster 16 then virtual cluster tree was built by connecting CH nodes observing the 
same phenomenon which was identified using locally linear embedding LLE algo
rithm 17 The virtual cluster tree based distributed data classification strategy in
cludes two phases the virtual cluster tree formation and classifier generation  
using LLE 
31 
The Network Model 
The network model of the strategy was assumed that a set of N energyconstrained 
heterogeneous sensor nodes were randomly deployed in MM twodimensional field 
Each node covers an area of the monitoring region and is responsible for collecting 
environment data All sensor nodes are not mobile and unaware of their location The 
immobile Sink node is only and considered to be a powerful node endowed with en
hanced communication and computation capabilities and no energy constraints Sink 
node received the messages from the cluster head nodes using clusterbased hierar
chical routing approach in WSN A subset of cluster head CH nodes is selected to 
facilitate communication functions with a certain task The communication radius of 
sensor nodes is more than a multiple of the cluster radius for implementing the direct 
communication between cluster head nodes 
32 
The Virtual Cluster Tree Formation  
After the distributed cluster was formed by using EADEEG protocol the network 
topology was built by multihop communication between the CH nodes The virtual 
cluster tree can be formed by providing logical connectivity among these collabora
tive communication CH nodes the CH nodes can communicate directly each other 
Fig1 illustrates the ideal cluster detecting two different events happened Two moni
toring events are located around clusters CH3 CH4 CH15 CH6 CH8 CH9 and 
CH10 CH nodes can be grouped into different virtual cluster tree based on the phe
nomenon they track eg rockslides vs animal crossing or the task they perform  
558 
X Song et al 
 
Sink
CH1
CH2
CH3
CH4
CH5
CH6
CH7
CH8
CH9
CH10
CH11
CH12
CH13
CH14
CH15
CH16
CH17
CH18
 
Fig 1 The ideal clusters detecting two different events happened 
Assume that a cluster member node in cluster CH10 first detects the interested 
event It sends a virtual cluster tree formation message towards the root node sink 
node Firstly the message is forwarded to its own CH10 This is the first time that 
CH10 is receiving this interested event information therefore it marks its self as de
tecting the event happened It then forwards the message to its parent CH9 CH9 
caches the event information about the new virtual cluster tree and forwards the mes
sage to its parent CH6 Finally the message is forwarded to the root node sink 
Meanwhile other nodes in cluster CH8 will also detect the interested event CH8 also 
marks itself as detecting the event The message is then forwarded to its parent CH6 
However CH6 is already aware of the interested event and has already informed its 
parent sink node Therefore CH6 will not forward the message any further CH6 
keeps track that CH6 is also in the interested event Similarly cluster CH15 CH3 and 
CH4 can form another logical tree The virtual trees formed were shown in Fig2 
 
CH1
Sink 
CH2
CH3
CH4
CH5
CH6
CH7
CH8 CH9
CH10
CH11
CH12 CH13
CH15 CH16
CH17
CH14
CH18
 
Fig 2 Virtual cluster tree that connect CH members with special task 
If these trees monitor the same interested event they belong to the same virtual 
cluster tree If not they can be considered as two separate virtual cluster trees Our 
strategy can also enable multiple logical structures to communicate with each other 
because each virtual tree is guaranteed to meet at the root sink node 
 
Virtual Cluster Tree Based Distributed Data Classification Strategy 
559 
 
33 
The Classifier Generation Using Locally Linear Embedding Algorithm 
One technique is to implement the distributed data classification and enforce different 
parts of the sensor network to be active for different application at different times In 
general the energy consumption of computation is significantly smaller than that of 
data transmission and the information representation of monitoring environment are 
formed by processing large numbers of sensory inputs Note the monitoring data of 
the sensors with multifunction has multidimensional and nonlinear feature There
fore our approach utilizes LLE algorithm as the basis of classification for classifier 
generation The LLE algorithm can establish the mapping relationship between the 
observed data and the corresponding lowdimensional data Here is a brief description 
of LLE algorithm That is the main principle of LLE is to obtain the corresponding 
lowdimensional data 

d 
Y Y
⊂ R
 of the training set



N
X X
R
N
d
⊂

 Here is a brief 
description of LLE algorithm 
Step1 Discover the adjacency information For each
ix find its n nearest neighbors 
in the dataset

12
 
jx
j
n
=

 
Step2 Construct the approximation matrix Suppose each data point and its neigh
bors to lie on or close to a locally linear patch We characterize the local geometry of 
these patches by linear coefficients that reconstruct each data point from its neighbors 
Reconstruction errors are measured by the cost function as function 1 
2


n
i
ij
j
i
j
E W
x
W x
=
−


  
 1
The weigh
ij
W summarize the contribution of the th
j
data point to the th
i
reconstruc
tion The weigh
ij
W were computed by minimizing the cost function subject to the con
straints that
1
j
W ij
=

for each i  Each data point
ix is reconstructed only from its 
neighbors enforcing
0
ij
W = if
jx does not belong to neighbors set of
ix  
Step3 Map to embedded coordinates Each highdimensional observation
ix is 
mapped to a lowdimensional vector
iy by minimizing the cost function 2 
2
 
n
i
ij
j
i
j
y
y
W y
φ
=
−


 
 2
The optimal embedding is found by computing the bottom
1
d + eigenvectors of ma
trix

 

T
M
I
W
I
W
=
−
−
 The bottom eigenvector of this matrix which we discard is the 
unit vector with all equal components     
We let the sensor nodes in WSN be organized by multiple virtual cluster trees A 
leaf node builds the classifier with the local sensed training data and sends the inter
ested event to the parent The intermediate CH node periodically checks if there is any 
new class from children If yes it will mark and combine them with its local classifier 
for forming a new virtual cluster tree The base station will build the global classifier 
The processing model of distributed data classification in WSN was shown in Fig 3 
560 
X Song et al 
 
 
Fig 3 The processing model of distributed data classification 
4 
Experiment Results and Performance Evaluation 
In the experimental scenario 200 sensor nodes are randomly distributed be
tween 
0
0
x
y
=
=
and 
400
400
x
y
=
=
with the sink node at location 
200
200
x
y
=
=
 
Each node begins with only 6J of energy and an unlimited amount of data to send to 
the sink The simulation time is 1000 seconds Fig4 shows the nodes deployment and 
the cluster trees that are formed in nodes detecting the same phenomenon  
 
 
Fig 4 Virtual cluster tree by nodes detecting the same phenomenon  
We supposed that two interested event occurred within two different monitoring 
regions For homogeneous data they were buffered in the CH node as training data 
for data classification For heterogeneous data the CH node executed the LLE based 
classifier to recognize for forwarding new marker to parent node We investigate the 
energy consumption which is one of the most important concerns in WSN Fig5 
shows the total energy dissipation of nodes obtained with separate noncooperation 
WSN and two virtual cluster trees formed in implementing different phenomenon 
monitoring From the figure we find that the energy consumption of our strategy is 
much lower than the conventional approach Due to enhanced data classification  
 
 
Virtual Cluster Tree Based Distributed Data Classification Strategy 
561 
 
 
Fig 5 The total energy dissipation of two strategies 
function of CH node the effective dimensionality of the data may be significantly 
lower than the total number of sensor measurements while decreasing the communi
cation requirements 
5 
Conclusion 
In this paper we have proposed a novel virtual cluster tree based distributed data 
classification using locally linear embedding in wireless sensor network The strategy 
can realize the structure representation of original sensed data by LLE to meet the 
classification for forming the virtual cluster tree of the various monitoring task Vir
tual cluster tree is an emerging concept that supports collaborative resource efficient 
and multipurpose wireless sensor networks The experimental results show that the 
proposed strategy can maintain very low communication overhead and decrease the 
energy dissipation We are also interested in evaluating the performance with real 
wireless sensor network 
 
Acknowledgment The research work was supported by open research fund of Key 
Laboratory of Complex System and Intelligence Science Institute of Automation 
Chinese Academy of Sciences under grant No20100106  
References 
1 Hong Y Xu S Wu H Study on Distributed Data Mining Model in Wireless Sensor 
Networks In International Conference on Intelligent Computing and Integrated Systems 
pp 866–869 IEEE Press Guilin 2010 
2 Lantow B Applying Distributed Classification Algorithm to Wireless Sensor Networks
A Brief View into the Application of the SPRINT Algorithm Family In 7th International 
Conference on Networking pp 52–59 IEEE Press Cancun 2008 
562 
X Song et al 
 
3 Parenrenq JM Syarif MI Djanali S et al Performance Analysis of Resourceaware 
Framework Classification Clustering and Frequent Items in Wireless Sensor Networks In 
2011 International Conference on eEducation Entertainment and eManagement pp 
117–120 IEEE Press Bali 2011 
4 Tezcan N Wang W A Lightweight Classification Algorithm for Energy Conservation 
in Wireless Sensor Network In 14th International Conference on Computer Communica
tions and Networks pp 87–92 IEEE Press San Diego 2005 
5 Zhao C Wang Y A New Classification Method on Information Fusion of Wireless 
Sensor Networks In 2008 International Conference on Embedded Software and Systems 
Symposia pp 231–236 IEEE Press Chengdu 2008 
6 Boyd AWF Balasubramaniam D Dearle A et al On the Selection of Connectivity
Based Metrics for WSNs Using a Classification of Application Behavior In International 
Conference on Sensor Networks Ubiquitous and Trustworthy Computing pp 268–275 
IEEE Press Newport Beach 2010 
7 Costa N Pereira A Serodio C Virtual Machines Applied to WSN’s the Stateofthe
art and Classification In International Conference on System and Networks Communica
tions pp 50–57 IEEE Press Cap Esterel 2007 
8 Gök S Yazici A Cosar A et al Fuzzy Decision Fusion for Single Target Classifica
tion in Wireless Sensor Networks In International Conference on Fuzzy Systems pp 1–
8 IEEE Press Barcelona 2010 
9 Imran N Khan A Identifier Based Graph Neuron A Light Weight Event Classification 
Scheme for WSN In Wong KW Mendis BSU Bouzerdoum A eds ICONIP 2010 
Part II LNCS vol 6444 pp 300–309 Springer Heidelberg 2010 
10 Imran N Khan A A Single Shot Associated Memory Based Classification Scheme for 
WSN In Liu D Zhang H Polycarpou M Alippi C He H eds ISNN 2011 Part 
III LNCS vol 6677 pp 94–103 Springer Heidelberg 2011 
11 Yan Z Ansari N Wei S Optimal Decision Fusion Based Automatic Modulation Clas
sification by Using Wireless Sensor Networks in Multipath Fading Channel In Proceed
ings of the 2011 IEEE Global Communications Conference GLOBECOM pp 1–5 IEEE 
Press Kathmandu 2011 
12 Cheng X Xu J Pei J Liu J Hierarchical Distributed Data Classification in Wireless 
Sensor Networks Computer Communications 33 1404–1413 2010 
13 Rajkamal R Vanaja Ranjan P Packet Classification for Network Processors in WSN 
Traffic Using ANN In The IEEE International Conference on Industrial Informatics pp 
707–710 IEEE Press Daejeon 2008 
14 Xu J Su W Zhou M Distributed Automatic Modulation Classification with Multiple 
Sensors IEEE Sensors Journal 1011 1779–1785 2010 
15 Dilum Bandara HMN Jayasumana AP Illangasekare TH Cluster Tree Based Self 
Organization of Virtual Sensor Networks In IEEE Globecom Workshops GLOBECOM 
pp 1–6 IEEE Press New Orleans 2008 
16 Liu M Cao J Chen G et al EADEEG An EnergyAware Data Gathering Protocol 
for Wireless Sensor Networks Journal of Software 185 1092–1109 2007 in Chinese 
17 Roweis ST Saul LK Nonlinear Dimensionality Reduction by Locally Linear Embed
ding Science 29022 2323–2326 2000 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 563–569 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Fault Detection and Isolation in Wheeled Mobile Robot 
Ngoc Bach Hoang1 HeeJun Kang2 and YoungShick Ro2 
1 Graduate School of Electrical Engineering University of Ulsan 
680749 Ulsan South Korea 
hoangngocbachgmailcom 
2 School of Electrical Engineering University of Ulsan 
680749 Ulsan South Korea 
hjkangysroulsanackr 
Abstract This paper presents a fault detection and isolation scheme for 
wheeled mobile robots A nonlinear observer is designed based on the mobile 
robot dynamic model The fault is detected when at least one of the residuals 
exceeds its corresponding threshold After that three observers are activated to 
isolate three types of faults right wheel fault left wheel fault and the other 
changed dynamic faults  
Keywords wheeled mobile robots nonlinear observer fault detection fault 
isolation 
1 
Introduction 
Over last two decades several researches have been investigated about WMR fault 
detection and fault tolerant control 1 2 Until now most of papers in field of fault 
diagnosis for mobile robots only use kinematics model with nonholonomic constraints 
and neglect robot dynamic model to design robot controller As proposed in 3 robot 
dynamic model needs to be considered Recently some approaches have been 
presented in which mobile robot dynamics is taken into account 3 4 On the other 
hand in the robot manipulator diagnosis field faults are successfully detected and 
isolated based on manipulator dynamic models 5 6 However those fault diagnosis 
design techniques have never been applied to the mobile robot system Therefore we 
intend to apply them to the mobile robot system by taking its dynamics into account  
In this paper we present a fault detection and isolation scheme for wheeled mobile 
robots This work is about the extension of the results in 5 8 for wheeled mobile 
robots First a nonlinear observer is designed based on the WMR dynamics The fault 
is detected by comparing the error state observer with its corresponding derived 
threshold Second three observers are constructed for three types of faults right 
wheel fault left wheel fault and all the other dynamic faults Finally the computer 
simulation has been performed and its results show that the developed scheme is very 
effective for the fault detection and isolation for the WMR 
This paper is organized as follows In section 2 the robot kinematics and dynamics 
are described and the fault diagnosis problem is formulated In section 3 the fault 
                                                           
 Corresponding author 
564 
NB Hoang HJ Kang and YS Ro 
 
detection and isolation scheme are discussed The simulation results are shown in 
section 4 Section 5 has some conclusions and future works 
2 
Problem Formulation 
cI  Moment of inertia of the mobile platform about Pc
Im
 Moment of inertia of 
wheel about its diameter
w
I  Moment of inertia of wheel about its rotation axle
c
m  
Mass of the mobile platform without wheels
w
m  Mass of each wheel Oxy  The 
world coordinates system PXY The coordinate system fixed to mobile platform Pc 
Center of mass of WMR without wheels d  Distance from P to Pc b  Distance 
from each wheel to P 
Rr  Radius of right wheel 
Lr  Radius of left wheel 
 
2b
2r
d
φ
Pc
P
o
x
Xc
y
Yc
Free
wheel
 
Fig 1 Differentialdriven WMR 
The configuration of the WMR can be described by five generalized coordinates 
T
l
r
0



y
 x
q
φ θ θ
=
 
The kinematic and dynamic models of WMR are 
S q q
q
0
0


 =
 
1
τ
τ
=
+
+
+
d
F q 
V q 
Mq



  
2
where  
T
L
L
L
R
R
R
0
1
0
2b
r
2 sin
r
2 cos
r
0
1
2b
r
2 sin
r
2 cos
r
S q 










−
=
φ
φ
φ
φ
 








+
+
−
−
+
+
=
2
2
R
2
L
R
2
R L
2
2
R
b
4
r
2
w
b
4
r r
2
b
4
r r
2
b
4
r
2
w
I 
mb
I
I 
mb

I 
mb
I 
 mb
I
M









−
−
=
r q 
r q q

r q 
r q q
Vq
2
R 1
L 1 2
b
4
dr r
m
2
2
L
R 1 2
b
4
dr r
m
2
R L
c
2
R L
c




 

 
S  q F  q 
F q 
0
0
0
T


 =
With 
m b 
2 I
m d 
 I
I
2
w
w
2
c
c
+
+
+
=
and 
w
c
2m
m
m
+
=
 
 
 
Fault Detection and Isolation in Wheeled Mobile Robot 
565 
 
In the presence of fault the WMR dynamic model can be represented as 
T   q q  
t

F q 
V q 

M
q
d
1
τ
ψ
β
τ
τ




−
+
−
−
−
=
−
 
3
The term
T   q q  
t
τ
ψ
β

−
is the fault function The fault time profile is modeled by 
T 
t
i
e i
1
T 
t
−
− −
=
−
α
β
 if 
t ≥ T 
 and 
0
T 
i t
=
−
β
 if 
t  T 
i=12 The 
modeling uncertainty is assumed to be bounded 
η
τ
≤
+
−

 F q 
M
d
1

 
3 
Fault Detection and Isolation Scheme 
31 
Fault Detection 
Let
t 
q
x
T
=
 The dynamic model of eq2 can be rewritten as 

 q q 
T 
 t

F  q 
V  q 

M
x
d
1
τ
ψ
β
τ
τ




−
+
−
−
−
=
−
 
4
The following estimated model is considered 
x 
A xˆ
V q 

M
xˆ
1
−
+
−
=
−


τ
 
5
where 
xˆ ∈ R2

diag a  a 
A
− 1 − 2
=
with 
0
a
a
2
1

are the estimation velocity vector of 
x  and the stable matrix respectively From eqs4 and 5 we get the error dynamic 

 q q 
T 
 t

 F  q 
M
x 
A xˆ
e
d
1
τ
ψ
β
τ



−
+
+
−
−
=
−
 
6
The threshold bound can be chosen as 
M
1
t
0
e
d
 M
exp A t
e
=
−
≤
−

η τ
τ
 
7
Fault detection decision scheme The decision on the occurrence of fault is made 
when at least one of the residual e exceeds its corresponding threshold bound eM  
32 
Fault Isolation 
In this paper we classify all that reasons into three types of faults Right wheel 
fault ψ
1
 right wheel radius change Left wheel fault ψ
2
 left wheel radius change the 
other faults ψ
3
 all remaining faults which make the dynamics change Each fault 
function is described by
3 
 21
s
q 
g 
q 

T
j
s
ij
s
s
=
=


τ
θ
ψ τ
 where 
ij
sθ  i =1 2 j 
=1 m is an unknown matrix assumed to belong to a known compact 
set
2 m
ij
s
Ω ∈ R ×

m
T
j
s
R
g  q 
τ  ∈
is a known smooth vector field More details about 
fault functions see appendix 1 To isolate after a fault is detected three isolation 
estimators are activated 8 Each estimator has the following form as 
q 
g 
ˆ
V q 

M
x 
A  xˆ
xˆ
T
j
s
ij
s
1
s
s
s



τ
θ
τ
+
−
+
−
=
−
 
8
566 
NB Hoang HJ Kang and YS Ro 
 
where
s ˆθij
i =1 2 j =1 m is the estimate of fault parameter
sθij
 
diag a  a 
A
2
1
s
−
−
=
where 
0
a
a
2
1

is a stable matrix The online updating law for 
ij
s ˆθ is derived by using Lyapunov synthesis approach with the projection operator 
restricting 
ij
s ˆθ  to the corresponding known set
2 m
ij
s
Ω ∈ R ×
8 
g e 

P
ˆ
i
i
s
ij
ij
s
s ij
γ
θ
Ω
 =
 
9
where 
ij
γ are the positivedefinite learning rate 
i
sg  are the corresponding smooth 
vector field and 
i
i
i
xˆ
x
e
−
=
 i = 12 are the error state The error dynamics is given 
q 

g
ˆ

 q q 
T 
 t

 F  q 
M
Ae
e
T
j
s
ij
s
d
1




τ
θ
τ
ψ
β
τ
−
−
+
+
−
=
−
 
10
Thus each element of state estimation error is given by 




=
−
−
+
−
−
−
−
−
−
+
−
−
−
−
−
=
t
d
T
2
l 1
l
l
il
i
t
d
T
T
j
s
ij
s

ti
i
t
d
T
T
j
s
ij
s
i
d
i
d
i
i
d
mv  f

exp a t
q d
g 

e
1
a t
exp
d
q
g 
 ˆ
exp a t
T e T 
exp a t
e
τ
τ
τ
τ
τ
θ
τ
τ
τ
θ
τ
τ
α


 11
The following functions are defined to use later for deriving the threshold as 







≥
=

≥
=
0 
q 
 g 
q 
g 
0 
q 
 g 
0
0  h
q 
 g 
0
0 
q 
 g 
q 
g 
h
j
s
j
s
j
s
j
2
j
s
j
s
j
s
1 j






τ
τ
τ
τ
τ
τ
 
12
Because
2 m
ij
s
s ij
θ ∈ Ω ∈ R ×
 there exist two values 
ij
M
m ij
θ  θ
 satisfy the inequality 
ij
M
ij
s
ij
m
ij
M
ij
s
m ij
0
or
0
θ
θ
θ
θ
θ
θ
≤
≤
≤

≤
≤
 
13
In the incipient fault case α is the lower bound of fault evolution rate which 
satisfies
i
i
α ≥ α
 see8 So that the following inequality can be established 
1
e
1
e
1
0
T 
t
T 
t
d
i
d
i
≤
−
≤
−
≤
−
−
−
−
α
α
 
14
From eqs14 and 15 if
0
ij
m
M ij
θ ≥ θ ≥
 we have 16aif
ij
m
0 M ij
 θ ≥ θ
 we have 16b 
ij
ij
M
ij
s

 t T
ij
m

 t T
ij
M

e
1

e
1
m
d
i
d
i
=
≤
−
≤
−
=
−
−
−
−
θ
θ
θ
α
α
 
15
ij
ij
M
T 
t
ij
s
T 
t
ij
ij m
M

e
1

e
1
m
d
i
d
i
=
−
≤
−
≤
=
−
−
−
−
θ
θ
θ
α
α
 
16b
Now let’s combine eqs13 and 16 
j
2
ij
j
1
ij
j
s
ij
s

T
 t
j
2
ij
j
ij 1
h
m
h
M
g

e
 1
h
M
h
m
d
i
+
≤
−
≤
+
−
−
θ
α
 
17
 
 
Fault Detection and Isolation in Wheeled Mobile Robot 
567 
 
Thus the following thresholds can be chosen for isolation decision 



−
−
+
+
−
−
+
−
−
−
−
−
=
t
T
i
i
t
T
j
2
ij
j
1
ij
i
t
T
T
j
s
ij
s
i
d
i
d
i
i
U
d
d
d
d

t
a
exp
h d
m
h
 M
a  t
exp
g  q d
 ˆ
t
a
exp
T e  T 
t
a
exp
e
τ
η
τ
τ
τ
τ
τ
θ
τ

 
18



−
−
−
+
−
−
+
−
−
−
−
−
=
t
T
i
i
t
T
j
2
ij
j
1
ij
i
t
T
T
j
s
ij
s
i
d
i
d
i
i
L
d
d
d
d

t
a
exp
h d
M
h
 m
t
a
exp
g  q d
 ˆ
t
a
exp
T e T 
t
a
exp
e
τ
η
τ
τ
τ
τ
τ
θ
τ

 
19
Fault isolation scheme If one of the state estimation errors 
ie  exceeds its 
threshold
i
L
U i
e  e
 for some time t  Td then the occurrence of that fault is excluded 
4 
Simulation Results 
The WMR parameters Ic = 085Kgm2 Im = 00061Kgm2 Iw = 0012Kgm2 mc = 
29Kg mw = 354Kg d = 0053m b = 0149m rR = 008m rL =008m The 
values of
F q  d
 τ

0 05
F q 
 +τd ≤
Three observers will be used to isolate faults 
O1 right wheel fault O2 left wheel fault O3 other fault First right wheel radius 
suddenly changes from 008m to 007m at T = 10s The fault is detected at Td = 
101s as shown in Fig2ab The outputs of three observers are shown in O1 2c 
2d O2 2e 2f O3 2g 2h The right wheel fault is successfully isolated at Tiso = 
102s Second left wheel radius changes from 008m to 007m at T = 10s The 
fault is detected at Td = 101s as shows in Fig3a 3b The left wheel fault is 
successfully isolated at Tiso = 1015s by observing O1 3c 3d O2 3e 3f O3 
3g 3h Third a fault occurs at T = 10s and results 2kg loss in mass of platform 
The fault is detected at Td = 101s as shown in Fig4a 4b The fault is isolated at Tiso 
= 1022s by observing O1 4c 4d O2 4e 4f O3 4g 4h 
 
Fig 2 Fault detection and Isolation in case of right wheel fault 
568 
NB Hoang HJ Kang and YS Ro 
 
 
Fig 3 Fault detection and Isolation in case of left wheel fault  
 
Fig 4 Fault detection and Isolation in case of the other faults 
5 
Conclusions and Future Works 
In this paper the fault detection and isolation schemes have been proposed for 
wheeled mobile robot By using three observers three types of faults right wheel 
fault left wheel fault and the other faults change robot dynamics are successfully 
isolated after a fault is detected For future research we will find a suitable fault 
accommodation control scheme and test the proposed algorithm in a real mobile 
robot 
 
Acknowledgement The authors would like to express financial supports from Minis
try of Knowledge Economy under Human Resources Development Program for Con
vergence Robot Specialists and under Industrial Core Technology Project 
Appendix 1 The Derivation of the Fault Functions 
We temporarily remove the effect of friction and disturbances in the absence of fault 
 
V q 

M
q
1


−
=
− τ
 
The dynamic model in the presence of fault 
 
V
M 
 M
V 
M
M M 
 M
V 

M
q
1
1
1
Δ
Δ
τ
Δ
Δ
τ
−
−
−
+
−
−
+
−
−
 =
 
 
 
Fault Detection and Isolation in Wheeled Mobile Robot 
569 
 
From 30 and 31 the fault function will have the following form 
 
V
M 
V   M
M
M M 
 M
q 

1
1
Δ
Δ
τ
Δ
Δ
τ
ψ
−
−
+
−
−
+
 = −
 
Right wheel fault 




−
+
+
=
2
1
2
1
R
2
L
R
R
c
R
R
q
q q
r
4b
r
r
m d r
V
r
r
V

 
Δ
Δ
Δ
Δ
 
 


q 
g 
q q
q
V
V
V
V


q 

T
j
1
ij
1
T
2
1
2
1
2
1
2
2
1
1
26
1
21
1
16
1
11
1
1

 


τ
θ
τ
τ
θ
θ
θ
θ
ψ τ
=
−
−





= 
 
Left wheel fault 





−
+
+
=
2
1
2
2
L
2
L
L
R
c
L
L
q
q
q
r
4b
r 
m dr  r
V
r
r
V



Δ
Δ
Δ
Δ
 
 


g  q 
q
q q
V
V
V
V


q 

T
j
2
ij
2
2 T
2
2
1
2
1
2
2
1
1
26
2
21
2
16
2
11
2
2


 

τ
θ
τ
τ
θ
θ
θ
θ
ψ τ
=
−
−





= 
 
The other fault  
nV
V
Δ
Δ
=
 
 


q 
g 
V
V
V
V


q 

T
j
3
ij
3
T
2
1
2
2
1
1
24
3
21
3
14
3
11
3
3


τ
θ
τ
τ
θ
θ
θ
θ
ψ τ
=
−
−





= 
 
References 
1 
Ji M Sarkar N Supervisory Fault Adaptive Control of a Mobile Robot and Its Applica
tion in SensorFault Accommodation IEEE Transactions on Robotics 231 174–178 
2007 
2 
Meng J Hybrid Fault Adaptive Control of a Wheeled Mobile Robot IEEEASME Trans
actions on Mechatronics 82 226–233 2003 
3 
Dongkyoung C Tracking Control of DifferentialDrive Wheeled Mobile Robots Using a 
BacksteppingLike Feedback Linearization IEEE Transactions on Systems Man and Cy
bernetics Part A Systems and Humans 406 1285–1295 2010 
4 
Fierro R Lewis FL Control of a Nonholonomic Mobile Robot Backstepping Kinemat
ics into Dynamics Journal of Robotic Systems 149–163 1997 
5 
Huang SN Kiang TK Fault Detection Isolation and Accommodation Control in Ro
botic Systems IEEE Transactions on Automation Science and Engineering 53 480–489 
2008 
6 
Vemuri AT Polycarpou MM Diakourtis SA Neural Network Based Fault Detection 
in Robotic Manipulators IEEE Transactions on Robotics and Automation 142 342–348 
1998 
7 
Vemuri AT Polycarpou MM Neuralnetworkbased Robust Fault Diagnosis in Robot
ic Systems IEEE Transactions on Neural Networks 86 1410–1420 1997 
8 
Zhang XD Parisini T Polycarpou MM Adaptive Faulttolerant Control of Nonlinear 
Uncertain Systems an Informationbased Diagnostic Approach IEEE Transactions on Au
tomatic Control 498 1259–1274 2004 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 570–578 2012 
© SpringerVerlag Berlin Heidelberg 2012 
The Research on Mapping from DAML+OIL Ontology  
to BasicElement and ComplexElement of Extenics 
Wen Bin12 
1 School of Mechanical Electric  Information Engineering China University of Mining  
Technology Beijing China 
2 School of Information Science and Technology Yunnan Normal University Kunming China 
wenbin315163com 
Abstract DAML+OIL is an important ontology language Basicelement and 
complexelement are the formalization basis of Extenics and used to solve the 
contradiction problems We firstly introduced the DAML+OIL and the basic
element and complexelement Secondly we analyzed the element of 
DAML+OIL and the basicelement and complexelement then gave the 
corresponding relation of them Finally we gave the mapping rules from 
DAML+OIL to the basicelement and complexelement We studied how to 
transform DAML+OIL ontology to the basicelement and complexelement in 
syntax and gave a way to solve the contradiction problem of the knowledge 
domain  
Keywords DAML+OIL Ontology Extenics Basicelement Mapping rules 
Introduction 
For AI communitythere are many definitions of an ontology1An ontology is an 
explicit specification of a conceptualisation2 An ontology consits of the description 
of conceptsthe desciption of properties of each concept and  a  set  of  individual  
instances  of  classes in a domain of discourse1345OILOntology Interference 
Layer is the production of the OnToKnowledge plan OIL unifies three important 
aspects formal semantics and efficient reasoning support as provided by Description 
Logics epistemologically rich modeling primitives as provided by the Frame system 
and a standard proposal for syntactical exchange notations as provided by the Web 
languagesXML and RDFS6 DAMLDARPA Agent Markup Languageis the 
web language which is created by  DARPA DAML allowed users to mark the se
mantic information on their data and it let the computer can “understand” the marked 
information resource7A joint committee with two organizations combined DAML 
and OIL and named it DAML+OIL In December 20008 DAML+OIL is a standard 
description language of Semantic Web910 DAML+OIL is set up based on the 
description logics which only can represent the static knowledge by concepts and 
roles and it is difficult to represent the dynamic knowledge by description logic So 
DAML+OIL can’t describe the essential change of thing which is caused by the inter
nal attribute change and it is difficult to solve the contradiction problem by 
DAML+OIL  
Mapping from DAML+OIL Ontology to BasicElement and ComplexElement of Extenics 
571 
In Extenics we research the possibility of extending the things and the rules and me
thods of development and innovation with the formalization models and use them to 
solve the contradiction problems1112We use the matterelement affairelement rela
tionelement and complexelement to describe the concepts and their relationships In a 
certain level there is a similarity between the ontology and the basicelement and com
plexelementIf the ontology can be mapped to the basicelement and complexelement 
it will help us to use Extenics engineering to solve contradiction problems of the domain 
relvant the ontologyThe Mapping from OWL lite to complexelments is researched in 
article14but OWL Lite is the restrict ontolgoy languageDAML+OIL is an important 
ontology language of the Semantic Web If the DAML+OIL ontology can be mapped to 
the basicelement and complexelement it is useful to solve contradiction problems of 
the Semantic Web In this paper we researched the mapping from DAML+OIL to the 
basicelement and complexelement in syntax and gave the corresponding relation be
tween their elements and their mapping rules    
1 
DAML+OIL 
DAML+OIL is an ontology description language and it describe the domain structure 
which include classes and attributes The basis of DAML+OIL is the RDF triple col
lection DAML+OIL use its vocabulary to express the RDF triple meansThe elements 
of DAML+OIL language are show in table1table2 and table35891013 
Table 1 Property elements 
DAML+OIL element 
statement 
damlObjectProperty 
To state the object property 
damlObjectProperty 
To state the datatype property 
rdfssubPropertyOf 
To define the level relation of properties 
damltoClass 
The universal quantification restriction 
damlhasClass 
The existential quantification restriction 
damlhasValue 
A speceial case of hasClass using enumeration 
damlmaxCardinality 
The maxcardinality restriction 
damlminCardinality 
The mincardinality restriction 
damlcardinality 
The cardinality restriction 
rdfsdomain 
To state the property domain 
rdfsrange 
To state the property range 
damlsamePropertyAs 
To assert one property is equivalent to another property 
damlinverseOf 
To assert one property is inverse to another property 
damlTransitiveProperty 
To assert one property is transitive 
damlUniqueProperty 
To assert one property is unique 
Table 2 Instance elements 
DAML+OIL element 
statement 
damlsameIndividualAs 
To state that objects are same 
damldifferentIndividualFrom 
To state that objects are distinct 
572 
B Wen 
Table 3 Class elements 
DAML+OIL element 
statement 
damlClass 
To define an object class 
rdfssubClassOf 
To define the level relation of classes 
damlsameClassAs 
To assert one class is equal to another class 
damldisjointWith 
To assert one class is disjoint with another class 
damlintersectionOf 
To define the intersection of classes 
damlunionOf 
To define the union of classes 
damlcomplementOf 
To define the complement of class 
2 
BasicElement and ComplexElement of Extenics 
Matterelement and affairelement are the basic concepts of Extenics Extension trans
formation is the basic tool to solve the contradiction problems Extension analysis is 
the basis for exploring the extension transformation1112 Using these we can ana
lyze the extension possibilities of things from a qualitative point of view 
21 
BasicElement 
Matterelement describes the thingsaffairelement describes the eventsrelation
element describes the relationship between the matterelements or affairelements 
Matterelement affairelement and relationelement are collectively called basic
element it is described in detail in article12 
Given an object class O for any O∈O there is vi = ciO∈Vi on the property 
of ci i = 12  n the basicelement set 
 
Oc1V1 
              B=OCV=         c2V2                                  1 
…… 
cnVn 
 
is called class basicelement 1415 
22 
ComplexElement 
The compounded form of matterelement affairelement and relationshipelement are 
collectively called complexelementComplexelement has seven formsmatter
element composites with matterelement matterelement composites with affair
element matterelement composites with relationelement etc The relevant 
description of all complexelement in detail is in the article11  
Given an object class Ocm for any Ocm∈Ocm there is vcmi=ccmiOcm∈Vcmi on 
the property ccmi i = 12  n the complexelement set 
 
 
Mapping from DAML+OIL Ontology to BasicElement and ComplexElement of Extenics 
573 
                         Ocmccm1Vcm1 
CM =OcmCcmVcm=      ccm2Vcm2                                         2 
                               …… 
                             ccmnVcmn 
 
is called class complexelement1415 
3 
Mapping from DAML+OIL to the BasicElement and 
ComplexElement 
31 
Corresponding Relation between DAML+OIL and the BasicElement and 
ComplexElement 
Table 4 The corresponding relation between DAML+OIL element and basicelement and 
complexelement 
DAML+OIL element 
BasicelementComplexelement 
Class 
Class basicelement or class complexelement 
ObjectProperty 
Propery of basicelement or complexelement 
DatatypeProperty 
Propery of basicelement or complexelement 
Datatype Range of one Property 
Text 
Object Range of one Property 
Class basicelement or class complexelement 
Individual 
Basicelement or complexelement 
32 
Mapping Rules of Class 
1damlClass According whether it has the object property or not it is mapped to 
class basicelement or class complexelement If it has the object property it is 
mapped to class complexelement otherwise it is mapped to class basicelement 
2rdfssubClassOf This is mapped to the logical relation ⊆  of classesFor  
example 
damlClass rdfID=Man 
    rdfssubClassOf rdfresource=Person 
damlClass  
Its mapping is CMMan ⊆ CMPerson 
3damlsameClassAs If there is damlsameClassAs between class A and class 
Bits mapping is CMA=CMBFor example 
damlClass rdfID=HumanBeing 
    damlsameClassAs rdfresource=Person 
damlClass 
Its mapping is CMHumanBeing=CMPerson 
4 damldisjointWith If there is damldisjointWith between class A and class Bits 
mapping is CMA ∩ CMB=∅  
 
574 
B Wen 
5damlintersectionOf This is mapped to  ∧ of ExtenicsFor example 
damlClass rdfID=Woman 
  damlintersectionOf rdfparseType=damlcollection 
    damlClass rdfabout=Person 
    damlClass rdfabout=Female 
  damlintersectionOf 
damlClass 
Its mapping is CMWoman=CMPerson∧CMFemale 
6damlunionOf This is mapped to ∨ of ExtenicsFor example 
damlClass rdfID=Parents 
  damlunionOf parseType=damlcollection  
    Class rdfresource=Mother 
    Class rdfresource=Father  
   damlunionOf 
damlClass 
Its mapping is CMParent=CMMather∨CMFather 
7damlcomplementOf  This is mapped to ¬ of Extenics For example 
complementOf  
           Class  
               Class rdfresource=Man 
           Class 
complementOf 
Its mapping is ¬CMMan 
33 
Mapping Rules of Property 
1damlObjectProperty This is mapped to the property whose range is class basic
element or class complexelement of class complexelement 
2damlDatatypeProperty This is mapped to the property whose range is textof 
class basicelement or class complexelement 
3damlhasValue This is mapped to the all range of the property of class basic
element or class complexelement 
4rdfsdomain This is mapped to the domain of the property of class basicelement 
or class complexelement 
5rdfsrange This is mapped to the range of the property of class basicelement or 
class complexelement 
6damlsamePropertyAs This is mapped to the equal property of the property of 
class basicelement or class complexelement 
7damlinverseOf This is mapped to the inverse property of the property of class 
basicelement or class complexelement 
34 
Mapping Rules of Instance 
Individual of instance is mapped to the individual that is basicelement or complex
element of some class basicelement or class complexelementFor example 
Mapping from DAML+OIL Ontology to BasicElement and ComplexElement of Extenics 
575 
damlClass rdfID=Person 
Person rdfID=Wenbin 
Person is mapped to class basicelement BPerson=OPersonnameVname where Vname is 
the collection of all person name Wenbin is mapped “to basicelement BWen
bin=OPersonname “Wenbin” and BWenbin∈BPerson 
DamlsameIndividualAs is mapped to that the basicelements or complex
elements are equal DamldifferentIndividualFrom is mapped to that the basic
elements or complexelements are distinct 
4 
Example of the Mapping from DAML+OIL Ontology to 
BasicElement and ComplexElement 
As basicelement and complexelement have the extended property which include the 
divergence the relevance the implication and the extension We can analyze the ex
tension of basicelement and complexelement to get many approaches to solve the 
contradiction12 After transforming DAML+OIL ontology to basicelement and 
complexelement and extendedly analyzing the intrinsic property of the concept and 
relation which are represented by basicelement and complexelement the contradic
tion problems can be solved The following example illustrates this 
A petroleum company set up the information ontology of its petrol sales with 
DAML+OIL the class and property of the information ontology show in Fig 1 and 
the instance of the information ontology shows in Fig 2 
Using the above mapping rules the DAML+OIL ontology in Fig1 and Fig2 is 
mapped to basicelement and complexelement as follows  
 
 
 
Fig 1 Definition of class and property  
576 
B Wen 
 
Fig 2 Definition of instance  
                    OProvinceSalesRegionNameVProvinceSalesRegionName 
CMProvinceSalesRegion=                 RegionVRegion                                              3 
                               AreaCodeVAreaCode 
                             SalesVolumeVSalesVolume 
Where VProvinceSalesRegionName is the set of all province sales region names 
VProvinceSalesRegionNameVRegionVAreaCode VSalesVolume is the description text respectively 
 
                   OCitySalesRegionNameVCitySalesRegionName 
CMCitySalesRegion=                 RegionVRegion                                                   4 
                             AreaCodeVAreaCode 
                          SalesVolumeVSalesVolume 
 
WhereVCtitySalesRegionName is the set of all city sales region namesVCitySalesRegionName 
VRegionVAreaCodeVSalesVolume is the description text respectivelyand CMCitySalesRe
gion ⊆ CMProvinceSalesRegion 
 
                   OYunnanName“Yunnan” 
      BYunnan=          Region“Yunnan”                                      5 
                     AreaCode“087” 
                 SalesVolume“10 million tons” 
Mapping from DAML+OIL Ontology to BasicElement and ComplexElement of Extenics 
577 
 
                  OYunnanProvinceName“YunnanProvince” 
 
BYunnanProvince=               Region“YunnanProvince”                        6 
                         AreaCode“087” 
                      SalesVolume“10 million tons” 
 
                  OKunmingName“Kunming” 
       BKunming=        Region“Kunming”                                    7 
                      AreaCode“0871” 
                  SalesVolume“4 million tons” 
 
                 OKunmingCityName“KunmingCity” 
BKunmingCity=             Region“KunmingCity”                            8 
                      AreaCode“0871” 
                  SalesVolume“4 million tons” 
 
                 OZhaotongName“Zhaotong” 
 BZhaotong=          Region“Zhaotong”                                        9 
                 AreaCode“0870” 
               SalesVolume“2 million tons” 
 
                   OZhaotongCityName“ZhaotongCity” 
  BZhaotongCity=             Region“ZhaotongCity”                            10 
                         AreaCode“0870” 
                      SalesVolume“2 million tons” 
 
BKunmingBKunmingCityBZhaotongBZhaotongCity∈CMCitySalesRegion ⊆ CMProvinceSalesRegion 
BYunnan BYunnanProvince∈CMProvinceSalesRegion 
BKunming=BKunmingCityBZhaotong=BZhaotongCityBYunnan= BYunnanProvince 
 
If you find the sales volume of some region is too low and the petrol is surplus and 
the sales volume of other region is too large and the petrol is shortage we can exten
dedly analyze the property of the above basicelement and complexelement and find 
the way to solve the problem of the petrol shortage and overstock 
5 
Conclusion 
DAML+OIL and the basicelement and complexelement can describe the concepts 
and their relationship in some domain But DAML+OIL only can describe the static 
concept the basicelement and complexelement which can be with the parameter 
variables can describe the dynamic things and it conforms to the reality DAML+OIL 
can’t solve the contradiction problems and the basicelement and complexelement 
have the extension ability In this paper we researched the mapping from 
DAML+OIL to the basicelement and complexelement in syntax we firstly analyzed 
the 
corresponding 
relation 
from 
DAML+OIL 
to 
the 
basicelement 
and  
578 
B Wen 
complexelement then gave the mapping rules from DAML+OIL to the basic
element and complexelement In the future we will study their mapping in semantic 
and the application of conversion from DAML+OIL to the Extenics 
References 
1 Noy NF McGuinness DL Ontology Development 101 A Guide to Creating Your 
First Ontology Stanford Knowledge Systems Laboratory Technical Report KSL0105 
and Stanford Medical Informatics Technical Report SMI2001—0880 2001 
2 Gruber TR A Translation Approach to Portable Ontology Specifications Journal of 
Knowledge Acquisition 5 199–220 1993 
3 Yu LC Wu CH Jang FL Psychiatric Document Retrieval Using a DiscourseAware 
Model Journal of Artificial Intelligence 173 817–829 2009 
4 Yu LC Chan CL Lin CC Lin IC Mining Association Language Patterns Using a 
Distributional Semantic Model for Negative Life Event Classification Journal of Biomed
ical Informatics 44 509–518 2011 
5 Lai YS Wang RJ Hsu WT A DAML+OILCompliant Chinese Lexical Ontology 
In 19th International Conference on Computational Linguistics Taipei 2002 
6 Fensel D et al OIL An Ontology Infrastructure for the Semantic Web Journal of IEEE 
Intelligent Systems 16 38–45 2001 
7 The DARPA Agent Markup Language DAML EBOL httpwwwDamlorg 
8 Yi QW Li SP Semantic Web Language DAML+OIL and its Initiatory Application 
Journal of Computer Science 30 139–141 2003 
9 Mcguinness DL Fikes R Hendler J Stein LA DAML+OILAn Ontology Language 
for The Semantic Web Journal of IEEE Intelligent Systems 17 72–80 2002 
10 Horrocks I PatelSchneider PF van Harmelen F Reviewing The Design of DAML + 
OIL An Ontology Language for The Semantic Web In Proc of the 18th National Confe
rence on Artificial Intelligence pp 792–797 2002 
11 Cai W Yang CY He B Extension Logic Initial Science Press Beijing 2003 
12 Yang CY Cai W Extension Engineering Science Press Beijing 2007 
13 van Harmelen F et al Reference Description of the DAML+OIL Ontology Markup 
Language 2001 httpwwwdamlorg200103referencehtml 
14 Liu ZM Li WH Tan JX Research on Mapping OWL Ontology to Complex
Elements Journal of Guangdong University of Technology 26 78–83 2009 
15 Tan JX Li WH Liu ZM Research on Database Retrieval for ComplexElement 
Journal of Guangdong University of Technology 25 57–61 2008 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 579–585 2012 
© SpringerVerlag Berlin Heidelberg 2012 
The Study of Codeswitching in Advertisements 
Wang Hua 
Department of BasicsChinese People’s Armed Police Force Academy Langfang Hebei 
065000 China 
wanghuahuawang126com 
Abstract Codeswitching has become one of the most common language  
phenomena in these days Researchers have launched various studies on codes
witching from different perspectives and thus achieved different findings The 
ChineseEnglish codeswitching in Chinese advertising discourse can be catego
rized into three types insertional codeswitching alternational codeswitching 
and diglossia codeswitching Moreover the insertional codeswtiching can be 
further divided into letter insertion lexical and phrasal insertion clausal inser
tion and discourse insertion ChineseEnglish codeswitching in ads is a normal 
linguistic and social phenomenon The purpose of advertisers’ applying Chi
neseEnglish codeswitching is mainly to realize their final commercial goals 
Good ChineseEnglish codeswitching in ads can achieve good results while 
overuse and misuse of it may leave bad impressions on consumers Therefore 
proper guidance should be offered and other actions should be taken immediate
ly in order to address the problems in ChineseEnglish codeswitching in ads and 
to maintain a healthy development of Chinese advertising which will facilitate 
the harmonious coexistence between English and Chinese in Chinese advertis
ing thus contributing to the final social harmony 
Keywords codswitching advertiding reasons problems solutions 
1 
Introduction 
With the introduction of economic reform and opendoor policy China develops 
closer ties with the west especially the Englishspeaking countries for the political 
economic and cultural purposes which unavoidably leads to the language contact As 
one of the outcomes of language contact ChineseEnglish codeswitching appears 
constantly in people’s daily communications A number of scholars have launched 
various researches on codeswitching from different viewpoints but little attention has 
been paid to the ChineseEnglish codeswitching in advertising This paper attempts to 
further study the phenomenon of ChineseEnglish codeswitching in advertising What 
types could the ChineseEnglish codeswitching in ads be classified into What might 
be the reasons for which the advertisers make use of the ChineseEnglish codeswitch
ing during their composition of the advertising discourse What problems are existing 
in the ChineseEnglish codeswitching in ads 
2 
Codeswitching and Advertising  
It is Hans Vogt 1 who introduced codeswitching for the first time as a psychologi
cal phenomenon in 1954 codeswitching in itself is perhaps not a linguistic  
580 
H Wang 
phenomenon but rather a psychological one and its causes are obviously extra
linguistic Up to the present a variety of definitions have been proposed by different 
researchers 234 based on their research focuses and the data they use We prefer 
to choose a generally accepted definition codeswitching is the alternate use of two or 
more languages within the same conversation or utterance 
A French advertising agent Robert says that what we are breathing today are oxy
gen nitrogen and advertising we are swimming in the ocean of advertising It is no 
exaggeration As consumers we are exposed everyday to hundreds and even thou
sands of commercial messages in the form of newspaper ads publicity event spon
sorships or TV commercials No doubt advertising is getting increasingly important 
and has become an essential part of our life It is clear from this definition that adver
tising by its very nature is a kind communication with the purpose of convincing the 
target audience about something ideas goods or services especially persuading 
them into the action of purchasing 
3 
The Types of ChineseEnglish Codeswitching in Advertising 
Three kinds of codeswitching in Chinese advertising are insertional codeswitching 
alternational codeswitching and the diglossia codeswitching 5 
31 
Insertional Codeswitching in Ads 
Insertional codeswitching occurs when the language items from English are put into 
the grammatical framework of a discourse made up of Chinese According to the 
linguistic structure of the English items the classification of insertional codeswiching 
is shown in Table 1 
Table 1 The Classification of Insertional Codeswitching 
Insertional codeswitching 
Letter insertion Lexical and phrasal insertion Clausal insertion Discourse insertion 
 
As the name indicates letter insertion refers to the codeswitching in which an Eng
lish letter is embedded into the Chinese advertising If the pronunciations of the  
inserted letters get associated with some Chinese elements the effect may be more 
powerful especially when an English pronunciation is punned into a certain Chinese 
syntactic frame Lexical and phrasal insertion refers to the English elements at levels 
of lexicon and phrase that are interposed in the Chinese discourse In fact such inser
tion may be regarded as the socalled intrasentential codeswitching in Poplack’s 6 
division It involves switching languages at sentential boundaries and occurs within 
the same sentence or sentence fragment The clausal insertion includes the codes
witching in which one or more English sentences are put into the Chinese discourse 
Different from lexical and phrasal insertion it is similar to the socalled intersenten
tial codeswitching which involves switches of codes from one language to the other 
between sentences It is a fact that not all the ChineseEnglish codeswitching are the 
sheer lexical phrasal or the complete clausal insertion Usually in most ads there is a 
 
The Study of Codeswitching in Advertisements 
581 
mixture of the abovementioned two or even three types of insertions Discourse in
sertion refers to the codeswitching in which a discourse from English as an entire unit 
is inserted or embedded into another discourse from Chinese 
32 
Alternational Codeswiching in Ads 
Alternational codeswitching means that two or more languages English and Chinese 
in the present study appear alternately and symmetrically in the same clause or dis
course 
33 
Diglossia in Ads 
Diglossia originally is a term used to describe a situation existing in a speech commu
nity where people use two very different varieties of language to perform different 
functions But in this section we borrow diglossia to analyze the ads in which both 
Chinese and its English equivalent are employed to express the same meaning Apart 
from the complete diglossia there exist many ads in which only part of the Chinese 
elements is accompanied by the English translation We call this form of codeswitch
ing partial diglossia Comparatively speaking partial diglossia is more popular than 
complete diglossia And it appears most frequently in the switching of company 
names product brand names and slogans 
4 
The Reasons for the Codeswitching in Advertising 
41 
Codeswitching as Adaptation to the Linguistic Reality 
The adaptation to linguistic reality in the Chinese–English codeswitching means the 
adaptation to the linguistic existence and linguistic properties of both Chinese and 
English 
Every language possesses its own linguistic existence that other languages do not 
share It is the same with the language of Chinese and English By reasons of dissimi
lar geographical positions natural environment culture and social practice and the 
like the linguistic elements and structures of the two languages that are employed to 
express the same objective can not be in complete agreement Consequently when the 
linguistic existence possessed uniquely by either English or Chinese is needed it is no 
surprise that the language users including advertisers will resort to codeswitching to 
fill the lexical gap between the two languages so that the communication could be 
continued smoothly 
This specific adaptation mainly involves the codeswitching triggered by differenc
es in the semantic features contained by a particular Chinese expression and its Eng
lish equivalent 
Linguistically speaking different languages can express the same meaning But 
when it comes to the actual use this usually can not be true due to the complexity of 
meaning Sometimes though a word has a counterpart in other languages there still 
exist subtle semantic differences between them Once the language users become 
aware of the subtlety they will possibly switch to another language for a certain pur
pose As to the advertising discourse there are many instances of codeswitching  
582 
H Wang 
resulted from the advertisers’ desire to adapt to the linguistic features of Chinese and 
English 
42 
Codeswitching as Adaptation to the Social Factors 
Being a mirror of the society language use is always closely tied up with a complex 
social world The social world is so complex that there is no principled limit to the 
range of social factors to which the linguistic choices are interadaptable Nonethe
less just as Verschueren 7 remarks phenomenon of the utmost importance in the 
relationship between linguistic choices and the social world are the setting institu
tion or communityspecific communicative norms that have to be observed The 
social settings and norms can regulate the content and means of the actualization of a 
speech event greatly 
With regard to the Chinese advertising the social world to which the codeswitch
ing adapts is from two aspects One is the adaptation to the social environment on the 
macrolevel which concerns the widespread of English in China And the other is the 
adaptation to a certain social convention or custom in terms of the microlevel 
43 
Codeswitching as Adaptation to the Psychological Motivations 
Psychological motivations do work powerfully on the communicators’ language 
choice making since their motives or intentions influence or even decide not only 
what to say but also how to say for instance switching from one code to another 
Given that advertising is a sort of oneway communication from the advertisers to 
the consumers the psychological motivations in this study refer to the advertisers’ 
spontaneous motives or intentions behind their performing of ChineseEnglish co
deswitching in editing the advertising discourse Usually the advertisers make use of 
codeswitching which is itself a communicative strategy as has been indicated earlier 
to satisfy different motives or intentions including their assumptions on the expecta
tions of the addressee Once the assumptions are in conformity with the real situa
tions various functions will be fulfilled in the form of a series of communicative 
strategies and the codeswitching will prove to be a success 
The ChineseEnglish codeswitching as adaptation to psychological motivations in 
advertising is rather complicated as the analysis shows codeswitching as attention 
seeking strategy codeswitching as foreign flavor gaining strategy codeswitching as 
authenticitykeeping strategy codeswitching as solidaritybuilding strategy codes
witching as addressee selection strategy codeswitching as convention strategy and 
codeswitching as decoration strategy However it must be made clear that the adapta
tion to psychological motivations is rather complicated and the above list is far from 
being complete there is still more to be identified and discussed 
5 
Problems in the Code Switched Ads and Possible Solutions 
There is no doubt that the ChineseEnglish codeswitching as a communicative strategy 
can function to be much more persuasive if it is used by the advertisers rationally and 
efficiently But this is by no means an easy task The advertisers have to be very careful 
with the employment of English in their work otherwise they may make mistakes which 
will in the end destroy their work and also cause economic loss for the companies 
 
The Study of Codeswitching in Advertisements 
583 
51 
Problems 
The author finds there does exist some problems in the code switched Chinese adver
tising which are illustrated as follows 
First of all it seems that in some ads the English code is abused or overused so 
much that it makes the customers especially those who are less proficient in English 
feel hard to catch the main information conveyed  
The second problem lies in the advertisers’ misjudgments on public reactions to the 
code switched ads In China some advertisers normally overestimate the power of 
English and show a blind trust on codeswitching without considering the specific 
context Take “Da Bao SOD Mi” a cosmetic well known by Chinese for instance 
SOD here is a kind of chemical ingredient which is helpful to prevent the skin from 
premature senility The reason why the advertisers adopt “SOD” into the brand name 
as an important constituent of the ad is probably based on the psychological motiva
tion of implying the hitechnology to the consumers But unfortunately the spelling 
of “SOD” is happen to be identical with that of “sod” a swear word used mainly to 
express anger annoyance or contempt So how will the English native speakers react 
to such cosmetic if it is promoted in the international market  
The excessive craziness for the foreign flavor gives rise to another serious prob
lem If it is acceptable for those produced in the west to have English names so that 
the English name will enable the perfect match of the name and content then what do 
you think of the English brand names for some commodities which are manufactured 
for the domestic market As a Chinese sometimes we get so confused by a surge of 
English brand names in the ads such as “Canadian Garden” “Monte Carlo Villa” and 
“Manhattan Square” ect that we can not help wondering whether we are living in 
China Predictably such ads will leave no good impressions on the consumers and 
may even arouse their antipathy let alone motivate them to buy  
Besides the problems stemming from the advertisers’ less proficiency in English 
also spread all over the code switched ads Misspellings grammatical mistakes and 
inappropriate translations are not uncommon Translation problems in the diglossia 
ads or partial diglossia ads appear even more frequent The author once read a piece 
of diglossia ad in which “Bing Tang Yan Wo”is translated as “Bird’s nest” Such a 
nest is made of straw and mud and how can it be nourishing and nutritious 
52 
Solutions 
In spite of various problems mentioned above we have no reasons to reject all the 
code switched ads like throwing the baby out with the bath water On the contrary as 
an inevitable result of language contacts and social development the codeswitching in 
Chinese advertising does reflect the real life and has very much potential impact onto 
the social language Confronted with the negative influences what we should do is 
not to be annoyed with the language of code switched ads and argue that it does noth
ing good but destructing the purity of Chinese In fact it is not the language itself but 
the people who abuse the language deserve the blame So instead of forbidding the 
occurrence of English in Chinese ads more considered guidance should be provided 
so that such codeswitching can be used properly 
First efforts should be made to help people realize that codeswitching is a natural re
sult of language contacts also an inevitable and essential language phenomenon The 
584 
H Wang 
quality of a product actually has nothing to do with the language in which it is advertised 
As the advertiser they should set up good professional ethics and adopt the language of 
English into their ads in a proper way Meanwhile both the cultural factors and accepta
bility of the addressees should be taken into considerations by them when editing the ads 
As the consumer however they should develop rational consuming concepts but not 
merely pursue fashion and go after westernization blindly  
 Second as to those linguisticrelated errors they are very easy to be avoided by 
the advertisers through careful review of the translated version or seeking help from 
an English expert 
Last but not least a more efficient monitoring system should be established On the 
one hand the government should take measures to protect the consumers from being 
misled by the appearance of English code in some ads On the other hand the con
sumers themselves should keep an eye on and defend their rights against some false 
code switched ads when it is necessary 
Certainly there are many other different ways to solve the problems existing in the 
code switched ads But anyhow a balanced relationship between the embedded Eng
lish and the matrix Chinese is a must for the healthy development of Chinese advertis
ing And we still need make more efforts to facilitate the harmonious coexistence of 
the two languages and thus contribute to the final social harmony 
6 
Conclusion 
The ChineseEnglish codeswitching in ads can be classified into three types including 
the insertional codeswitching alternational codeswitching and diglossia codeswitch
ing The reason for which the advertisers switch codes between Chinese and English 
is that they need adapt to the contextual elements including the linguistic reality so
cial factors and psychological motivations so as to realize their communicative  
purposes But the boundaries among the adaptation to these three elements are not 
definitely clearcut because there exists overlapping in some cases Furthermore the 
author has found some problems from the advertisers’ application of ChineseEnglish 
codeswitching which involves English abuse or overuse linguistic errors and so on 
To fight against the negative influences reasonable guidance should be provided such 
as setting up good professional ethics and rational consuming concepts establishing 
efficient monitory system and advocating correct English ect but not simply prohi
biting the occurrence of ChineseEnglish codeswitching in ads 
Against the backdrop of growing economic globalization it is predictable that Chi
neseEnglish codeswitching in Chinese ads will become increasingly intense in the 
not too distant future Therefore for language policy makers instead of forbidding the 
occurrence of English in fields such as advertising they should look for ways in 
which people can prepare for their encounter with this phenomenon Nothing is more 
effective than education in making a population critically alert and empowered to deal 
with the ongoing realities of language contact 
Acknowledgments I’m indebted to a number of people for their invaluable assis
tance in the preparation and completion of this paper writing especially my teachers 
friends and family members whose loving consideration and helps are the source of 
my strength 
 
The Study of Codeswitching in Advertisements 
585 
References 
1 
Vogt H Language Contacts in Word pp 365–374 1954 
2 
Di Pietro R Codeswitching as A Verbal Strategy Among Bilinguals In Current Themes 
in Linguistics Bilingualism Experimental Linguistics and Language Typologies Hemis
phere Publishing 1977 
3 
Valde’s Fallis G Social Interaction and Codeswitching Patterns A Case Study of Span
ish English alternation In Bilingualism in the Bicentennial and Beyond Bilingual Press 
1976 
4 
Scotton C Ury W Bilingual Strategies The Social Functions of CodeSwitching Lin
guistics 193 5–20 1977 
5 
Muysken P Codeswitching and Grammatical Theory In Two Languages Cross
disciplinary Perspectives on Codeswitching Cambridge University Press 1995 
6 
Poplack S Sometimes I Start A Sentence in Spanish Y Ternimo En Espanol Towards A 
Typology of Codeswitching Linguistics 581–618 1980 
7 
Verschueren J Understanding Pragmatics Arnold 1999 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 586–593 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Powered Grid Scheduling by Ant Algorithm 
Feifei Liu12 and Xiaoshe Dong1 
1 Department of Computer Science and Technology Xi’an Jiaotong University Xi’an China  
2 Engineering University of Armed Police Force of China Xi’an China 
liuffwjstuxjtueducn xsdongmailxjtueducn 
Abstract The grid environment has the characteristics of distribution dynamic 
and heterogeneous How to schedule jobs is one of most important issues of 
computing grid To address this problem this paper presents a novel reputation
based ant algorithm in the computing grid scheduling The reputation is a 
comprehensive measure and used to reflect the ability of compute node or 
network for a longrunning stability The reputationbased ant algorithm 
introduce reputation index both in tasks and resources to the local and global 
pheromone Experimental results show that the reputationbased ant algorithm 
outperforms Round Robin MinMin and reputation based MinMin in 
makespan and system load balancing 
Keywords improved ACS reputation grid scheduling SimGrid 
1 
Introduction  
Grid computing is now being researched by more institutions and scholars as a new 
dynamic multivirtual organization domain heterogeneous resource sharing and col
laboration processing technology At the same time grid scheduling is getting more 
attention as an important function module of the grid 
Traditional algorithms of distributed grid scheduling aim to better complete task
resource mapping The dynamic distributed and heterogeneous characteristics of the 
grid environment decide that the instability and noncredit factors in the grid envi
ronment and these factors may be caused by the inherent instability of system re
sources or behavior of some malicious viruses Therefore paying more attention to 
reputation of physical resource in job scheduling will be brings better quality of grid 
service 
In this article we regard the reputation of running environment which users job 
expect as a request QoS and deal the reputation mapping between user job and 
physical resource An improved ant algorithm is being used to solve the resource 
allocation problem with the special QoS constrains The reputation mapping relation 
is reflected in the pheromone initialization and update process of ant colony algo
rithm 
This paper is organized as follows Section 2 will discuss the background and re
lated work of grid job scheduling In section 3 an improved ant scheduling algorithm 
is introduced In section 4 we present and discuss the simulation architecture and 
show the experimental results We conclude this study in section 5  
 
Powered Grid Scheduling by Ant Algorithm 
587 
 
2 
Related Works 
Early grid scheduling system often focused on how best to complete the mapping 
from tasks to resources But with the continuous development of grid technology as 
well as the proposed Open Grid Services Architecture Quality of Service QoS has 
became an important factor which should be considered in job scheduling Generally 
speaking QoS is a comprehensive metric to measure user’s satisfaction with the 
service It describes certain performance characteristics of a service and these 
performance features are visible to the user 
As an earlier research Qos was firstly introduced into grid job scheduling in 1 
The work presented by Zhiang et al 2 considers multidimensional QoS indicators 
in the grid scheduling and proposes user satisfaction function with more detailed 
description There are also a number of works which are based on special QoS 
metrics These indicators usually have a specific influence to efficient completion of 
user jobs In 3 based on the grid trust model and trust utilization functions a 
computational service scheduling problem based on trust QoS enhancement is 
proposed Although this method can effectively reduce the risk of failure of user job 
execution complicated relationship of mutual trust model also brings more 
complexity for resource allocation and job scheduling 
Since various features of the grid itself the traditional grid scheduling more likely 
use heuristics algorithm to solve the resource allocation problem The related work 
introduced by 46 indicate that heuristic algorithm also applies to the Marketbased 
job scheduling strategy With the introduction of the concept of QoS the authors of 
7 not only proposes a hierarchical structure of grid QoS but also apply these 
researches to improve on MinMin algorithm  
Ant algorithm was first introduced by Dorigo in 8 The work presented by Z Xu 
et al 9 apply ant colony algorithm to the grid scheduling and consider the reward 
and punishment factors in pheromone update A Balanced Ant Colony Optimization 
BACO algorithm presented in 10 has a major contributions to balance the entire 
system load while trying to minimize the makespan of a given set of jobs The scheme 
proposed in 11 is more focused on giving better throughput with a controlled cost 
In 12 a modified ant algorithm combined with local search is proposed which takes 
into consideration the free time of the resources and the execution time of the jobs to 
achieve better resource utilization Unlike those methods mentioned above we will 
focus on how to solve the job schedulig based reputation by ant algorithm in this 
article 
3 
Improved Ant Algorithm 
31 
The Scheduling Model 
Resources in grid environment are heterogeneous which includes not only high
performance cluster but also the ordinary household hosts and most of them run jobs 
which are communicationintensive or computingintensive 
588 
F Liu and X Dong 
 
This study abstracts and encapsulates a variety of heterogeneous resources in a grid 
environment Grid physical resources can be simplified by 
i 
G
= Node Network
  
where 

1
i
node i
M
∈
is the set of servers hosts Each node can be characterized by a 
set of independent parameters including of computing power and the reputation val
ue which can express as  
 Re
Node
Computation Power
putation
=
  
1
The core attribute of nodes is the computing power In addition the longrunning 
stability is also an important attribute of nodes which may be reflected in reputation 
of node 
Based on the definition of the node we can describe the grid links and networks 
with the aim of statistical analysis of communication overhead In essence the net
work is a weighted undirected graph each vertex represents a mesh node each edge 
represents a data link and the weight of edge is depended on many factors eg 
bandwidth and latency We can describe the network as 




 Re
 | 

i
j
Network
Node Node
Bandwidth Latency
putation
i j
M
=
∈
 
2
where reputation of network is a important value which reflect the stability of net
work between distinct nodes 
In the ServiceOriented grid environment how to improve the QoS of the grid and 
satisfy the users request is much more urgent to be resolve Generally the user only 
concerns whether their computation goals have been achieved From users viewpoint 
how to manage the gird and complete the job is insignificant At the same time some 
specific requirements for the QoS are also requested by user So user’s request can be 
described like the formula 3 
Request


= Task Input QoSrequest
  
 3
In our study we take the reputation as the mainly QoS indicator That is to say us
ers of grid hope that their tasks are dispatched to those nodes owned a reputation val
ue which exceeds a baseline So we can describe the QoS as 
Re
node Re
network
Qos
putation
putation
=
  
4
32 
The Algorithm Improvement 
Let 
1
2

 
m
R
r r
r
=
，
denotes M heterogeneous hosts in the grid and 
1
2

 
n
T
t t
t
=
，
de
notes N independent tasks in the grid In order to apply ant algorithm we should first 
map the ant colony to scheduling systems Ant is on behalf of a mapping solution of 
taskresource An ant goes from the task i to node j denotes that task i is scheduled to 
service host j and executed on it The ACS algorithm based reputation is described in 
Table 1  
 
 
Powered Grid Scheduling by Ant Algorithm 
589 
 
Table 1 The ACS algorithm based reputation 
Initiate 



e 
α β ρ C Cp
 
While NC  NCMAX do 
    Initialize Allowed 
    for all ants 
ka in 
k
Allowed  
        for all tasks 
it in T  
            find the largest value 
ij
τ  in τ  
            update the local pheromone 
        end for 
        Wait for all the tasks to finish when it finishes  
        update the local pheromone 
        update the reputation of node 
    end for 
update the global pheromone 
end while 
In this process pheromone denotes the weight of the path from task to resource The 
higher the pheromone the greater the weight also the possibility of the corresponding 
allocation solution is higher The pheromone matrix is as follows 
11
1
1
m
n
nm
τ
π
τ
τ
τ
=














 
5
where every 
ij
τ represents the degree of match between the node i and task j The ini
tial value of 
ij
τ can be calculated by 
1

Re


init
ij
ij
ij
ij
TT
WT
ET
putation
Match
τ
−
=
+
+
+
 
6
In formula 6 
ij
ET  represents the expected execution time for task 
it  to execute on 
node
jh  given that 
jh  is idle 
ij
TT  denotes the expected transmission time for task 
it to transmit to node 
jh  and 
ij
WT  is the time that task 
it  need to wait before node 
jh  begin to execute it 
ij
ET  and 
ij
TT can be calculated respectively by 
ij
Computation size
ET
Computing Power
=
 
 7
ij
Communication size
TT
Latency
Bandwidth
=
+
 
8
590 
F Liu and X Dong 
 
The smaller 
ij
ET  
ij
TT  and 
ij
WT  can be the higher possibility allocating the task to 
this resource is Similarly the task is hopeful to assigned to host in which the task and 
resource reputation value is more closer 
Stochastic transmission probability P
k
ij denotes the probability of the kth ant sche
duling job i to host j P
k
ij  can be calculate by formula 9 where
k
allowed denotes the 
tasks that kth ant can schedule next step 
1

 



 

P
0
ij
ij
k
m
k
ir
ir
ij
r
i
allowed
otherwise
α
β
α
β
τ
η
τ
η
=
∈
=





 
 9
Visibility factor 
ij
η is given by 
1
1
1

Re

ij
j
Latency
putation
Bandwidth
Power
η
−
=
+
+
+
 
10
The rule of job scheduling and state transfer is as follows the kth ant which just sche
duled job s selects job t that will be scheduled next step and the corresponding execu
tion node r through the rule given in 11 
0
12 
arg max 


 


9
k
tr
tr
t allowed
r
m
if q
q
t r
T R
otherwise use
α
β
τ
η
∈
∈
⋅


=




 
11
In formula 11 q  is a random number uniformly distributed in 01 
0q is a parame
ter 
0
0
1
≤ q
≤
 

 T R
 is a random variable selected from the probability distribu
tion given in 9 
Local pheromone update formula is as 
1

ij
ij
τ
ρ τ
=
−
+
Δτij
0
1
 ρ
  After the job 
is scheduled and before it is executed update the pheromone by 
1

Re


ij
ij
ij
K
TT
ET
putation
Match
τ
−
Δ
=
=
+
+
 
12
After the task execution succeed the reputation value of the node would update 
meanwhile the pheromone would update as follows 
ij
Ce
K
Δτ
=
×
 which 
e
C  is the 
encourage factor If job execution failed the reputation value of the node would up
date meanwhile the pheromone would update as follows 
ij
Cp
K
Δτ
=
×
 which 
p
C  
is punish factor 
4 
Experimental Test 
Based on SimGrid simulation library this paper developed RoundRobin MinMin 
Reputation based MinMin and Reputation based ACS scheduling algorithm The ant 
algorithm improved in this paper is based on the implementation of the Ant Colony 
 
Powered Grid Scheduling by Ant Algorithm 
591 
 
System ACS uses the pseudorandomproportional rule to replace state transition rule 
for decreasing computation time of selecting paths and update the pheromone on the 
optimal path only It is proved that it helps ants search the optimal path 
41 
Simulation Architecture 
There are many kind of projects related to grid simulator such as SimGrid GridSim 
ChicSim GSSIM Alea etc They are especially powerful for development and testing 
of new grid scheduling algorithms SimGrid13 is a toolkit that provides core func
tionalities for the simulation of distributed applications in heterogeneous distributed 
environments The specific goal of the project is to facilitate research in the area of 
distributed and parallel application scheduling on distributed computing platforms 
ranging from simple network of workstations to Computational Grids It provides 
highlevel user interfaces for researchers to use in either C or Java It seeks a com
promise between execution speed and simulation accuracy because the main perfor
mance matrix in the gird domain is makespan of every application Our experiment is 
based on SimGrid 
 
 
Fig 1 Simulation Architecture 
The Simulation Architecture is illustrated in Fig1 It consists of a scheduling agent 
that implements different scheduling strategies in the computing grid The deploy
ment and platform scheme of the system which include tasks resources and their 
properties are all loaded form the XML files Information Agent is responsible for 
collecting these data and storage them which is very useful for the scheduling agent 
to retrieve It is the prediction agent’s responsible to provide TT WT and CT  
42 
Implementation Environment 
Job attributes We suppose that there are 200 independent tasks We only consider 
onedimensional QoS reputation Reputation is a random value in 0 1 The task 
computing size is a random value in 0 100000000 the task transmission size is also 
a random valued in 0100 This fully guarantees the heterogeneity of tasks  
Resource attributes We suppose that there are 15 resources Resource computing 
power network bandwidth and delay value save as a configuration file that comes 
with the simulator The reputation is also a random value in 0 1 
592 
F Liu and X Dong 
 
Table 2  Simulation Parameter 
PARAMETER 
NUM OF ANTS 
NCMAX 
α  
β  
ρ  
e
C  
p
C  
VALUES 
20 
200 
05 
05 
01 
11 
08 
 
Parameter Setup There are lots of parameters to setup in the simulator We set them 
just as 11 in Table 2 
43 
Experiment Results 
Makespan As can be seen from the Fig 2a RACS performs the best  
 
    
 
Fig 2 Comparison of scheduling algorithms 
Standard deviation of load We use the standard deviation of the resource finish time 
as load balance metrics which is defined as follows 
2
1
1
  
 
M
i
i
f t
f t
N
δ
=
=
−

 
 13
where
 i
f t
it the time when node i finishes all the tasks that dispatched to it 
 
f t  is 
the average finish time of all nodes in the grid system  
If the standard deviation value of an algorithm is small it means that the difference 
of each load is small The small standard deviation tells that the load of the entire 
system is balanced The lower value the standard deviation has the more load ba
lanced the system is 
As can be seen from Fig 2b Reputation based MinMin performs well in terms 
of makespan but it is not good in terms of load balancing But the reputation based 
ACS outperforms others three algorithm both in makespan and load balancing  
5 
Conclusion And Future Work 
In this paper we proposed a novel QoS metric named reputation In contract value 
of Reputation is a simplified description about the stability and credibility of variety 
 
Powered Grid Scheduling by Ant Algorithm 
593 
 
of grid resources In the context of the independent batch mode job scheduling prob
lems we analyzed the current ant colony system algorithm combined it with the 
reputation mechanism and proposed an improved ant system algorithm Its main 
contribution is introducing reputation mechanism to the pheromone change taking 
into account both the node computing power and the network link capacity We use 
SimGrid simulator to simulate the heterogeneous resources and algorithm The expe
riment results show that the reputation based ACS not only outperforms other three 
algorithms in makespan but also in system load balancing 
This study aimed at the independent task scheduling in a real grid environment 
there are always communication and data dependencies between tasks meanwhile 
due to the dynamic nature of grid resources the availability of the resource increasing 
cause for concern Our future research will focus on resource availability and relative 
task scheduling in the complex grid environment 
References 
1 He XS Sun XH Laszewski GV QoS Guided Minmin Heuristic for Grid Task 
Scheduling Journal of Computer Science and Technology 18 442–451 2003 
2 Wu ZA Luo JZ Dong F Measurement Model of Grid QoS and Multidimensional 
QoS Scheduling In Shen W Luo J Lin Z Barthès JPA Hao Q eds CSCWD 
LNCS vol 4402 pp 509–519 Springer Heidelberg 2007 
3 Zhang WZ Fang BX et al A TrustQoS Enhanced Grid Service Scheduling Chinese 
Journal of Computers 7 1157–1166 2006 
4 Sonmez OO Gursoy A A Novel EconomicBased Scheduling Heuristic for Computa
tional Grids International Journal of High Performance Computing Applications 211 
21–29 2007 
5 Kumar S Dutta K Mookerjee V Maximizing Business Value by Optimal Assignment 
of Jobs to Resources in Grid Computing European Journal of Operational Research 194 
856–872 2009 
6 Vanderstera DC Dimopoulosb NJ et al Resource Allocation on Computational Grids 
Using a Utility Model and the Knapsack Problem Future Generation Computer Sys
tems 251 35–50 2009 
7 Wu ZA Luo JZ Song AB QoSBased Grid Resource Management Journal of 
Software 1711 2264–2276 2006 
8 Dorigo M Maniezzo V Colorni A The Ant System Optimization by a Colony of 
Cooperating Agents IEEE Transactions on Systems Man and CyberneticsPart B 261 
29–41 1996 
9 Xu Z Hou X Sun J Ant AlgorithmBased Task Scheduling in Grid Computing In 
Canadian Conference on Electrical and Computer Engineering IEEE CCECE 2003 
10 Chang RS Changa JS Lina PS An Ant Algorithm for Balanced Job Scheduling in 
Grids Future Generation Computer Systems 25 20–27 2009 
11 Sathish K Reddy ARM Enhanced Ant Algorithm Based Load Balanced Task Sche
duling in Grid Computing International Journal of Computer Science and Network Securi
ty 810 219–223 2008 
12 Kousalya K Balasubramanie P Ant Algorithm for Grid Scheduling Powered by Local 
Search International Journal of Open Problems Computational Mathematics 1 223–240 
2008 
13 Casanova H Legrand A Quinson M SimGrid A Generic Framework for LargeScale 
Distributed Experiments In Tenth International Conference on Computer Modeling and 
Simulation uksim 2008 126–131 2008 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 594–601 2012 
© SpringerVerlag Berlin Heidelberg 2012 
An Efficient Palmprint Based Recognition System  
Using 1DDCT Features  
GS Badrinath Kamlesh Tiwari and Phalguni Gupta 
Department of Computer Science and Engineering  
Indian Institute of Technology Kanpur 
Kanpur 208016 India 
badriktiwaripgcseiitkacin 
Abstract This paper makes use of one dimensional Discrete Cosine Transform 
DCT to design an efficient palmprint based recognition system It extracts the 
palmprint from the hand images which are acquired using a flat bed scanner at 
low resolution It uses new techniques  to correct the nonuniform brightness of 
the palmprint and to extract features using difference of 1DDCT coefficients of 
overlapping rectangular blocks of variable size and variable orientation 
Features of two palmprints are matched using Hamming distance while nearest 
neighbor approach is used for classification The system has been tested on 
three databases viz IITK CASIA and PolyU databases and is found to be 
better than the well known palmprint systems 
Keywords Biometrics Image Enhancement Palmprint 1DDCT Decidability 
index ROC curve 
1 
Introduction 
Biometrics helps to provide the identity of the user based on hisher physiological or 
behavioral characteristics Palmprint is the region between wrist and fingers and has 
features like principle lines wrinkles datum points delta point ridges minutiae 
points singular points and texture pattern which can be considered as biometric 
characteristics 17 Compared to other biometric systems palmprint has many 
advantages  Features of the human hand are relatively stable and unique It needs 
very less cooperation from users for data acquisition Collection of data is non
intrusive Low cost devices are sufficient to acquire good quality of data The system 
uses low resolution images but provides high  accuracy Compared to the fingerprint 
a palm provides larger surface area so that more features can be extracted Because of 
the use of lower resolution imaging sensor to acquire palmprint the computation is 
much faster at the preprocessing and feature extraction stages System based on hand 
features is found to be most acceptable to users Furthermore palmprint also serves as 
a reliable human identifier because the print patterns are found to be unique even in 
monozygotic twins 9 
There exist palmprint recognition systems which are based on Gabor filter 16 
Ordinal code 14 Local binary pattern 15 2DDCT 67 The system in 6 retains 
less than 50 of 2DDCT coefficients of palmprint image as its features and it has 
 
An Efficient Palmprint Based Recognition System Using 1DDCT Features 
595 
used partial PolyU database For the identification mode this system performs with 
CRR of 99 However the recognition rates on this partial database are not 
promising 
This paper proposes an efficient technique to extract palmprint features  The 
proposed technique to generate palmprint features uses the variation of 1DDCT 
coefficients of local rectangular blocks of variable size and variable orientation to 
represent the palmprint The variable size rectangular blocks are oriented at 45o for 
capturing characteristics of both horizontal and vertical direction The binary features 
of query and enrolled palmprints are matched using Hamming distance Classification 
is done using nearest neighborhood approach The performance of the system is 
studied using IITK CASIA 1 and PolyU 2 databases where the hand images are 
obtained under different environmental circumstances 
The rest of the paper is organized as follows Section 2 describes the use of the 
Discrete Cosine Transform DCT in the proposed system to extract features of the 
palmprint Next section presents the proposed system Performance of the system has 
been discussed in Section 4 Conclusion is presented in the last section 
2 
Discrete Cosine Transform 
The objective of any palmprint feature extraction technique is to obtain good inter
class separation in minimum time Features should be obtained from the extracted 
enhanced and normalized palmprint for recognition Discrete Cosine Transform 
DCT 5 has variance distribution Further DCT is also source independent which 
means that the DCT basis vectors are fixed and independent of data Thus DCT is 
more suitable for systems where the reference database is dynamic over the life of the 
system Moreover DCT has increased tolerance to variation of illumination 13 One 
can use DCT to extract palmprint features 
DCT 5 is a real valued transformation There exist variants of DCT such as DCT
I to DCTVIII But DCTII has certain advantages over other DCT types It can be 
applied for real sequence of any positive length It has strong energy compaction 
property 12 It has been used efficiently for image compression in JPEG MPEG and 
for pattern recognition problems like biometrics 8 11 As a result DCTII has been 
considered for feature extraction In 3 it is shown that the 1DDCT coefficients 
CT0 CT1 CT2 ldots CTN1 of length N using DCTII 1 can be generated as 
ܥܶ௡=2
ܰݓሺ݇ሻ ෍ݔ௡
ேିଵ
௡ୀ଴
ܿ݋ݏ൤ߨ
ܰ ൬݊ ൅ 1
2൰ ݇൨ 
݇ = 0 …  ܰ െ 1 
1
And 
ݔ௡= ෍ݓሺ݇ሻ
ேିଵ
௞ୀ଴
ܥܶ௞ܿ݋ݏ൤ߨ
ܰ ൬݊ ൅ 1
2൰ ݇൨ 
݇ = 0 …  ܰ െ 1 
2
 
                                                           
1  Henceforth DCTII is refereed as DCT 
596 
GS Badrinath K Tiwari and P Gupta 
 
 
 
 
a Scanned Image  b Extracted Palmprint 
c Enhanced Palmprint 
Fig 1 Palmprint Extraction Process 
where ݔ௡is a real sequence of length N and 
ݓሺ݇ሻ = ൝
1
√2
݂݅ ݇ = 0
1 ܱݐ݄݁ݎݓ݅ݏ݁
 3
3 
Proposed System 
This section discusses the proposed palmprint based system Like other biometrics it 
consists of five major tasks Hand image has been acquired with the help of a low cost 
flat bed scanner Desired area of palm termed as palmprint or region of interest 
ROI has been extracted from the hand image and subsequently enhanced to 
improve its texture as described in 4 The enhanced palmprint is segmented into 
oriented  overlapping rectangular blocks and features are extracted using the 
difference of 1DDCT coefficients of adjacent blocks Features of two palmprints are 
matched using Hamming distance and decision is made based on a threshold 
31 
Feature Extraction  
Conventional image transformation is done by applying 2DDCT on whole image 
But if the image has lot of textures or randomness the transformation of larger image 
results in severe ringing artifacts around edges and the 2DDCT coefficients may not 
represent the image effectively 10 Since the palmprint is texture in nature the 2D
DCT coefficients obtained from transforming whole palmprint may not represent 
properly and it results in poor performance This paper considers a nonconventional 
approach of applying 1DDCT on rectangular blocks of variable size and variable 
orientation for extracting features This nonconventional method can take the 
advantages of varying the block size either to large size or small size and varying the 
orientation to any direction according to the regional properties such that performance 
is optimized Further the system based on fixed block is a special case of the 
proposed nonconventional method of rectangular blocks 
The enhanced palmprint is segmented into overlapped rectangular blocks of 
variable size with orientation at particular direction ߠ The oriented rectangular block 
is of size W×H where W is the width and H is the height of the block The rectangular 
block as shown in Fig 2 is the basic structure used to extract features of the palmprint 
 
An Efficient Palmprint Based Recognition System Using 1DDCT Features 
597 
in the proposed system The figure also shows the schematic diagram for segmenting 
the palmprint and other parameters that can be used for feature extraction 
 
 
Fig 2 Overlapping Rectangular Blocks at an angle ߠ and other parameters  
The segmented block is averaged across its height to obtain 1Dintensity signal of 
width W Formally the rectangular block R of width W and height H is averaged 
across width to obtain 1Dintensity signal ܀ of width W as 
܀௝=෍ܴ௝௞
ு
௞ୀଵ
݆ = 12 …  ܹ 
4
Averaging helps to smooth the image thus it reduces the noise This 1D intensity 
signal ܀  is windowed using Hanning window and then is subjected to 1DDCT 
Since the signal type of the palmprint is not known Hanning window is chosen The 
difference of 1DDCT coefficients from each vertically adjacent block is computed 
That is if  ܀૚ and ܀૛ are the 1DDCT coefficients of vertically adjacent rectangular 
blocks of width W the difference of 1DDCT coefficients ۲ of these vertically 
adjacent blocks are computed as 
ܦ௝= ܀૚ܒ െ܀૛ܒ                j =12W 
5
Further the difference of 1DDCT coefficients ۲ obtained from each of the vertically 
adjacent rectangular blocks are binarised using zero crossing to obtain a binary vector 
۰ as 
۰ ௝ = ൜ 1  ݂݅ ۲ܒ൐0
1   ܱݐ݄݁ݎݓ݅ݏ݆݁=12…ܹ 6
The resulted Wbit binary vector ۰ is considered as subfeature vector and subfeature 
vectors from all adjacent blocks create the feature vector of palmprint 
32 
Matching  
The extracted feature vector from the query palmprint is matched with that of each 
enrolled palmprint stored in the database for verification or identification In this 
paper nearest neighbor approach is used for matching If both the query and the 
enrolled palmprints are from same class then it is a genuine NonFalse match 
otherwise it is a imposer False match The distance between the feature vectors is 
computed using Hamming distance 
598 
GS Badrinath K Tiwari and P Gupta 
Formally let ܮ= ൛݈௜௝ൟ and ܧ= ൛݁௜௝ൟ   be binarised matrix of subfeature vectors 
from query and enrolled palmprints Hamming distance between feature vector of 
query palmprint L and that of enrolled palmprint E is computed as 
ܪܦ=
∑∑
൬∑
݈௜௝ሺ݇ሻ۪݁௜௝ሺ݇ሻ
ܹ
ௐ
௞ୀଵ
൰
௡
௝ୀଵ
௠
௡௜ୀଵ
݉ × ݊
 
7
The query and the enrolled palmprints are considered to be matched if the normalized 
Hamming distance between L and E is less than the predefined threshold Th 
4 
Experimental Results 
The proposed system has been tested on three hand image databases These databases 
are obtained from The Indian Institute of Technology Kanpur IITK The Chinese 
Academy of Sciences Institute of Automation CASIA 2 and The Hong Kong 
Polytechnic University PolyU 1 
Database of the IITK consists of 549 hand images obtained from 150 users 
corresponding to 183 different palms with the help of low cost flat bed scanner at 
spatial resolution of 200 dpi with 256 gray levels Three images per palm are 
collected in the same session The palmprint region is extracted and normalized to 
size of the 448 × 448 pixels and then enhanced One image per palm is considered  
for training and remaining two images are used for testing  The CASIA database 1 
contains 5239 hand images captured from 301 subjects corresponding to 602 palms 
For each subject around 8 images from left hand and 8 images from right hand are 
collected All images collected using CMOS and 256 graylevels The device is pegs 
free so user is free to place his hand facing the scanner The palmprint region is 
extracted and normalised to size of  256 × 256 pixels and then enhanced Two 
images per palm are considered for training while remaining images are used for 
testing  The PolyU database 2 consists of 7752 grayscale images from 193 users 
corresponding to 386 different palms Around 17 images per palm are collected in 
two sessions The images are collected using CCD at spatial resolution of 75 dots per 
inch and 256 graylevels Images are captured placing pegs Palmprints are 
normalised to size of 176 × 176 pixels and then enhanced The database is classified 
into training set and testing set Six images per palm are considered for training and 
remaining images are used for testing 
The performance of the proposed system is measured for both identification and 
verification mode For identification mode the Correct Recognition Rate CRR of 
the system is measured as 
ܥܴܴ=ܰଵ
ܰଶ
× 100 
8
Where ܰଵ denotes the number of correct NonFalse recognition of palmprints and 
ܰଶଵ is the total number of palmprints in the testing set For verification mode Equal 
Error Rate EER is used to measure accuracy 
 
An Efficient Palmprint Based Recognition System Using 1DDCT Features 
599 
The performance of the proposed system is compared with the best known systems 
proposed in 6 7 14 16 for all three databases The systems in 6 7 have used 2D
DCT to extract palmprint features It is reported that systems in 6 and 7 are 
evaluated using 500 images and 198 images of PolyU database respectively 
However experiments for evaluating the performance of the systems in 6 7 14 16 
are carried out on the same set of extracted and enhanced palmprint images used for 
the proposed system 
CRR and ERR of all the systems are given in Table 1 It can be observed that CRR 
and EER of the proposed system are better than the  best known systems 6 7 14 
16 Further the proposed system performs better with IITK database which has the 
better quality images as compared to CASIA and PolyU databases It should also be 
noted that it performs with 100 CRR for CASIA which has the worst quality images 
among the three databases 
Table 1 Performance Comparison of the Proposed System 
 
IITK 
CASIA 
PolyU 
 
CRR 
ERR 
DI 
CRR 
ERR
DI 
CRR 
ERR 
DI 
Palmcode 16 
10000 
5210 
1800 
99619 
3673 
3073
99916 
0533 
5580 
Ordinal code 14 
10000 
1188 
2129 
99843 
1754 
3396
10000 
0070 
6678 
Kipsang etal6 
92769 
10234 
09281 
95326 
6768 
0644
8584 
2011 
0798 
Dale et al 7 
89097 
1025 
1025 
93544 
7712 
2563
9090 
1190 
2269 
Proposed 
10000 
6490 
6490 
1000 
1004 
5526
1000 
0033 
7750 
 
The Receiver Operating Characteristics ROC curve which is used as graphical 
depiction of performance of a system for verification mode is generated by plotting 
FAR against FRR at different thresholds Fig 4 shows the ROC curves of various 
systems for IITK CASIA and PolyU databases It is found to have minimum FRR at 
low FAR Hence from Table 1 and Fig 3 it can be inferred that the proposed system 
performs better than the systems in 6 7 14 16 
 
 
 
a ROC curves for IITK  
b ROC curves CASIA   
c ROC curves PolyU 
Fig 3 Comparing ROC Curves  
Since EER is near to zero significant conclusion cannot be drawn So decidability 
index DI which gives the measure of the separability of genuine and imposter 
matching scores is used Decidability index is defined as 
600 
GS Badrinath K Tiwari and P Gupta 
ܦܫ=
|ߤଵ െߤଶ|
ටߪଵ
ଶ ൅ߪଶ
ଶ
2
 
9
where ߤଵ and  ߤଶ are the means and ߪଵ and ߪଶ are standard deviations of genuine and 
imposter scores respectively DIs for various systems for all three databases are 
shown in Table 1 It can be observed that the proposed system has the highest DI 
compared to other systems for the corresponding databases        
5 
Conclusion   
This paper has presented an palmprint based recognition system using 1DDCT 
coefficients of local region Palmprints are extracted from hand images with the help 
of low cost flat bed scanner at IITK which is free of constraints pegs So user is free 
to place hand independent of orientation and translation on scanner bed Extracted 
palmprint is found to be robust to translation and orientation of placement of hand on 
scanner bed A nonconventional technique to extract palmprint features based on 
variation of 1DDCT coefficients of adjacent rectangular blocks of variable size and 
variable orientation has been proposed and experimentally evaluated Hamming 
distance is used as classifier The system has been tested using IITK database of 549 
CASIA database of 5239 and PolyU database of 7752 hand images which are 
acquired using flat bed scanner CCD and CMOS respectively 
Experimentally optimum parameters of the proposed system are determined to 
achieve CRR of 100 for all three databases Also it is observed that EERs are 
000 100 and 003 for IITK CASIA and PolyU databases respectively The 
system has been compared with the best known systems in 14 16 and those in 6 7 
which have used 2DDCT to extract palmprint features The proposed system is found 
to perform better than all these systems 
Acknowledgement Authors acknowledge the support provided by the Department of 
Information Technology Government of India to carry out this work 
References 
1 The CASIA palmprint database httpwwwcbsriaaccn 
2 The PolyU palmprint database httpwwwcomppolyueduhkbiometrics 
3 Ahmed N Natarajan T Rao KR Discrete cosine transform IEEE Transaction on 
Computers 233 90–93 1974 
4 Badrinath G Gupta P Palmprint based recognition system using phase diference 
information Future Generation Computer Systems 281 287–305 2012 
5 Britanak V Yip PC Rao KR Discrete Cosine and Sine Transforms General 
Properties Fast Algorithms and Integer Approximations Academic Press 2006 
6 Choge HK Oyama T Karungaru S Tsuge S Fukumi M Palmprint Recognition 
Based on Local DCT Feature Extraction In Leung CS Lee M Chan JH eds 
ICONIP 2009 Part I LNCS vol 5863 pp 639–648 Springer Heidelberg 2009 
 
An Efficient Palmprint Based Recognition System Using 1DDCT Features 
601 
7 Dale MP Joshi MA Gilda N Texture based palmprint identification using DCT 
features In International Conference on Advances in Pattern Recognition pp 221–224 
2009 
8 Hafed ZM Levine MD Face recognition using the discrete cosine transform 
International Journal of Computer Vision 433 167–188 2001 
9 Kong A Zhang D Lu G A study of identical twins’ palmprints for personal 
authentication Pattern Recognition 3911 2149–2156 2006 
10 Mao X He Y Image subjective quality with variable block size coding In International 
Video Coding and Video Processing Workshop pp 26–28 2008 
11 Monro D Rakshit S Zhang D DCTbased iris recognition IEEE Transactions on 
Pattern Analysis and Machine Intelligence 294 586–595 2007 
12 Rao KR Yip P Discrete Cosine Transform Algorithms Advantages Applications 
Academic Press 1990 
13 Schwerin B Paliwal K LocalDCT features for facial recognition In International 
Conference on Signal Processing and Communication Systems pp 1–6 2008 
14 Sun Z Tan T Wang Y Li SZ Ordinal palmprint representation for personal 
identification In Computer Vision and Pattern Recognition pp 279–284 2005 
15 Wang X Gong H Zhang H Li B Zhuang Z Palmprint identification using boosting 
local binary pattern In International Conference on Pattern Recognition pp 503–506 
2006 
16 Zhang D Kong AW You J Wong M Online palmprint identification IEEE 
Transactions on Pattern Analysis and Machine Intelligence 259 1041–1050 2003 
17 Zhang DD Palmprint Authentication International Series on Biometrics Springer
Verlag New York Inc Secaucus 2004 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 602–609 2012 
© SpringerVerlag Berlin Heidelberg 2012 
An Efficient Algorithm for Deduplication  
of Demographic Data 
Vandana Dixit Kaushik1 Amit Bendale2 Aditya Nigam2 and Phalguni Gupta2 
1 Department of Computer Science  Engineering Harcourt Butler Technological Institute 
Kanpur 208002 India 
vandanadixitkyahoocom 
2 Department of Computer Science  Engineering Indian Institute of Technology Kanpur 
Kanpur 208016 India 
bendalenadityapgiitkacin 
Abstract This paper proposes an efficient algorithm to deduplicate based on 
demographic 
information 
which 
contains 
two 
name 
strings 
viz 
and  of individuals The algorithm consists of two 
stages  enrolment and deduplication In both stages all name strings are re
duced to generic name strings with the help of phonetic based reduction rules 
Thus there may be several name strings having same generic name and also 
there may be many individuals having the same name The generic name with 
all name strings and their Ids forms a bin At the enrolment stage a database 
with demographic information is efficiently created which is an array of bins 
and each bin is a singly linked list At the deduplication stage name strings are 
reduced and all neighbouring bins of the reduced name strings are used to de
termine the top best matches In order to see the performance of the proposed 
algorithm we have considered a large demographic database of 485136 indi
viduals It has been observed that the phonetic reduction rules could reduce both 
the name strings by more than 90 Experimental results reveal that there is 
very high hit rate against a low penetration rate 
Keywords Deduplication Demographic Data Edit Distance Levenshtein 
Distance Phonetics 
1 
Introduction 
Individual recognition has gained great importance recently for personal as well as 
national security Techniques of recognition can be broadly classified on the basis of 
the data used into two categories biometric and demographic Biometric data makes 
uses of physical characteristics like face fingerprint iris etc for recognition Demo
graphic data on the other hand is the data in the form of textstring which can be 
used for identification For example in electoral department any voter can be identi
fied uniquely with the help of his name his fathers name his date of birth and his 
permanent residential address all in the form of text the combination of which is 
very less likely to be matching with the other voter 
 
An Efficient Algorithm for Deduplication of Demographic Data 
603 
Demographic data in general refers to selected population characteristics as used in 
government marketing or opinion research Commonlyused demographics include 
race age income disabilities mobility in terms of travel time to work or number of 
vehicles available educational attainment home ownership employment status and 
even location 
In computing data deduplication is a process to eliminate redundant data to im
prove storage utilization In the deduplication process duplicate data is deleted leav
ing only one copy of the data to be stored along with references to the unique copy of 
data In this paper by demographic data of an individual we mean a record consisting 
of two string fields viz GivenName and Surname of the individual demographic 
deduplication process determines whether there exists any set of records in the demo
graphic  database which are matched with the record of a query data within some 
tolerable  range For any new demographic data a negative background search is 
performed to obtain all its close matches  To accomplish this task every string ie 
first name and last name of the query individual is considered to be found in the 
database if any of the following criteria satisfies 
• Identical strings in the database 
• String having similar phonetics but difference with alphabets 
• String which can be constructed from new string by few transformations like inser
tion deletion or substitution 
Data deduplication has been approached previously using machine learning ap
proaches 6 8 9 In these approaches a labeled set of duplicates and non
duplicates is provided to learn the deduplication function and is used to predict on 
testing set These approaches depend heavily on providing an exhaustive and diverse 
set of duplicates and nonduplicates that bring out the subtlety of the deduplication 
function  
In this paper we propose an efficient algorithm for deduplication of demographic 
data We have considered GivenName and Surname as demographic data Thus be
side Id of each individual each record in the database contains two string fields  
GivenName and Surname The proposed deduplication search is based on the ap
proximate string matching like editdistance Next section discusses the method of 
edit distance which has been used in this paper for deduplication Section 3 describes 
the proposed algorithm Experimental results have been analyzed in the next section 
Finally conclusions are given in the last section 
2 
Edit Distance 
Editdistance is used commonly for approximate string matching 3 In information 
theory and computer science the edit distance between two strings of characters is the 
number of operations required to transform one of them into the other There are sev
eral ways to define an edit distance depending on which edit operations are allowed 
replace delete insert transpose 1 2 4 5 7 In this paper we have used Levenshtein 
distance to measure the amount of difference between two sequences ie an edit 
distance The Levenshtein distance between two strings is the minimum number of 
604 
VD Kaushik et al 
edits that are required to transform one string into the other with the help of edit op
erations like insertion deletion or substitution of a single character This edit distance 
is normalized by dividing the maximum of the length of two name strings Cost of 
computing this distance is proportional to the product of the two string lengths Let A 
and B be two name strings of size m and n respectively The complete algorithm 
which computes the normalized edit distance between A and B NEDAB is de
scribed in Algorithm 1 
Algorithm 1 NormalisedEditDistanceAB 
Require String A   of length m String B  of length n 
  
Ensure normalised Levenshtein Editdistance of A   and B 
1 Create 2D array d0m 0n 
 for all i and j dij  holds the Levenshtein distance 
between the first i characters of A and the first j char
acters of B note that d   has m + 1 x n + 1 values 
2 for i = 0 to m do 
3    di0  =  i   distance of null substring of B 
from A1i 
4 end for 
5 for j = 0 to n do 
6    d0j  = j   distance of null substring of A from 
B1j 
7 end for 
8 for j = 1 to n do 
9     for i = 1 to m do 
10      if Ai == Bj then 
11         dij = di1 j1 no editing required 
12    else 
13     dij  =   mindi1j dij1 di1j1+1 
deletion insertion substitution 
14     end if 
15  end for 
16 end for 
17  NEDAB= dmnmax|A||B|  
18 return NEDAB   Normalized edit distance 
3 
Proposed Algorithm 
In this section we have proposed an efficient demographic deduplication algorithm 
It consists of two major components 1 Enrolment of demographic data of an indi
vidual in the database and 2 Searching the database for a query demographic data to 
find potential duplicates By the term of potential duplicates we mean the duplicates 
with some tolerable distance Each demographic data is represented by two string 
fields viz GivenName and Surname It has been observed that one can write or  
pronounce a name in several ways and as a result the database size becomes  
 
An Efficient Algorithm for Deduplication of Demographic Data 
605 
unnecessarily very large which leads to a very large amount of searching time In 
order to reduce the search space in the database each name is reduced to its generic 
name  
Phonetic reduction rules to convert a name string into a generic name string are 
based on the above said rules There may be several name strings which are converted 
into a single generic name A bin can be defined by this generic name which contains 
the set of all such name strings Further one can observe that there may be several 
individuals having same name but different Ids So one can represent the bin in the 
form of a singly linked list where each node contains a name string with different Ids  
Thus a demographic database consists of an array of records where each record is 
a singly linked list with a header node marked by the reduced name Each node in the 
linked list has several Id fields one name string field and a link field At enrolment 
stage the reduced name of a name string is used to get the desired bin ie singly 
linked list where the name string is to be inserted If there exists a field in a node of 
the list having the same name string then the Id of the name string is inserted in one of 
the empty Id fields otherwise a new node with the name string and its Id is inserted in 
the appropriate position of the linked list The entire scheme to reduce the name 
strings is depicted in Figure 1 
 
Fig 1 Preprocessing  Creation of bins 
At the time of verification or looking for the existence of duplicate entries with 
some tolerable range each name string of the query demographic data Given
NameF SurnameS is reduced and all bins having reduced names whose norma
lized edit distanceNED from the reduced name of the name string of the query data is 
within the tolerable range are selected for further processing It can be noted that edit 
distance enables to capture two similar strings that cannot be identified simply by 
606 
VD Kaushik et al 
using the phonetical rules of reduction It can effectively detect all those duplications 
where there are some modifications in a string and do not conform to phonetical rules 
All name strings in these selected bins having the normalized edit distance from the 
name string of the query data less than the tolerable range are considered as existence 
of the name string of the query data in the database Ids of all these name strings are 
the probable candidates of the duplicates with some tolerable range  
These probable candidates are considered for obtaining top k best matches Let Q 
and X be the demographic data of a query and of a probable candidate in the database 
respectively One could have obtained sum of the normalized edit distances of Given 
Name and Surname between Q and X to find its matching score But it has been ob
served that individuals need not be consistent in providing their demographic infor
mation By inconsistency we mean any individual may interchange the strings of his 
name This may be due to make an attempt to fraud the environment or to make use of 
different styles of writing his name etc In order to tackle such type of scenarios nor
malized edit distances between every pair of name strings have been considered 
More clearly let Qrf and Qrs be the generic form of GivenName and the Surname 
of a query individual Further it is assumed that X is a record in the database with 
whom the normalized edit distances are to be computed If Xrf and Xrs  be the generic 
form of GivenName and the Surname of X respectively the  NEDQrfXrf 
NEDQrsXrs NEDQrfXrs NEDQrsXrf are computed where NEDAB is the normalized 
edit distance between two name strings A and B To get the best match minimum 
normalized distance between two name strings has been considered That means 
minDistQX = minNEDQrfXrf+NEDQrsXrs NEDQrfXrs+NEDQrsXrf 
1
Further there are some individuals who sometime use middle name in providing de
mographic information It may be intentional also to avoid duplication in the data
base In this paper we have considered only GivenName and Surname But to distin
guish between a name with and that without middle name we have considered the 
factor of difference between length of two name strings multiplied by a constant 
Thus the matching score SQX  between Q and X is given by 
SQX = minDistQX + w  ||Q||X|| 
2
where w is a predefined weight In this paper we have considered w = 001 
These matching scores of the demographic data of Q against all probable candi
dates in the database are arranged in ascending order Candidates with top k scores are 
selected as top k best matches 
4 
Experimental Results 
In order to analyze the performance of the proposed algorithm we have used our 
database consisting of 485136 individuals each having his GivenName and Surname 
as his demographic data The database size is large enough and is sufficient to prove 
its robustness and efficiency At the enrollment stage name strings are reduced at two 
stages one with GivenName and other with Surname It has been seen that percen
 
An Efficient Algorithm for Deduplication of Demographic Data 
607 
tage of reduction is 9094 in case of GivenName and is 9440 in case of Surname 
string Number of distinct bins containing reduced name strings is found to be 43941 
and 27184 with respect to GivenName and Surname respectively Further maximum 
number of nodes in a bin of GivenName is 16 and that of Surname is 15 
Detailed statistics of both type of bins is given in Table 1 and Table 2 We have not 
shown those bins having negligible densities These tables indicate that there is a lot 
of redundancies of names if exact spellings and phonetics are ignored 
Table 1 Bin Size Statistics 
Name 
String 
Number 
of Bins 
Minimum 
Bin size 
Maximum 
Bin size 
Average 
Bin size 
Standard 
deviation 
Reduction 
Percentage 
Given 
Name 
43941 
1 
16 
1278 
0808 
9094 
Surname 
27184 
1 
15 
1288 
0797 
9440 
Table 2 Bin Density 
Bin Size 
1 
2 
3 
4 
5 
GivenName 
36461 
5090 
1288 
532 
270 
Bin Density  
83 
12 
3 
1 
1 
Surname 
22349 
3222 
9321 
371 
152 
Bin Density  
82 
12 
3 
1 
1 
 
To analyze the performance of the proposed deduplication algorithm 500 query 
records are chosen at random from above mentioned database to create gallery demo
graphic database DB An editdistance of i i = 1 to 6 units is artificially induced in 
each full name consisting of first and last name of DB and inserted in query dataset 
DBi alongwith the corresponding ID In this manner six query datasets  DB1 DB2 
DB3 DB4 DB5 DB6 are created each having 500 queries Thus the dissimilarity of 
query datasets and gallery dataset DB increases from DB1 to DB6 A random sample 
of consonants from the name are randomly substituted or augmented with another 
consonant or deleted to make the modified name at the desired editdistance from 
original one Matching of queries from these size sets is carried out with DB and topk 
matches for each query are reported 
If the correct ID for a query is found in the topk matches it is reported as a hit 
otherwise it is considered as a miss The parameter t1 is the relative tolerance thre
shold for normalized editdistance between generic form of GivenName string of a 
query data and that of a record in the database whereas t2 is the threshold for Sur
name For different tolerance thresholds t1 t2 hit rates for DB1 DB2 DB3 DB4 DB5 
DB6 with k = 5 are found and shown in Figure 2 and Table 3 From the figure we can 
see that matching performance degrades with an increase in dissimilarity between 
query and gallery datasets  
608 
VD Kaushik et al 
Table 3 Hitrates with k = 5 for varying thresholds 
Dataset 
t1=04 t2=04 
t1=05 t2=05 
t1=06 t2=06 
DB1
990 
990 
980 
DB2
922 
972 
976 
DB3
826 
928 
952 
DB4
746 
898 
938 
DB5
668 
830 
904 
DB6
568 
796 
882 
 
Fig 2 Performance Analysis  
Penetration rate for a given query is the average fraction of total database in  
scanned to get top k best matches Penetration is directly proportional to threshold 
values t1 t2 It can be seen from the Figure 2b that increasing penetration rate gener
ally increases the hit rate for all the datasets It can be inferred that hit rate improves 
greatly by increasing thresholds but at the cost of increased penetration 
5 
Conclusion 
This paper has proposed an efficient deduplication algorithm for demographic data 
It is a challenging task when two name strings with phonetical and other random dif
ferences within the tolerable range are referring to the same individual Simple pho
netic reduction and exact matching cannot identify potential duplicates due to their 
rigidity Thus this paper has reduced each name string to a generic name string so 
that the name string is put in a bin with the reduced name string Finally name string 
of a query data is searched in the corresponding bin and its neighbors to compute the 
top k best matches We have used a large database of 485136 demographic data It 
has been observed that there is a reduction of 916 in case of GivenName and more 
than 94 reduction in case of Surname In order to test the performance of the algo
rithm a test set of 500 demographic data has been used It has been found that it has 
achieved 94 average accuracy over DB1 through DB6 for top 5 best matches using 
best thresholds 
 
An Efficient Algorithm for Deduplication of Demographic Data 
609 
Acknowledgement Authors acknowledge the support provided by the Department of 
Information Technology Government of India to carry out this work 
References 
1 
Jaro M Advances in Recordlinkage Methodology as Applied to Matching the 1985  
Census  
2 
Levenshtein V Binary Codes Capable of Correcting Deletions Insertions and Reversals 
Soviet Physics Doklady 10 707–710 1966 
3 
Navarro G A Guided Tour to Approximate String Matching ACM Computing Surveys 
CSUR 331 31–88 2001 
4 
Oommen B Loke R Pattern Recognition of Strings with Substitutions Insertions Dele
tions and Generalized Transpositions Pattern Recognition 305 789–800 1997 
5 
Sankoff D Kruskal JB eds Time Warps String Edits and Macromolecules The 
Theory and Practice of Sequence Comparison AddisonWesley Publication Reading 
1983 
6 
Sarawagi S Bhamidipaty A Interactive Deduplication Using Active Learning In Pro
ceedings of the Eighth International Conference on Knowledge Discovery and Data Min
ing ACM SIGKDD pp 269–278 ACM 2002 
7 
Ukkonen E Online Construction of Suffix Trees Algorithmica 143 249–260 1995 
8 
Winkler W Matching and Record Linkage Wiley Online Library 1993 
9 
Winkler W The State of Record Linkage and Current Research Problems Statistical Re
search Division US Census Bureau Citeseer 1999 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 610–617 2012 
© SpringerVerlag Berlin Heidelberg 2012 
A Transportation Model with Interval Type2 Fuzzy 
Demands and Supplies 
Juan C FigueroaGarcía1 and Germán Hernández2 
1 Universidad Distrital Francisco José de Caldas Bogotá – Colombia 
2 Universidad Nacional de Colombia Sede Bogotá – Colombia 
jcfigueroagudistritaleduco 
gjhernandezpgmailcom 
Abstract This paper presents a basic transportation model TM where its de
mands and supplies are defined as Interval Type2 Fuzzy sets IT2FS This 
kind of constraints involves uncertainty to the membership function of a fuzzy 
set so we called this model as Interval Type2 Transportation Model IT2TM 
Using convex optimization techniques a global solution of this problem can be
found To do so we define a general model for IT2TM and then we present an 
application example to illustrate how the algorithm works 
1 
Introduction and Motivation 
Transportation problems are interesting ones due to its applicability to logistics pro
duction and assignment scenarios There are some possible situations where the de
mands of the customers and availabilities of the suppliers are neither constant nor well 
defined In this way Fuzzy Linear Programming FLP appears as an important tool 
to find solutions of uncertain problems using Type1 and Type2 fuzzy sets 
Optimization with IT2FS is a recent approach that allows us to deal with linguistic 
uncertainty of a Type1 fuzzy set The main scope of this paper is to define a TM 
model with IT2FS compute its optimal solutions by using classical algorithms and 
finally use a Typereduction strategy for obtaining a crisp solution of the problem 
Some fuzzy uncertain optimization methods have been proposed by Figueroa 1 
2 3 and 4 who solved LP problems with uncertain fuzzy constraints The prob
lem addressed in this paper is the problem of having both uncertain demands and 
supplies which can be extended to VRP TSP and related problems 
The paper is divided as follows In Section 1 a brief introduction and motivation is 
presented Section 2 presents the IT2TM model in Section 3 an application example 
is introduced and Section 4 presents the concluding remarks of the study 
2 
The Uncertain TM Model 
FLP problems have well known solution procedures For further information about 
FLP problems see Klir  Yuan in 5 Lai  Hwang in 6 Kacprzyk  Orlovski in 
7 Pandiant in 8 and Zimmermann in 9 and 10 In this way the classical TM 
with crisp parameters is 
 
A Transportation Model with Interval Type2 Fuzzy Demands and Supplies 
611 
 
min
ij
ij
z
= c x
 
1
st 
1
m
ij
j
n
i
x
a
j
=
∀
∈



 
2
1
n
ij
i
m
j
x
d
i
=
∀
∈



 
3
Index Sets 
m
 is the set of all “i” resources 

12


m
m
i
m
∈
=



 
n
  is the set of all “j” products 

12

n
n
j
n
∈
=



 
Decision Variables 
ijx  = Quantity of product to be shipped from the supplier “i” to the customer “j” 
Parameters 
ja  = Quantity of product available by the supplier “j” 
ib  = Quantity of product required by the customer “i” 
 
In this problem all their parameters are crisp sets and they have no uncertainty 
sources involved A common issue here is that the demands of the customers and the 
available units that each supplier has can be represented by fuzzy numbers so the 
TM becomes a more complex one 
Now the uncertain TM called IT2TM uses two partial orders  and  with dif
ferent membership functions This implies that its supplies and demands are defined 
as IT2FS so an Interval Type2 fuzzy parameter is defined as follows 
Definition 1 Uncertain constraint Let 
jb 
 a constraint of an IT2FLP The mem
bership function which represents the fuzzy space1 supp
jb 
 is 
1


01
j
j
bj
j
j
b
b
u J
b
u
b
J
∈
∈


=
⊆





 

 
 4
The difference between a Type1 and Type2 fuzzy sets lies in the Footprint of Uncer
tainty FOU of a Type2 fuzzy set which can be understood as uncertainty in the 
membership grades of a fuzzy set which is an interval itself as defined in 4 A 
graphical representation of an IT2FS is shown in Figure 1 
 
                                                           
1  A Fuzzy Space is defined by the interval 
j
j
b
b
⊆   
612 
JC FigueroaGarcía and G Hernández 
 
 
Fig 1 Interval Type2 fuzzy constraint with Joint Uncertain Δ   ∇  
This Figure depicts a linear IT2FS note that its FOU is weighted by 1 this means 
that its primary membership functions 
bJ  are decomposed into Lower and Upper 
primary membership functions namely 
b
μ   and 
b
μ   respectively In particular each 
value b has an intervalvalued membership grade modeled by following interval 
 
 
  
b
b
b
b
b
b
b
μ
μ
μ


∈
∈





  
5
Definition 2 Uncertain supplies and demands Consider the demands 
id 
 and 
supplies 
ja 
 of the crisp TM Their uncertain counterparts according to the Defini
tion 1 are defined as linear IT2FS namely 
id 
 and 
ja 
 
The transportation model with uncertain demands and supplies 
id 
 and 
ja 
 is 
min
ij
ij
z
= c x
 
6
st 
1
m
ij
j
n
i
x
a
j
=
∀
∈




 
7
1
n
ij
i
m
j
x
d
i
=
∀
∈




 
8
where 


n m
ij
ij
c
x ∈
 
ja  and 
id  are IT2FS whose supports are defined over the real 
numbers    and  are Type2 fuzzy partial orders 
 
A Transportation Model with Interval Type2 Fuzzy Demands and Supplies 
613 
 
In this model 
ja  is a linear IT2FS with parameters 


j
j
j
a
a
a
Δ
Δ
∇  and 
ja ∇  and 
id  
is a linear IT2FS with parameters 


i
i
i
d
d
d
Δ
Δ
∇  and 
id ∇  analogous to Figure 1 
This problem can be solved through the proposals of Figueroa 13 and Figueroa 
and Hernández 4 by computing a set of optimal solutions called Z  and applying a 
Typereduction strategy to find an optimal solution through the Zimmermanns me
thod In next section we provide more details about its computation 
21 
Solution Procedure 
Firstly we need to compute a fuzzy set of optimal solutions Z  through the bounda
ries of each fuzzy supply and demand in the following way 

i
j
d
a
z
∇
∇
Δ
→
 
9
i 
j
d
a
z
Δ
Δ
∇
→
 
10
i 
j
d
a
z
∇
∇
Δ
→
 
11
i 
j
d
a
z
Δ
Δ
∇
→
 
12
Then we define the following auxiliary variables 
 
Definition  3 Consider an uncertain supply 
ja  defined as IFS with linear member
ship function Then 
j
Δ  is defined as the distance between 
jaΔ and 
ja Δ 
j
j
j
a
a
Δ
Δ
Δ =
−
 
and 
j
∇  is defined as the distance between 
ja∇  and 
ja ∇  
j
j
j
a
a
∇
∇
∇ =
−
 
Similarly for an uncertain demand 
id  
i
Δ  is defined as the distance between 
id Δ  
and 
id Δ  
i
i
i
d
d
Δ
Δ
Δ =
−
 and 
i
∇  is defined as the distance between 
id ∇  and 
id ∇  
i
i
i
d
d
∇
∇
∇ =
−
 
 
First we describe the Zimmermanns Soft Constraints method as follows 
Algorithm 1 Zimmermann’s Soft Constraints method 
1 Calculate an inferior bound called Z Minimum 
z by solving a LP model with b  
2 Calculate a superior bound called Z Maximum 
z by solving a LP model with b  
3 Define a Fuzzy Set 
 
z x 

with bounds z and z  and linear membership function 
This set represents the degree that any feasible solution has regarding the optimiza
tion objective 
614 
JC FigueroaGarcía and G Hernández 
 
4 If the objective is to minimize then its membership function2 is 
1
   

0
z
c x
z
z
c x
x z z
z
c x
z
z
z
c x
z
μ
′


− ′

′
= 
−


′






 
13
─ Thus solve the following LP model3 
 
max α  
14
st 


c x
z
z
z
′ +α
−
=
 
15


Ax
b
b
b
−α
−

 
16
0
x  
 where α  is the overall satisfaction degree of all fuzzy sets 
Now Figueroa 13 proposed a method for solving IT2FLP problems which uses Δ  
and ∇  as auxiliary variables with weights C Δ  and C∇  respectively to find an op
timal fuzzy set enclosed into the FOU of Z  and then solve it by using the Soft Con
straints model Its description is presented next 
Algorithm 2 Figueroa’s method for IT2FLP 
─ Calculate an inferior boundary called Z minimum  
z  by using b ∇ + ∇  as a fron
tier of the model where ∇  is an auxiliary set of variables weighted by C∇  which 
represents the lower uncertainty interval subject to the following statement  
b
b
∇
∇
∇
−

 
17
─   Calculate a superior boundary called Z maximum  
z
 by using b Δ + Δ  as a 
frontier of the model where Δ  is an auxiliary set of variables weighted by C Δ  
which represents the upper uncertainty interval subject to the following statement 
b
b
Δ
Δ
Δ
−

 
18
Find the final solution using the third and subsequent steps of the Algorithm 1 
In this algorithm Δ  and ∇  operate as Typereducers this means that for each 
uncertain 


i
d aj


 we obtain a fuzzy set embedded on its FOU where 

iΔ  and 

i
∇  
                                                           
2  For a maximization problem its membership function is defined as the complement of 
   
z x z z
μ 
 
3  In the same way a minimization problem is defined as the complement of a maximization 
problem 
 
A Transportation Model with Interval Type2 Fuzzy Demands and Supplies 
615 
 
leads to 
id  and 
id  and 

j
Δ  and 

j
∇  leads to 
ja  and 
ja  in the Zimmermanns 
method 

j
ja
Δ →
 
19

j
ja
∇ →
 
20

i
id
Δ →
 
21

i
∇ → id
 
22
Then we apply the Algorithm 1 finding an optimal solution in terms of α  
It is important to note that the analyst needs a solution in terms of x∈  so the 
uncertain problem must be Typereduced by using the Algorithm 2 and then a real 
valued solution is found by using the Algorithm 1 This means that we come from an 
IT2FLP problem to an FLP and finally to a crisp solution embedded into Z  
3 
Application Example 
We present an application of our proposal to a small example where all its demands 
and supplies are IT2FS so its optimal solution is given by an α cut The obtained 
solution is a set enclosed into Z  where its typereduced solution is  provided by the 
Algorithm 2 All parameters of 
ja  and 
ib  are shown in following matrices 
 
10
13
12
16
11
14
13
17
12
18
15
21
i
i
i
i
d
d
d
d
∇
Δ
∇
Δ
















=
=
=
=
























 
 
20
14
24
16
32
25
37
30
24
18
29
23
j
j
j
j
a
a
a
a
∇
Δ
∇
Δ
















=
=
=
=
























 
 
2
3
1
4
1
3
2
05
4
05
3
4
1
6
1
6
1
1
8
1
6
3
05
7
05
7
2
05
6
1
6
4
2
Cij
C
C
Δ
∇
 
 
 


 


 
 


 


 
 


 


 
 


 


 
=  
=
Δ =
=
∇ =


 


 
 


 


 
 


 


 
 


 


 
 


 


 
 
 
 
 
616 
JC FigueroaGarcía and G Hernández 
 
This example is composed by three suppliers and three customers which parameters 
are IT2FS so we apply the Algorithm 2 to find a crisp solution of the problem The 
obtained fuzzy set Z  is defined by the following boundaries 
 
76
91
56
69
z
z
z
z
Δ
∇
Δ
∇
=
=
=
=
 
The values of 
z  and 

z  are 655 and 79 respectively After applying the Zimmer
manns method we obtain a crisp solution of 

α = 01053
 and 

z = 7757
 The ob
tained solution in terms of x is 
 




11
13
22
31
21053
183158
143158
112105
x
x
x
x
=
=
=
=
 
 






1
2
3
1
2
3
3
3
6
4
4
6
i
i
i
i
i
i
=
=
=
=
=
=
Δ
=
Δ
=
Δ
=
∇
=
∇
=
∇
=
 
The resultant set of optimal solutions is displayed next 
 
 
Fig 2 Fuzzy Set Z  embedded into the FOU of Z  
These results indicate that the analyst should send 

ijx  units from its suppliers to 
customers as shown above A global satisfaction degree of 

α = 01053
 is computed 
over the Typereduced fuzzy set of optimal solutions z  which is embedded into the 
FOU of Z  as shown in Figure 2 
In this problem all suppliers availability and customers demands are considered as 
IT2FS We apply both Algorithm 1 and 2 to find a crisp solution of the problem 
which is what the analyst needs to make a decision about how many units has to send 
getting an optimal minimum transportation cost 
In this way the IT2TM can be handled with classical optimization techniques and 
the analyst can involve linguistic uncertainty provided by the experts of the system to 
find an operation point namely α  which satisfies both customers and suppliers 
4 
Concluding Remarks 
The presented model is a tool which deals with uncertainty of a fuzzy set involving 
the perception of different experts into IT2FS Its computation and solution are  
 
A Transportation Model with Interval Type2 Fuzzy Demands and Supplies 
617 
 
explained through a small example Although the example presented in this paper is 
small it is intended to be easy for implementation and checking its results by readers 
The presented IT2TM deals with uncertainty coming from the opinion and percep
tion of the experts about the availability of suppliers and customers demands getting 
an optimal solution which allows us make decisions under fuzzy uncertainty 
Finally the presented model can be extended to other scenarios and models as 
TSP VRP and its extensions by applying a similar reasoning than the presented here 
41 
Further Topics 
The theory of Generalized Interval Type2 Fuzzy Sets GT2 FS can be considered as 
a new optimization focus In this field an additional degree of freedom should be con
sidered in the modeling process this feature is the secondary membership function 
xf   
u
u  which induces to new directions 
References 
1 Figueroa JC Linear Programming with Interval Type2 Fuzzy Right Hand Side Parame
ters In 2008 Annual Meeting of the IEEE North American Fuzzy Information Processing 
Society NAFIPS 2008 
2 Figueroa JC Solving Fuzzy Linear Programming Problems with Interval Type2 RHS 
In 2009 Conference on Systems Man and Cybernetics IEEE 2009 
3 Figueroa JC Interval Type2 Fuzzy Linear Programming Uncertain constraints In 
IEEE Symposium Series on Computational Intelligence pp 1–6 IEEE 2011 
4 FigueroaGarcía JC Hernandez G Computing Optimal Solutions of a Linear Pro
gramming Problem with Interval Type2 Fuzzy Constraints In Corchado E Snášel V 
Abraham A Woźniak M Graña M Cho SB eds HAIS 2012 Part I LNCS 
vol 7208 pp 567–576 Springer Heidelberg 2012 
5 Klir GJ Yuan B Fuzzy Sets and Fuzzy Logic Theory and Applications Prentice Hall 
1995 
6 Lai YJ Hwang C Fuzzy Mathematical Programming Springer 1992 
7 Kacprzyk J Orlovski SA Optimization Models Using Fuzzy Sets and Possibility 
Theory Kluwer Academic Press 1987 
8 Pandiant MV Application of Fuzzy Linear Programming in Production Planning Fuzzy 
Optimization and Decision Making 23 229 2003 
9 Zimmermann HJ Fuzzy programming and Linear Programming with Several Objective 
Functions Fuzzy Sets and Systems 11 45–55 1978 
10 Zimmermann HJ Fullér R Fuzzy Reasoning for Solving Fuzzy Mathematical Pro
gramming Problems Fuzzy Sets and Systems 601 121–133 1993 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 618–625 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Unstructured Scene Object Localization Algorithm  
Based on Sparse Overcomplete Representation 
Peng LuYuhe Tang Eryan Chen Huige Shi and Shanshan Zhang 
School of Electrical Engineering Zhengzhou University Zhengzhou China 
lupengzzueducn 
Abstract Unstructured scene has many uncertainties and unpredictable states 
It brings difficulties to the object localization which is pixelbased processing 
The method of analog visual information processing is an effective way to solve 
the problem mentioned above Sparse overcomplete representation is an image 
representation model which is more in line with visual mechanism However 
the overcomplete representation not only increases the combinatorial search 
difficulty of sparse decomposition but also changes the symmetry between 
input and code space Furthermore it makes the model solution and calculation 
method complicated In order to solve the afore mentioned problem and effec
tively use this model to achieve automatic image object localization this paper 
takes the unstructured scenes object localization as the background Firstly the 
overcomplete representation computational model which is based on energy 
model and score matching method is established Then an automatic object 
localization method based on the neuronal response and dynamic threshold 
strategy is proposed and applied to the movement object localization On this 
basis the error analysis is done Experimental results show that the method can 
achieve the movement object localization 
Keywords Unstructured scenes sparse overcomplete representation object  
localization score matching 
1 
Introduction 
Unstructured scene has many uncertainties and unpredictable states 1 which leads to 
the pixel values in a fixed position change constantly It brings difficulties to the 
method pixelbased processing Data processing of highdimensional heterogeneous in 
the unstructured scene involves complex computational process and needs an over
whelming computational amount Thus calculation method with efficient processing of 
unstructured scene is needed Excitedly visual computing method has advantages in 
environmental perception and cognitive abilities 2It is an important way to solve the 
above problems Therefore simulating the visual perception mechanism plays an 
important role in solving the moving object localization in unstructured scenes 
                                                           
  This work is supported by National Natural Science Foundation of China No60841004  
60971110 61172152 and Zhengzhou Science and Technology Development Project 
112PPTGY2198 
 
Unstructured Scene Object Localization Algorithm 
619 
 
Recently the sparse coding is commonly used as the visual computing model 3 
This model assumes that the code space is equal to input space dimension 4 However 
the image sparse representation based on overcomplete mechanism 5 meets the below 
three properties sparse divisibility and the shift invariance when it is used to represent 
the image data The simulation of overcomplete mechanism plays an important role in 
solving image processing problems such as object localization 
However the overcomplete representation increases the combinatorial search 
difficulty of sparse decomposition meanwhile changes the symmetry between input 
space and code space thus makes the model solution and calculation method 
complicated6 In order to solve the afore mentioned problem and effectively use the 
overcomplete representation model to achieve image object localization this paper 
takes object localization of unstructured scenes as its problem background Firstly we 
design the algorithm to obtain overcomplete receptive fields of simple cells in primary 
visual cortex from unstructured scenes sequences based on sparse overcomplete 
representation model Secondly we design the algorithm to achieve object localization 
Finally the effectiveness of the model and algorithm is tested through the unstructured 
scenes experiments The results show that this method can achieve the movement 
object localization 
2 
Computational Model 
The basic sparse coding model is 





=
=
m
i
i
i
A x y s
x y
I
1


 
1
Where
  
I x y
is a 
n × n
dimensional image
  
Ai x y
 is a basis function with n
dimensional column vector m  is the number of basis functions 
is is the response 
coefficient If
m  n
and A  is full rank then A  is overcomplete 
From the physiological point formula 1 is modeling the respond process of cells 
of visual cortex Basis function set A simulates the receptive fields of visual cortex 
Increase m to meet
m  n
 A  would contains more basic characteristics of the joint 
space such as localization orientation and frequency We can clearly see that sparse 
overcomplete representation would enhance the robustness of system 
Howeverif 
m  n
two problems will be caused The first one is that sparse 
decomposition about
  
I x y
based on overcomplete needs combination search 7So 
the approximation algorithm such as relaxation optimization algorithm 8 and greedy 
tracking algorithm 9 to solve this problem is needed But the calculation of 
approximation algorithm is very complicated The second problem is that  
overcomplete sparse coding model doesnt like the complete ICA model in which row 
vector
W
A
−1 =
is defined as the receptive fieldTherefore a new method must be 
adopted to solve the problem of model solution 
In this paper we estimated the receptive field through overcomplete sparse coding 
model based on energy model 10 Meanwhile we defined an objective function to  
 
620 
P Lu et al 
 
measure the sparseness and estimated the optimal receptive field W through 
maximizing the sparseness First we used the nonnormalized loglikelihood function 
to define the energy model 


 



=
+
−
=
n
i
T
i
i
n
G w x
Z W
w
x w
L
1
1
log


log
 
2
Where x is a single observation data 
 
Z W
is the normalization constant 
of
i
w 



=1


1
w dx
L x w
n

 




∏
=
=
n
i
T
i
i
G w x dx
W
Z
1
exp
The classic log cosh function 
G which is used to replace the logarithm probability density of 
is  is defined as  
 
 
u
u
G
i
i
= −α logcosh
 
3
Where 
i
α are parameters that are estimated following with
i
w  
Maximization likelihood estimation needs to calculate 
 
Z W
firstlyHowever under 
situation of overcomplete the calculation is complicated Therefore score matching 
11 is used to directly estimate the linear receptive field
k
w The log density function of 
the data vector is defined as 
 



n 
n
m
k
T
k
k
Z
G
p
α
α
α



log
1
1
1
w
w
w x
x
+
= 
=
 
4
Where n  is the dimension of the data m is the number of components which is the 
number of the receptive fieldsAnd the number of components m is larger than the 
dimension of the data n  x is the single sample data vector that is an image 
patchwhere the vector

kn 
k
k
w w
1
w =
is constrained to the unit norm 
Use the gradient of the logdensity function of data vector to define the score 
functionThe model score function is defined as  






1
1
1



log



m
m
m
T
k
k
k
k
α
α
p
α
α
α
g
ψ
=
= ∇
=
x
x W
x W
w
w x
 
5
Where g  is the first derivative of G  
We used the square of distance between model score function and data score 
function to get the objective function 
 


 


 






=
=
=
=
+
=
m
k
j
T
t
T
j
T
k
k
T
j
k
j
m
K
T
t
T
k
k
t
t g
g
T
t
g
T
J
1

1
1
1

1
2
1
1
w x
w x
w w
w x
α α
α
 
6
Where T  is the number of samples denoted by    
 
x T
x
x
2 
1 
 
We used the gradient descent algorithm to make the objective function minimization 
 

1
 

1
 
w w t
J w
w t
w t
t
w
η
=
−
∂
=
−
−
∂
 
7
Where  t
η
is the step of the negative gradient direction which is the learning rate 
 
Unstructured Scene Object Localization Algorithm 
621 
 
3 
Algorithm 
31 
Learning Algorithm of Receptive Field Overcomplete Set 
Input sample images 
Output the receptive field W  
Steps 
Step1 random sampling to the images to obtain the training samples 

nx 
x x
X


2
1
=
  
Step2 remove DC component of the
ix whiten the samples X  by the principal 
component analysis PCA method and project X  into whitenization space 
Z = VX
 
Step3 use unit row vector to initialize the
s
W  
Step4 calculate
  
J w 0 
 move along the negative gradient direction 
21 
t =
 
update W according to the formula 7 and normalize the unit vector meanwhile 
update parametersα  
Step5 If the result is convergent or the number of iterations reaches the set value 
then stop iteration otherwise return to step 4 
Step6 Project 
s
W  back into the original image space 
W V
W
s
=
 
Then we can obtain overcomplete receptive field of simple cell use this way 
According to the properties of competitive response and visual sparse 12 only a 
small amount of neurons is activated to constitute the internal representation of image 
So the object locating algorithm OLA selects N  neurons which have larger response 
to achieve image object localization 
32 
Image Locating Algorithm 
Input test image
1I  and its corresponding background image
2I  
Output results of object locating  
Steps 
Step1 sequentially sampling image
1I and
2I
 and obtain the image patches 
Xn
X
X


2
1
 
Step2 remove DC component of the
i
X calculate the neurons response according to 
the formula 
i
i
S =WX
 
Step3 retain N  larger responses in each group 
iS  
Step4 obtain the perception results 
g
S and
d
S of 
1I and
2I  
Step5 
gi
di
S
S
h
−
=
 set dynamic thresholds
n
S
S
n
i
gi
di

=
−
=
1
δ
 if 
h ≥ δ
 then 
i
X  is 
the object patch Store the coordinates of
i
X  
Step6 display the results of object locating 
622 
P Lu et al 
 
4 
Experiments and Data Analysis 
We selected the unstructured traffic images as test object of the above algorithm 
41 
Data Preprocessing and Learning the Receptive Field 
We select 14 images as the training images for learning the receptive fields Using 
algorithm 1 for learning the receptive field We can get 50000 image patches using 16 
 16 window Every patch is converted to a column vector Input data set X  
25650000 is acquired We preprocess the data just as algorithm 1 and retain 128 prin
cipal components after dimension reduction Then a representation with 512 receptive 
fields is estimated based on the energy model and score matching The resulting recep
tive fields are shown in Fig1 The number of receptive fields is four times when  
compared to the  PCA dimensions These results are exactly similar with the characte
ristics that found in the simple cell receptive fields of V1 area 
 
 
Fig 1 Overcomplete set of receptive fields  
42 
Experiment of Image Object Localization 
We select background image and test images of 512512 Using algorithm 2 for con
tentaware Firstly sampling the test images sequentially starting from the top left
hand corner apex of the image and adopting1616 pixels space sub window to sam
ple we can get the first image patch And then shifting the window to right 16 pixels 
we can get the second image patch Repeat the above step until we gathered all 
patches of first line Then let 16 pixels shift downward from the apex coordinate posi
tion and image patches of the second line are collected At last we could sample the 
entire image 
Response of neurons caused by one image patch is shown in Fig2 a N neurons 
with larger response are shown in Fig2 b The number N is determined by experi
ment 
The first experiment selected images with complex background The images are 
taken from video supervision sequence we do the grayscale processing Fig3 a is the 
test images The corresponding results are shown in Fig3 b The area of rectangular 
box is the target area marked by OLA algorithm 
 
Unstructured Scene Object Localization Algorithm 
623 
 
 
a The neurons response of an image          b The result of neurons response when 
N = 5
 
Fig 2 The neurons response aroused by one image patch 
 
aTest image         b the result of targeting 
Fig 3 Test results of the first experiments 
The second experiment selected vehicle image at highspeed The test image is 
shown in Fig4 The corresponding results are shown in Fig5 The area of rectangular 
box is the target area marked by OLA algorithm 
 
 
aone car                  bmany cars 
Fig 4 Two test images 
 
a the test results when 
N = 5
                   b the test results when 
N =1
 
Fig 5 Test results of the second experiments 
624 
P Lu et al 
 
The third experiment is comparison test Under the same experimental conditions 
respectively with Template Matching TM method and the edge detection based on 
wavelet method for the contrast experiment The results can be seen in Fig6 
 
 
 
       aOLA algorithm     b IM algorithm   c edge detection algorithm 
Fig 6 Contrast experiment 
43 
Results Analysis 
As can be seen in Fig6 OLA algorithm achieves the object localization accurately the 
TM and edge detection methods all have erroneous judgment 68 images were chosen 
for other experiments Statistics results of three algorithms are shown in Table 1  
Table 1 The statistics results of three algorithms  
algorithms 
image 
numbers 
located  correctly 
numbers 
located error 
numbers 
the accurate rates 
OLA 
68 
65
3
9559 
TM 
68 
52 
16 
7647 
edge de
tection 
68 
42 
26 
6176 
 
It can be seen from the Table 1 the accurate rate of object localization based on 
OLA algorithm has improved greatly compared with TM and edge detection methods 
From the above experiments we can see that the OLA algorithm can achieve the 
object localization under different conditions However there are still small amount of 
error patches from Table 1 we know that the error have little effect to results The 
edge of object exist error is attributed to vehicle and pedestrian in close distance the 
error of leaves is caused by the swing The error patches in Fig5 a less than Fig5 b 
shows that keeping N  neurons have more strong antijamming than retaining neuron 
with largest response 
Experiment results show that OLA methods based on sparse overcomplete represen
tation is effective for image object localization It has a low error rate and high accura
cy rate under unstructured background And has antijamming and relatively strong 
robustness to leaves swing 
5 
Conclusion 
By simulating visual information processing mechanism and method we established 
the image sparse overcomplete representation model and then put forward OLA  
 
Unstructured Scene Object Localization Algorithm 
625 
 
algorithm The algorithm solved the problem of the increasing of sparse decomposition 
search difficulty and is effectively used to realize image object localization The algo
rithm has an important theoretical and practical application value to traffic flow statis
tics The furthermore work is to solve the problem of error image patches caused by 
leaves swing and shadow 
References 
1 Rodriguez M Ali S Kanade T Tracking in Unstructured Crowded Scenes In IEEE 
12th International Conference on Computer Vision pp 1389–1396 2009 
2 Lu P Li YQ Faultimage Detection Algorithm Based on Visual Perception Chinese 
Journal of Scientific Instrument 319 1997–2002 2010 
3 Zheng M Bu JJ Graph Regularized Sparse coding for Image Rrepresentation IEEE 
Transactions on Image Processing 205 1327–1336 2011 
4 Geisler WS Visual Perception and the Statistical Properties of Natural Scenes Annual 
Review of Psychology 59 167–192 2008 
5 Thang ND Rasheed T Contentbased Facial Image Retrieval Using Constrained Inde
pendent Component Analysis Information Sciences 18115 3162–3174 2011 
6 Tseng P Further Results on Stable Recovery of Sparse Overcomplete Representations in 
the Presence of Noise IEEE Trans Inf Theory 552 888–899 2009 
7 Cai ZM Lai JZ An Overcomplete Learned DictionaryBased Image Denoising Me
thod Acta Electronica Sinica 372 347–350 2009 
8 Mohimani H BabaieZadeh M Jutten C A fast approach for overcomplete sparse de
composition based on smoothed L0norm IEEE Trans Signal Processing 571 289–301 
2009 
9 Needell D Vershynin R Uniform Uncertainty Principle and Signal Recovery via Regu
larized Orthogonal Matching Pursuit Foundations of Computational Mathematics 93 
317–334 2009 
10 Valgaerts L Bruhn A Dense Versus Sparse Approaches for Estimating the Fundamen
tal Matrix Int J Comput Vision 962 212–234 2012 
11 Joder C Essid S Richard G A Conditional Random Field Framework for Robust and 
Scalable Audiotoscore Matching IEEE Trans Audio Speech Lang Process 198 
2385–2397 2011 
12 Tiesinga PH Buia CI Spatial Attention in Area V4 is Mediated by Circuits in Primary 
Visual Cortex Neural Networks 228 1039–1054 2009 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 626–633 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Solving the Distribution Center Location Problem Based 
on Multiswarm Cooperative Particle Swarm Optimizer 
Xianghua Chu1 Qiang Lu1∗ Ben Niu2 and Teresa Wu3  
1 Shenzhen Graduate School Harbin Institute of Technology Shenzhen 518055 China 
2 College of Management Shenzhen University Shenzhen 518060 China 
3 School of Computing Informatics Decision Systems Engineering  
Arizona State University Tempe USA 
Drniubenqiangluhomegmailcom 
Abstract The discrete location of distribution center is a NPhard issue and has 
been studying for many years Inspired by the phenomenon of symbiosis in 
natural ecosystems multiswarm cooperative particle swarm optimizer 
MCPSO is proposed to solve the location problem In MCPSO the whole 
population is divided into several subswarms which keeps a well balance of 
the exploration and exploitation in MCPSO By competition and collaboration 
of the individuals in MCPSO the optimal location solution is obtained The 
experimental results demonstrated that the MCPSO achieves rapid convergence 
rate and better solutions 
Keywords discrete location distribution center improved PSO 
1 
Introduction 
Determination of optimal distribution center location is an abiding challenge for 
researchers and professionals in the logistics sector The location of distribution 
center however is a NPhard problem due to its various variables and complex 
constraints which makes it hard to be solved within polynomial time Some 
mathematical methods have been developed to cope with the location problem such 
as Gravity Method GM 1 Lagrangian Relaxation Algorithm LRA 24 Branch 
and Bound Method BBM 5  
In recent years with the rapid development of heuristic stochastic optimization 
algorithms more and more techniques have been introduced to solve the location 
problem of distribution center for instance Genetic Algorithm 67 Simulated 
Annealing 8 Neural Network 9 etc 
Based on the simulation of simplified social models particle swarm optimization 
PSO is first introduced in 10 as a novel evolutionary computation tool that has 
been applied in many engineering fields Nevertheless PSO suffers premature 
convergence as a result of weak exploitation and exploration  
                                                           
  Corresponding authors 
 
Solving the Distribution Center Location Problem Based on MCPSO 
627 
 
In this paper the distribution center location problem is studied with an improved 
PSO Multiswarm Cooperative PSO MCPSO 11The Section 2 describes the 
location problem modeling Section 3 provides a brief introduction of PSO Section 4 
presents MCPSO algorithm Section 5 introduces MCPSObased location algorithm 
and experimental study followed by conclusions in Section 6 
2 
The Discrete Location Model for Distribution Centers 
The formulation extends the Medianbased location model to include turnover cost 
and setup cost The goal is to select a certain number of sites out of a finite set of 
potential sites to locate distribution centers while minimize total cost of the supply 
network Before formulating the objective function the issue is concentrated on the 
following situations 
 
1 The decision for distribution center location is based on a set of candidate sites 
2 The capacity of factory is capable of satisfying the demand of ending 
customers 
3 The potential distribution centers have capacity limitation 
4 The demand of customer is predictable 
5 Each customer only supplied by one distribution center and each distribution 
center satisfies at least one customer 
6 Transportation cost is directly proportional to the quantity of transportation 
7 Total cost includes turnover cost setup cost and transportation cost 
 
Some relevant notations are explained as follows K  is a set of factories I  is a set 
of candidate sites J  represents the demand notes the unit cost of shipping from 
factory k  to distribution center i  is given by 
ki
c  and that cost from distribution 
center i  to customer j  is given by 
ijx  The turnover cost in site i  is given by 
ig  
Letting 
if  be the fixed facility cost of site i  and pq  denotes the predefined 
number of established distribution centers 
To formulate our problem the following decision variables are introduced for any 
i
∈ I
 j
∈ J
 
ijy  
iz  are a binary decision variables 
1
ijy =  if demand note j  is 
supplied by facility i  and 0 otherwise 
1
iz =  if a facility is located at candidate i   
and 0 otherwise The transportation volume that from factory k  to distribution center 
i  and from distribution center i  to customer j  is denoted as 
wki
 and 
ijx  
respectively 
ijt  denotes  whether center i  covers demand note j  it take on the 
value 1 if distribution center i  provides goods with customer j  otherwise it is 0 
The formulation for the above discrete location problem is 
1
1
1
1
1
1
1
min
l
m
m
n
l
m
m
ki
ki
ij
ij
i
ki
i
i
k
i
i
j
k
i
i
U
c w
h x
g w
z f
=
=
=
=
=
=
=
=
+
+
+




 
1
subject to 
628 
X Chu et al 
 
ki
k
i I
w
A
∈
≤

 k
∈ K
 
2
ki
i
k K
w
M
∈
≤

 i
I
∈  
3
i I i
z
pq
∈
≤

 
4
ij
j
i I
x
D
∈
≥

 j
∈ J
 
5
1
ij
i I
y
∈
=

 j
∈ J
 
6
ij
ki
j J
k K
x
w
∈
= ∈


 i
I
∈  
7


ijt ∈ 0 1
 
i
I
∀ ∈  
j
∀ ∈ J
 
8
0
ki
w ≥
 
ijx ≥ 0
 k
∈ K
 i
I
∈  j
∈ J
 
9
Constraints 2 limit the supply of factories within their capacities Constraints 3 
ensure that each distribution center receives input that within its processing capacity 
Constraint 4 states that the selected sites should not exceed the predefined number 
Constraints 5 stipulate that each demand note is satisfied Constraints 6 ensure that 
each customer is supplied by only one distribution center Constraints 7 state that 
the input goods of each distribution center equals to its output 8 and 9 are 
constraints for decision variables 
3 
Standard PSO 
PSO is a populationbased stochastic optimization algorithm based on the idea of a 
swarm moving over a given landscape In PSO algorithm each member of the 
population is called a “particle” and each particle flies around in the ddimensional 
search space with a velocity which is constantly updated by the particle’s own 
experience and the experience of the whole swarm The Standard PSO SPSO can be 
described as follows 
1
1
2
2

1
 

 

 
d
d
d
d
d
d
i
i
i
i
i
v
t
w v
t
c
r
pbest
x
t
c
r
gbest
x
t
+
=
×
+
×
×
−
+
×
×
−
 
10

1
 

1
d
d
d
i
i
i
x
t
x
t
v
t
+
=
+
+
 
11
For 
1

i
s
=

 and
1

d
n
=

 where s  is the population size n  is the problem 
dimension 
1c  and 
2c  are acceleration coefficients 
ivd  
t
 is particle velocity 
ixd  
t  is particle i ’s current position 
1r  and 
2r  is uniformly distributed function in 
the interval 01 w  is the inertia weight which provides the necessary diversity to 
the swarm by changing the momentum of particles 
id
pbest  is the location of the best 
 
Solving the Distribution Center Location Problem Based on MCPSO 
629 
 
solution vector found by i  and 
d
gbest  represents the best solution found so far in 
the population 
In SPSO there is only one swarm for evolution that the communication among 
particles is so simple thus the whole population is easy to be trapped into the local 
optima It is necessary to improve the convergence of SPSO especially where exists a 
large number of suboptima 
4 
Multiswarm Cooperative PSO 
In ecosystems many species have developed cooperative interactions with other 
species to improve their survival which is called symbiosis The phenomenon of 
symbiosis can be found in all forms of life from simple cells to birds and mammals 
Inspired by this phenomenon a master–slave mode is incorporated into the SPSO 
and the multiswarm species cooperative optimizer MCPSO is thus developed 
1112  
In MCPSO a population consists of one master swarm and several slave swarms 
The slave swarms mainly explore the feasible space while the master swarm focuses 
on exploitation The masterslave communication is showed in Fig 1 
 
Fig 1 The masterslave model 
Each slave swarm executes a single PSO or its variants within MCPSO When all 
slave swarms are ready with the new generations each slave swarm then sends their 
best individual to the master swarm and the best fitness of all received individuals 
would be selected and employed for evolution according to the following equations 
1
1
2
2

1
 
 
 
 


 
 
M
M
M
M
M
M
id
id
id
id
gd
id
v
t
wv
t
c
r p
t
x
t
c
r
p
t
x
t
+
=
+
−
+
−
 
3
 3

 
 
S
M
gd
id
c
r p
t
x
t
+
−
 
12

1
 

1
M
M
M
id
id
id
x
t
x
t
v
t
+
=
+
+
 
13
where 
M  
pid
t  is the best previous position of  particle i  in master swarm 
M  
pgd
t  
is the best previous position of the master swarm 
S  
pgd
t  is the best previous position 
in the slave swarms 
3c
 is acceleration coefficient which determines the 
communication intensity that the slave swarms impact the master swarm 
630 
X Chu et al 
 
5 
MCPSOBased Location Algorithm 
51 
Particle Coding 
The key to implement MCPSObased location is to develop an efficient particle 
representation method In the above location model there are two types of variables 
the binary variables eg 
iz  and 
ijy  and the numeric variables eg 
ki
w  and 
ijx  
For the binary decision variables discrete binary method is convenient to code the 
particle However it may be inefficient for the numeric variables eg for a company 
with l factories m candidate sites and n demand notes then the potential problem 
scale of 
ki
w  and 
ijx  is l
×m
 and m
×n
 respectively Apparently the algorithm 
efficiency would deteriorate dramatically with the increase of problem scale  
Besides the representation of floating number by binary method is lack of  
accuracy Therefore a hybrid parallel coding method is employed as shown in  
Table 1 
Table 1 Particle coding structure 
Variable 
Candidate 
1 
2 
3 
…… 
m 
yij 
00…0 
y21y21…y2n
y31y31…y3n 
…… 
00…0 
wki 
0 
wk2 
wk3 
0 
52 
Constraint Handling and Fitness Function 
We make use of annealing approach 8 to establish the penalty factor τ  ie 
0 1
t
τ
=τ
+
 where 
0
τ  is the initial value then penalty coefficient is given by 14 
0
1

2

t
r
τ
+
=
×
 
14
According to 14 an infeasible solution may not be penalized enough at the 
beginning while it may be penalized seriously in the lately stage hence population 
likely to concentrate on local exploitation 
With the proposed penalty function method the location model problem can be 
transformed into an unconstrained one and the fitness function is built as 15  
2
0
1
 
2
q
i A
t
fitness
U
d
x
τ
∈
+
=
+
×
 
15
where 
dq  
x  is a realvalued function  
1
2
3
4
5
 
max0





dq
x
nc nc nc nc nc
=

12
1
2
q
l
m
n
=
+ +
+

 
16
 
Solving the Distribution Center Location Problem Based on MCPSO 
631 
 
where 
ii
nc 
ii =12345
 is given as follows
1
ki
k
i I
nc
w
A
∈
=
−

 k
∈ K
 
2
ki
i
k K
nc
w
M
∈
=
−

 i
I
∈  
3
i I i
nc
z
p
∈
=
−

 
4
j
i I ij
nc
D
x
∈
=
−
 j
∈ J
 
5
|
|
ij
ki
j J
k K
nc
x
w
ε
∈
∈
=
−
−


 i
I
∈  
53 
Experimental Study and Discussion 
A company has established a factory in southern China and it aims to build no more 
than 4 distribution centers to serve 15 demand notes The number of candidate sites is 
10 The detailed data and parameter settings are given in 13 
Experiments are conducted to compare MCPSO with SPSO When conducting 
MCPSObased algorithm one master swarm and three slave swarms are set up in a 
population For master swarm 
1
2
3
1333
c
c
c
=
=
=
 and for slave swarms the 
coefficients are set at 2 The population size for each slave swarm is 50 and the 
maximum generation is set at 100 Comparing with MCPSO the population size of 
SPSO is set at 200 Besides SPSO has the same parameters settings as well as 
MCPSO 50 runs for each algorithm are carried out The evolution and convergence 
of MCPSO and SPSO are shown in Fig 2 and the results are presented in Table 2 
Fig 2 a and b illustrate how the algorithms evolve and converge during the 
evolutions In Fig 2 a although the master swarm gets inferior values at the 
beginning its fitness figure decreases steeply and overtakes the slave swarms at about 
18th iteration then the master swarm continuously evolves With collaboration 
between the master swarms and the slave swarms MCPSO gain a superior 
performance According to Fig 2 b SPSO is trapping into local optima in the 9th 
iteration and the whole algorithm gets stagnated that no better solution is found  
Comparing with SPSO as shown in Table 2 MCPSO generates a feasible solution 
that without any penalty cost which has practical values in the facility location 
Comparison is also conducted between the proposed algorithm and GA GA is 
carried out with the population size of 50 for maximum of 100 iterations the 
Roulette Wheel Selection is used to select the best individuals The probability of 
crossover and mutation is set at 04 and 005 respectively The results in Table 3 are 
the average of minimum values and standard deviations for 30 trials 
In Table 3 the MCPSObased algorithm achieves the best mean result and 
standard deviation over the other algorithms SPSO gains the second place in the 
average cost while its standard deviation is worse than GA This phenomenon is 
probably attributed to its premature convergence and stochastic instability  
In addition comparing with some traditional location methods such as 1 14 
the proposed MCPSObased algorithm not only demonstrates the turnover of  
each selected location center but also presents additional valued information eg 
service coverage transportation volume for the decision makers to optimize supply 
network 
 
632 
X Chu et al 
 
 
a 
b 
Fig 2 The median convergence characteristics a MCPSObased algorithm 2 SPSO 
Table 2 Solution results 
Algorithm 
Decision 
Coverage of each facility 
Total cost 
Penalty cost 
MCPSO 
5 
35111214 
291279 
0 
6 
2467 
8 
189101315 
SPSO 
5 
3491114 
329090 
13333 
8 
2561215 
10 
1781013 
Table 3 The results of MCPSO SPSO and GA 
Algorithm 
MCPSO 
SPSO 
GA 
Average cost 
293565 ± 5993 
316582 ± 18562 
328962 ± 10256 
6 
Conclusion 
In this paper we considered an extension of the traditional Medianbased distribution 
center location model in terms of fixed cost turnover cost and transportation cost 
which is highly applicable in distribution center location A MCPSObased algorithm 
is proposed to solve the NPhard location model Due to the discrete characteristics of 
this location model a hybrid parallel coding method is developed and the SA factor is 
integrated into penalty coefficient A realworld location problem is employed to 
assess to the performance of MCPSObased algorithm In the experimental 
simulation MCPSObased location algorithm shows a superior result and overtakes 
that of GA and SPSO algorithm  
There are still several problems remaining to be investigated such as how to make 
the swarms more adaptive in evolution and how to apply the algorithms of this kind 
to more realworld engineering problems 
 
 
Solving the Distribution Center Location Problem Based on MCPSO 
633 
 
Acknowledgements This work is supported by the National Natural Science 
Foundation of China Grants No 71171064 No71001072 Science and Technology 
Project of Shenzhen Grant No JC201005280492A The Natural Science Foundation 
of Guangdong Province Grant no 9451806001002294 
References 
1 Plastria F Facility Location A Survey of Application and Method pp 25–80 Springer 
New York 1995 
2 Beasley JE Lagrangean Heuristics for Location Problems European Journal of 
Operational Research 65 383–399 1993 
3 Agar M Salhi S Lagrangean Heuristics Applied to a Variety of Large Capacitated Plant 
Location Problem Journal of the Operational Research Society 49 1072–1084 1998 
4 Aykin T Lagrangian Relaxation Based Approaches to Capacitated Hubandspoke 
Network Design Problem European Journal of Operational Research 79 501–523 1994 
5 Land AH Doig AG An Automatic Method of Solving Discrete Programming 
Problems Econometrica 28 497–520 1960 
6 Topcuoglu H Corut F Ermis M Yilmaz G Solving the Uncapacitated Hub Location 
Using Genetic Algorithms Computers and Operations Research 32 967–984 2005 
7 Qian J Pang XH et al An Improved Genetic Algorithm for Allocation Optimization 
of Distribution Centers Journal of Shanghai Jiaotong University 9 73–76 2004 
8 AbdinnourHelm S Using Simulated Annealing to Solve the Phub Median Problem Int 
Journal of Physical Distribution and Logistics Management 31 203–220 2001 
9 Smith KA Krishnamoorthy M Palaniswami M Neural versus Traditional Approaches 
to the Location of Interacting Hub Facilities Location Science 4 155–171 1996 
10 Eberhart R Kenedy J Particle Swarm Optimization In Proceedings of IEEE Int Conf 
Neural Networks Piscataway pp 1114–1121 1995 
11 Niu B Zhu YL He XX MCPSO A MultiSwarm Cooperative Particle Swarm 
Optimizer Applied Mathematics and Computation 185 1050–1062 2007 
12 Niu B Xue B Li L Chai Y Symbiotic Multiswarm PSO for Portfolio Optimization 
In Huang DS Jo KH Lee HH Kang HJ Bevilacqua V eds ICIC 2009 
LNCS LNAI vol 5755 pp 776–784 Springer Heidelberg 2009 
13 httpsdocsgooglecomopenid=0B0LeEyhqhWOGbVdTeGRHR3VWT2c 
14 Huo H Research on Distribution Center Location Problems Logistic Science 
Technology 27 50–52 2004 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 634–640 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Improved Bacterial Foraging Optimization  
with Social Cooperation and Adaptive Step Size 
Xiaohui Yan12 Yunlong Zhu1 Hanning Chen1 and Hao Zhang12 
1 Key Laboratory of Industrial Informatics Shenyang Institute of Automation  
Chinese Academy of Sciences 110016 Shenyang China 
2 Graduate School of the Chinese Academy of Sciences 100039 Beijing China 
yanxiaohuiylzhuchenhanningzhanghaosiacn 
Abstract This paper proposed an Improved Bacterial Foraging Optimization 
IBFO algorithm to enhance the optimization ability of original Bacterial  
Foraging Optimization In the new algorithm Social cooperation is introduced 
to guide the bacteria tumbling towards better directions Meanwhile adaptive 
step size is employed in chemotaxis process The new algorithm is tested on a 
set of benchmark functions Canonical BFO Particle Swarm Optimization and 
Genetic Algorithm are employed for comparison Experiment results show that 
the IBFO algorithm offers significant improvements over the original BFO al
gorithm and is a competitive optimizer for numerical optimization 
Keywords bacterial foraging optimization social cooperation adaptive search 
strategies 
1 
Introduction 
Bacterial Foraging Optimization BFO is a novel swarm intelligence algorithm first 
proposed by Passino in 2002 1 It is inspired by the foraging and chemotactic beha
viors of bacteria Recently BFO algorithm and its variants have been used for many 
numerical optimization 2 or engineering optimization problems 34 
However original BFO algorithm has some weaknesses First the tumble angles 
are generated randomly Useful information can’t be shared between bacteria Second 
the step size in the original BFO is a constant If the step size is large at the end stage 
it is hard to converge to the optimal point In this paper we proposed an Improved 
Bacterial Foraging Optimization IBFO Two adaptive strategies are used in IBFO to 
improve its optimization ability First social cooperation is introduced to enhance the 
information sharing between bacteria Then adaptive step size is employed which 
could make the bacteria use different search step sizes in different stages  
The rest of the paper is organized as followed Section 2 introduces the original 
BFO algorithm In section 3 the proposed IBFO algorithm is described in detail In 
Section 4 the IBFO algorithm is tested on a set of benchmark functions compared 
with several other algorithms Results are presented and discussed Finally conclu
sions are drawn in Section 5 
                                                           
 This research is partially supported by National Natural Science Foundation of China 
61174164 supported by National Natural Science Foundation of China 61003208 and sup
ported by National Natural Science Foundation of China 61105067 
 
Improved BFO with Social Cooperation and Adaptive Step Size 
635 
 
2 
Original Bacterial Foraging Optimization 
The E coli bacterium is one of the earliest bacterium which has been researched It 
has a plasma membrane cell wall and several flagella which are randomly distributed 
around its cell wall 1 By the rotation of the flagella E coli can “tumble” or “run” in 
the nutrient solution By simulating the foraging process of bacteria Passino proposed 
the BFO algorithm The main steps of BFO are explained as followed 
21 
Chemotaxis 
In BFO the position updating which simulates the chemotaxis procedure is used the 
Eq 1 as followed θt is the position of the bacterium in the tth chemotaxis step Ci 
presents the step size Φi is a randomly produced unit vector which stands for the 
tumble angle  
   
1
i
C i
t
i
it
φ
θ
θ
+
+ =
 
 1
In each chemotactic step the bacterium generated a tumble direction firstly Then the 
bacterium moves in the direction using Eq 1 If the nutrient concentration in the new 
position is higher than the last position it will run one more step in the same direction 
This procedure continues until the nutrient get worse or the maximum run step is 
reached The maximum run step is controlled by a parameter called Ns 
22 
Reproduction 
For every Nc times of chemotactic steps a reproduction step is taken in the bacteria 
population The bacteria are sorted in descending order by their nutrient values Bacte
ria in the first half of the population will split into two Bacteria in the residual half of 
the population die By this operator individuals with higher nutrient are survived and 
reproduced which guarantees the potential optimal areas are searched more carefully 
23 
Eliminate and Dispersal 
After every Nre times of reproduction steps an eliminatedispersal event happens For 
each bacterium it will be moved to a random place according to a certain probability 
known as Pe This operator enhances the diversity of the algorithm 
3 
Improved Bacterial Foraging Optimization 
31 
Social Cooperation 
As we known swarm intelligence is emerged by the cooperation of simple individuals 
5 However the social cooperation hasn’t been used in original BFO algorithm In 
chemotactic steps the tumble directions are generated randomly Information carried 
636 
X Yan et al 
 
by the bacteria in richnutrient positions is not utilized In our IBFO the tumble direc
tions are generated using Eq 2 Where θgbest is the global best of the population found 
so far θi pbest  is the ith bacterium’s personal historical best The tumble direction is 
then normalized as unit vector and the position updating is still using the Eq 1 





i
i pbest
i
gbest
i
θ
θ
θ
θ
−
+
−
Δ =
 
 2
The direction generating equation is similar with the velocity updating equation of 
PSO algorithm 6 They all used the global best and personal best However they are 
not the same First there is no inertia term in Eq 2 In chemotactic steps of bacteria  
inertia term will enlarge the difference of between θgbest θi pbest and the current position 
tremendously Second there are no learning factors in Eq 2 as the direction will be 
normalized to unit vector By social cooperation the bacteria will move to better areas 
with higher probability as good information is fully utilized 
32 
Adaptive Step Size 
As it mentioned above the constant step size will make the population hard to converge to 
the optimal point In an intelligence optimization algorithm it is important to balance its 
exploration ability and exploitation ability Generally in the early stage of an algorithm 
we should enhance the exploration ability to search all the areas In the later stage of the 
algorithm we should enhance the exploitation ability to search the good areas intensively 
There are various step size varying strategies 78 In IBFO we use the decreasing 
step size The step size will decrease with the iteration as shown in Eq 3 Cs is the 
initial step size Ce is the ending step size NowEva is the current function evaluations 
count TotEva is the total function evaluations In the early stage of IBFO algorithm 
we use larger step size to guarantee the exploration ability And at the end stage small
er step size is used to make sure the algorithm can converge to the optimal point 
NowEva TotEva
C
C
C
C
e
s
s



×
−
−
=
  
3
4 
Experiments 
In this section we tested the optimization ability of IBFO algorithm on six benchmark 
functions Original BFO PSO and Genetic Algorithm GA were employed for  
comparison  
41 
Benchmark Functions 
The six benchmark functions are listed in Table 1 They are widely adopted by other re
searchers to test their algorithms in many works 910 Dimension of all functions are 20 
To compare algorithms fairly we use number of function evaluations FEs as a 
measure criterion in this paper It is also used in many other works 1112 All algo
rithms were terminated after 60000 function evaluations 
 
Improved BFO with Social Cooperation and Adaptive Step Size 
637 
 
42 
Parameter Settings for the Involved Algorithms 
The population sizes S of all algorithms are 50 In original BFO and IBFO algorithm 
the parameters are set as followed Nc=50 Ns=4 Nre=4 Ned=10 Pe=025 C=01 
Sr=S2=25 The initial step size of IBFO Cs=01UbLb ending step Ce=000001Ub
Lb where Lb and Ub refer the lower bound and upper bound of the variables of the 
problems In PSO algorithm ω decreased from 09 to 07 C1=C2=20 13 
Vmin=01×Lb Vmax=01×Ub In GA Pc is 095 and Pm is 01 
Table 1 Benchmark functions used in the experiment 
Function 
Formulation 
Variable ranges fx 
Sphere 

=
=
D
i
ix
x
f
1
2
1
 
 
512 512 
0 
Rosenbrock 



−
=
+
−
+
−
=
1
1
2
2
1
2
2

1

100


D
i
i
i
i
x
x
x
x
f
 
−15 15 
0 
Rastrigin 



=
+
−
=
D
i
i
i
x
x
x
f
1
2
3
 10
10cos2
 
π
 
10 10 
0 
Ackley 














−

−

+ −
=
=
=
D
i
i
D
i
i
x
D
x
D
e
e
e
x
f
1
1
2

cos2
1
1
20
4
20
20


π
 
−32768 
32768 
0 
Griewank 
1

cos
4000
1


1
1
2
5
 +




 − 





=
∏

=
=
D
i
i
D
i
i
i
x
x
x
f
 
−600 600 
0 
Schwefel222

∏
=
=
+
=
n
i
n
i
i
i
x
x
x
f
1
1
6
 
 
1010 
0 
43 
Experiment Results and Statistical Analysis 
The results of IBFO BFO PSO and GA on the benchmark functions are listed in Table 
2 Best values of them on each function are marked as bold Convergence plots of the 
algorithms on these functions are shown in Fig 1 
It is clear from table 2 that IBFO obtained the best values on five of all six func
tions PSO is best on the rest one BFO and GA performed worst On Rosenbrock and 
Rastrigin functions all algorithms didn’t performed well However the results of 
IBFO are a little better than that of PSO On Ackley function BFO PSO and GA all 
performed badly while only IBFO obtained remarkable results IBFO converged fast at 
the end stage and seemed was able to continue improving its result On Griewank func
tion it got a rank of 2 However it is only a little worse than PSO On Schwefel222 it 
performed better than the other three algorithms too Overall IBFO shows significant 
improvement over the original BFO algorithm And its optimization ability is better 
than the classic PSO and GA algorithms on most functions 
638 
X Yan et al 
 
 
 
0
2
4
6
x 10
4
5
0
5
evaluationCount
Fitnesslog
 
 
IBFO
BFO
PSO
GA
 
0
2
4
6
x 10
4
2
4
6
8
evaluationCount
Fitnesslog
 
 
IBFO
BFO
PSO
GA
 
a Sphere 
b Rosenbrock 
0
2
4
6
x 10
4
1
2
3
4
evaluationCount
Fitnesslog
 
 
IBFO
BFO
PSO
GA
 
0
2
4
6
x 10
4
2
1
0
1
2
evaluationCount
Fitnesslog
 
 
IBFO
BFO
PSO
GA
 
c Rastrigin 
d Ackley 
0
2
4
6
x 10
4
2
0
2
4
evaluationCount
Fitnesslog
 
 
IBFO
BFO
PSO
GA
 
0
2
4
6
x 10
4
0
5
10
evaluationCount
Fitnesslog
 
 
IBFO
BFO
PSO
GA
 
e Griewank 
f Schwefel222 
Fig 1 Convergence plots of IBFO BFO PSO and GA algorithms on functions 
 
 
 
Improved BFO with Social Cooperation and Adaptive Step Size 
639 
 
Table 2 Results obtained by the IBFO BFO PSO and GA algorithms 
Function 
IBFO 
BFO 
PSO 
GA 
f1 
Mean 
316004e007 
617635e001 
452322e006 
661625e001 
Std 
150001e007 
194300e001 
334529e006 
234433e001 
f2 
Mean 
205631e+001 
202447e+003 
249117e+001 
656345e+002 
Std 
171408e+001 
862087e+002 
212081e+001 
455687e+002 
f3 
Mean 
132458e+001 
494922e+002 
335838e+001 
680152e+001 
Std 
541958e+000 
674151e+001 
107949e+001 
177908e+001 
f4 
Mean 
100880e002 
195483e+001 
110071e+000 
186919e+001 
Std 
147724e003 
371818e001 
914650e001 
130767e000 
f5 
Mean 
496128e002 
300705e+000 
389587e002 
311937e+000 
Std 
397896e002 
45062e001 
313277e002 
855760e001 
f6 
Mean 
851024e003 
543634e+001 
139941e001 
492696e+000 
Std 
171202e003 
120626e+001 
187068e001 
109457e+000 
5 
Conclusions 
This paper analyzes the shortages of original BFO algorithm To overcome its short
ages an Improved Bacterial Foraging Optimization IBFO algorithm is proposed 
Social cooperation and adaptive step size strategies are used in IBFO In the chemo
tactic steps the tumble angles are no longer generated randomly Instead they are 
produced using the information of the bacteria’s global best the bacterium’ personal 
best and its current position The step size in chemotactic processes decreases linearly 
with iterations too which could balance its exploration ability and exploitation ability 
To test the optimization ability of IBFO algorithm it is tested on a set of bench
mark functions compared with original BFO PSO and GA algorithms The results 
show that IBFO algorithm performed best on five functions of all six On the rest one 
it is only a little worse than PSO In general the proposed IBFO algorithm offers 
significant improvements over original BFO and is a competitive algorithm for opti
mization compared other algorithms 
References 
1 Passino KM Biomimicry of Bacterial Foraging for Distributed Optimization and Con
trol IEEE Control Systems Magazine 22 52–67 2002 
2 Chen H Zhu Y Hu K Cooperative Bacterial Foraging Optimization Discrete Dynam
ics in Nature and Society Article ID 815247 17 pages 2009 
640 
X Yan et al 
 
3 Wu C Zhang N Jiang J Yang J Liang Y Improved Bacterial Foraging Algorithms 
and Their Applications to Job Shop Scheduling Problems In Beliczynski B Dzielinski 
A Iwanowski M Ribeiro B eds ICANNGA 2007 Part I LNCS vol 4431 pp 562–
569 Springer Heidelberg 2007 
4 Majhi R Panda G Majhi B Sahoo G Efficient Prediction of Stock Market Indices 
Using Adaptive Bacterial Foraging Optimization ABFO and BFO Based Techniques 
Expert Systems with Applications 366 10097–10104 2009 
5 Eberhart RC Shi Y Kennedy J Swarm Intelligence Morgan Kaufmann 2001 
6 Shi Y Eberhart RC A Modified Particle Swarm Optimizer In Proceedings of the 
IEEE Congress on Evolutionary Computation Piscataway pp 303–308 1998 
7 Chen H Zhu Y Hu K SelfAdaptation in Bacterial Foraging Optimization Algorithm 
In Proceedings of the 3rd International Conference on Intelligent System  Knowledge 
Engineering Xiamen China pp 1026–1031 2008 
8 Zhou B Gao L Dai Y Gradient Methods with Adaptive StepSizes Computational 
Optimization and Applications 351 69–86 2006 
9 Karaboga D Basturk B A Powerful and Efficient Algorithm for Numerical Function 
Optimization Artificial Bee Colony ABC Algorithm Journal of Global Optimiza
tion 393 459–471 2007 
10 Zou W Zhu Y Chen H Sui X A Clustering Approach Using Cooperative Artificial 
Bee Colony Algorithm Discrete Dynamics in Nature and Society Article ID 459796 16 
pages 2010 
11 Yan X Zhu Y Zou W A Hybrid Artificial Bee Colony Algorithm for Numerical 
Function Optimization In 11th International Conference on Hybrid Intelligent Systems 
pp 127–132 2011 
12 Liang JJ Qin AK Suganthan PN Baskar S Comprehensive Learning Particle 
Swarm Optimizer for Global Optimization of Multimodal Functions IEEE Transcations 
on Evolutionary Computing 10 281–295 2006 
13 Shi Y Eberhart RC Empirical Study of Particle Swarm Optimization In Proceedings 
of the IEEE Congress on Evolutionary Computation Piscataway NJ USA vol 3 pp 
1945–1950 1999 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 641–648 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Root Growth Model for Simulation of Plant Root System 
and Numerical Function Optimization 
Hao Zhang1 2 Yunlong Zhu1 and Hanning Chen1 
1 Key Laboratory of Industrial Informatics Shenyang Institute of Automation of Chinese  
Academy of Sciences 110016 Shenyang China 
2 Graduate School of the Chinese Academy of Sciences 100039 Beijing China 
zhanghaoylzhuchenhanningsiacn 
Abstract This paper presents the study of modelling root growth behaviours in 
the soil The purpose of the study is to investigate a novel biologically inspired 
methodology for optimization of numerical function A mathematical frame
work is designed to model root growth patterns Under this framework the inte
ractions between the soil and root growth are investigated A novel approach 
called “root growth algorithm” RGA is derived in the framework and simula
tion studies are undertaken to evaluate this algorithm The simulation results 
show that the proposed model can reflect the root growth behaviours and the 
numerical results also demonstrate RGA is a powerful search and optimization 
technique for numerical function optimization 
Keywords Root growth simulation numerical function optimization  
modelling 
1 
Introduction 
Nature ecosystems have been the rich source of mechanisms for designing computa
tional systems to solve difficult engineering and computer science problems Model
ing is an important tool for the comprehension of complex systems such as nature 
ecosystems and the model inspired from nature ecosystem is instantiated as an opti
mizer for numerical function and engineering optimization 
Computational root growth models or “virtual root system of plant” are increasing
ly seen as a useful tool for comprehending complex relationships between plant phy
siology root system development and the resulting plant form Therefore accurate 
root growth models for simulation of rootsoil interactions are of major significance 
In the field of modeling root system of plant there already exists a variety of elabo
rate approaches 13 However the complexity of the root–soil system requires an 
accurate and detailed description not only of each subsystem but also of their mutual 
linkage and influence 
                                                           
  This research is partially supported by National Natural Science Foundation of China 
61174164 supported by National Natural Science Foundation of China 61003208 and sup
ported by National Natural Science Foundation of China 61105067 
642 
H Zhang Y Zhu and H Chen 
 
In pursuit of finding solution to the optimization problems many researchers have 
been drawing inspiration from the nature 4 A lot of such biologically inspired algo
rithms have been developed namely Genetic Algorithm GA 5 Particle Swarm 
Optimization PSO 6 Artificial Immune System AIS 7 and Artificial Bee Colo
ny ABC 8 Plant growth simulation algorithm PGSA is a powerful evolutionary 
algorithm simulating plant growth that has been proposed for solving integer pro
gramming 9 These algorithms with their stochastic means are well equipped to 
handle such problems However the algorithms inspired from plant root models are 
very limited  
This study presents a root growth model which can simulate plant root and formu
lated as an optimization algorithm for numerical function optimization This paper is 
structured as follows root growth model is presented in Section 2 The selfadaptive 
growth of root hairs is simulated in Section 3 In Section 4 experimental settings and 
experimental results are given Finally Section 5 concludes the paper 
2 
Proposed Root Growth Model 
21 
Description for Growth Behavior of Root System 
At the end of the 1960s A Lindenmayer introduced Transformationalgenerative 
grammar into biology and developed a variant of a formal grammar namely L
Systems most famously used to model the growth processes of plant development L
Systems are based on simple rewriting rules and branching rules and successfully 
make a formal description for the plant growth We use LSystems to describe growth 
behavior of root system as follows  
• A seed germinates in the soil partly becoming stems of plant above the earths 
surface  The other part grows down becoming plant root system New root hairs 
grow from the root tips 
• More new root hairs grow from the root tips of old root hairs The behavior of root 
system which is repeated is called as branching of the root tips 
• Most root hairs and root tips are similar to each other The entire root system of 
plant has selfsimilar structure The root system of each plant is composed of nu
merous root hairs and root tips with similar structure  
22 
Plant Morphology 
The uneven concentration of nutrients in the soil makes root hairs growing towards dif
ferent directions This characteristic of root growth relates to the morphogenesis model in 
biological theory When the rhizome of root system grows three or four growing points 
with different rotation directions will be generated at each root tip The rotation diversi
fies the growth direction of the root tip Root tips from which root hairs germinate con
tain undifferentiated cells These cells are considered as fluid bags in which there are 
homogeneous chemical constituents One of chemical constituents is a version of the 
growth hormone called as morphactin The morphactin concentration determines wheth
er cells start to divide When cells start to divide root hairs appear  
 
Root Growth Model for Simulation of Plant Root System 
643 
 
With regard to the process of root growth there have been the following conclu
sions in biology 
• If root system of plant have more than one root tip which root tips could germi
nate root hairs depends on their morphactin concentration The probability of new 
root hairs germinating is higher from root tips with larger morphactin concentra
tion than root tips with less ones 
• The morphactin concentration in cells is not static but depends on its surround
ings After new root hairs germinate and grow the morphactin concentration will 
be reallocated among new root tips in line with the new concentration of nutrients 
in the soil 
In order to simulate the above process it is assumed that the sum of the morphactin 
concentration of is constant considered as 1 in the morphactin state space of the 
multicellular closed system If there are n root tips xi i=1 2 ⋯ n which are D
dimensional vectors the morphactin concentration of any cell is defined as Ei i=1 2 
⋯ n The morphactin concentration of each root tip can be expressed as 
 
 
1
1
1
i
i
n
i
i
f x
E
f x
=
=

     
    
 1
where f  is objective function which represents the spatial distribution of nutrients 
in the soil In expression 1 the morphactin concentration of each root tip is deter
mined by the relative position of each point and environmental information objective 
function value at this position When new root hairs germinate the morphactin con
centration may be changed 
23 
Branching of the Root Tips 
There are four rules for branching of the root as follow 
• Plant growth begins from a seed  
• In each cycle of growth process some excellent root tips which have larger mor
phactin concentration values are selected to branch 
• The distance should not be close between the selected root tips in order to make 
spatial distribution of the root system wider and increase the diversity of the fit
ness values  
• If the number of the root tips selected equals the predefined value the loop of the 
selection process terminates 
In order to produce a new growing point from the old root tip in memory the pro
posed model uses the following expression 


2
1
ij
ij
lj
ij
x
j
k
pg
x
j
k
δ

+
×
=

= 
≠

   
  2
where k∈1 2 ⋯ D are randomly chosen indexes and j∈1 2 ⋯ D pgl i=1 
2 ⋯ S are S new growing points δij is a random number between 1 1 
644 
H Zhang Y Zhu and H Chen 
 
24 
Root Hair Growth 
Root hair growth depends on its growth angle and growth length The growth angle is 
a vector for measuring the growth direction of root hair The growth angle of each 
root hair φi i=1 2 ⋯ n which is produced randomly can be expressed as  




1
 2


D
rand D
φ φ
φ
=

 
3


1
2
2
2
2
1
2
D



D
i
φ φ
φ
ϕ
φ
φ
φ
=
+
+
+


 
 4
The growth length of each root hair is defined as δi i=1 2 ⋯ n which is an impor
tant parameter in the root growth model Some strategies of tuning the parameter can 
produce multiple versions of the root growth model After growing a new root tip is 
produced by the following expression 
i
i
i
i
x
x
δ ϕ
=
+
   
5
25 
Root Growth Algorithm 
The root growth model proposed is instantiated as root growth algorithm RGA for 
simulation of root system of plant and numerical function optimization The threshold 
of the distance between root tips and the growth length of each root hair are important 
parameters for RGA The flowchart of the RGA is presented in Figure 1  
 
Fig 1 The flowchart of the RGA 
3 
Simulating Selfadaptive Growth of Root Hairs 
Simulating selfadaptive growth of root hairs is important for learning relationships 
between the nutrient concentration of the soil and the growth length of root hairs 
Rastrigin function is used as soil environment in computer Rastrigin function is pre
sented in Section 41 and the setting of the parameters is the same as one in Section 
 
Root Growth Model for Simulation of Plant Root System 
645 
 
42 The growth length of each root hair δi i =1 2 ⋯ n is the master variable for 
selfadaptive growth of root hairs The growth length of all root hairs change as their 
fitness values change The pseudocode and the formula of the way of selfadaptive 
growth are listed in Table 1 in which τ is a nonnegative number Figure 6 shows the 
simulation of the selfadaptive growth way The roots of plant grow towards the same 
direction with the high concentration of nutrients From the view point of the optimi
zation the selfadaptive growth way can make all points moving toward the area with 
the largest fitness value and then these points move around back and forth for inten
sive search So RGA using the way of selfadaptive growth can obtain better optimal 
solution easily and quickly  
Table 1 Pseudocode for selfadaptive growth way  
Set δi = Initial value 
FOR each generation 
FOR each point xi 
Set δi = |Ei|  |Ei + τ| 
END FOR 
END FOR 
 
Fig 2 The simulation of selfadaptive growth way 
4 
Numerical Experiments for Optimization 
41 
Benchmark Functions 
The set of benchmark functions contains five functions that are commonly used in 
evolutionary computation literature 10–12 to show solution quality and convergence 
rate The five function are Sphere function Rosenbrock function Ackley function 
Griewank function and Rastrigin function The first two functions are unimodal prob
lems and the remaining functions are multimodal The functions are listed below The 
dimensions initialization ranges global optimum and the criterion of each function 
are listed in Table 2 
646 
H Zhang Y Zhu and H Chen 
 
Table 2 Parameters of the benchmark functions 
Function  
Dimensions 
Initial range  
Minimum 
Sphere 
30 
−100 100D 
0 
Rosenbrock 
30 
−30 30D 
0 
Ackley 
30 
−32768 32768 D 
0 
Griewank 
30 
−600 600D 
0 
Rastrigin 
30 
−1010 D 
0 
42 
Settings 
For the experiments the value of the common parameter total evaluation number 
used in each algorithm was chosen to be the same The maximal number of fitness 
function evaluations was 200000 for all functions  
For GA a binary coded standard GA were employed The rate of single point cros
sover operation was 08 Mutation rate was 001 The selection method was stochastic 
uniform sampling technique Generation gap is the proportion of the population to be 
replaced Chosen generation gap value in experiments was 09 For DE F was set to 
05 Crossover rate was chosen to be 09 as recommended in 13 For PSO inertia 
weight was 06 and cognitive and social components were both set to18 14  
The parameter settings of RGA for simulation and optimization are listed in Table 3 
All parameter values have been tested many times to obtain better simulation and 
optimal solution and then were used  
Table 3 Parameter setting of RGA 
Simulation settings 
Optimization settings 
Value 
The number of seed 
The number of initial population 
1 
The maximum number of root tips 
The maximum number of population 100 
The initial length of each root fair δi 
The initial value of the parameter δi 
1 
The distance threshold between root tips  
A parameter 
1 
Branching number of each root tip selected A parameter 
4 
S 
S 
4 
α 
α 
12 
β 
β 
08 
τ 
τ 
02515 
λ 
λ 
200 
43 
Numerical Results and Comparison 
The GA DE PSO and RGA algorithms are compared on five functions described in 
the previous section and are listed in Table 2 Each of the experiments in this section 
was repeated 30 times The mean values the standard deviation the minimum values 
and the maximum values produced by the algorithms have been recorded in Table 4 
The best values obtained by the four algorithms for each function are marked as bold  
As shown in Table 4 RGA algorithm can obtain better performance than the other 
three algorithms on Sphere Rosenbrock Rastrigin and Griewank functions while 
PSO algorithm shows better performance on Ackley On Sphere function PSO and 
 
Root Growth Model for Simulation of Plant Root System 
647 
 
RGA obtained satisfying results However RGA algorithm showed the best perfor
mance Rosenbrock function is a unimodal nonseparable function On Rosenbrock 
function RGA algorithm showed the best performance and PSO was a little worse 
than RGA GA and DE cannot obtain better performance on this function Ackley and 
Griewank functions are two multimodal nonseparable functions On these two func
tions the results obtained by RGA and PSO were significantly better than GA and DE 
On Ackley function RGA showed a better converge performance than PSO but on 
Griewank function RGA converged better PSO algorithm converged fast at the be
ginning and trapped in the local optimum soon on Griewank function Rastrigin func
tion is a multimodal variableseparable function The convergence profile of RGA 
was significantly better than the other three algorithms and GA DE and PSO also 
trapped in the local optimum soon on Rastrigin function 
Overall RGA algorithm can obtain good performance on most functions with high
erdimension especially on the multimodal variableseparable functions RGA based 
on root growth model is appropriate for numerical function optimization 
Table 4 Comparison of results obtained by GA DE PSO and RGA 
Function 
GA 
DE 
PSO 
RGA 
Sphere 
Mean 
092505 
226724 
225409e008 
109852e008 
Std 
129811 
286836 
176046e008 
161596e008 
Min 
050220 
369461 
106366e008 
590561e024 
Max 
319916 
556673 
427632e008 
295410e008 
Rosenbrock 
Mean 
430021e+003 
118596 
360147 
212087 
Std 
638258 
522157 
371823 
0808404 
Min 
366262e+003 
717021 
121717 
203453 
Max 
493916e+003 
174864 
788579 
219481 
Ackley 
Mean 
490392 
128558 
644018e007 
0532827 
Std 
042856 
106169 
468484e007 
0460087 
Min 
457918 
116323 
242885e007 
000157042 
Max 
538955 
135347 
11589e006 
080068 
Griewank 
Mean 
181473 
138865 
00338252 
00180136 
Std 
020679 
0290332 
00483038 
00214339 
Min 
158355 
105381 
498359e006 
353763e007 
Max 
198206 
157037 
00891463 
00417191 
Rastrigin 
Mean 
871626 
156074 
285221 
428574e004 
Std 
124927 
704707 
902802 
969091e005 
Min 
791588 
968458 
218891 
335807e004 
Max 
101558 
234011 
388034 
529153e004 
5 
Conclusion 
In this paper we present a root growth model based on root growth behaviours in the 
soil By using this model as a computational metaphor we propose a novel algorithm 
called “root growth algorithm” RGA for simulation of plant root system and  
648 
H Zhang Y Zhu and H Chen 
 
numerical function optimization The selfadaptive growth of root hairs are simulated 
and he characteristics of root growth are showed in the form of images The numerical 
results obtained from the proposed algorithm have been compared with those obtained 
from GA DE and PSO It is seen from the comparison that RGA performs better than 
GA DE and PSO RGA can optimize numerical function better 
References 
1 Gerwitz A Page ER An Empirical Mathematical Model to Describe Plant Root Sys
tems Journal of Applied Ecology 112 773–781 1974 
2 Hodge A Root Decisions Plant Cell and Environment 326 628–640 2009 
3 Leitner D Klepsch S Bodner G Schnepf A A Dynamic Root System Growth Model 
Based on LSystems Plant Soil 332 177–192 2010 
4 Eberhart RC Shi Y Kennedy J Swarm Intelligence Morgan Kaufmann 2001  
5 Holland JH Adaptation in Natural and Artificial Systems University of Michigan Press 
Ann Arbor 1975 
6 Kennedy J Eberhart RC Particle Swarm Optimization In 1995 IEEE International 
Conference on Neural Networks vol 4 pp 1942–1948 IEEE Press New York 1995 
7 Castro DLN Zuben VFJ Artificial Immune Systems Part I Basic Theory and Appli
cations Technical Report Rt Dca 0199 FeecUnicamp Brazil 1999 
8 Karaboga D Basturk B On the Performance of Artificial Bee Colony ABC Algo
rithm Applied Soft Computing 81 687–697 2008 
9 Cai W Yang W Chen X A Global Optimization Algorithm Based on Plant Growth 
Theory Plant Growth Optimization In International Conference on Intelligent Computa
tion Technology and Automation ICICTA vol 1 pp 1194–1199 2008 
10 Krink T Vestertroem JS Riget J Particle Swarm Optimization with Spatial Particle 
Extension In Proceedings of the IEEE Congress on Evolutionary Computation Honolulu 
Hawaii pp 1474–1479 IEEE Press New York 2002 
11 Shi Y Ebrehart RC A Modified Particle Swarm Optimizer In Proceeding of the 1998 
IEEE International Conference on Evolutionary Computation Piscataway NJ pp 69–73 
1998 
12 Shi Y Eberhart RC Empirical Study of Particle Swarm Optimization In Proceedings 
of the 1999 IEEE Congress on Evolutionary Computation Piscataway NJ pp 1945–1950 
IEEE Press New York 1999 
13 Corne D Dorigo M Glover F New Ideas in Optimization McGrawHill 1999 
14 Vesterstrom J Thomsen R A Comparative Study of Differential Evolution Particle 
Swarm Optimization and Evolutionary Algorithms on Numerical Benchmark Problems 
In IEEE Congress on Evolutionary Computation CEC 2004 pp 1980–1987 IEEE 
Press New York 2004 
 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 649–656 2012 
© SpringerVerlag Berlin Heidelberg 2012 
BacterialInspired Algorithms for Engineering 
Optimization 
Ben Niu1234 Jingwen Wang1 Hong Wang1 and Lijing Tan5 
1 College of Management Shenzhen University Shenzhen 518060 China  
2 Hefei Institute of Intelligent Machines Chinese Academy of Sciences Hefei 230031 China  
3 eBusiness Technology Institute The University of Hongkong Hongkong China  
4 Institute for Cultural Industries Shenzhen University Shenzhen 518060 China 
5 Management School Jinan University Guangzhou 510632 China 
drniubengmailcom 
Abstract Bioinspired optimization techniques using analogy of swarming 
principles and social behavior in nature have been adopted to solve a variety of 
problems In this paper Bacterial foraging optimization BFO was employed 
to achieve highquality solutions to engineering optimization problems Two 
modifications of BFO BFO with linear decreasing chemotaxis step BFO
LDC and BFO with nonlinear decreasing chemotaxis step BFONDC were 
proposed to further improve the performance of the original algorithm In order 
to illustrate the efficiency of the proposed method BFOLDC and BFONDC 
for engineering problem an engineering design problem was selected as testing 
functions and the performance is compared against some stateoftheart 
approaches The experimental results demonstrated that the modified BFOs are 
of greater efficiency and can be used as general approach for engineering 
problems 
Keywords Engineering problem constrained handling optimization bacterial 
foraging linear decreasing chemotaxis nonlinear decreasing chemotaxis 
1 
Introduction 
In 2002 a new optimization algorithm based on foraging behavior of bacteria was 
introduced by Passino 1 Although lots of significant research articles published so 
far have focused on analysis of the foraging behavior and self adaptability properties 
of BFO as an unconstrained optimizer till date little such analysis has been done for 
solving the constrained engineering optimization problems 
Constrained Optimization Problems arise in numerous applications 2 Yet 
constraint handing remains to be one of the most difficult parts encountered in 
practical engineering design optimization Realworld limitations frequently introduce 
multiple nonlinear and nontrivial constraints on a design so that feasible solutions 
would be restricted to a small subset of the design space  In general engineering 
optimization problem can be defined as follows 
                                                           
  Corresponding authors 
650 
B Niu et al 
 
Minimize 


n
n
R
x
x x
X
f X
∈
=





2
1

 
subject to  
p
i
gi X

21
0



=
≤
 
and      
m
i
hi X

21
0



=
=
 
where    
n
u i
x
l
i
i
i

21


=
≤
≤
 
1
The objective function f is defined on an ndimensional search space in
Rn
 
Constraints
ig and equality constraints
ih are called active constraints  
Many approaches especially evolution algorithms have received a lot of attention 
regarding their potential for solving constrained engineering optimization problems 
over the past several years The society and civilization simulation proposed by Ray 
 Liew 3 made use of intra and intersociety interactions within a formal society 
and civilization model to solve engineering optimization problems Belegundu 4 
used Various mathematical programming methods for optimal design of structural 
systems CCoello 5 proposed an evolution strategy which is called Simple multi
membered Evolution Strategy SMES to solve global nonlinear optimization 
problems However most of the work is centered on some algorithms such as Particle 
Swarm Optimization PSO Genetic Algorithm GA or other evolutionary 
algorithms In this paper the performance of two Bacterialinspired Algorithms 
proposed by Niu B et al 6 BFOLDC and BFONDC in solving constrained 
engineering problems was investigated and compared with results from other 
methods  
The paper is organized as follows the description of the two improved algorithms
BFOLDC and BFONDC is provided in section 2 Then the process of solving 
constrained engineering optimization problems by two bacterialinspired algorithms 
will be presented in section 3 together with constraint handling pseudocode of 
algorithm experimental settings and the results Finally the paper ends with 
conclusions and ideas for future work reported in section 4 
2 
Modified Bacterial Foraging Optimization 
Bacterial Foraging Optimization BFO algorithm has been applied to model E coli 
foraging behavior and optimization problems Bacteria have the tendency to migrate 
toward the nutrientrich areas and this behavior is termed ‘chemotaxis’ In the original 
BFO each bacterium updated its position of chemotaxis procedure is the key step 
However the chemotaxis step length C  is a constant If this rule holds true 
balancing between the global and local search ability will be difficult and as a result 
the accuracy and searching speed will be affected To solve this problem after solving 
multiobjective optimization 7 and portfolio optimization by symbiotic multiswarm 
PSO 8 Niu B et al 6proposed a simple scheme to modulate the chemotaxis step 
size to improve its convergence behavior In the following section two novel variants 
of original BFO were proposed ie BFOLDC that employed linear variation and 
BFONDC with nonlinear variation of chemotaxis step length 
 
BacterialInspired Algorithms for Engineering Optimization 
651 
 
21 
BFO with Linear Decreasing Chemotaxis Step BFOLDC 
In this method a linearly varying chemotaxis step length over iterations is used The 
chemotaxis step length starts with a high value 
Cmax
 and linearly decreases to 
Cmin
 at the maximal number of iterations The mathematical representations of the 
BFO method are given as shown in  
max


max
min
min
max
iter
iter
C
C
C
C
j
iter
−
=
+
−
 
2
where 
itermax
 is the maximal number of iterations j  is the current number of 
iterations  jth  is the chemotaxis step With
max
min
C
= C
 the system becomes a 
special case of fixed chemotaxis step length as the original proposed BFO algorithm 
From hereafter this BFO algorithm will be referred to as bacterial foraging optimizer 
with linear decreasing chemotaxis BFOLDC 
22 
BFO with Nonlinear Decreasing Chemotaxis Step BFONDC 
Different from the linearly decreasing strategy a nonlinear function modulated 
chemotaxis step adaptation with time is used to improve the performance of BFO 
algorithm The key element is the determination of the chemotaxis step length as a 
nonlinear function of the present iteration number at each time step The proposed 
adaptation of C is given as 
2
exp

 


max
min
min
max
iter
C
C
C
C
j
iter
λ
=
+
−
×
×
−
 
3
where
itermax
 iter  is the same as linearly decreasing strategy λ  the nonlinear 
modulation index The system starts with a high initial chemotaxis step length 

Cmax
 which should allow it to explore new search areas aggressively and then 
decreases it gradually according to 4 following different paths for different values of 
λ  to reach 
Cmin
at
max
iter
= iter
 
3 
Solving Engineering Optimization Problems by Bacterial
Inspired Algorithms 
31 
Constraint HandlingPenalty Function Approach 
Penalty function approach is the most widely used method for problems with 
evolutionary algorithm This approach converts the constrained problem to an 
unconstrained one by introducing a penalty term into the original objective function to 
penalize constraint violations Through this unconstrained optimization algorithm 
652 
B Niu et al 
 
the violations can be minimized If the penalty values are high the minimization 
algorithms are likely to be trapped in local minima On the other hand if penalty 
values are low feasible optimal solutions can hardly be detected  
Penalty functions can be grouped into two main categories stationary and non
stationary Stationary penalty functions use fixed penalty values throughout the 
minimization while nonstationary penalty functions use dynamically modified 
penalty values In this paper the stationary one is adopted 
The basic form of the penalty function is as follows 
 
 
 
 
H x
M
G x
M
f x
F x
×
+ Σ
×
+ Σ
=
 
a
g x
G x
×
=
 
max 0
 
 
b
h x
H x
×
=
 
 
                    
4
f x
is the original objective function of the constrained optimization problem 
Fx
is the new objective function  M is a constant called penalty factor An initial 
value of M has to be provided by the user and there are no rigorous ways of finding 
this initial value This getting the fittest penalty factor is the most difficult part in 
using the method we may face 
Gx
and 
H x
are functions of  the constraints 
gx
and 
hx
are the original constrains Finally a and b are both constant the 
value is one or two In our experiments a stationary assignment penalty function was 
used which means M is fixed  
32 
BFOs for Engineering Optimization 
We illustrate the pseudocode of the modified BFOs The main features of BFOLDC 
and BFONDC can be summarized in Table 1 
33 
Settings 
Bacterial foraging optimization algorithm includes six parameters S is population 
size Nc  is chemotactic step Nre  is the number of reproduction steps to be 
taken Ned is the number of eliminationdispersal events and for each elimination
dispersal event each bacterium in the population is subjected to eliminationdispersal 
with probability
ed
P
 N s is the largest number of step along the random search 
direction 
For 
the 
engineering 
problem 
in 
the 
experiment 
we 
choose
S = 50

Nc = 1000

Nre = 5

Ned = 2

Ns = 4
and 
025
ed
P
=
The 
chemotaxis step length starts with a high value 
02
Cstart
=
 and linearly decreases 
to
min
001
C
=
in BFOLDC  and 
06
Cstart
=

min
005
C
=
in BFONDC In penalty 
function 
1
50000
=
=
=
b
a
M
 
 
BacterialInspired Algorithms for Engineering Optimization 
653 
 
 
Table 1 The pseudocode of BFOLDC and BFONDC 
INITIALIZE  
Set parameters S 
s
N 
c
N 
re
N 
ed
N 
ed
P 
i
θ 
Cstart

Cmin
 λ 
C i
 
The chemotaxis step start length
Cstart
 and it decreases 
linearly to the chemotaxis step length
Cmin
 nonlinear 
modulation index  λ  
Constrain handing by Penalty Function Approach as 
penalty function 4 
WHILE the termination conditions are not met 
FOR 
1
ed
l
N
=
  EliminationDispersal loop 
FOR 
1
re
k
N
=
  Reproduction loop 
FOR 
1
c
j
N
=
  Chemotaxis loop 
     For each bacterium i 
Tumble Generate a random vector  
n
i
R
Δ
∈
 
Move Let 
  

 
 
   
1  

i
i
i
c i
j k l
k l
j
T
i
i
Δ
Δ
Δ
+
=
+
θ
θ
             
This results in a step of the start size 
cstart
in the direction of the tumble for 
bacteriai  
Swim Let 
m = 0
 counter for swim length 
While 
m Ns

 
1
m
= m
+
 
If 
Jlast
k l
J i j

+
1  

 let
 
1  
Jlast
J i j
k l
=
+
 
Calculate the new  
J i j 1  
+ k l
using

i j 1  
k l
θ
+
 
Else 
let 
s
m
= N
 
    END 
END 
END FOR  Chemotaxis loop end 
Select highest
i
Jhealth
bacteria and reproduce 
END FOR  Reproduction loop end 
With probability Ped  eliminates and disperse each 
bacteria 
END FOR  Elimination and Dispersal loop end 
END WHILE 
 
654 
B Niu et al 
 
34 
Experiments and Results 
In this section a wellknown engineering problemA tensioncompression spring 
designfrom the realworld optimization literature is used This problem posed a 
challenge for constrainthandling engineering problem and is good measurements for 
testing the ability of the proposed algorithms 
The constrained optimization problem is the minimization of the weight of spring 
It consists of minimizing the weight of a tensioncompression spring subject to 
constraints on shear stress surge frequency and minimum deflection The design 
variables are the mean coil diameter


D = 1x
 the wire diameter 


d = 2x
and the 
number of active coils


N = 3x
 and the problem involves four nonlinear inequality 
constrains The best feasible solution found by BFOLDC and BFONDC is 
0 01273838
 
=
f x
and 
0 01275064
 
=
f x
 respectively The problem has been 
studied by Ray and Liew 4 CDE 9 Belegundu 3 Arora 10 and Mahdavi et al 
11 The problem can be described as follows 
Minimize 
l
x
x
f x
×
+
=

2 2


2
1
                           5 
Subject to 
0
2
2
2


2
1
2
1
2
1
1
≤
−
+
+
=
σ
P
x x
x
x
x
g x
 
0
2
2


2
1
2
1
2
2
≤
−
+
=
σ
P
x x
x
x
x
g
 
0
2
1


1
2
3
≤
−
+
=
σ
P
x
x
x
g
 
where 
1
0
≤ x1 ≤
and
1
0
≤ x2 ≤

cm
l
=100

2

2
KN cm
P =
 and
2

2
σ = KN cm
 
Five independent runs are performed in MATLAB 70 And then we measured the 
quality of results and the robustness of BFOLDC and BFONDC the standard 
deviation values by compared with other methods The statistical results are 
summarized in Table 2 As it can be seen both BFOLDC and BFONDC 
outperformed the two compared approaches proposed by Belegundu and Mahdavi et 
al Our method and Arora’s showed similar performances Although Ray and Liew 
and CDE gave better results than BFOLDC or BFONDC our method showed better 
performance in terms of robustness When the computational cost ie the number of 
ﬁtness functions evaluationsFFEs is concerned it could be noted that the results of 
CDE requires 240000 FFEs while those of BFOLDC and BFONDC were obtained 
after only 50000 FFEs So we can conclude that the computational cost of BFOLDC 
and BFONDC is less than that of CDE 
 
BacterialInspired Algorithms for Engineering Optimization 
655 
 
Table 2 Comparing of tensioncompression spring design problem results of BFOLDC and 
BFONDC with respect to the other stateoftheart algorithms 
Design problem
Best
Mean
Worst
SD 
BFOLDC
001273838
001275498
001277329
13774e005 
BFONDC
001275064
001277656
001279454
19135e005 
BFO
001273111
001276133
001281023
29571e005 
Ray and Liew
0012669249
0012922669
0016717272
59e04 
CDE
00126702
0012703
0012790
27e05 
Belegundu
0012833
NA
NA
NA 
Arora
0012730
NA
NA
NA 
Mahdavi et al
00128874
NA
NA
NA 
4 
Conclusions and Future Work 
It is well known that nearly all engineering optimization problems in the real world 
would involve multiple nonlinear and nontrivial constraints due to many 
limitations From an engineering standpoint a better faster cheaper solution is 
always desired 
In this paper two novel variants of the original BFO were proposed BFOLDC 
that employed linear variation and BFONDC with nonlinear variation of chemotaxis 
step length The experimental results showed that the proposed BFOLDC and BFO
NDC algorithms are effective methods for handling constraints of engineering 
problems 
Future works should focus on comparing the two proposed methods BFOLDC and 
BFONDC with PSO GA and other algorithms and effort should be made to optimize 
the performance of them In addition studying of the applications in more complex 
practical optimization problems in engineering is necessary for thorough investigation 
of the properties and performances of BFOLDC and BFONDC Meanwhile we are 
also setting about to explore other proposed methods for chemotaxis step to improve 
the performance of BFO 
Acknowledgements This work is supported by National Natural Science Foundation 
of China Grant No71001072 60905039 China Postdoctoral Science Foundation 
Grant No 20100480705 Science and Technology Project of Shenzhen Grant No 
JC201005280492A The Natural Science Foundation of Guangdong Province Grant 
no 9451806001002294 
References 
1 Passino KM Biomimicry of Bacterial Foraging for Distributed Optimization and 
Control IEEE Control Systems Magazine 223 52–67 2002 
2 Michalewicz Z Schoenauer M Evolutionary Algorithms for Constrained Parameter 
Optimization Problems Evolutionary Computation 41 1–32 1996 
3 Ray T Liew KM Society and Civilization An Optimization Algorithm Based on the 
Simulation of Social Behavior IEEE Transactions on Evolutionary Computation 74 
386–396 2003 
656 
B Niu et al 
 
4 Belegundu AD A Study of Mathematical Programming Methods for Structural 
Optimization Science and Engineering 4312 383 1983 
5 Coello CAC Constrainthandling in Genetic Algorithms Through The Use of 
Dominancebased Tournament Selection Advanced Engineering Informatics 16 193–203 
2002 
6 Niu B Fan Y Wang H Li L Wang XF Novel Bacterial Foraging Optimization 
with 
Timevarying 
Chemotaxis 
Step 
International 
Journal 
of 
Artificial 
Intelligence 7A11 257–273 2011 
7 Niu B Wang H Tan LJ Xu J Multiobjective Optimization Using BFO Algorithm 
In Huang DS Gan Y Premaratne P Han K eds ICIC 2011 LNCS LNBI 
vol 6840 pp 582–587 Springer Heidelberg 2012 
8 Niu B Xue B Li L Chai Y Symbiotic Multiswarm PSO for Portfolio Optimization 
In Huang DS Jo KH Lee HH Kang HJ Bevilacqua V eds ICIC 2009 
LNCS vol 5755 pp 776–784 Springer Heidelberg 2009 
9 Amirjanov A The Development of a Changing Range Genetic Algorithm Computer 
Methods in Applied Mechanics and Engineering 195 2495–2508 2006 
10 Arora JS Introduction to Optimum Design McGrawHill New York 1989 
11 Mahdavi M Fesanghary M Damangir E An Improved Harmony Search Algorithm for 
Solving Optimization Problems Applied Mathematics and Computation 1882 1567–
1579 2007 
DS Huang et al Eds ICIC 2012 LNCS 7389 pp 657–664 2012 
© SpringerVerlag Berlin Heidelberg 2012 
Multiobjective Dynamic MultiSwarm Particle Swarm 
Optimization for EnvironmentalEconomic Dispatch 
Problem 
JaneJing Liang1 WeiXing Zhang1 BoYang Qu2 and TieJun Chen1 
1 School of Electrical Engineering Zhengzhou Univerisity Zhengzhou China 
2 School of Electric and Information Engineering  
Zhongyuan University of Technology Zhengzhou China 
liangjingtchenzzueducn 
 boystarboy163com e070088entuedusg 
Abstract This paper presents a new multiobjective particle swarm optimization 
MOPSO technique to solve environmentaleconomic dispatch EED problem 
The EED problem is a nonlinear constrained multiobjective optimization prob
lem The Multiobjective Dynamic MultiSwarm Particle Swarm Optimizer 
DMSMOPSO proposed employs novel pbest and lbest updating criteria 
which are more suitable for solving multiobjective problems In this work the 
standard IEEE 30bus sixgenerator test system is used and simulation results 
showed that the proposed approach is efficient and confirms its potential to 
solve the multiobjective EED problem 
Keywords environmentaleconomic dispatch particle warm optimization 
multiobjective optimization evolutionary algorithm 
1 
Introduction 
The passage of the clean air act amendments in 1990 has forced the utilities to reduce 
their SO2 and NOX emissions by 40 percent from 1980 levels 1Therefore not only 
cost but also emission objective have to be considered EnvironmentalEconomic 
dispatch EED is a multiobjective problem having conflicting objectives as the 
minimization of cost and minimization of the pollution This leads to a tradeoff anal
ysis to define admissible dispatch policies for any demand level 2 
In the past decade the metaheuristic optimization methods have been used to 
solve EED problems primarily due to their nice feature of populationbased search 
3 With the development of multiobjective evolutionary a number of effective 
multiobjective evolutionary search strategies 4such as the novel Nondominated 
Sorting Genetic Algorithm NSGA 5 Niched Pareto Genetic Algorithm 
NPGA6 Strength Pareto Evolutionary Algorithm SPEA7 and NSGAII 8 
have been successful used to solve EED problem 
Particle swarm optimization PSO is an effective search method that based on 
swarm intelligence Although PSO has been applied to many areas 913 few  
658 
JJ Liang et al 
efficient works have been reported to implement MOPSO for solving EED problems 
In this paper the Multiobjective Dynamic MultiSwarm Particle Swarm Optimizer is 
proposed for EnvironmentalEconomic Dispatch Problem The DMSMOPSO 14 
algorithm uses an external archive and a novel pbest and lbest updating strategy to 
solve the EED problem 
The rest of the paper is organized as follows In section 2 we present the problem 
formulation Section 3 and 4 gives a brief description about the DMSMOPSO algo
rithm and constrain method used respectively The experimental results and discus
sions are provided in Section 5 and Section 6 concludes the paper 
2 
Problem Formulation 
21 
Problem Objectives  
Objective 1 The total fuel cost 
 G 
F P
can be expressed as 




2
1
i
i
G
i
N
i
G
i
i
G
P
c
P
b
a
P
F
∗
+
∗
+
= 
=
 
 1
Where N is the number of generators
ia  
ib  
ic  are the cost coefficients of the ith  
generator and 
Gi
P is the real power output of the ith  generator 
G
P  is the vector of 
real power outputs and defined as 
T
G
G
G
G
P N
P
P
P





2
1

=
 
 2
Objective 2 The total emission 
 G 
E P
can be expressed as  

exp


10


2
1
2
i
i
i
G
i
i
G
i
G
N
i
i
G
P
P
P
E P
λ
ξ
γ
β
α
+
+
+
= 
−
  
3
Where
i
α 
iβ 
iγ 
iξ 
iλ  are coefficients of the ith generator emission characteristics 
22 
Problem Constraints 
Constraint 1 Generation capacity constraint 
For stable system operation real power output of each generator is restricted by lower 
and upper limits as follows 
N
i
P
P
P
i
i
i
G
G
G

1
max 
min

=
≤
≤
 
 4
Where 
min
PGi
and 
max
PGi
 are the minimum and maximum power generated by the 
ith generator 
 
 
 
DMSMOPSO for EnvironmentalEconomic Dispatch Problem 
659 
Constraint 2 Power balance constraint 
The total power generation must cover the total demand PD and the real power loss in 
transmission lines Ploss This relation can be expressed as 
0
1
=
−
−

=
loss
N
i
D
G
P
P
P
i
    
 5
As a matter of fact the power loss in transmission lines can be calculated by different 
methods such as B matrix loss formula method The system loss formula can be ex
pressed as follows 
00
1
0
1
1
B
B
P
B P
P
P
N
i
i
G
G
N
i
N
j
ij
G
loss
i
i
i
+
+
=

 
=
=
=
  
 6
where 
ij
B is the transmission loss coefficient
0
iB  is the ith  element of the loss coef
ficient vector 
B00
 is the loss coefficient constant Bcoefficients are needed they are 
shown as follows 


















−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
=
0 02978
0 00012
0 0045
0 00792
0 0028
0033
0
0 0001
0 0216
0 0098
0 0118
0 00026
00055
0
0 0045
0 0098
0 0265
0 01328
0 00179
0011
0
0 0079
0 0118
0 01328
0 02459
0 0002
0 0004
0 0028
0 00026
0 00179
0 0001
0 01704
0107
0
0 0033
0 00055
0 0011
0 00036
0 0107
0218
0
Bij
 

3
5 5503
3
1 3832
3
3 8453
3
4 0645
3
1 7704
3
0 010731
0
−
−
−
−
−
−
−
=
e
e
e
e
e
e
Bi
     
B00 = 0 0014
 
23 
The Standard IEEE 30Bus 6Generator Test System 15  
In this paper the standard IEEE 30bus 6generator system is used The power sys
tem’s total demand is 2834 pu The system parameters are listed in Table1 including 
fuel cost and emission coefficients and each generator’s limit  
Table 1 Parameters of the standard IEEE 30bus sixgenerator test 
 
G1 
G2 
G3 
G4 
G5 
G6 
a 
10 
10 
20 
10 
20 
10 
b 
200 
150 
180 
100 
180 
150 
c 
100 
120 
40 
60 
40 
100 
α  
4091 
2543 
4258 
5326 
4258 
6131 
β  
5554 
6047 
5094 
355 
5094 
5555 
γ  
649 
5638 
4586 
338 
4586 
5151 
ξ  
20e4 
50e4 
10e6 
20e3 
10e6 
10e5 
λ  
2857 
3333 
8000 
2000 
8000 
6667 
Pi
min 
005 
005 
005 
005 
005 
005 
Pi
max 
050 
060 
100 
120 
100 
060 
660 
JJ Liang et al 
3 
DMSMOPSO  
The dynamic multiswarm particle swarm optimizer DMSPSO 1617 is con
structed based on the local version of PSO In the proposed approach the population 
was divided into small sized swarms and each swarm uses its own members to search 
the space Every R generations the population is regrouped randomly and starts 
searching using a new configuration of swarms R is called regrouping period In this 
way the good information obtained by each swarm is exchanged among the swarms 
and the diversity of the population is increased simultaneously  
Base on DMSPSO an extended version for multiobjective optimization is pro
posed in 14 An external archive is added to keep a historical record of the non
dominated solutions obtained during the search process The maximum size of the 
archive Nmax is predefined The technique of updating the external archive is similar to 
the NSGAII Different from other multiobjective PSOs in DMSMOPSO lbest are 
chosen from the best nondominated solutions set in the external archive  
4 
Constraints Handling 
To solving multiobjective optimization problems with constraints we need to adopt 
certain constraint handling method In this paper the superiority of feasible solutions 
technique is used 18 In this technique feasible solutions always dominate infeasi
ble Thus in order to simplify the constraints handling mechanism we modify the 
objective functions of EED problems as follows 





G
G
P
E P
F P
Minmize
G
 
8






G
G
G
V P
g P
h P
=
+
                     
V
E P
E P
V
F P
F P
V P
if
E P
E P
F P
F P
V P
if
G
G
G
G
G
G
G
G
G
G
+
=
+
=

=
=
=

max 

   

max 


 0


    



                  



 0

    
 
 
Here max FPG and  maxEPG are the maximum F and E value found so far 
5 
Experimental Results and Discussions 
51 
Experimental Setup  
To demonstrate the effectiveness of the proposed approach two cases have been con
sidered which are listed as follows 
 
Case 1 The generation capacity and the power balance constraints without consi
dering Ploss 
Case 2 The generation capacity and the power balance constraints with consider
ing Ploss 
The results obtained by proposed approach are compared with FCPSO 6 NSGA 5 
NPGA 7 SPEA 10 MOPSO 19 IMOPSO 19 As reported in literature for all 
 
DMSMOPSO for EnvironmentalEconomic Dispatch Problem 
661 
the six compared algorithms the size of the archive is set as 50 the population size is 
also selected as 50 and the maximum generation number is 7000 For DMSMOPSO 
the subswarm number is set to 5 and in each subswarm there are five particles  
Thus the population size is 25 which is just a half of others And the maximum num
ber of iteration is set to 3000 which means we only use about 21 fitness evaluations 
for DMSMOPSO 
52 
Experimental Results 
Fig 1 shows the paretooptimal front of the proposed approach of case 1 Table 2 and 
3 show the two nondominated solutions that represent the best cost and best emission 
for case 1 It can be seen from these tables DMSMOPSO is able to generate better or 
similar performance with much less number of function evaluations 
600
605
610
615
620
625
630
635
640
019
0195
02
0205
021
0215
022
0225
Fuel Costhr
Emissiontonhr
 
Fig 1 DMSMOPSO Paretooptimal front in case 1 
Table 2 The result of best cost in case 1 
 
NPGA 
6 
NSGA 
5 
SPEA 
7 
FCPSO 
10 
MOPSO 
19 
IMPSOD
E19 
DMS
MOPSO 
P1 
01080 
01567 
01099 
01070 
01018 
01060 
01102 
P2 
03284 
02870 
03186 
02897 
02900 
03063 
02998 
P3 
05386 
04671 
05400 
05250 
05404 
05209 
05250 
P4 
10067 
10467 
09903 
10150 
10175 
10140 
10169 
P5 
04949 
05037 
05336 
05300 
05292 
05262 
05231 
P6 
03574 
03729 
036507 
03673 
03548 
03602 
03590 
Cost 
600259 
600572 
60022 
600131 
600142 
600118 
6001116 
Emission 
022116 
022282 
02206 
022226 
022282 
02220 
02222 
Table 3 The result of best emission in case 1 
 
NPGA 
6 
NSGA 
5 
SPEA 
7 
FCPSO 
10 
MOPSO 
19 
IMPSOD
E 19 
DMS
MOPSO 
P1 
04002 
04394 
04240 
04097 
03993 
04018 
03964 
P2 
04474 
04511 
04577 
04550 
04536 
04583 
04572 
P3 
05166 
05105 
05301 
05363 
05478 
05426 
05318 
P4 
03688 
03871 
03721 
03842 
03801 
03844 
04095 
P5 
05751 
05553 
05311 
05348 
05408 
05415 
05367 
P6 
05259 
04905 
05180 
05140 
05124 
05052 
05025 
Cost 
639182 
639231 
64042 
638357 
637969 
637775 
6354356 
Emission 
019433 
019436 
01942 
01942 
019421 
01942 
01942 
662 
JJ Liang et al 
Beside best cost and best emission the most important criterion is to select the best 
compromise solution from the final nondominated solutions The best solution will 
be extracted by using a fuzzybased mechanism 20 that defined as follows 







≥
≤
≤
−
−
≤
=
max
max
min
min
max
max
min
0
 
    
          
1
i
i
i
i
i
i
i
i
i
i
i
i
F
F
if
F
F
if F
F
F
F
F
F
F
if
μ
 
9
where 
iF is the ith  objective function of a solution in the Paretooptimal 
iF max
 
and 
iF min
 are the maximum and minimum values of the objective function 
i
μ stands for the membership value of the ith  function
iF
 For each non
dominated solution k  the normalized membership value 
μk
 is calculate using 
 
 

=
=
=
=
N
j
M
i
i
M
i
i
j
k
k
1
1
1




μ
μ
μ
 
10
M is the number of objectives N is the number of solutions in paretooptimal front 
The best compromise solution is the one with the maximum
μk
 The best compro
mise solutions of these six algorithms for case 1 are calculated listed in the Table 4 
Table 4 Compromise solution in case 1 
 
NPGA 
5 
NSGA 
4 
SPEA 
6 
MOPSO 
19 
IMPSODE
19 
DMS 
MOPSO 
P1 
02696 
02571 
02623 
02484 
02445 
02415 
P2 
03673 
03774 
03765 
03779 
03771 
03741 
P3 
05594 
05381 
05428 
05409 
05416 
04974 
P4 
06496 
06872 
06838 
07117 
07190 
07270 
P5 
05396 
05404 
05381 
05324 
05229 
05631 
P6 
04486 
04337 
04305 
04224 
04346 
04273 
Cost 
612127 
610067 
610300 
608737 
608414 
6081710 
Emission 
019941 
020060 
02004 
02015 
02018 
02022 
 
From Table 4 it is clear that using the proposed DMSMOPSO method can give 
us a stable result with much less number of function evaluations The better perfor
mance is due to the high diversity and fast converge speed generated by the dynamic 
multiple swarms 
Fig 2 shows the paretooptimal front obtained by the proposed approach for case 
2 The best cost best emission and compromise solutions found by DMSMOPSO 
for case 2 are presented in Table 5 
 
DMSMOPSO for EnvironmentalEconomic Dispatch Problem 
663 
605
610
615
620
625
630
635
640
645
019
0195
02
0205
021
0215
022
Fuel Costhr
Emissiontonhr
 
Fig 2 DMSMOPSO Paretooptimal front in case 2 
Table 5 Results achieved for case 2 
 
Best Cost 
Best Emission 
Compromise Solution 
P1 
01214 
03891 
02423 
P2 
03012 
04565 
03672 
P3 
06203 
05505 
05806 
P4 
09592 
04173 
07496 
P5 
05157 
05392 
05234 
P6 
03528 
05127 
04031 
Cost 
6088074 
6416982   
6141244 
Emission 
02186 
  01942 
02033 
6 
Conclusions 
In this work a novel DMSMOPSO has been employed to solve the environmental  
economic dispatch problem The proposed algorithm is compared with a number of 
algorithms in the literature Due to the high diversity and fast converge speed generat
ed by the dynamic multiple swarms DMSMOPSO is able to generate stable and 
satisfactory performance over multiobjective problems From the experimental re
sults we can observe that the proposed algorithm performs well on the EED prob
lems  
Acknowledgment This research is partially supported by National Natural Science 
Foundation of China Grant NO 60905039 and Postdoctoral Science Foundation of 
China Grants 20100480859 
References 
1 IEEE Current Operating Problems Working Group Potential Impacts of Clean Air Regula
tions on System Operations pp 647–656 1995 
2 Zahavi J Eisenberg L An Application of the Economicenvironmental Power Dispatch 
IEEE Trans Syst Man Cybernet 523–530 1977 
664 
JJ Liang et al 
3 Wang LF Singh C Stochastic Economic Emission Based Load Dispatch Through a 
Modified Particle Swarm Optimization Algorithm Electric Power Systems Research 78 
1466–1476 2008 
4 Niu B Wang H Tan L Xu J Multiobjective Optimization Using BFO Algorithm 
In Huang DS Gan Y Premaratne P Han K eds ICIC 2011 LNCS LNBI 
vol 6840 pp 582–587 Springer Heidelberg 2012 
5 Abido MA A Novel Multiobjective Evolutionary Algorithm for Environmen
talEconomic Power Dispatch Electric Power Systems Research 71–81 2003 
6 Abido MA A Niched Pareto Genetic Algorithm for EnvironmentalEconomic Power 
Dispatch Electric Power Systems Research 97–105 2003 
7 Abido MA Multiobjective Evolutionary Algorithms for Electric Power Dispatch Prob
lem IEEE Trans Evolut Comput 103 315 2006 
8 Basu M Dynamic Economic Emission Dispatch Using Nondominated Sorting Genetic 
AlgorithmII Electric Power Energy Systems 302 140–210 2008 
9 Niu B Xue B Li L Symbiotic Multiswarm PSO for Portfolio Optimization In 
Huang DS Jo KH Lee HH Kang HJ Bevilacqua V eds ICIC 2009 LNCS 
LNAI vol 5755 pp 776–784 Springer Heidelberg 2009 
10 Shubham A Panigrahi BK Tiwari MK Multiobjective Particle Swarm Algorithm 
with Fuzzy Clustering for Electrical Power Dispatch IEEE Trans Evolutionary Computa
tion 529–541 2008 
11 Lu S Sun C Lu Z An Improved Quantumbehaved Particle Swarm Optimization Me
thod for Shortterm Combined Economic Emission Hydrothermal Scheduling Energy 
Conversion and Management 561–571 2010 
12 Wang L Singh C EnvironmentalEconomic Power Dispatch Using a Fuzzified Multi
objective Particle Swarm Optimization Electric Power Systems Research 1654–1664 
2007 
13 Cai J Ma X Li Q Li L Peng H A Multiobjective Chaotic Particle Swarm Optimi
zation for Environmentaleconomic Dispatch Energy Conversion and Management 1318–
1325 2009  
14 Liang JJ Qu BY Suganthan PN Dynamic MultiSwarm Particle Swarm Optimiza
tion for MultiObjective Optimization Problems Has been accepted by IEEE Congress on 
Evolutionary Computation 2012 
15 Hemamalini S Sishaj PS Emission Constrained Economic Dispatch with ValvePoint 
Effect using Particle Swarm Optimization In TENCON 2008 – 2008 IEEE Region 10 
Conference pp 1–6 2008 
16 Liang JJ Suganthan PN Dynamic MultiSwarm Particle Swarm Optimizer with Local 
Search In Proceedings of IEEE Congress on Evolutionary Computation CEC 2005 
vol 1 pp 522–528 2005 
17 Liang JJ Suganthan PN Dynamic MultiSwarm Particle Swarm Optimizer In Pro
ceedings of IEEE International Swarm Intelligence Symposium SIS 2005 pp 124–129 
2005 
18 Deb K An Efficient Constraint Handling Method for Genetic Algorithms Computer Me
thods in Applied Mechanics and Engineering 186 311–338 2000 
19 Wu YL Xu LQ Zhang J Multiobjective Particle Swarm Optimization Based on Dif
ferential Eevolution for EnvironmentalEconomic Dispatch problem In Control and Deci
sion Conference CCDC Chinese pp 1498–1503 2011 
20 Abido MA EnvironmentalEconomic Power Dispatch Using Multiobjective Evolutio
nary Algorithms IEEE Trans Power Systems 1529–1537 2003 
Author Index
Alighale Saeed
95
An Zhenzhou
80
Azami Hamed
95
Badrinath GS
594
Bai Luyi
49
Ben Kerong
284
Bendale Amit
602
Binh Huynh Thi Thanh
137
Bu FanLiang
458
Chen Bo
490
Chen Eryan
618
Chen Hanning
634 641
Chen Hanwu
309
Chen Hua
451
Chen Ling
309
Chen Sheng
1
Chen TieJun
657
Chen Yuehui
168 319
Cherkaoui Mohamed
528
Cho Tae Ho
293 547
Chu Xianghua
626
Cielecki Lukasz
10
Cui Baotong
174
Dat Nguyen Ngoc
137
Ding Lixin
25
Ding Zuohua
475
Dong WenYong
401
Dong Xiaoshe
586
Dong Yinli
103
Douiri Moulay Rachid
528
Du Chun
221
Duan Hongjun
269
Erturk Hayrettin
407
Fang Liying
342
Feng Yong
497
FigueroaGarc´ıa Juan C
610
Fok Chong Yin
538
Geng Huantong
128
Gong Zengtai
65
Gulez Kayhan
407
Guo Lili
168
Guo Qing
120
Guo Zhaolu
25
Gupta Phalguni
594 602
Ha Cheolkeun
415
Ha Nguyen Sy Thai
137
Ha Xuan Vinh
415
Han Fengling
497
Han Wei
475
Hern´andez Germ´an
610
Hoang Ngoc Bach
563
Hong Xia
1
Hu Jiankun
497
Hu Xi
555
Hu Xuegang
214
Hu Yurong
25
Huai Wenjun
365 379
Huang Guoliang
80
Huang Minlie
301
Huang Ze
182
Huang ZhaoTong
18 483
Huo Bofeng
206
Huo Guanying
357
Iliopoulos Costas S
120
Jin Hongwei
301
Kang HeeJun
563
Kaushik Vandana Dixit
602
Khosravi Alireza
95
Kim Seongho
152 160
Kobayashi Yoshinori
423
Koo Imhoi
160
Kuno Yoshinori
423
Lee Byungjeong
505
Lee Jewon
415
Lee Yang Weon
87
Lei Lin
442
Lei Xuejiao
319
Li Bo
401
Li Ding
327
666
Author Index
Li Hao
357
Li Lilin
497
Li Qingwei
269
Li Rui
327
Li Xiaolai
145
Li Xiaoyuan
260
Li Xueling
145
Li ZhiZeng
18 483
Liang JaneJing
657
Liu Feifei
586
Liu GuoDong
277
Liu Hanqiang
373
Liu Jin
401
Liu Wei
309
Liu Xingtong
442
Liu Yiran
490
Liu Zheng
342
Long Pham Vu
137
Lou Ke
174
Lou Xuyang
174
Lu Peng
618
Lu Qiang
626
Lu Xianghua
42
Lui Shufen
386
Luo Guoyun
244
Lv Panpan
451
Ma Jiaying
475
Ma Zongmin
49
Malekzadeh Milad
95
McClain Craig
152
Mei Haibin
513
Meng Xiangbin
451
Min Hai
466
Moon Soo Young
293
Mumcu Tarik Veli
407
Nam Su Man
547
Nigam Aditya
602
Niu Ben
626 649
Peng Chunlin
252
Qi Bin
260
Qin Yong
490
Qiu Daowen
57
Qu BoYang
657
Ro YoungShick
563
Shang Li
365 379
Shao Yabin
65
Shi Huige
618
Shi Xinling
80
Shi Xue
152
Shu ZhengHua
277
Song Jiatao
350
Song Xin
555
Song Xiuchao
244
Song XueYan
18 483
Su Pingang
365 379
Sun He
80
Sun Jixiang
221 442
Sun JiZhou
18 483
Tan Lijing
649
Tang Yuhe
618
Tao Wenlin
386
Tari Zahir
497
Tiwari Kamlesh
594
Tuna Gurkan
407
Unold Olgierd
10
Vai Mang I
538
Wang BingXin
277
Wang Cong
555
Wang Cuirong
555
Wang Hong
649
Wang HongQiang
145 327
Wang Hua
579
Wang Jianxin
520
Wang Jingwen
649
Wang Kaijun
236
Wang Liang
350
Wang Lu
260
Wang Pu
342
Wang Shenwen
25
Wang Shulin
145
Wang Wei
350
Wang Weijing
42
Wang XiaoFeng
393 466
Wang Xin
451
Wang Xingjun
520
Wang Yuzhou
244
Wei Xiaoli
152
Wen Bin
570
Wu Teresa
626
Wu Tingting
128
Xia Liang
190
Xiao Yulan
206
Author Index
667
Xie Chengwang
25
Xie Datong
25
Xie Qing
277
Xie Yaoqin
433
Xing Hongyan
57
Xing Rui
128
Xu Qingwei
284
Yamazaki Akiko
423
Yamazaki Keiichi
423
Yan Jianzhuo
342
Yan Tingqin
386
Yan Xiaohui
634
Yan Zhenguo
190
Yang Fengying
73
Yang LiWei
393
Yang Liying
236
Yang Yali
244
Ye Qian
174
Yousuf Mohammad Abu
423
Yu Haiping
73
Yu Mingwei
342
Yu Xinghuo
497
Yuan Bo
182
Zha Hua
252
Zhang Hao
634 641
Zhang Hui
120
Zhang Lei
334
Zhang Lizhi
190
Zhang Minghua
513
Zhang Shanshan
618
Zhang Tao
505
Zhang WeiXing
657
Zhang WenSheng
401
Zhang Xiang
152 160
Zhang Xing
112
Zhang Yuhong
214
Zhang Zhanpeng
433
Zhao DengJi
277
Zhao Feng
373
Zhao Jingjing
221 442
Zhao Junlong
230
Zhao Kewen
198
Zhao Yaou
319
Zhao YuNing
458
Zheng Di
284
Zhou Changxiong
386
Zhou Huijuan
490
Zhou Quan
214
Zhou Shilin
221
Zhou Shuisheng
103
Zhou Tianpei
34
Zhu Haifeng
128
Zhu Min
145
Zhu Qingsong
433
Zhu Xiaoyan
301
Zhu Yunlong
634 641
